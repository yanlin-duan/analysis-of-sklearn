match,path,repo_name,count
,,,5983670
"from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


# classification models
classifiers = {'K-Nearest Neighbors (Braycurtis norm)':
               KNeighborsClassifier(n_neighbors=3, algorithm='auto',
                                    metric='braycurtis'),
               'Random Forest':
               RandomForestClassifier(n_estimators=80, n_jobs=1),
               'SVM': SVC(gamma=2, C=1),
               'Linear Support Vector Machine': SVC(kernel=""linear"", C=0.025),
               'Decision Tree': DecisionTreeClassifier(max_depth=5),
               'Ada Boost': AdaBoostClassifier(n_estimators=80,
                                               learning_rate=0.4),
               'Naive Bayes': GaussianNB(),
               }
vc = VotingClassifier(estimators=list(classifiers.items()), voting='hard')
",src/utils_classification.py,ThorbenJensen/wifi-locator,1
"from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

classifers_dict = {'AdaBoost': AdaBoostClassifier(n_estimators=50,
                                                  learning_rate=1),
                    'Random Forest': RandomForestClassifier(n_estimators=10,
                                                            # max_depth=None,
                                                            min_samples_split=50,
                                                            random_state=0),
                    'Naive Bayes': GaussianNB(),
                    'SVM': SVC(C=1.5, kernel='poly', degree=2, gamma=0.),
                    'Tree': DecisionTreeClassifier(),
                    'KNN': KNeighborsClassifier(n_neighbors=22)
                    }
                    ",Intro to Machine Learning/choose_your_own/your_algorithm.py,myselfHimanshu/Udacity-DataML,1
"    # train_data = train_data[train_data['Credit_History'] == 1.0]
    # print train_data['Month_Pay'].value_counts()

    baseline_prediction = train_data.apply(classify, axis=1)
    baseline_score = metrics.accuracy_score(baseline_prediction, train_data['Loan_Status'])
    print ""Baseline: {:.3f}%"".format(baseline_score*100)
    print confusion_matrix(train_data['Loan_Status'], baseline_prediction)

    # model = LogisticRegression(C=1, class_weight='balanced')
    # model = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=5)
    # model = RandomForestClassifier(n_estimators=100)
    # model = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(10, 3), random_state=1)
    # model = RandomForestClassifier(n_estimators=25, min_samples_split=25, max_depth=7, max_features=1)
    # model = SVC(C= 1.0, kernel= 'rbf', class_weight={'Y': 4, 'N':1})
    # model = SGDClassifier(class_weight='balanced')
    # model = GaussianNB()

    # params = {'n_estimators': 100, 'max_depth': 5, 'subsample': 0.5,
    #       'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}
    # model = ensemble.GradientBoostingClassifier(**params)",LoanPrediction/scripts/Loan_Classifier.py,Zhenxingzhang/AnalyticsVidhya,1
"from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

from skml.problem_transformation import LabelPowerset
from skml.datasets import load_dataset

X, y = load_dataset('yeast')
clf = LabelPowerset(RandomForestClassifier())
clf.fit(X, np.array(y))
y_pred = clf.predict(X)

print(""real: "", y.shape)
print(""y_pred: "", y_pred.shape)

print(""hamming loss: "")
print(hamming_loss(y, y_pred))
",examples/example_lp.py,ChristianSch/skml,1
"df = pd.read_csv(""parsed.csv"")
y1 = df[""admission_type_id""].values
y2 = df[""discharge_disposition_id""].values
columns = list(df)[1:4] + list(df)[8:49]
print columns
X = df[columns].values
      
X1_train, X1_test, y1_train, y1_test = train_test_split(X, y1, test_size = 0.20)
X2_train, X2_test, y2_train, y2_test = train_test_split(X, y2, test_size = 0.20)

clf1 = RandomForestClassifier()
clf2 = RandomForestClassifier()

clf1.fit(X1_train, y1_train)
clf2.fit(X2_train, y2_train)
y1_pred = clf1.predict(X1_test)
y2_pred = clf2.predict(X2_test)

acc1 = accuracy_score(y1_test, y1_pred)
acc2 = accuracy_score(y2_test, y2_pred)",data/kyle/validation.py,isabellewei/deephealth,1
"    count_enrollment = df_sub['3COURSEID'].value_counts()
    #print ""Number of %s enrollment: %s""%(subject,count_enrollment)

    A = df_sub.as_matrix()
    X = A[:,4:]
    X = X.astype(np.int64, copy=False)
    y = A[:,2]
    y = y.astype(np.int64, copy=False)

    #Training data
    forest = RandomForestClassifier(n_estimators=10, max_depth=None, 
            min_samples_split=1, random_state=None, max_features=None)
    clf = forest.fit(X, y)
    scores = cross_val_score(clf, X, y, cv=5)
    print scores
    print ""Random Forest Cross Validation of %s: %s""%(subject,scores.mean())
    precision_rf[subject] = scores.mean()
    df_precision.loc[subject]=precision_rf[subject]
    print ""-----------------------------------""
    ",pae/forcast/src/train_scikit.py,wasit7/book_pae,1
"
    plt.xlabel('Relative Importance')
    plt.title('%s: Top Features' %(dataName))
    plt.grid('off')
    plt.ion()
    plt.show()
    plt.savefig(str(dataName)+'TopFeatures.png',dpi=200)

def altPlotFeaturesImportance(X,y,featureNames,dataName):
    ""http://nbviewer.ipython.org/github/cs109/2014/blob/master/homework-solutions/HW5-solutions.ipynb""
    clf = RandomForestClassifier(n_estimators=50)

    clf.fit(X,y)
    importance_list = clf.feature_importances_
    # name_list = df.columns #ORIG
    name_list=featureNames

    importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))
    plt.barh(range(len(name_list)),importance_list,align='center')
    plt.yticks(range(len(name_list)),name_list)",ProFET/feat_extract/VisualizeBestFeatures.py,ddofer/ProFET,1
"# strings.
train_data_features = vectorizer.fit_transform(train_data).toarray()

print(train_data_features.shape)


#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

# Initialize a Random Forest classifier with trees
# use as many CPU cores as possible (n_jobs=-1)
forest = RandomForestClassifier(n_estimators = 1000, n_jobs=-1, verbose=2) 

# Fit the forest to the training set, using the bag of words as 
# features and the sentiment labels as the response variable
#
# This may take a few minutes to run
forest = forest.fit(train_data_features, train[""sponsored""])


#Run the prediction...",calculateEstimate_scikit.py,carlsonp/kaggle-TrulyNative,1
"    y2 += [dep(test)]
  return x1,y1,x2,y2

def learns(tests,trains,indep=lambda x: x[:-1],
                    dep = lambda x: x[-1],
                    rf  = Abcd(),
                    lg  = Abcd(),
                    dt  = Abcd(),
                    nb  = Abcd()):
  x1,y1,x2,y2= trainTest(tests,trains,indep,dep) 
  forest = RandomForestClassifier(n_estimators = 50)  
  forest = forest.fit(x1,y1)
  for n,got in enumerate(forest.predict(x2)):
    rf(predicted = got, actual = y2[n])
  logreg = linear_model.LogisticRegression(C=1e5)
  logreg.fit(x1, y1)
  for n,got in enumerate(logreg.predict(x2)):
    lg(predicted = got, actual = y2[n])
  bayes =  GaussianNB()
  bayes.fit(x1,y1)",src/tools/axe/learn.py,rahlk/RAAT,1
"    :param selectKBest: The number of best features to select
    :type selectKBest: int
    :param kfold: The number of folds to use in K-fold CV
    :type kfold: int
    :return: A list of predicted labels across the k-folds
    """"""
    try:
        # Prepare data
        X, y = numpy.array(X), numpy.array(y)
        # Define classifier
        clf = ensemble.RandomForestClassifier(n_estimators=estimators, criterion=criterion, max_depth=maxdepth)
        if selectKBest > 0:
            X_new = SelectKBest(chi2, k=selectKBest).fit_transform(X, y)
            predicted = cross_val_predict(clf, X_new, y, cv=kfold).tolist()
        else:
            predicted = cross_val_predict(clf, X, y, cv=kfold).tolist()
    except Exception as e:
        prettyPrintError(e)
        return []
",data_inference/learning/ScikitLearners.py,aleisalem/Aion,1
"from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier

# Get some data.
digits = load_digits()
X, y = digits.data, digits.target

# Build a classifier.
clf = RandomForestClassifier(n_estimators=20)

# Utility function to report best scores.
def report(results, n_top=3):
    for i in range(1, n_top + 1):
        candidates = np.flatnonzero(results['rank_test_score'] == i)
        for candidate in candidates:
            print(""Model with rank: {0}"".format(i))
            print(""Mean validation score: {0:.3f} (std: {1:.3f})"".format(
                  results['mean_test_score'][candidate],",sw_dev/python/rnd/test/machine_learning/scikit_learn/sklearn_model_selection.py,sangwook236/SWDT,1
"
test_x = all_x[indices[indices >= train_cut]]
test_y = all_y[indices[indices >= train_cut]]

my_normalization = preprocessing.StandardScaler().fit(train_x)
train_x = my_normalization.transform(train_x)
test_x = my_normalization.transform(test_x)

#train random forest classification
#print ""fitting forest""
clf = RandomForestClassifier(n_estimators=15)
clf = clf.fit(train_x, train_y)
print ""mean accuracy on test set: %f""%(clf.score(test_x,test_y))

print ""processing reads""
f = open(args.taxonomer_input,'r')
out = open(args.output,'w')
for line in f:
	if line[0] == ""U"":
		out.write(line)",taxonomer/scripts/apply_learning.py,Yandell-Lab/taxonomer_0.5,1
"
    transformer = skf.DfmfTransform(max_iter=200, init_type=""random_vcol"", random_state=0)
    transformer.transform(chemical, fusion_graph, fuser)
    return transformer


def predict_action(train_idx, test_idx, action_idx):
    fuser = fuse(train_idx)
    X_train = profile(fuser, fuser)
    y_train = pharma[chemical][action][0].data[train_idx, action_idx]
    clf = ensemble.RandomForestClassifier(n_estimators=200)
    clf.fit(X_train, y_train)

    transformer = transform(fuser, test_idx)
    X_test = profile(fuser, transformer)
    y_pred = clf.predict_proba(X_test)[:, 1]
    return y_pred


def main():",examples/pharma_chaining.py,marinkaz/scikit-fusion,1
"res = cross_val_score(bagging, X, y, cv=10).mean()
print res
out('4_3.txt', str(res))

tree = DecisionTreeClassifier(max_features=int(np.sqrt(d)))
bagging = BaggingClassifier(base_estimator=tree, n_estimators=100)
res = cross_val_score(bagging, X, y, cv=10).mean()
print res
out('4_4.txt', str(res))

rf = RandomForestClassifier()

grid_1 = {'n_estimators': np.arange(10, 140, 20)}
grid_2 = {'max_features': np.arange(0.05, 1, 0.05)}
grid_3 = {'max_depth': np.arange(2, 20, 2)}

gs1 = GridSearchCV(rf, grid_1, scoring='accuracy', cv=10)
gs2 = GridSearchCV(rf, grid_2, scoring='accuracy', cv=10)
gs3 = GridSearchCV(rf, grid_3, scoring='accuracy', cv=10)
",course2/week4/task4_1.py,astarostin/MachineLearningSpecializationCoursera,1
"            
            acc = round(metrics.accuracy_score(test_labels, pred_labels), 6)
            #print acc
            scores.append(acc)
        score = round(sum(scores)/len(scores),4)
        return score
    
       
       
    def rf_evaluator (self, subset):
        rf = RandomForestClassifier(n_estimators = 1)
        scores = []
        #print ""subset "", subset
        sample_data = self.data.ix[:,subset]
        for i in range(len(self.train_indices)):
            train_data = sample_data.ix[self.train_indices[i]] ##train sample data
            train_labels = self.labels[self.train_indices[i]] ##train labels
            
            test_data = sample_data.ix[self.test_indices[i]] ##test data
            test_labels = self.labels[self.test_indices[i]] ##test labels",Asynchronous_SFS/ML_data.py,JSilva90/MITWS,1
"from sklearn.ensemble import RandomForestClassifier

param_grid = {'max_depth': [1, 3, 5, 7, 10], 'max_features': [5, 8, 10, 20]}

grid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid)
grid.fit(X_train, y_train)
print(""best parameters: %s"" % grid.best_params_)
print(""Training set accuracy: %s"" % grid.score(X_train, y_train))
print(""Test set accuracy: %s"" % grid.score(X_test, y_test))

scores = [x.mean_validation_score for x in grid.grid_scores_]
scores = np.array(scores).reshape(5, 4)
plt.matshow(scores)
plt.xlabel(""max_features"")",solutions/grid_search_forest.py,amueller/advanced_training,1
"import operator
import matplotlib.pyplot as plt

############ basic random forest setup
predictors = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'SibSp', 'Parch']

#random forest algorithm initialization
#n_estimators = number of trees
#min_samples_split = min number of rows to make the split
#min_samples leaf = min number of samples at end of tree branch
alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)

#tweak parameters:
# enhance accuracy by increasing the number of trees
# reduce overfitting by increasing min samples on the split and on the leaf
alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=4, min_samples_leaf=2)

############ more data wrangling
predictors = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'SibSp', 'Parch', 'FamilySize', 'NameLength']
",random_forest.py,lucasosouza/dataquest,1
"        else:
            newtesty = clf.predict(testx)
        # Transform them back if we need to
        if configs['benchmark_log_target']:
            newtesty = np.exp(newtesty)
        newtesty = np.transpose(np.array([newtesty]))
        return np.hstack((testy, newtesty))

    # Add a bunch of classifiers
    if configs['is_classification']:
        clfs = [RandomForestClassifier(n_estimators=25, n_jobs=4),
                LogisticRegression()]
    else:
        clfs = [RandomForestRegressor(n_estimators=25, n_jobs=4),
                linear_model.LinearRegression(),
                linear_model.Ridge()]
    for clf in clfs:
        testy = __clf_pred(clf, classification=False)

    # Average the models together",scripts/prototyper.py,mcraig2/pyanalysis,1
"
    def set_datamodel(self, dm):
        self.dm = dm
        return None
    #enddef


    def get_classifier_list(self):
        clfs = [{CLASSIFIER:svm.SVC(kernel='rbf',class_weight='auto'), \
                STRING:'SVC_RBF_CLASS_WEIGHT_AUTO'},\
                {CLASSIFIER:ensemble.RandomForestClassifier(\
                max_depth=4,class_weight='auto'), \
                STRING:'RANDOM_FOREST_DEPTH_4_CLASS_WEIGHT_AUTO'},\
                ]
        return clfs
    #enddef


    def get_svm(self):
        clfs = [{CLASSIFIER:svm.SVC(kernel='rbf',class_weight='auto'), \",gigs/src/experimenter.py,narendergupta/gigster,1
"from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import naive_bayes


def extra_trees_classifier():
    return ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)


def random_forest():
    return RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)


def knn():
    """"""
        very fast and slightly more accurate than AdaBoost
    """"""

    return KNeighborsClassifier(n_neighbors=3, leaf_size=125)
",hal/ml/models/classification.py,sirfoga/hal,1
"
        # Merge Domains
        all_domains = pd.concat([alexa_df, dga_df], ignore_index=True)

        # Features
        all_domains['length'] = [len(x) for x in all_domains['domain']]
        all_domains = all_domains[all_domains['length'] > 6]
        all_domains['entropy'] = [self.entropy(
            x) for x in all_domains['domain']]

        self.clf = sklearn.ensemble.RandomForestClassifier(n_estimators=20)

        self.alexa_vc = sklearn.feature_extraction.text.CountVectorizer(
            analyzer='char', ngram_range=(3, 5), min_df=1e-4, max_df=1.0)
        counts_matrix = self.alexa_vc.fit_transform(alexa_df['domain'])
        self.alexa_counts = np.log10(counts_matrix.sum(axis=0).getA1())

        # Read in word dictionary for trigrams
        word_df = pd.read_csv(self.word_dict_path, names=['word'], header=None, dtype={
                              'word': np.str}, encoding='utf-8')",flare/data_science/features.py,austin-taylor/flare,1
"        training_label = [arr[idx_imb] for idx_arr, arr in enumerate(label_bal)
                         if idx_arr != idx_lopo_cv]
        # Concatenate the data
        training_data = np.vstack(training_data)
        training_label = label_binarize(np.hstack(training_label).astype(int),
                                        [0, 255])
        print 'Create the training set ...'

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_bal.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/balancing/pipeline_classifier_dce.py,I2Cvb/mp-mri-prostate,1
"            col_data = pd.get_dummies(col_data, prefix = col)  
        
        # Collect the revised columns
        output = output.join(col_data)    
    return output

X = preprocess_features(X)
random.seed(0)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8,  random_state=0)

clf = RandomForestClassifier(random_state=0) # just a basic random forest model
clf.fit(X_train, y_train)

## predict on the test set:
probs = clf.predict_proba(X_test)
score=[probs[x][1] for x in range(len(probs)) ]

# ## 5. getTestResults; storeTestResults; pull some statistics
test_results = get.getTestResults(score, y_test)
store.storeTestResults(test_results)",docker/Template.py,kpn-advanced-analytics/modelFactoryPy,1
"# Parameters for trees
random_state = 5342
n_jobs = 8
verbose = 1
n_estimators = 89
# ExtraTreesClassifier - classifier 1
clf1 = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf2 = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
# RandomForestClassifier - classifier 2
n_estimators = 89
clf3 = RandomForestClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf4 = RandomForestClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
# AdaBoostClassifier - classifier 3
estimator = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf5 = AdaBoostClassifier(base_estimator=estimator, random_state=random_state, learning_rate=0.8)

# Start training
print('training started')
clf1.fit(X, y)
X_new1 = clf1.transform(X, '1.25*median')",solution4b6.py,canast02/microsoft-malware-classification-challenge,1
"X, X_calibration, y, y_calibration = train_test_split(data,
                                      labels,
                                      test_size=0.20,
                                      random_state=0)

parameters = {'max_depth': (3, 4, 5, 6, 7, 8, 9, 10, 11),
              'min_samples_split': (50, 100, 500, 1000),
              'max_features': (30, 50, 100, 150, 200, 300, 500)}

f1_scorer = make_scorer(f1_score, greater_is_better=True, average='weighted')
rfc = RandomForestClassifier(n_estimators=200,  n_jobs=4)
clf = RandomizedSearchCV(rfc, # select  the best hyperparameters
                         parameters,
                         n_jobs=4,
                         n_iter=40,
                         random_state=42,
                         scoring=f1_scorer)

clf.fit(X, y)
clf.best_params_",src/models/randomforest.py,Kebniss/TalkingData-Mobile-User-Demographics,1
"  score0 = model.score(test0X, test0y)
  print(""Interactions         "" + str(score1))
  print(""Non-interactions     "" + str(score0))
  print(""Accuracy             "" + str((score0 * test0Count + score1 * test1Count) / testCount))
  a, b, c, d = score1 * test1Count, score0 * test0Count, (1.0 - score1) * test1Count, (1.0 - score0) * test0Count
  tss = (a * b - c * d) / ((a + c) * (b + d))
  print(""TSS                  "" + str(tss))

# Train random forests with the best parameters found by randomized search

rf = RandomForestClassifier(n_estimators = 100, bootstrap = True, min_samples_leaf = 10, criterion = 'entropy', max_depth = None)
rf.fit(X, y)
score_and_show(rf, ""\nRandom Forest"")

support = svm.SVC()
support.fit(X, y)
score_and_show(support, ""\nSVM"")
",1_Netflix/Supervised Learning/rf.py,PhDP/Articles,1
"    # Numpy arrays are easy to work with, so convert the result to an
    # array
    train_data_features = train_data_features.toarray()

    # ******* Train a random forest using the bag of words
    #
    print ""Training the random forest (this may take a while)...""


    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators = 100)

    # Fit the forest to the training set, using the bag of words as
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit( train_data_features, train[""sentiment""] )


",pycode/BagOfWords.py,CharLLCH/Word2Vec,1
"         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA""]

classifiers = [
    KNeighborsClassifier(),
    LinearSVC(),
    SVC(),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True,
        copy_X_train=False),
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    MLPClassifier(),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]

# classifiers = [
#     KNeighborsClassifier(3),
#     SVC(kernel=""linear"", C=0.025),
#     SVC(gamma=2, C=1),",data-scripts/mongo-all-classifiers.py,JohnStarich/github-code-recommendations,1
"        df = features.numerical.date(df)
        df = features.numerical.delete_nonnumerical(df)
        df = features.numerical.large_apartments_to_same_group(df)
        df = features.numerical.price_deviation(df, self.avg_prices_per_bedroom)
        return df

    def train_model(self, df):
        X, y = data.read_data.X_y_split(df)
        X = self.prepare_data(X)

        clf = RandomForestClassifier(n_estimators=1000)
        clf.fit(X, y)

        return clf

    def test_model(self, clf, df):
        X, y = data.read_data.X_y_split(df)
        X = self.prepare_data(X)

        y_pred = clf.predict_proba(X)",two-sigma-rental-listing/src/models/numerical_features.py,aufziehvogel/kaggle,1
"        raise Exception('This example is data analysis for Kaggle Titanic Competition.' +
                        'For trying this example, you should download ""train.csv"" from https://www.kaggle.com/c/titanic.')

    train = True
    full_cv = True
    test = False

    train_dr = DataReader('train.csv')
    bclf = LogisticRegression(random_state=1)
    clfs = [
            RandomForestClassifier(n_estimators=50, criterion = 'gini', random_state=1),
            ExtraTreesClassifier(n_estimators=50, criterion = 'gini', random_state=1),
            ExtraTreesClassifier(n_estimators=50, criterion = 'gini', random_state=2),
            GradientBoostingClassifier(n_estimators=25, random_state=1),
            GradientBoostingClassifier(n_estimators=40, random_state=1),
            Ridge(random_state=1),
            #KNeighborsClassifier(n_neighbors=4)
            #LogisticRegression(random_state=1)
            ]
    sl = StackedClassifier(bclf, clfs, n_folds=3, verbose=2)",stacked_generalization/example/kaggle_titanic.py,fukatani/stacked_generalization,1
"'''
Now let's use the selected 5-feature subset and see how well KNN performs
'''
knn.fit(X_train_std[:, k5], y_train)
print 'Training accuracy: ', knn.score(X_train_std[:, k5], y_train)
print 'Test accuracy: ', knn.score(X_test_std[:, k5], y_test)

# Accesing feature importance with random forests
from sklearn.ensemble import RandomForestClassifier
feat_labels = df_wine.columns[1:] # 0 index is y column
forest = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1 )
forest.fit(X_train, y_train)
importances = forest.feature_importances_
indices = np.argsort(importances)[::-1] # accending sort index, then change to decending index

for f in range(X_train.shape[1]):
    print ""%2d) %-*s %f"" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]])
    
plt.title('Feature Importances')
plt.bar(range(X_train.shape[1]), importances[indices], color='lightblue', align='center')",self_practice/Chapter 4 Wine data set.py,wei-Z/Python-Machine-Learning,1
"# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division

import sklearn.ensemble

import submissions
from data import *

rf = sklearn.ensemble.RandomForestClassifier(n_estimators=1000, oob_score=True, n_jobs=2)
rf.fit(train, target)
pred = rf.predict(test)

submissions.save_csv(pred, ""random_forest.csv"")",digit_recognizer/random_forest.py,wjfwzzc/Kaggle_Script,1
"# clf = svm.SVC(kernel='linear', C=1)

# K Nearest Neighbors
# clf = KNeighborsClassifier(3)

# Naive Bayes
# clf = GaussianNB()

## Ensemble Methods
# Random Forest ('Generalized' DT)
# clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)
# AdaBoost (also usually based on DTs)
clf = AdaBoostClassifier()

scores = cross_validation.cross_val_score(clf,train_X,train_y, cv=2)
print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

# write_tree(dt_model, features) # output the model for graphviz visualization
# train_predicted = dt_model.predict(train_X)
",learn.02.py,knmnyn/orbital16-scikit,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/median_aggregate_only.py,diogo149/CauseEffectPairsPaper,1
"    y_true = testing['Happiness']
    # mat = metrics.confusion_matrix(y_true, y_pred, labels=[0, 1])
    # print(mat)
    # print('% Unhappy correctly predicted:', (mat[0][0]/sum(mat[0])*100))
    # print('% Happy correctly predicted:', (mat[1][1]/sum(mat[1])*100))

    # report
    print(metrics.classification_report(y_true, y_pred))

    # Random forest
    rf = RandomForestClassifier(n_estimators=100)
    rf.fit(train.drop('Happiness', 1), train['Happiness'])
    y_pred = rf.predict(testing.drop('Happiness', 1))
    y_true = testing['Happiness']
    # report
    print(metrics.classification_report(y_true, y_pred))

    # Logit with transformed X using RF
    frf = SelectFromModel(rf, prefit=True)
    df_red = frf.transform(train.drop('Happiness', 1))",yps.py,amal029/kaggle,1
"        X[index] = metrics[tid].matrix_row
        y.append(score)

    if args.regress is True:

        clf = RandomForestRegressor(n_estimators=int(len(MetricEntry.metrics)/3),
                                    max_depth=None,
                                    n_jobs=10,
                                    random_state=0)
    else:
        clf = RandomForestClassifier(n_estimators=int(len(MetricEntry.metrics)/3),
                                        max_depth=None,
                                        n_jobs=10,
                                        random_state=0)

    clf.fit(X, y)
    clf.metrics = MetricEntry.metrics
    importances = clf.feature_importances_
    # std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)
    # indices = np.argsort(importances)[::-1]",util/self_training.py,lucventurini/mikado,1
"        aggregate_axis = 1 - axis
        features_non_null_series = df.count(axis=aggregate_axis)
        # Whenever count() differs from row_length it implies a null value exists in feature column and a False in mask
        mask = row_length == features_non_null_series
        return mask

    @staticmethod
    def estimate_by_mice(df):
        df_estimated_var = df.copy()
        random.seed(129)
        mice = MICE()  # model=RandomForestClassifier(n_estimators=100))
        res = mice.complete(np.asarray(df.values, dtype=float))
        df_estimated_var.loc[:, df.columns] = res[:][:]
        return df_estimated_var

    def feature_scaling(self, df):
        df = df.copy()
        # Standardization (centering and scaling) of dataset that removes mean and scales to unit variance
        standard_scaler = StandardScaler()
        numerical_feature_names_of_non_modified_df = TwoSigmaFinModTools._numerical_feature_names",two_sigma_financial_modelling.py,MizioAnd/PortfolioTimeSeriesAnalysis,1
"            (indexes, target, obs) = featureGetter.getTargetVector(coordinates, namesObservations, train)
            print ""Saving images""
            imageSaver = ImageSaver(coordinates[indexes], namesObservations[indexes],
                                    imageCollections, featureGetter.patchSize, target[indexes])
            imageSaver.saveImages()
            print ""Executing wndchrm algorithm and extracting features""
            (train, target) = wndchrmWorker.executeWndchrm()
        else:
            (train, target) = wndchrmWorker.loadWndchrmFeatures()
        print ""Training the model""
        model = RandomForestClassifier(n_estimators=500, verbose=2, n_jobs=1, min_samples_split=30, random_state=1, compute_importances=True)
        model.fit(train, target)
        print model.feature_importances_
        print ""Saving the classifier""
        data_io.save_model(model)

    def runWithoutWndchrm(self):
        print ""Reading in the training data""
        imageCollections = data_io.get_train_df()
        print ""Getting features""",Trainer.py,kosklain/MitosisDetection,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/no_fit.py,diogo149/CauseEffectPairsPaper,1
"rfe = rfe.fit(trainData[0::, 1::], trainData[0::, 0])
# summarize the selection of the attributes
print(rfe.support_)
print(rfe.ranking_)

trainData = SelectKBest(f_classif, k=3).fit_transform(trainData[0::,1::], trainData[0::,0])
print 'F Values: ', f_classif(trainData[0::,1::], trainData[0::,0])[0]
print 'P Values: ', f_classif(trainData[0::,1::], trainData[0::,0])[1]
'''
print 'Training...'
forest_v = RandomForestClassifier(max_depth=16,n_estimators=256, oob_score=True)
forest = forest_v.fit(trainData[0::,1::], trainData[0::,0])

# Feature importances
#importances = forest.feature_importances_
#print ""Feature Importances: "", importances

print 'Predicting...'
output = forest.predict_proba(testData).astype(float)
",algorithms/RandomForest.py,kv-kunalvyas/sfcc,1
"    text_expl, _ = expls = format_as_all(res, clf)
    for expl in expls:
        assert 'atheists' in expl
        assert 'atheism' in expl
        assert 'space' not in expl
        assert 'BIAS' in expl
    assert '<BIAS>' in text_expl


@pytest.mark.parametrize(['clf'], [
    [RandomForestClassifier(n_estimators=100, random_state=42)],
    [ExtraTreesClassifier(n_estimators=100, random_state=24)],
    [GradientBoostingClassifier(random_state=42)],
    [AdaBoostClassifier(learning_rate=0.1, n_estimators=200, random_state=42)],
    [DecisionTreeClassifier(max_depth=3, random_state=42)],

    # FIXME:
    # [OneVsRestClassifier(DecisionTreeClassifier(max_depth=3, random_state=42))],
])
def test_explain_tree_classifier(newsgroups_train, clf, **explain_kwargs):",tests/test_sklearn_explain_weights.py,TeamHG-Memex/eli5,1
"    print(""Extracting features"")
    fea = features.extract_features(feature_names, data)
    
    print(""Joining features"")
    fea = fea.join(codeFea)
    fea = fea.join(textFea)

    print(""Training the model"")
    # n_estimators = 50 means ""create 50 decision trees in the forest""
    # n_jobs = -1 means ""automatically detect cores/threads and parallelize the job""
    rf = RandomForestClassifier(n_estimators=100, verbose=2, compute_importances=True, n_jobs=-1) # This just creates the object - nothing else happens
    rf.fit(fea, data[""OpenStatus""]) # This line trains the classifier; it fits the data, mapped to features, to the classifier
    # training the classifier means minimizing gini impurity (by default)
    # look it up on Wikipedia (if you care)

    print(""Reading test file and making predictions"")
    data = cu.get_dataframe(test_file)
    
    code = preprocess.get_code(data)
    text = preprocess.get_text(data)",basic_benchmark.py,cgearhart/SO-Competition,1
"    Examples
    --------
    >>> from costcla.probcal import ROCConvexHull
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.metrics import brier_score_loss
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> f = RandomForestClassifier()
    >>> f.fit(X_train, y_train)
    >>> y_prob_test = f.predict_proba(X_test)
    >>> f_cal = ROCConvexHull()
    >>> f_cal.fit(y_test, y_prob_test)
    >>> y_prob_test_cal = f_cal.predict_proba(y_prob_test)
    >>> # Brier score using only RandomForest
    >>> print brier_score_loss(y_test, y_prob_test[:, 1])
    0.0577615264881
    >>> # Brier score using calibrated RandomForest",costcla/probcal/probcal.py,madjelan/CostSensitiveClassification,1
"    while time_spent <= time_budget * 0.75:
        if quit_next_time:
            break
        # Exponentially scale the amount of data
        n_data = int(np.exp2(cycle + 4))
        if n_data >= D.data['X_train'].shape[0]:
            n_data = D.data['X_train'].shape[0]
            quit_next_time = True
        print('n_data = %d' % n_data)
        # Fit estimator to subset of data
        M = RandomForestClassifier(n_estimators=1000, min_samples_leaf=msl, n_jobs=1)
        M.fit(D.data['X_train'][:n_data, :], D.data['Y_train'][:n_data])
        M.fit(D.data['X_train'], D.data['Y_train'])
        vprint( verbose,  ""[+] Fitting success, time spent so far %5.2f sec"" % (time.time() - start))
        # Make predictions
        if 'X_valid' in D.data:
            Y_valid = M.predict_proba(D.data['X_valid'])[:, 1]
        if 'X_test' in D.data:
            Y_test = M.predict_proba(D.data['X_test'])[:, 1]
        vprint( verbose,  ""[+] Prediction success, time spent so far %5.2f sec"" % (time.time() - start))",automl.py,jamesrobertlloyd/automl-phase-2,1
"        of trees from 100, 250, and 500.
        debug (boolean) : Verbosity of output (default False)

        Returns
        -------
        Nothing. Output to console describes model accuracy.
        """"""

        # TODO: refactor, such that each algo doesn't need an if/else tree
        if self.modeltype == 'classification':
            algo = RandomForestClassifier(n_estimators=trees,
                                          verbose=(2 if debug is True else 0))

        elif self.modeltype == 'regression':
            algo = RandomForestRegressor(n_estimators=trees,
                                         verbose=(2 if debug is True else 0))

        else:  # Here to appease pep8
            algo = None
",healthcareai/develop_supervised_model.py,HealthCatalystSLC/healthcareai-py,1
"    lbl_enc = preprocessing.LabelEncoder()
    y_train = lbl_enc.fit_transform(y_train)
    y_valid = lbl_enc.transform(y_valid)
    
    return (x_train, y_train, x_valid, y_valid, tfidf)
    
    
# train a random forest classifier
def train_rf(x, y, n_estimators=10):
    print 'training a random forest'
    rf_model = ensemble.RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators)
    rf_model.fit(x, y)
    return rf_model
    
# train a Extra Trees Boost classifier
def train_extree(x, y, n_estimators=10):
    print 'training an extra tree'
    rf_model = ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=n_estimators)
    rf_model.fit(x, y)
    return rf_model",kaggle-otto/model_test.py,ryanswanstrom/Sense.io-Projects,1
"    # Numpy arrays are easy to work with, so convert the result to an
    # array
    train_data_features = train_data_features.toarray()

    # ******* Train a random forest using the bag of words
    #
    print(""Training the random forest (this may take a while)..."")


    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators=100)

    # Fit the forest to the training set, using the bag of words as
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit(train_data_features, train[""sentiment""] )


",doc_ref/NLP/word2vec-nlp-tutorial/DeepLearningMovies/BagOfWords.py,gtesei/fast-furious,1
"from sklearn.ensemble import RandomForestClassifier
import utils

def main():
    training, target = utils.read_data(""../files/train.csv"")
    training = [x[1:] for x in training]
    target = [float(x) for x in target]
    test, throwaway = utils.read_data(""../files/test.csv"")
    test = [x[1:] for x in test]

    rf = RandomForestClassifier(n_estimators=100, min_samples_split=2)
    rf.fit(training, target)
    predicted_probs = rf.predict_proba(test)
    predicted_probs = [[min(max(x,0.001),0.999) for x in y]
                       for y in predicted_probs]
    print utils.logloss(predicted_probs, test)
    predicted_probs = [[""%f"" % x for x in y] for y in predicted_probs]
    utils.write_delimited_file(""../files/rf_benchmark.csv"",
                                predicted_probs)
",Eye Movement Tracking/src/rfBenchmark.py,shaileshahuja/MineRush,1
"def hyperopt_rf_opt(params):
    print ""Training with params : ""
    print params

    # create features and del parameter
    cv = CountVectorizer(max_features=params['features'])
    cv_train_data = cv.fit_transform(train_data['NAME'])
    del params['features']

    # learn classifier
    rf = RandomForestClassifier(**params)
    scores = cross_val_score(rf, cv_train_data, train_data['GROUP_ID'], cv=3, scoring='accuracy')
    print ""Scores : ""
    print scores.mean()
    return scores.mean()

def f(params):
    acc = hyperopt_rf_opt(params)
    return {'loss': 1-acc, 'status': STATUS_OK}
",experiment/countvect/cv_random_forest.py,shiron/evotor-ml-champ,1
"from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

svm = SVC()
bnb = BernoulliNB(alpha=0.2)
mnb = MultinomialNB(alpha=0.4)
gnb = GaussianNB()
rf = RandomForestClassifier(n_jobs=4,n_estimators=17)
knn = KNeighborsClassifier(n_neighbors=5)

trainDataFile = ""train.csv""
trainLabelFile = ""train_labels.csv""

fLabel = open(trainLabelFile,""r"")
labels = fLabel.read()
class_labels = labels.split()
train_set = np.zeros((len(class_labels),100))",KaggleEmailSpam.py,rupakc/Kaggle---Email-Spam,1
"        'model': MLPClassifier(max_iter=1000),
        'params': [
            {
                'hidden_layer_sizes': [(20, 10)],
                'random_state': [77]
            }
        ]
    },
    {
        'name': 'Random Forest',
        'model': RandomForestClassifier(),
        'params': [
            {
                'max_depth': [8],
                'n_estimators': [20],
                'min_samples_split': [5],
                'criterion': ['gini'],
                'max_features': ['auto'],
                'class_weight': ['balanced'],
                'random_state': [77]",scripts/classifier.py,yohanesgultom/knowledge-extractor-id,1
"			x_col=['housing','loan','duration','pdays'])

dh.create_sub('default', col_dummy=['housing','loan'])

dh.use_ds('bank', 'default', new=True)

dh.partition(0.5)

mod = LogisticRegression()
#mod = GradientBoostingClassifier()
#mod = RandomForestClassifier()

mod.fit(dh.x, dh.y)

dh.partition(1.0)

y_hat = mod.predict_proba(dh.x)

dt.summary(dh.y.values, y_hat[:, 1], prob_hist=True, thres=0.1)",depr/0.2.5/examples/bank_demo.py,rosspalmer/DataTools,1
"    for threshold in [""gobbledigook"", "".5 * gobbledigook""]:
        model = SelectFromModel(clf, threshold=threshold)
        model.fit(data, y)
        assert_raises(ValueError, model.transform, data)


def test_input_estimator_unchanged():
    """"""
    Test that SelectFromModel fits on a clone of the estimator.
    """"""
    est = RandomForestClassifier()
    transformer = SelectFromModel(estimator=est)
    transformer.fit(data, y)
    assert_true(transformer.estimator is est)


@skip_if_32bit
def test_feature_importances():
    X, y = datasets.make_classification(
        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,",projects/scikit-learn-master/sklearn/feature_selection/tests/test_from_model.py,DailyActie/Surrogate-Model,1
"X_CSF_FDG = mci_df[csf_fdg_cols].as_matrix()
X_ALL = mci_df.as_matrix()

def writeSensAndSpec(fpr, tpr, thresh, filename):
    specificity = 1-fpr
    a = numpy.vstack([specificity, tpr, thresh])
    b = numpy.transpose(a)
    numpy.savetxt(filename, b, fmt='%.5f', delimiter=',')

print('CSF_ONLY')
RUSRFC_CSF_ONLY = RUSRandomForestClassifier.RUSRandomForestClassifier(n_Forests=200, n_TreesInForest=500)
predClasses_CSF_ONLY, classProb_CSF_ONLY, featureImp_CSF_ONLY, featureImpSD_CSF_ONLY = RUSRFC_CSF_ONLY.CVJungle(X_CSF_ONLY, Y, shuffle=True, print_v=True)
cm_CSF_ONLY = confusion_matrix(Y, predClasses_CSF_ONLY)
print('Final Accuracy')
print(cm_CSF_ONLY)
print(featureImp_CSF_ONLY)
featureImpScale_CSF_ONLY = [featureImp_CSF_ONLY[csf_cols.index(i)] if i in csf_cols else 0 for i in all_list]
featureImpScaleSD_CSF_ONLY = [featureImpSD_CSF_ONLY[csf_cols.index(i)] if i in csf_cols else 0 for i in all_list]
plt.figure()
plt.title('Feature Importance CSF ONLY')",Python/RUSRandomForest/runClassification.py,sulantha2006/Conversion,1
"from stacked_generalization.lib.joblibed import JoblibedClassifier


iris = datasets.load_iris()
rng = check_random_state(0)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]

# Joblibed model
rf = RandomForestClassifier(n_estimators=40,
                            criterion='gini',
                            random_state=1)
clf = JoblibedClassifier(rf, ""rf"")


train_idx, test_idx = list(StratifiedKFold(iris.target, 3))[0]

xs_train = iris.data[train_idx]
y_train = iris.target[train_idx]",stacked_generalization/example/joblibed_classification.py,fukatani/stacked_generalization,1
"    if percent_of_null == 100.0:
        df_new.drop(col, axis=1, inplace=True)
        #print ('dropping',col)
    elif null_count >0:
        df_new[col+'_is_null'] = df_new[col].isnull()
       # df_new = df_new.merge(col+'isnull',left_index=True,right_index=True)
        df_new[col].fillna(-99999.99,inplace=True)


x_train,x_test,y_train,y_test = train_test_split(df_new,y,test_size = 0.2)
clf_rf = RandomForestClassifier(n_estimators=100,max_depth=80)
clf_rf.fit(x_train,y_train)
y_pred = []
prob_score = clf_rf.predict_proba(x_train)
a = prob_score[:,1]
for idx,item in enumerate(a):
    if item>= 0.55:
        item = 1
    else:
        item =0",WorldBank2015/Code/data_pipeline_src/rank_gen.py,eredmiles/Fraud-Corruption-Detection-Data-Science-Pipeline-DSSG2015,1
"cvcode = np.asarray([0]*ytrain.shape[0] + [1]*ytest.shape[0])
assert checkX(X)
assert X.shape[0] == y.shape[0], ""X and y length mismatch""
assert X.shape[0] == cvcode.shape[0], ""X and cvcode length mismatch""

# CV
cv = StratifiedKFold(cvcode, n_folds=2, indices=True)

# Classifier
if args.clf == ""RandomForestClassifier"":
    clf = RandomForestClassifier(
            n_estimators=500, max_features=None
            )
elif args.clf == ""GradientBoostingClassifier"":   
    clf = GradientBoostingClassifier(
            n_estimators=100, learning_rate=1.0, 
            max_depth=1, random_state=prng
            )
else:
    raise ValueError(""--clf not understood"")",meta/between.py,parenthetical-e/wheelerexp,1
"
auc_results = pd.DataFrame()
for c in [""gini"", ""entropy""]:
	for max_f in np.arange(0.0, 1, 0.05):
		if max_f == 0.0:
			max_f = None
		#for min_s_split in range(1, 10, 2):
			#for min_s_leaf in range(1, 10, 2):
				#for min_w_f_leaf in np.arange(0, 0.5, 0.1):
		for bs in [True, False]:
			clf = RandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=0,
			criterion=c,
			max_features=max_f,
			#min_samples_split=min_s_split,
			#min_samples_leaf=min_s_leaf,
			#min_weight_fraction_leaf=min_w_f_leaf,
			bootstrap=bs)
			clf.fit(X_train, y_train)
			# Determine the false positive and true positive rates
			fpr, tpr, _ = roc_curve(y_test, clf.predict_proba(X_test)[:,1])",find_best_parameters.py,carlsonp/kaggle-TrulyNative,1
"    dataDescrs_array = np.asarray(property_list_list)
    dataActs_array = np.array(TL_list)

    for randomseedcounter in range(1, 11):
      if self.verbous:
        print(""################################"")
        print(""try to calculate seed %d"" % randomseedcounter)
      X_train, X_test, y_train, y_test = cross_validation.train_test_split(
        dataDescrs_array, dataActs_array, test_size=.4, random_state=randomseedcounter)
      #            try:
      clf_RF = RandomForestClassifier(n_estimators=100, random_state=randomseedcounter)
      clf_RF = clf_RF.fit(X_train, y_train)

      cv_counter = 5

      scores = cross_validation.cross_val_score(clf_RF, X_test, y_test, cv=cv_counter,
                                                scoring='accuracy')

      accuracy_CV = round(scores.mean(), 3)
      accuracy_std_CV = round(scores.std(), 3)",Contrib/pzc/p_con.py,mcs07/rdkit,1
"    ----------
    imputed: [(str, np.ndarray), (str, np.ndarray)...]
       List of tuples containing (imputation algorithm name, imputed data)
    RETURNS
    ------
    .txt file of classification results on imputed data
    """"""
    clfs = [[""SVC"", SVC()],
            [""KNeighbours"", KNeighborsClassifier(2)],
            [""GaussianNB"", GaussianNB()],
            [""RandomForestClassifier"", RandomForestClassifier()]]

    results = {imputation_name: [] for imputation_name, _ in imputed}

    for imputation_name, data in imputed:
        X, y = data
        X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                            test_size=0.33,
                                                            random_state=42)
        print(imputation_name)",impyute/utils/compare.py,eltonlaw/impyute,1
"        from sklearn import linear_model
        brr = linear_model.BayesianRidge()
        brr.fit(datatrain,alivetrain)
        Output = brr.predict(datatest)

    #-------------
    # RANDOM FOREST
    elif choose_method == 4:
        print '\nRunning Random Forest Classifier...\n'
        from sklearn.ensemble import RandomForestClassifier
        Forest = RandomForestClassifier(n_estimators = 100) #1000 trees
        Forest = Forest.fit(datatrain,alivetrain)
        Output = Forest.predict(datatest)

    #-------------
    # EXTREMELY RANDOMIZED FOREST
    elif choose_method == 5:
        print '\nRunning Extremely Randomized Forest...\n'
        from sklearn.ensemble import ExtraTreesClassifier
        extratrees = ExtraTreesClassifier(n_estimators = 100) #1000 trees",bikerides.py,jesford/bike_sharing,1
"        X, y = self.extraction.return_test_dataset()
        assert X.shape == (1797, 64)
        assert y.shape == (1797,)


class TestReturnEstimator(unittest.TestCase):
    def setUp(self):
        self.base_learner_origin = models.BaseLearnerOrigin(
            source=''.join([
                ""from sklearn.ensemble import RandomForestClassifier\n"",
                ""base_learner = RandomForestClassifier(random_state=8)""
            ])
        )

    def test_return_estimator_from_json(self):
        est = self.base_learner_origin.return_estimator()
        assert isinstance(est, RandomForestClassifier)",xcessiv/tests/test_models.py,reiinakano/xcessiv,1
"import numpy as np

class RandomForrest(PersonalClassifier):
    def __init__(self, data_set, labels):
        super(RandomForrest, self).__init__(data_set, labels)

    def train(self):
        x = self.data_set
        y = self.labels

        clf = RandomForestClassifier(n_estimators=10)
        clf = clf.fit(x, y)
",personal_classifier/random_forrests.py,BavoGoosens/Gaiter,1
"
from lime.lime_tabular import LimeTabularExplainer


class TestLimeTabular(unittest.TestCase):
    def test_lime_explainer_bad_regressor(self):
        iris = load_iris()
        train, test, labels_train, labels_test = train_test_split(
            iris.data, iris.target, train_size=0.80)

        rf = RandomForestClassifier(n_estimators=500)
        rf.fit(train, labels_train)
        lasso = Lasso(alpha=1, fit_intercept=True)
        i = np.random.randint(0, test.shape[0])
        with self.assertRaises(TypeError):
            explainer = LimeTabularExplainer(
                train,
                feature_names=iris.feature_names,
                class_names=iris.target_names,
                discretize_continuous=True)",lime/tests/test_lime_tabular.py,marcotcr/lime,1
"
		kernel_train = arc_cosine(param[i], Mat2, temp2, temp1)
		kernel_test = arc_cosine(param[i], Mat3, temp3, temp1)

		trainX_kpca = kpca.transform(kernel_train)
		testX_kpca = kpca.transform(kernel_test)


		#fit the random forest model to evaluate featues
		forest = ExtraTreesClassifier(n_estimators=400, random_state=0, n_jobs=-1)
		#forest = RandomForestClassifier(n_estimators=300, n_jobs=-1)
		#forest = DecisionTreeClassifier()
		forest.fit(trainX_kpca, trainY)
		print forest.n_features_

		importances = forest.feature_importances_
		indices = np.argsort(importances)[::-1]
		#for f in range(kpcaX.shape[1]):
		#	print(""%d. feature %d (%f)"" % (f + 1, indices[f], importances[indices[f]]))
			#chumma = raw_input('### wait for key press ### ')",kpcaWithTreeFS/mnistBackRandom/mnistRANDwithFS.py,akhilpm/Masters-Project,1
"    x_tr = data.values[:, 1:].astype(float)#The other columns are the pixels of each image and we have 28,000 images
    y_tr = data.values[:, 0]#The first column is the label that drawn by user

    scores = list()#list data type and it can be change
    scores_std = list()

    print('Start learning...')
    n_trees = [10, 20, 50, 100]#the number of random trees
    for n_tree in n_trees:
        print(n_tree)
        recognizer = RandomForestClassifier(n_tree)
        score = cross_val_score(recognizer, x_tr, y_tr)
        scores.append(np.mean(score))
        scores_std.append(np.std(score))#compute the standard deviation 

    sc_array = np.array(scores) #change a list to array
    std_array = np.array(scores_std)#numpy array 
    print('Score: ', sc_array)
    print('Std  : ', std_array)
",Digit Recoginer/Digit Recognier in Random Forest/code/Digit recognier in Random forest.py,0Steve0/Kaggle,1
"len(train_Y),
len(test_Y),
str(sklnr).replace('\n',' '))

# choose different learners
learner = [
naive_bayes.GaussianNB(),
linear_model.SGDClassifier(),
svm.SVC(),
tree.DecisionTreeClassifier(),
ensemble.RandomForestClassifier(),
# neural_network.MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10, 2), random_state=1)
]

# run
forlinlearner:
fit_and_predict(l, train_X, train_Y, test_X)",ml_test.py,james-jz-zheng/jjzz,1
"        self.imputer = skl_preprocessing.Imputer()
        self.imputer.fit(X)
        X = replace_nan(X, self.imputer)

        params = dict(self.params)
        max_features = params[""max_features""]
        if isinstance(max_features, numbers.Integral) and \
                X.shape[1] < max_features:
            params[""max_features""] = X.shape[1]

        rf_model = skl_ensemble.RandomForestClassifier(**params)
        rf_model.fit(X, Y.ravel())
        return RandomForestClassifier(rf_model, self.imputer)


class RandomForestClassifier(SklModel):
    def __init__(self, clf, imp):
        self.clf = clf
        self.imputer = imp
",Orange/classification/random_forest.py,jzbontar/orange-tree,1
"# instances is optimized. The naming convention is a tribute to that heritage.)
# This argument will be a integer between 0 and num_instances (defined below).
# Note that this increases the computational effort as SMAC now estimates the 
# quality of a parameter setting for multiple instances.
def random_forest(n_estimators,criterion, max_features, max_depth, instance):

	# Use the requested fold
	train, test = kfold[instance]
	X_train, Y_train, X_test, Y_test = X[train], Y[train], X[test], Y[test]
	
	predictor = sklearn.ensemble.RandomForestClassifier(n_estimators, criterion, max_features, max_depth)
	predictor.fit(X_train, Y_train)
	
	return -predictor.score(X_test, Y_test)

# We haven't changed anything here.
parameter_definition=dict(\
		max_depth   =(""integer"", [1, 10],  4),
		max_features=(""integer"", [1, 20], 10),
		n_estimators=(""integer"", [1,100], 10, 'log'),			",examples/sklearn_example_advanced_crossvalidation.py,belkhir-nacim/smac_python,1
"            X_train_set.append(X_train[i])
            Y_train_set.append(Y_train[i])

    clf1 = tree.DecisionTreeClassifier(criterion='gini',min_samples_split=250)
    clf2 = GaussianNB()
    clf3 = tree.DecisionTreeClassifier(criterion='entropy',min_samples_leaf=39)
    clf4 = tree.DecisionTreeClassifier(criterion='gini',max_depth=4)
    # clf4 = BernoulliNB()
    # clf5 = MultinomialNB()
    # clf = LogisticRegression(C=.01)
    # clf = RandomForestClassifier(n_jobs=-1, n_estimators=50,max_depth=16)
    # clf4 = Classifier(
    #     layers=[
    #         Layer(""Tanh"", units=100),
    #         Layer(""Sigmoid"", units=100),
    #         Layer('Softmax')],
    #     learning_rate=0.1,
    #     learning_rule='momentum',
    #     learning_momentum=0.9,
    #     batch_size=100,",src/main.py,Luke092/MLDM_SFCrime,1
"ids = test_df['PassengerId'].values
# Remove the Name column, Cabin, Ticket, and Sex (since I copied and filled it to Gender)
test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1)

# The data is now ready to go. So lets fit to the train, then predict to the test!
# Convert back to a numpy array
train_data = train_df.values
test_data = test_df.values

print 'Training...'
forest = RandomForestClassifier(n_estimators=100)
forest = forest.fit(train_data[0::,1::], train_data[0::,0])

print 'Predicting...'
output = forest.predict(test_data).astype(int)

print ""Score...""
print forest.score(X=train_data[0::,1::], y=train_data[0::,0])

predictions_file = open(""testresult.csv"", ""wb"")",Titanic- Machine Learning from Disaster/train_with_RandomForest.py,SakhawatSumit/Kaggle-Competitions-,1
"# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division

import sklearn.ensemble

import submissions
from data import *

rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=2)
rf.fit(train, target)
pred = rf.predict(test)

submissions.save_csv(pred, ""random_forest.csv"")",titanic/random_forest.py,wjfwzzc/Kaggle_Script,1
"    #SVM1 ~ 0.494
    #(Xtrain,ytrain,Xtest,labels) = prepareDataset(nsamples='shuffle',featureFilter=None,addFeatures=False,standardize=True,log_transform=True)
    #model = SVC(kernel='rbf',C=10.0, gamma=0.0, verbose = 0, probability=True)
    #xmodel = XModel(""svm1_r3"",classifier=model,Xtrain=Xtrain,Xtest=Xtest,ytrain=ytrain,class_names=sorted(list(set(labels))))
    #ensemble.append(xmodel)   
    
    #RF1 ~ 0.487
    #all_features=[u'feat_1', u'feat_2', u'feat_3', u'feat_4', u'feat_5', u'feat_6', u'feat_7', u'feat_8', u'feat_9', u'feat_10', u'feat_11', u'feat_12', u'feat_13', u'feat_14', u'feat_15', u'feat_16', u'feat_17', u'feat_18', u'feat_19', u'feat_20', u'feat_21', u'feat_22', u'feat_23', u'feat_24', u'feat_25', u'feat_26', u'feat_27', u'feat_28', u'feat_29', u'feat_30', u'feat_31', u'feat_32', u'feat_33', u'feat_34', u'feat_35', u'feat_36', u'feat_37', u'feat_38', u'feat_39', u'feat_40', u'feat_41', u'feat_42', u'feat_43', u'feat_44', u'feat_45', u'feat_46', u'feat_47', u'feat_48', u'feat_49', u'feat_50', u'feat_51', u'feat_52', u'feat_53', u'feat_54', u'feat_55', u'feat_56', u'feat_57', u'feat_58', u'feat_59', u'feat_60', u'feat_61', u'feat_62', u'feat_63', u'feat_64', u'feat_65', u'feat_66', u'feat_67', u'feat_68', u'feat_69', u'feat_70', u'feat_71', u'feat_72', u'feat_73', u'feat_74', u'feat_75', u'feat_76', u'feat_77', u'feat_78', u'feat_79', u'feat_80', u'feat_81', u'feat_82', u'feat_83', u'feat_84', u'feat_85', u'feat_86', u'feat_87', u'feat_88', u'feat_89', u'feat_90', u'feat_91', u'feat_92', u'feat_93']
    #addedFeatures_best=[u'row_median',u'arg_max',u'row_max',u'non_null',u'arg_min']
    #(Xtrain,ytrain,Xtest,labels) = prepareDataset(nsamples='shuffle',addFeatures=True,final_filter=all_features+addedFeatures_best)
    #basemodel = RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=1,n_jobs=4,criterion='entropy', max_features=20,oob_score=False)
    #model = CalibratedClassifierCV(basemodel, method='isotonic', cv=3)
    #xmodel = XModel(""rf1_r3"",classifier=model,Xtrain=Xtrain,Xtest=Xtest,ytrain=ytrain,class_names=sorted(list(set(labels))))
    #ensemble.append(xmodel)
    #all_features=[u'feat_1', u'feat_2', u'feat_3', u'feat_4', u'feat_5', u'feat_6', u'feat_7', u'feat_8', u'feat_9', u'feat_10', u'feat_11', u'feat_12', u'feat_13', u'feat_14', u'feat_15', u'feat_16', u'feat_17', u'feat_18', u'feat_19', u'feat_20', u'feat_21', u'feat_22', u'feat_23', u'feat_24', u'feat_25', u'feat_26', u'feat_27', u'feat_28', u'feat_29', u'feat_30', u'feat_31', u'feat_32', u'feat_33', u'feat_34', u'feat_35', u'feat_36', u'feat_37', u'feat_38', u'feat_39', u'feat_40', u'feat_41', u'feat_42', u'feat_43', u'feat_44', u'feat_45', u'feat_46', u'feat_47', u'feat_48', u'feat_49', u'feat_50', u'feat_51', u'feat_52', u'feat_53', u'feat_54', u'feat_55', u'feat_56', u'feat_57', u'feat_58', u'feat_59', u'feat_60', u'feat_61', u'feat_62', u'feat_63', u'feat_64', u'feat_65', u'feat_66', u'feat_67', u'feat_68', u'feat_69', u'feat_70', u'feat_71', u'feat_72', u'feat_73', u'feat_74', u'feat_75', u'feat_76', u'feat_77', u'feat_78', u'feat_79', u'feat_80', u'feat_81', u'feat_82', u'feat_83', u'feat_84', u'feat_85', u'feat_86', u'feat_87', u'feat_88', u'feat_89', u'feat_90', u'feat_91', u'feat_92', u'feat_93']
    #addedFeatures_best=[u'row_median',u'arg_max',u'row_max',u'non_null',u'arg_min']
    #(Xtrain,ytrain,Xtest,labels) = prepareDataset(nsamples='shuffle',addFeatures=True,final_filter=all_features+addedFeatures_best)
    #basemodel = RandomForestClassifier(n_estimators=1000,max_depth=None,min_samples_leaf=1,n_jobs=4,criterion='entropy', max_features=24,oob_score=False)
    #model = CalibratedClassifierCV(basemodel, method='isotonic', cv=3)
    #xmodel = XModel(""rf1_r1"",classifier=model,Xtrain=Xtrain,Xtest=Xtest,ytrain=ytrain,class_names=sorted(list(set(labels))))",competition_scripts/ensemble_otto.py,chrissly31415/amimanera,1
"import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_svmlight_file
from sklearn.metrics import accuracy_score
import time

dataset_X, dataset_Y = load_svmlight_file('cod-rna')
model = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_features=""auto"")
start = time.time()
model.fit(dataset_X, dataset_Y)
end = time.time()

score = 1-accuracy_score(dataset_Y, model.predict(dataset_X))",examples/Benchmark/scicit-learn/Random_Forrest.py,egomeh/Shark,1
"        test_data[i,7] = np.median(test_data[(test_data[0::,7] != '') &\
                                             (test_data[0::,0] == test_data[i,0])\
            ,7].astype(np.float))

test_data = np.delete(test_data,[1,6,8],1) # Remove the name data, cabin and ticket


# The data is now ready to go. So lets train then test!

print 'Training '
forest = RandomForestClassifier(n_estimators = 1000)

forest = forest.fit(train_data[0::,1::],\
                    train_data[0::,0])

print 'Predicting'
output = forest.predict(test_data) #predict results using our CLEANED data


# Write Results to fie",Python Examples/agcfirstforest.py,michaelulin/kaggle-titanic,1
"            # work out desired resampling - this needs to be inheritied
            y_counts = pd.Series(y_train).value_counts().values

            # use the correct resampling with the same ratio as found in the clf main
            target_resample = (int(y_counts[0]/self.downsample_bl_factor),y_counts[1]*self.upsample_seizure_factor)
            print('CV fold:'+str(fold)+' . Resampling '+str(y_counts)+ ' to '+str(target_resample)+ \
                  ' with upsample seizures factor: ' + str(self.upsample_seizure_factor) + ',and downsample bl factor: '+ str(self.downsample_bl_factor))
            samp_y, samp_x = self.resample_training_dataset(self.labels, self.features, sizes = target_resample)

            # train clf on resampled
            rf = RandomForestClassifier(n_jobs=-1, n_estimators= self.ntrees, oob_score=False, bootstrap=True)
            rf.fit(samp_x, np.ravel(samp_y))

            # now get the emissions
            test_emitts  = rf.predict(X_test)

            # work out emission probabilities from the difference between annotations and emissions
            binary_states = np.where(y_test==0,0,1) # this is for when multiple labels
            emission_matrix = hmm.get_state_emission_probs(test_emitts, binary_states)
            print('CV fold: '+str(fold)+ ' emission matrix:')",pyecog/ndf/classifier.py,jcornford/pyecog,1
"
  #initial parameters
  test_size = 1
  amount = 10000 
  inv_made = 0
  market_earn = 0
  pred_invest = 0
  X,y= buildDataSet()

  #Create Model
  clf=RandomForestClassifier(max_features=None, oob_score=True)
  clf.fit(X[:-test_size],y[:-test_size]) 

  data_df = pd.DataFrame.from_csv(""forward_sample_full.csv"")
  data_df = data_df.replace(""NaN"",0).replace(""N/A"",0)
  X = np.array(data_df[features].values)
  X = preprocessing.scale(X)
  Z = data_df[""Ticker""].values.tolist()
  invest_list = []
",get_invests.py,aditya-pai/stockPrediction,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans10.py,diogo149/CauseEffectPairsPaper,1
"import numpy as np
from sklearn.ensemble import RandomForestClassifier
X, y = np.load('train_X.npy'), np.load('train_y.npy')
print X.shape, y.shape
clf = RandomForestClassifier(max_features = 200, n_estimators = 15000, n_jobs = -1)
clf.fit(X, y)
print 'training done'
print clf.score(X, y)

from sklearn.externals import joblib
joblib.dump(clf, 'big_forest.pkl')",run_big_forest.py,artiomka/kaggle,1
"def read_data(target, data_type):
    return cached_data_loader.load('data_%s_%s_%s'%(data_type,target,FEATURES),None)


## Predict

# In[45]:

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=3000, min_samples_split=1, bootstrap=False,max_depth=10,
                             n_jobs=-1) #min_samples_leaf=4


# In[63]:

from sklearn import preprocessing
from nolearn.dbn import DBN
from sklearn.pipeline import Pipeline
",140930-DBN-CV.py,udibr/seizure-prediction,1
"y = iris.target

X_train, X_test, y_train, y_test = \
         train_test_split(X, y, test_size=0.3, random_state=0)

X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))

# n_estimators defaults to 10, so we are training 10
# decision trees via 2 processor cores (n_jobs = 2)
forest = RandomForestClassifier(criterion='entropy', random_state = 1, n_jobs=2)
forest.fit(X_train, y_train)

PlotFigures.plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105, 150))
plt.show()

for X_test_val in X_test:
    print(forest.predict_proba(X_test_val.reshape(1, -1)))",Chapter3/RandomForest.py,southpaw94/MachineLearning,1
"	model.fit( X_train, y_train, X_test, y_test, nb_classes)
	mlp_score = model.score( X_test, y_test)
	print( ""DNN:"", mlp_score)

	model = kkeras.CNNC( param_d[""n_cv_flt""], param_d[""n_cv_ln""], param_d[""cv_activation""],
		l=[X_train.shape[1], 30, 10, nb_classes])
	model.fit( X_train, y_train, X_test, y_test, nb_classes)
	cnn_score = model.score( X_test, y_test)
	print( ""DCNN:"", cnn_score)

	model = ensemble.RandomForestClassifier( n_estimators=10)
	model.fit( X_train, y_train)
	rf_score = model.score( X_test, y_test)
	print( ""RF:"", rf_score)

	return dt_score, sv_score, mlp_score, cnn_score, rf_score


def GET_clsf2_by_clst( nb_classes):
	def clsf2_by_clst( Xpart_cf, Xpart_ct):",kcell_r2.py,jskDr/jamespy_py3,1
"    # Numpy arrays are easy to work with, so convert the result to an
    # array
    train_data_features = train_data_features.toarray()

    # ******* Train a random forest using the bag of words
    #
    print ""Training the random forest (this may take a while)...""


    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators=100)

    # Fit the forest to the training set, using the bag of words as
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit(train_data_features, train[""sentiment""])


",src/main/python/BagOfWords.py,Chaparqanatoos/kaggle-knowledge,1
"    class_weight = None
    if balanced:
        class_weight = 'balanced'

    # Make appropriate delegatation
    if 'lr' in method:
        estimator = LogisticRegression()
    elif 'svm' in method:
        estimator = SVC(probability=True)
    elif 'rf' in method:
        estimator = RandomForestClassifier()
    else:
        raise ValueError(""Not implemented for method {}"".format(method))

    estimator = estimator.set_params(**{'class_weight': class_weight, 'random_state': random_state})
    if hasattr(estimator, 'n_jobs'):
        estimator.set_params(**{'n_jobs': 1})

    if 'bagged' in method:
        for l in labels:",interactome_predict.py,daniaki/ppi_wrangler,1
"        training_label = np.ravel(label_binarize(
            np.hstack(training_label).astype(int), [0, 255]))
        print 'Create the training set ...'

        # Perform the classification for the current cv and the
        # given configuration
        # Feature selector
        sel = SelectPercentile(f_classif, p)
        training_data = sel.fit_transform(training_data, training_label)
        testing_data = sel.transform(testing_data)
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data, training_label).predict_proba(
            testing_data)

        results_cv.append([pred_prob, crf.classes_])
        feat_imp_cv.append(sel.get_support(indices=True))

    results_p.append(results_cv)
    feat_imp_p.append(feat_imp_cv)
",pipeline/feature-classification/exp-3/selection-extraction/anova/pipeline_classifier_adc.py,I2Cvb/mp-mri-prostate,1
"print ""EXP TRAIN TYPE:"",type(exp_train)
print ""EXP TRAIN SHAPE:"",exp_train.shape # (236,9803)
#print exp_test.shape   # (59,9803)
print ""EXP TRAIN: \n"",exp_train
print ""SURV TRAIN SHAPE: "",surv_train.shape #(236,1)
print ""SURV TRAIN RAVEL SHAPE: "",surv_train.ravel().shape #(236,)
print ""SURV TRAIN TYPE: "",type(surv_train) # numpy.ndarray
print ""SURV TRAIN: \n"",surv_train


model = RandomForestClassifier(n_estimators = 100)
model = model.fit(exp_train,surv_train.ravel())

output = model.predict(exp_test)

print ""OUTPUT:\n"",output
print ""OUTPUT TYPE:"",type(output) # numpy.ndarray
print ""OUTPUT SHAPE:"",output.shape
print ""surv_test:\n"",surv_test
",python/scikitlearn/survivaltest/scripts/kaggleinspired.py,jdurbin/sandbox,1
"        if len(self.__train_dfs) != 0:
            #---------------------------------------------------------------
            #   Create and train a tree model using sklearn api
            #---------------------------------------------------------------
            #create an instance of a tree model.
            if self.__treeType == ""DecisionTreeEntropy"":
                self.__tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 10)
            elif self.__treeType == ""DecisionTreeGini"":
                self.__tree = tree.DecisionTreeClassifier(criterion='gini', max_depth = 10)
            elif self.__treeType == ""RandomForestEntropy"":
                self.__tree  = RandomForestClassifier(criterion='entropy', n_estimators=25, max_depth = 10)
            elif self.__treeType == ""RandomForestGini"":
                self.__tree  = RandomForestClassifier(criterion='gini', n_estimators=25, max_depth = 10)
            elif self.__treeType == ""GaussianNB"":
                self.__tree = GaussianNB()
            elif self.__treeType == ""MLP"":
                self.__tree = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,40), random_state=1)

            if dontFit == False:
                #fit the model using the numeric representations of the training data",Codes/Tree.py,Texju/DIT-Machine_Learning,1
"# print(""DIR "",dir(text_clf))
# print(""GOT "",text_clf.predict(data_test[0]))
# print(""GOT "",text_clf.predict_proba(data_test[0]))
# sys.exit()


if True:
    # Create the pipeline
    text_clf = Pipeline([('vect', vectorizer),
                         #('clf', MultinomialNB()),
                         ('clf', RandomForestClassifier(n_estimators=100)),
                         #('clf', LinearSVC()),
                         #('clf', SGDClassifier()),
                         #('clf', SVC(probability=True)), # probability slows method but allows retrievel of probabilities
                         ])
    
    t0 = time()
    text_clf.fit(data_train, target_train)
    train_time = time() - t0
    print(""train time: %0.3fs"" % train_time)",sklearn_separate.py,linucks/textclass,1
"# instances is optimized. The naming convention is a tribute to that heritage.)
# This argument will be a integer between 0 and num_instances (defined below).
# Note that this increases the computational effort as SMAC now estimates the 
# quality of a parameter setting for multiple instances.
def random_forest(n_estimators,criterion, max_features, max_depth, bootstrap, instance):

	# Use the requested fold
	train, test = kfold[instance]
	X_train, Y_train, X_test, Y_test = X[train], Y[train], X[test], Y[test]
	
	predictor = sklearn.ensemble.RandomForestClassifier(n_estimators = n_estimators, criterion=criterion, max_features = max_features, max_depth = max_depth, bootstrap=bootstrap)
	predictor.fit(X_train, Y_train)
	
	return -predictor.score(X_test, Y_test)


# Convenience function to model compute the true mean accuracy across all
# 10 folds.
def true_accuracy(**config):
	accuracy = 0.",examples/sklearn_example_advanced_crossvalidation.py,sfalkner/pySMAC,1
"                print(t+1, 'records loaded')
    print('training set loaded')

    del labels

    # Parameters for Randomforest
    random_state = 123
    #n_jobs = 5
    n_jobs = 5
    verbose = 2
    clf = RandomForestClassifier(random_state=random_state, n_jobs=n_jobs, verbose = verbose)

    # Start training
    print('training started')
    clf.fit(train[:,:-1], train[:,-1])
    print('training completed')

    # We don't need training set now
    del train
",solution.py,virajkulkarni14/MalwareClassify,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/meta_only.py,diogo149/CauseEffectPairsPaper,1
"logger.addHandler(fh)


def define_hyper_params():
    """"""
        Esta funcin devuelve un diccionario con
        los clasificadores que vamos a utilizar y
        una rejilla de hiperparmetros
    """"""
    clfs = {
        'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),
        'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),
        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=""SAMME"", n_estimators=200),
        'LR': LogisticRegression(penalty='l1', C=1e5),
        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),
        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),
        'NB': GaussianNB(),
        'DT': DecisionTreeClassifier(),
        'SGD': SGDClassifier(loss=""hinge"", penalty=""l2""),
        'KNN': KNeighborsClassifier(n_neighbors=3) ",Tareas/tarea_3/magic_loop_pipeline.py,rsanchezavalos/compranet,1
"def game(x_train, x_test, y_train, y_test, algo='rf', show_train_scores=True):
    """"""Standard Alogrithms fit and return scores.

    * Default Random State is set as 192 when posible.
    * Available models - dc, rf, gb, knn, mc_ovo_rf, mc_ova_rf
    """"""
    if algo is 'dc':
        clf = clf = DummyClassifier(strategy='most_frequent', random_state=192)

    elif algo is 'rf':
        clf = RandomForestClassifier(n_jobs=-1, random_state=192)

    elif algo is 'gb':
        clf = GradientBoostingClassifier(random_state=192)

    elif algo is 'knn':
        clf = KNeighborsClassifier()

    elif algo is 'mc_ovo_rf':
        clf = OneVsOneClassifier(RandomForestClassifier(n_jobs=-1,",scripts/tools.py,msampathkumar/datadriven_pumpit,1
"    class_weight = None
    if balanced:
        class_weight = 'balanced'

    # Make appropriate delegatation
    if 'lr' in method:
        estimator = LogisticRegression()
    elif 'svm' in method:
        estimator = SVC(probability=False)
    elif 'rf' in method:
        estimator = RandomForestClassifier()
    else:
        raise ValueError(""Not implemented for method {}"".format(method))

    estimator = estimator.set_params(**{'class_weight': class_weight, 'random_state': random_state})
    if hasattr(estimator, 'n_jobs'):
        estimator.set_params(**{'n_jobs': 1})

    if 'bagged' in method:
        for l in labels:",recall_precision.py,daniaki/ppi_wrangler,1
"	f.close()

# def main():
fn = sys.argv[1]
X,Y = load_svmlight_file(fn)

rf_parameters = {
	""n_estimators"": 500,
	""n_jobs"": 1
}
clf = RandomForestClassifier(**rf_parameters)
X = X.toarray()

print clf

print ""Starting Training""
t0 = time()
clf.fit(X, Y)
train_time = time() - t0
print ""Training on %s took %s""%(fn, train_time)",sklrf.py,ryanbressler/ClassWar,1
"        return [features[index] for index in sorted_features]
    return sorted_features


def ova_forest_importance(X, cluster_labels, features=None, top_k=None):
    """"""Determine distinguishing cluster features based on
    RandomForest feature importances.
    """"""
    # fit a One-Vs-Rest classifier to distinguish clusters
    cluster_classifier = OneVsRestClassifier(
        estimator=RandomForestClassifier(n_estimators=100, n_jobs=-1))
    cluster_classifier.fit(X, cluster_labels)

    feature_importance = [top_k_features(estimator,
                                         features=features,
                                         top_k=top_k) for estimator in
                          cluster_classifier.estimators_]

    return feature_importance
",clumpy/importance.py,joshloyal/ClumPy,1
"        
        # # Initialize hyper-parameter space
        # param_grid = [
        #     {'criterion': ['gini'], 'max_depth': [None, 5, 6, 7, 8, 9, 10], 'n_estimators': [10, 20, 30, 40, 50, 75, 100, 150, 200],
        #      'max_features': [None, int, float, 'auto', 'sqrt', 'log2']},
        #     {'criterion': ['entropy'], 'max_depth': [None, 5, 6, 7, 8, 9, 10], 'n_estimators': [10, 20, 30, 40, 50, 75, 100, 150, 200],
        #      'max_features': [None, int, float, 'auto', 'sqrt', 'log2']}
        # ]

        # # Optimize classifier over hyper-parameter space
        # clf = grid_search.GridSearchCV(estimator=ensemble.RandomForestClassifier(), param_grid=param_grid, scoring='accuracy')
        # self.classifiers[2] = clf

    def predict(self, examples):
        # Extract results from the three chained random forest
        results = self.classifiers[0].predict_proba(examples)
        results = self.classifiers[1].predict_proba(results)",project/HierarchicalRandomForest_sklearn.py,grantathon/computer_vision_machine_learning,1
"# dflist = []
# keylist = []
# for key, value in df.iteritems():
#     temp = value
#     tempk = key
#     dflist.append(temp)
#     keylist.append(tempk)
# Y = dflist[0]
# X = dflist[2:]

#clf = RandomForestClassifier(n_estimators=30)
clf = tree.DecisionTreeClassifier()
clf = clf.fit(train_samps.values, train_labels.values)
# print test_samps
# print test_samps.values
anses = clf.predict(test_samps.values)

# print anses
# print test_labels.values
# print sum(anses)/len(anses)",learning/decisiontree2.py,ucsd-progsys/ml2,1
"        '''Boruta cannot handle missing values. Either run impute_values
        before feature_selection, or the following function runs mean
        imputation prior to running Boruta'''
        if np.any(np.isnan(self.train)):
            warnings.warn('RandomForestClassifier requires no missing data,\
                           features being imputed by mean')
            X = self.train
            imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
            imp.fit(X)
            self.train = imp.transform(X)
        rf = RandomForestClassifier(n_jobs=-1, oob_score=True, max_depth=5)
        feat_selector = boruta_py.BorutaPy(rf, n_estimators='auto',
                                           verbose=verbose, seed=seed)
        feat_selector.fit(self.train, self.predictors)
        self.feature_support = feat_selector.support_
        filtered_names = [i for indx, i in enumerate(self.feature_names) if self.feature_support[indx]]
        self.feature_names = filtered_names
        self.train = feat_selector.transform(self.train)

    def impute_values(self, distance, verbose, k=5):",bcml/Parser/build_training.py,sandialabs/BioCompoundML,1
"

iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target


def test_EnsembleVoteClassifier():

    np.random.seed(123)
    clf1 = LogisticRegression()
    clf2 = RandomForestClassifier()
    clf3 = GaussianNB()
    eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='hard')

    scores = cross_val_score(eclf,
                             X,
                             y,
                             cv=5,
                             scoring='accuracy')
    scores_mean = (round(scores.mean(), 2))",mlxtend/classifier/tests/test_ensemble_vote_classifier.py,rasbt/mlxtend,1
"
    p = 0.7
    gene.rank = p * dicty[gene][go_term][0].data.shape[0]
    exp_cond.rank = p * dicty[gene][exp_cond][0].data.shape[1]
    go_term.rank = p * dicty[gene][go_term][0].data.shape[1]
    fuser.fuse(fusion_graph)
    X = fuser.complete(fusion_graph[gene][exp_cond][0])

    X_train = X[train_idx, :]
    y_train = dicty[gene][go_term][0].data[train_idx, term_idx]
    clf = ensemble.RandomForestClassifier(n_estimators=200)
    clf.fit(X_train, y_train)
    X_new = X[test_idx, :]
    y_pred = clf.predict_proba(X_new)[:, 1]
    return y_pred


def rf(train_idx, test_idx, term_idx):
    X_train = dicty[gene][exp_cond][0].data[train_idx, :]
    y_train = dicty[gene][go_term][0].data[train_idx, term_idx]",examples/dicty_factorization.py,marinkaz/scikit-fusion,1
"
f = open(""filename.txt"")
f.readline()  # skip the header
train = np.loadtxt('~/NetBeansProjects/extractor/generated/avro/259/1281/256/train.csv', skip_header=1, names=""file1Id, file1, file2Id, file2, id, issueId, commitId, issueKey, issueType, issuePriority, issueAssignedTo, issueSubmittedBy, commenters, devCommenters, issueAge, wordinessBody, wordinessComments, comments, networkId, networkIssueId, networkCommitId, btwMdn, clsMdn, dgrMdn, efficiencyMdn, efvSizeMdn, constraintMdn, hierarchyMdn, size, ties, density, diameter, commitMetricId, commitMetricCommitId, revision, committer, fileMetricId, fileId, committers, commits, fileAge, addedLines, deletedLines, changedLines, cochanged"", delimiter=';', dtype=[('file1Id', '<i4'), ('file1', '|S1024'), ('file2Id', '<i4'), ('file2', '|S1024'), ('id', '<i4'), ('issueId', '<i4'), ('commitId', '<i4'), ('issueKey', '|S32'), ('issueType', '|S32'), ('issuePriority', '|S32'), ('issueAssignedTo', '|S128'), ('issueSubmittedBy', '|S128'), ('commenters', '<i4'), ('devCommenters', '<i4'), ('issueAge', '<i4'), ('wordinessBody', '<i4'), ('wordinessComments', '<i4'), ('comments', '<i4'), ('networkId', '<i4'), ('networkIssueId', '<i4'), ('networkCommitId', '<i4'), ('btwMdn', '<f8'), ('clsMdn', '<f8'), ('dgrMdn', '<f8'), ('efficiencyMdn', '<f8'), ('efvSizeMdn', '<f8'), ('constraintMdn', '<f8'), ('hierarchyMdn', '<f8'), ('size', '<i4'), ('ties', '<i4'), ('density', '<f8'), ('diameter', '<i4'), ('commitMetricId', '<i4'), ('commitMetricCommitId', '<i4'), ('revision', '|S128'), ('committer', '|S128'), ('fileMetricId', '<i4'), ('fileId', '<i4'), ('committers', '<i4'), ('commits', '<i4'), ('fileAge', '<i4'), ('addedLines', '<i4'), ('deletedLines', '<i4'), ('changedLines', '<i4'), ('cochanged', '<i4')])
print(train)

#from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier

#modelo = MultinomialNB()
modelo = RandomForestClassifier(n_estimators=10)

modelo.fit(dados, marcacoes)

misterioso1 = [1, 1, 1]
misterioso2 = [1, 0, 0]
misterioso3 = [0, 0, 1]

teste = [misterioso1, misterioso2, misterioso3]
",recominer-extractor/src/main/resources/scripts/classification.py,rodrigokuroda/recominer,1
"
    xgboost_params = {'objective': 'binary:logistic', 'booster': 'gbtree', 'eval_metric': 'auc', 'eta': 0.01,
                      'subsample': 0.75, 'colsample_bytree': 0.68, 'max_depth': 7, 'silent': 0}

    results = xgb.cv(xgboost_params, xgtrain, num_boost_round=5, nfold=5, metrics={'error'}, seed=0, show_stdv=False)

    # Luke messing up sebastian's pristine code
    y = target.astype(int).values
    X = preds.values

    rf = RandomForestClassifier()
    nb = BernoulliNB()

    cv1 = cross_val_score(rf, X, y)
    cv2 = cross_val_score(nb, X, y)

    print(results['test-error-mean'].mean(), sum(cv1) / len(cv1), sum(cv2) / len(cv1))


def main():",Reddit/reddit_analysis.py,PTSD-Syntaxing/PTSD_Syntax,1
"print (train_data_features.shape)

vocab = vectorizer.get_feature_names()
dist = np.sum(train_data_features, axis = 0)
print (vocab)


# Using Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

rforest = RandomForestClassifier(n_estimators=100)
rforest = rforest.fit(train_data_features, df[""sentiment""])

X = np.asarray(df['sentiment'])

predict = rforest.predict(X)
print(predict)",src/bow.py,switchkiller/ml_imdb,1
"

def train_and_test(X, y, percents):
    test_size = int(round(len(X) * 0.05))
    print('Size of test-set: ' + str(test_size))

    print('Training the model')
    # Try both with kernel = 'linear' and 'rbf'
    # clf = svm.SVC(kernel='linear', C = 0.01)
    clf = svm.LinearSVC(C = 1.0, dual = False)
    # clf = ensemble.RandomForestClassifier(n_jobs=-1,
    #     n_estimators=200,
    #     criterion='entropy')
    clf.fit(X[:-test_size], y[:-test_size])

    correct_count = 0
    stock_percent = 0
    result_percent = 0
    is_bougth = False
",modules/stock_analyzer.py,dasovm/olga,1
"X = sparse[:classes.shape[0]]
X_CV = X[-300:]
X = X[:-300]

X_test = sparse[classes.shape[0]:]

Y = classes_to_Y(classes)
Y_CV = Y[-300:]
Y = Y[:-300]

RF = RandomForestClassifier()
RF.fit(X, Y)
print 'done fitting'
preds = RF.predict(X_CV)

mistakes = 0
for i in range(len(preds)):
    if preds[i] != Y_CV[i]:
        mistakes += 1
        print Y_CV[i]",walter/basic_classify.py,sandias42/mlware,1
"    degree_result = {}
    trainFeature, testFeature, trainIdList, testIdList, trainTarList = merge_feature()
    # tmp = [t<32 for t in trainTarList]
    # tmp = np.array(tmp)
    # trainFeature = trainFeature[tmp]
    target = np.array(trainTarList)
    # target = target[tmp]
    trainIdList = np.array(trainIdList)
    # trainIdList = trainIdList[tmp]
    cFeature = trainFeature.columns[:]
    clf = RandomForestClassifier(n_estimators=200, min_samples_split=9)
    clf.fit(trainFeature[cFeature], target)
    preds = clf.predict(testFeature)
    right = 0
    for i in range(len(preds)):
        degree_result[testIdList[i]] = preds[i]
    return degree_result


if __name__ == '__main__':",position_predict/predict_code/degree_predict.py,yinzhao0312/Position-predict,1
"from __future__ import print_function
from sklearn.ensemble import RandomForestClassifier
from pyscript.pyscript import ArffToArgs, uses

@uses([""num_trees""])
def train(args):
    X_train = args[""X_train""]
    y_train = args[""y_train""].flatten()
    num_trees = args[""num_trees""] if ""num_trees"" in args else 10
    rf = RandomForestClassifier(n_estimators=num_trees, random_state=0)
    rf = rf.fit(X_train, y_train)
    return rf

def describe(args, model):
    return ""RandomForestClassifier with %i trees"" % model.n_estimators

def test(args, model):
    X_test = args[""X_test""]
    return model.predict_proba(X_test).tolist()",scripts/scikit-rf.py,chrispy645/weka-pyscript,1
"    print ""using %d features (%d columns) on %d rows and target %s. Split %f."" % (
    len(features), len(df.columns), len(df), target, split)
    # print ""unused features: "", '\n\t\t'.join([f for f in df.columns if f not in features])
    # print ""columns: "", '\n\t\t'.join(df.columns)
    df['is_train'] = np.random.uniform(0, 1, len(df)) <= split
    train, test = df[df['is_train'] == True], df[df['is_train'] == False]

    clf = Pipeline([
        (""imputer"", Imputer(strategy=""mean"", axis=0)),
        ('feature_selection', SelectKBest(k=200)),
        (""forest"", RandomForestClassifier(
            min_samples_leaf=1, min_samples_split=10, n_estimators=60, max_depth=None, criterion='gini'))])
    clf.fit(train[features], train[target])
    score = clf.score(test[features], test[target])
    predicted = clf.predict(test[features])

    cm = confusion_matrix(test[target], predicted)
    # print classification_report(test[target], predicted)

    return score, cm",occupancy/predict.py,datamindedbe/train-occupancy,1
"                        n_jobs=-1
                    else:
                        n_jobs=1
                        
                    #
                    
                    if inClassifier == 'RF':
                        param_grid_rf = dict(n_estimators=3**sp.arange(1,5),max_features=sp.arange(1,4))
                        y.shape=(y.size,)    
                        cv = StratifiedKFold(y, n_folds=3)
                        grid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid_rf, cv=cv,n_jobs=n_jobs)
                        grid.fit(x, y)
                        model = grid.best_estimator_
                        model.fit(x,y)        
                    elif inClassifier == 'SVM':
                        param_grid_svm = dict(gamma=2.0**sp.arange(-4,4), C=10.0**sp.arange(-2,5))
                        y.shape=(y.size,)    
                        cv = StratifiedKFold(y, n_folds=5)
                        grid = GridSearchCV(SVC(), param_grid=param_grid_svm, cv=cv,n_jobs=n_jobs)
                        grid.fit(x, y)",function_historical_map.py,lennepkade/HistoricalMap,1
"#clf = SGDClassifier(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'linearsvm accuracy ', accuracy_score(test_gt,pred)

#clf = LogisticRegression(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'logistic accuracy ', accuracy_score(test_gt,pred)

#clf = RandomForestClassifier(min_samples_leaf=20,n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'rfc accuracy ', accuracy_score(test_gt,pred)
#pred = clf.predict(raw_orig)
#with open('rfc.txt','w') as otf:
#    for p in pred:
#        otf.write(str(int(p)) + '\n')
from keras.utils.np_utils import to_categorical
",learning/nin-smaller-basic.py,leonidk/centest,1
"
# define features selection for pipe
kbest = SelectKBest(k=9)
pbest = SelectPercentile(percentile=20)
pca = PCA(n_components=5)

# define clfs for pipe nb, svc, knn, rf and ada
nb = GaussianNB()
svc = SVC()
knn = KNeighborsClassifier()
rf = RandomForestClassifier()
ada = AdaBoostClassifier()

# pipe for gs1
pipe_gs = Pipeline([
    (""scaler"", None),
    (""feature_selection"", None),
    (""clf"", nb)
])
",p5/final_project/poi_id.py,stefanbuenten/nanodegree,1
"    # plt.title('Precision, Recall, F1')
    # plt.legend(loc='best')
    # plt.savefig('images/Result_with_Thresholds.png')
    #
    # print list_results


        # def find_important_features(train_data, train_labels, test_data, test_labels, window_size=0):
        #     from sklearn.ensemble import RandomForestClassifier
        #     # train by Random Forest
        #     clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)
        #     forest = clf
        #
        #     X = np.concatenate((train_data, test_data), axis=0)
        #     Y = np.concatenate((train_labels, test_labels), axis=0)
        #
        #     forest.fit(X, Y)
        #
        #     importances = forest.feature_importances_
        #",support_functions.py,ducminhkhoi/PrecipitationPrediction,1
"
# print iris

kfold = KFold(n_splits=5, shuffle=True)
print kfold

for train_index, test_index in kfold.split(X):
    # print(""TRAIN:"", train_index, ""TEST:"", test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    clf = RandomForestClassifier(n_estimators=20,
                                 bootstrap=True,
                                 oob_score=True,
                                 min_samples_leaf=20,
                                 criterion=""entropy"")
    clf = clf.fit(X_train, y_train)
    oob_error = 1 - clf.oob_score_
    print ""oob error"", oob_error,
    cats = clf.predict(X_test)
    counts = collections.Counter(y_test==cats)",python/iris.py,parrt/AniML,1
"# CountVectorizer

# data, clean_train_review = load()
vectorizer = CountVectorizer(max_features=5000)
# # 
# train_data_features = vectorizer.fit_transform(clean_train_review)
# train_data_features = train_data_features.toarray()
# print(train_data_features.shape)

# 
# forest = RandomForestClassifier(n_estimators=100)
# forest = forest.fit(train_data_features, data[""sentiment""])
# print("""")
# with open(""data/forest.pickle"", ""wb"") as f:
#     pickle.dump(forest, f, -1)
# f.close()
with open(""data/forest.pickle"", 'rb') as f1:
    forest = pickle.load(f1)
f1.close()
",BagOfWordsMeetsBagsOfPopcorn/bofmbop.py,IgowWang/MyKaggle,1
"                               dce_training_data,
                               spa_training_data))
    testing_data = np.hstack((t2w_testing_data,
                               adc_testing_data,
                               mrsi_testing_data,
                               dce_testing_data,
                               spa_testing_data))

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    pred_prob = crf.fit(training_data,
                        np.ravel(training_label)).predict_proba(
                            testing_data)

    result_cv.append([pred_prob, crf.classes_])

# Save the information
path_store = '/data/prostate/results/mp-mri-prostate/exp-4/aggregation-modality'
if not os.path.exists(path_store):",pipeline/feature-classification/exp-4/pipeline_classifier_aggregation_modality.py,I2Cvb/mp-mri-prostate,1
"%time X_train_test_categ = pd.concat([get_dummies(d_train_test, col) for col in vars_categ], axis = 1)
X_train_test = pd.concat([X_train_test_categ, d_train_test.ix[:,vars_num]], axis = 1)
y_train_test = np.where(d_train_test[""dep_delayed_15min""]==""Y"", 1, 0)

X_train = X_train_test[0:d_train.shape[0]]
y_train = y_train_test[0:d_train.shape[0]]
X_test = X_train_test[d_train.shape[0]:]
y_test = y_train_test[d_train.shape[0]:]

#Airline delay using RandomForestClassifier
md = RandomForestClassifier(n_estimators = 150, n_jobs = -1)
%time md.fit(X_train, y_train)


%time phat = md.predict_proba(X_test)[:,1]
metrics.roc_auc_score(y_test, phat)

#Airline delay using LogisticRegression
md = LogisticRegression(tol=0.001, C=1000, n_jobs = -1)
%time md.fit(X_train, y_train)",cdsw_usecases/01_predict_airline_delays/01_benchml_rf_bt_lr_airline_.py,ravi9/aaisg-cdsw,1
"train = pd.read_json('../data/train.json')
train.head()

vectorizer = CountVectorizer(max_features = 2000)
recipes = train['ingredients']
model = gensim.models.Word2Vec(recipes, min_count=1)

# get matrix of vector array
recipes_train_word2vec = load_word_2_vec_mat(len(recipes),100,model,recipes, True)

forest = RandomForestClassifier(n_estimators = 30)
forest = forest.fit( recipes_train_word2vec, train[""cuisine""] )

# load data
test = pd.read_json('../data/test.json')
test.head()

test_recipes = test['ingredients']
recipes_test_word2vec = load_word_2_vec_mat(len(test_recipes),100,model,test_recipes, True)
",src/word_2_vec.py,Cospel/kaggle-cooking,1
"            scoring = 'accuracy'

            ######################################################
            # Use different algorithms to build models
            ######################################################

            # Add each algorithm and its name to the model array
            models = []
            models.append(('NB', GaussianNB()))
            models.append(('CART', DecisionTreeClassifier()))
            models.append(('RF', RandomForestClassifier()))
            models.append(('KNN', KNeighborsClassifier()))
            models.append(('SVM', SVC()))
            print(models)
            # Evaluate each model, add results to a results array,
            # Print the accuracy results (remember these are averages and std
            results = []
            names = []
            print(models[1:10])
            for name, model in models:",scripts/CaryLou_projectpart2_chidata.py,aburkard/501proj,1
"    ----------
    classes_ : array-like, shape = [n_predictions]

    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.naive_bayes import GaussianNB
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> y = np.array([1, 1, 1, 2, 2, 2])
    >>> eclf1 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='hard')
    >>> eclf1 = eclf1.fit(X, y)
    >>> print(eclf1.predict(X))
    [1 1 1 2 2 2]
    >>> eclf2 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='soft')
    >>> eclf2 = eclf2.fit(X, y)",trans4mers/ensemble/ensemble_classifier.py,rloliveirajr/sklearn_transformers,1
"#
#1.  2.  3.
from sklearn import cross_validation
from sklearn.ensemble import RandomForestClassifier

#
predictors = [""Pclass"",""Sex"",""Age"",""SibSp"",""Parch"",""Fare"",""Embarked""]
#random_state=1
#n_estimators:min_samples_split2
#min_samples_leaf
alg = RandomForestClassifier(random_state=1,n_estimators=10,min_samples_split=2,min_samples_leaf=1)
#
kf = cross_validation.KFold(titanic.shape[0],n_folds=3,random_state=1)
scores = cross_validation.cross_val_score(alg,titanic[predictors],titanic[""Survived""],cv=kf)
print (scores.mean())


# In[13]:

#1050",CSDN//.py,qiu997018209/MachineLearning,1
"from xgb import XGBoostClassifier


BasicSVM = Pipeline([(""SVM (linear)"", LinearSVC())])
NB = Pipeline([(""Gaussian NB Bayes"", GaussianNB())])
SGD = Pipeline([(""Stochastic Gradient Descent"", SGDClassifier())])
DTC = Pipeline([(""Decision Tree"", DecisionTreeClassifier())])
AdaBoost = Pipeline([(""Ada Boost"", AdaBoostClassifier())])
GradientBoosting = Pipeline([(""Gradient Boosting"", GradientBoostingClassifier())])
XGB = Pipeline([(""XGBoost"", XGBoostClassifier(num_class=2, silent=1))])
RandomForest = Pipeline([(""Random Forest"", RandomForestClassifier())])",titanic/classifier.py,lbn/kaggle,1
"    assert_equal(linear_clf.score(X_reduced, y), 1.)


def test_parallel_train():
    rng = check_random_state(12321)
    n_samples, n_features = 80, 30
    X_train = rng.randn(n_samples, n_features)
    y_train = rng.randint(0, 2, n_samples)

    clfs = [
        RandomForestClassifier(n_estimators=20, n_jobs=n_jobs,
                               random_state=12345).fit(X_train, y_train)
        for n_jobs in [1, 2, 3, 8, 16, 32]
    ]

    X_test = rng.randn(n_samples, n_features)
    probas = [clf.predict_proba(X_test) for clf in clfs]
    for proba1, proba2 in zip(probas, probas[1:]):
        assert_array_almost_equal(proba1, proba2)
",venv/lib/python2.7/site-packages/sklearn/ensemble/tests/test_forest.py,chaluemwut/fbserver,1
"    logging.info(""Beginning Decision Tree classification."")
    data = data.toarray()
    run_classification( dt, data, labels )
# }}}



# Perform RandomForest {{{
def do_randomForest( data, labels, config ):

    rf = ensemble.RandomForestClassifier()

    logging.info(""Beginning Random Forest classification."")
    data = data.toarray()
    run_classification( rf, data, labels )
# }}}



# Perform NaiveBayes {{{",code/classifier.py,jrouly/stackmining,1
"            elif estimator == 'logistic_regression':
                pipeline_steps.append(('estimator', LogisticRegression()))
            elif estimator == 'svm':
                pipeline_steps.append(('estimator', SVC()))
            elif estimator == 'polynomial_regression':
                pipeline_steps.append(('pre_estimator', PolynomialFeatures()))
                pipeline_steps.append(('estimator', LinearRegression()))
            elif estimator == 'multilayer_perceptron':
                pipeline_steps.append(('estimator', MLPClassifier(solver='lbfgs',alpha=1e-5)))
            elif estimator == 'random_forest':
                pipeline_steps.append(('estimator', RandomForestClassifier()))
            elif estimator == 'adaboost':
                pipeline_steps.append(('estimator', AdaBoostClassifier())) #AdaBoostClassifier(n_estimators=100)
        else:
            error = 'Estimator %s is not recognized. Currently supported estimators are:\n'%(estimator)

            for option in estimator_options:
                error += '\n%s'%(option)

            raise Exception(error)",pyplearnr/old.py,JaggedParadigm/pyplearnr,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/relative_only.py,diogo149/CauseEffectPairsPaper,1
"		# train_Y  = le.fit_transform(train_Y)

		test_index = test.loc[test.cellID == i].index
		test_label = test.loc[(test.cellID == i),'place_id'].values
		test_curr = test.loc[test.cellID == i].reset_index(drop = True)
		test_X = test_curr[FEATURE_LIST].as_matrix()

		# print len(train_curr)
		# print len(test_curr)

		rf = RandomForestClassifier(max_depth = 20, n_estimators = 30)
		#rf.fit(train_X, train_Y, sample_weight = weight)
		rf.fit(train_X, train_Y)
		test_predict = rf.predict(test_X)
		
		# test_predict = np.argsort(test_predict, axis=1)[:,::-1][:,:3]
		# test_predict = le.inverse_transform(test_predict)
		

		result_df = pd.DataFrame({'index': test_index, 'place_id': test_label, 'predict': test_predict})",facebook-kaggle/06_04.py,SonneSun/MYSELF,1
"    clf = None
    precision = []
    recall = []
    fscore = []
    if classifier == ""NN"":
       clf = MLPClassifier(hidden_layer_sizes=(50), activation='relu', solver='sgd', alpha=1e-2, random_state=None)   
    elif classifier == ""LR"":
        clf = linear_model.LogisticRegression(C=1e3)
        #clf = tree.DecisionTreeClassifier()
    if classifier == ""RF"":
        clf = RandomForestClassifier()
    elif classifier == ""NB"":
        clf = GaussianNB()
    elif classifier == ""SVM"":
        clf = LinearSVC()
    elif classifier == ""KNN"":
        clf = NearestCentroid()
    
    skf = StratifiedKFold(n_splits=nfold, shuffle=True)
    y_test_total = []",classification.py,bahmanh/DocumentClassification,1
"    def __call__(self, nn, train_history):
        if self.ls is None:
            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)

        epoch = train_history[-1]['epoch']
        new_value = float32(self.ls[epoch - 1])
        getattr(nn, self.name).set_value(new_value)

if __name__ == '__main__':       
    train_size = 0.75
    cls = RandomForestClassifier()
    train_df_orig = RevenueCompetition.load_data()
    y = train_df_orig['revenue'].values.astype('float32')
    del train_df_orig['revenue']

    test_df_orig = RevenueCompetition.load_data(train=False)

    full_df_orig = train_df_orig.append(test_df_orig)
    
    print(""Transforming..."")",revenue/dnn.py,ldamewood/kaggle,1
"test_data_features = test_data_features.toarray()
#print train_data_features.shape

vocab = vectorizer.get_feature_names()
#print vocab

print ""Training the random forest...""
from sklearn.ensemble import RandomForestClassifier

# Initialize a Random Forest classifier with 100 trees
forest = RandomForestClassifier(n_estimators = 100) 

# Fit the forest to the training set, using the bag of words as 
# features and the sentiment labels as the response variable
#
# This may take a few minutes to run
forest = forest.fit( train_data_features, train[""price_class""] )

# Use the random forest to make sentiment label predictions
result = forest.predict(test_data_features)",restate/NLP_part1.py,Najsztub/SzczecinHome,1
"features = np.array(features)

print ""All data loaded!""

n_neighbors = 20

ratio = 0.3
train_targets, test_targets, train_features, test_features = train_test_split(targets, features, test_size=1-ratio)

clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')
clf2 = ensemble.RandomForestClassifier(25, criterion=""entropy"")
clf3 = svm.SVC()

clf.fit(train_features, train_targets)
clf2.fit(train_features, train_targets)
clf3.fit(train_features, train_targets)

pickle.dump(clf, open('knn.pkl', 'w'))
pickle.dump(clf2, open('rfc.pkl', 'w'))
pickle.dump(clf3, open('svm.pkl', 'w'))",PythonCode/nearest.py,TheFightingMongooses/cloaked-octo-bugfixes,1
"# valData = valData[randIndices,...]
# valLabels = valLabels[randIndices,...]

# model.fit(trainData,newTrainLabels,batch_size=batchSize,nb_epoch=Numepochs,verbose=1,callbacks=[cb,history,checkpointer],validation_data=(valData,newValLabels),shuffle=True,show_accuracy=False)

# for epochs in range(Numepochs):
#     model.fit(trainData,newTrainLabels,batch_size=batchSize,nb_epoch=1,verbose=1,callbacks=[cb,history],validation_data=(valData,newValLabels),shuffle=True,show_accuracy=False)
model.load_weights(weightSavePath + ""bestWeightsAtEpoch_069.h5"")
# trainFeatures = model.predict(trainData,batch_size=batchSize,verbose=1)
# valFeatures = model.predict(valData,batch_size=batchSize,verbose=1)
# rfModel = RandomForestClassifier(n_estimators=50)
# rfModel.fit(trainFeatures, trainLabels)
# predictedValLabels = rfModel.predict(valFeatures)
# print ""Accuracy on Validation data-set is = "" + str(float(np.sum(valLabels == predictedValLabels))/float(len(valLabels))*100.0) + ""%""

model2 = Sequential()

model2.add(Activation(""linear"",input_shape=(100,)))
model2.add(Dense(100, trainable=True, init=initialization, W_regularizer=l2(regularizer), activation = ""relu""))
model2.add(Dropout(0.5))",train_imageQuality_classification_MDS.py,parag2489/Image-Quality,1
"	elif method == 'SVM':   
		return SVMClass(X_train, y_train, X_test, y_test)

	elif method == 'LDA':
		return LinearDA(X_train, y_train, X_test, y_test)

	elif method == 'QDA': 
		return QuadDA(X_train, y_train, X_test, y_test)

def RF(X_train, y_train, X_test, y_test):
	clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)
	clf.fit(X_train, y_train)
	accuracy = clf.score(X_test, y_test)
	return accuracy

def KNN(X_train, y_train, X_test, y_test):
	clf = neighbors.KNeighborsClassifier()
	clf.fit(X_train, y_train)
	accuracy = clf.score(X_test, y_test)
	return accuracy",machineLearning.py,hmn21/stockMarketPrediction,1
"
'''
x now consists of all the feature columns and y consists of the target/label
column.
'''
x = df.drop('classe', axis=1)
y = df.classe
x_train, __, y_train, __ = train_test_split(x, y, test_size=0.2)

#Already explained in Readme.md
clf = RandomForestClassifier(min_impurity_split=0.1, n_jobs = -1, max_depth=15)
score = cross_val_score(clf, x, y, cv=10)
print(""Mean k-fold cross_validation score for k = 10:"", score.mean())
score = cross_val_score(clf, x, y, cv=5)
print(""Mean k-fold cross_validation score for k = 5:"", score.mean())

input(""\n\nPress enter to exit."")",barbell_lift.py,Pranav-Rastogi/barbell-lift,1
"    return svc


def k_neighbors(x, y):
    neigh = KNeighborsClassifier(n_neighbors=40)
    neigh.fit(x, y)
    return neigh


def random_forest(x, y):
    forest = RandomForestClassifier(n_estimators=10)
    forest.fit(x, y)
    return forest


if __name__ == '__main__':
    crimes_train = pandas.read_csv('train.csv', parse_dates=['Dates'])
    crimes_test = pandas.read_csv('test.csv', parse_dates=['Dates'])

    x_train, y_train = pre_process_data(crimes_train, transform=False)",sf-crime/main.py,sergiy-evision/math-algorithms,1
"#!/usr/bin/env python
from __future__ import print_function

from sklearn.feature_extraction import FeatureHasher
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from sklearn.metrics import log_loss

import ctr

learner = RandomForestClassifier(verbose = False, n_jobs = -1)

for ID,x,y in ctr.data(ctr.train, batchsize = 1):",avazu-ctr/rf.py,ldamewood/renormalization,1
"
    Later: Add options for RBM + Logit; PCA; ICA; LDA.
    (Further) Feature selection should  be implemented within the CV pipeline, if you wish
    to avoid overfitting. (Note the effects of )
     See also
    http://scikit-learn-laboratory.readthedocs.org/en/latest/_modules/skll/learner.html

    Possible Scoreparams: scoreParam = 'f1','accuracy', 'precision', 'roc_auc'..
    '''

#    pipeline1 = Pipeline('clf', RandomForestClassifier() )

    pipeline1 = RandomForestClassifier(n_jobs=-1)
    pipeline2 = SVC(cache_size=1900)
    pipeline3 = GradientBoostingClassifier()
    pipeline4 = LogisticRegression()


    'RandomForestClassifier:'
    parameters1 = {",ProFET/feat_extract/OutPutRes.py,ddofer/ProFET,1
"
print ""Time Taken to Extract Features : "", end-start 


test_data = train_data[0:3000]
test_label = train_label[0:3000] 

bnb = BernoulliNB()
gnb = GaussianNB()
mnb = MultinomialNB()
randfor = RandomForestClassifier(n_jobs=4,n_estimators=23)
supvec = SVC() 

start = time.time()
bnb.fit(train_data,train_label)
gnb.fit(train_data,train_label)
mnb.fit(train_data,train_label)
randfor.fit(train_data,train_label)

end = time.time()",KaggleCooking.py,rupakc/Kaggle---What-s-Cooking,1
"    y2 += [dep(test)]
  return x1,y1,x2,y2

def learns(tests,trains,indep=lambda x: x[:-1],
                    dep = lambda x: x[-1],
                    rf  = Abcd(),
                    lg  = Abcd(),
                    dt  = Abcd(),
                    nb  = Abcd()):
  x1,y1,x2,y2= trainTest(tests,trains,indep,dep) 
  forest = RandomForestClassifier(n_estimators = 50)  
  forest = forest.fit(x1,y1)
  for n,got in enumerate(forest.predict(x2)):
    rf(predicted = got, actual = y2[n])
  logreg = linear_model.LogisticRegression(C=1e5)
  logreg.fit(x1, y1)
  for n,got in enumerate(logreg.predict(x2)):
    lg(predicted = got, actual = y2[n])
  bayes =  GaussianNB()
  bayes.fit(x1,y1)",wei/learn.py,ST-Data-Mining/crater,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/b_to_a_only.py,diogo149/CauseEffectPairsPaper,1
"#selecting best features  
with warnings.catch_warnings(): 
    warnings.simplefilter('ignore', UserWarning)
    warnings.simplefilter('ignore', RuntimeWarning)         
    selector = SelectKBest(f_classif, k = SELECTOR_K)
    selector.fit(features_train, labels_train)
    features_train = selector.transform(features_train)
    features_test = selector.transform(features_test)
    suspect = selector.transform(suspect)
# training machine learning
forest = RandomForestClassifier(n_estimators = 200) 
forest.fit(features_train, labels_train)     
pred = forest.predict(features_test)    
print('Classifier trained with accuracy score: ', accuracy_score(labels_test, pred))
print('Suspect is predicted as:')
suspect_classified = list(forest.predict(suspect))
for s in set(suspect_classified):
    print('{0} - {1:.2f}%'.format(s, suspect_classified.count(s)/len(suspect_classified)*100))   

",classification.py,ViliamV/stylometry,1
"    cnf_matrix = cnf_matrix + confusion_matrix(test_data_label, y_svm_predict, labels=[0,1,2])
    err_svm += score(y_svm_predict,test_data_label)
    
#Decision Tree
    dt = tree.DecisionTreeClassifier()
    dt = dt.fit(train_data_list, train_data_label['volumetype'])
    y_dt_predict = dt.predict(test_data_list)
    err_dt += score(y_dt_predict,test_data_label)

#Random Forest    
    rf = RandomForestClassifier()
    rf.fit(train_data_list, train_data_label['volumetype'])
    y_rf_predict = rf.predict(test_data_list)
    err_rf += score(y_rf_predict,test_data_label)
        
    j=j+1

#Confusion Matrix of SVM Model (Best model)
class_names=[""low"",""medium"",""high""]
plt.figure()",zipcode_classifications.py,siddharthhparikh/INFM750-project,1
"            self.modeldata.drop(self.modeldata[vba_feature_columns], axis=1, inplace=True)
            self.modeldata.drop(self.modeldata[tfidfcolumns], axis=1, inplace=True)


    def buildModels(self):
        '''
        After getLanguageFeatures is called, this function builds the models based on
        the classifier matrix and labels.
        :return:
        '''
        self.cls = RandomForestClassifier()
        # build classifier
        self.cls.fit(self.clf_X, self.clf_y)

        return self.cls


    def loadModelVocab(self):
        '''
        Loads vocabulary used in the bag of words model",mmbot/mmbot.py,egaus/MaliciousMacroBot,1
"    test_centroids = np.zeros(( test[""review""].size, num_clusters),dtype=""float32"")

    counter = 0
    for review in clean_test_reviews:
        test_centroids[counter] = create_bag_of_centroids( review,word_centroid_map)
        counter += 1


    # ****** Fit a random forest and extract predictions
    #
    forest = RandomForestClassifier(n_estimators = 100)

    # Fitting the forest may take a few minutes
    print(""Fitting a random forest to labeled training data..."")
    forest = forest.fit(train_centroids,train[""sentiment""])
    result = forest.predict(test_centroids)

    # Write the test results
    output = pd.DataFrame(data={""id"":test[""id""], ""sentiment"":result})
    output.to_csv(""BagOfCentroids.tsv"", index=False, quoting=3)",doc_ref/NLP/word2vec-nlp-tutorial/DeepLearningMovies/Word2Vec_BagOfCentroids.py,gtesei/fast-furious,1
"
# changes all the NaN values to the mean of the column 
for array in my_data:
	mean = round(np.nanmean(array), 3)
	for index in range(len(array)):
		if np.isnan(array[index]):
			array[index] = mean



clf = RandomForestClassifier(n_estimators=10)


def chooseRandom(data, labels, size):
	""""""Chooses test data and sample data randomly from the data""""""
	x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = size)
	return x_train, x_test, y_train, y_test 

def get_score(size, forest):
	""""""Gets scores for RF based on varying test imputs""""""",Justin/Random Forest.py,BIDS-collaborative/EDAM,1
"survival[survival < meanSurvival] = 0
survival[survival >= meanSurvival] = 1

survival = survival.astype(bool)

# Split data into test and train datasets
exp_train,exp_test,surv_train,surv_test = train_test_split(expression.values[1:,:],
                                                           survival,
                                                           train_size=0.8)

model = RandomForestClassifier(n_estimators = 100)
model = model.fit(exp_train,surv_train.ravel())
output = model.predict(exp_test)

print ""OUTPUT:\n"",output
print ""surv_test:\n"",surv_test.ravel()

print ""CONFUSION MATRIX:\n"",confusion_matrix(surv_test,output)",python/scikitlearn/survivaltest/scripts/kaggleinspired_take2.py,jdurbin/sandbox,1
"    elif(classifier_str == 'gnb'):
        cl = naive_bayes.GaussianNB()
    elif(classifier_str == 'mnb'):
        cl = naive_bayes.MultinomialNB()
    elif(classifier_str == 'bnb'):
        cl = naive_bayes.BernoulliNB()
    # Decision tree
    elif(classifier_str == 'dtree'):
        cl = tree.DecisionTreeClassifier()
    elif(classifier_str == 'rforest'):
        cl = ensemble.RandomForestClassifier()
    else:
        # raise error if classifier not found
        raise ValueError('Classifier not implemented: %s' % (classifier_str))

    return (cl)


#if __name__ == '__main__':
# TODO add test run",spice/classification.py,basvandenberg/spice,1
"  # set_trace()
  return _Abcd(before=actual, after=preds, show=False)[-1]


def rforest(train, test, tunings=None, smoteit=True, duplicate=True):
  ""RF ""
  # Apply random forest Classifier to predict the number of bugs.
  if smoteit:
    train = SMOTE(train, atleast=50, atmost=101, resample=duplicate)
  if not tunings:
    clf = RandomForestClassifier(n_estimators=100, random_state=1)
  else:
    clf = RandomForestClassifier(n_estimators=int(tunings[0]),
                                 max_features=tunings[1] / 100,
                                 min_samples_leaf=int(tunings[2]),
                                 min_samples_split=int(tunings[3])
                                 )
  train_DF = formatData(train)
  test_DF = formatData(test)
  features = train_DF.columns[:-2]",Prediction.py,ai-se/Tree-Learner,1
"        for ft in line[1:]:
            id, val = ft.split("":"")
            row.append(float(val))
        Xte.append(row)
    Xte = np.array(Xte)
    
    return Xtr, Ytr, Xte, names
    
            
def train_rfc(Xtr, Ytr):
    clf = RandomForestClassifier(n_estimators=15, random_state=0, n_jobs=-1).fit(Xtr,Ytr)
    return clf

def train_lr(Xtr, Ytr):
    lr = LogisticRegression().fit(Xtr, Ytr)
    return lr
    
def train_dtree(Xtr, Ytr):
    dtree = DecisionTreeClassifier().fit(Xtr, Ytr)
    return dtree",ML@Hackerrank/quora/quora_answer_classifier.py,ntduong/ML,1
"            np.hstack(training_label).astype(int), [0, 255]))
        print 'Create the training set ...'

        # Learn the PCA projection
        pca = PCA(n_components=sp, whiten=True)
        training_data = pca.fit_transform(training_data)
        testing_data = pca.transform(testing_data)

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_sp.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/selection-extraction/pca/pipeline_classifier_mrsi.py,I2Cvb/mp-mri-prostate,1
"
# Defining New Feature Set
explanatory_features = [col for col in df.columns if col not in ['field_goals_made', 'field_goals_allowed', 'good_team', 'wins', 'points', 'points_against', 'free_throws_made', 'three_pointers_made', 'offensive_rebounds']]
explanatory_df = df[explanatory_features]
explanatory_colnames = explanatory_df.columns




# Predicting wins using Random Forests 
rf = ensemble.RandomForestClassifier(n_estimators= 500)
roc_scores_rf = cross_val_score(rf, explanatory_df, response_series, cv=10, scoring='roc_auc', n_jobs = -1)
print roc_scores_rf.mean()


# Grid Search for best parameters
trees_range = range(10, 300, 10)
param_grid = dict(n_estimators = trees_range)
grid = GridSearchCV(rf, param_grid, cv=10, scoring='roc_auc', n_jobs = -1)
grid.fit(explanatory_df, response_series)",Project Random Forests.py,macohen2/MattsProject,1
"            settings.update({settings_field: default_directories[settings_field]})

        settings[settings_field] = os.path.abspath(settings[settings_field])


    # reads the settings classifier field and return the appropriate clf obj
    # using the model mapping dict

    classifier_objs = {
        'RandomForest':
            sklearn.ensemble.RandomForestClassifier(
                random_state=settings['R_SEED'],
                ),
        'ExtraTrees':
            sklearn.ensemble.ExtraTreesClassifier(
                random_state=settings['R_SEED'],
                ),
        'AdaBoost':
            sklearn.ensemble.AdaBoostClassifier(
                random_state=settings['R_SEED'],",python/utils.py,Neuroglycerin/hail-seizure,1
"    #boosting = AdaBoostClassifier(DecisionTreeClassifier(criterion=""gini"", max_depth=25), n_estimators=50,learning_rate=1.0)

    print ""    \tTrain\tTrain\tTrain\tTest\tTest\tTest""
    print "" #  \tACC\tC. AVG\tC. STD\tACC\tC. AVG\tC. STD""
    best_forest_ref = None
    best_forest_accuracy = None

    for i in range(times):
        start_training_time = time.time()
        #print ""Training #"" + str(i + 1)
        forest = RandomForestClassifier(n_estimators=n_trees,criterion=criterion_t,
                                        max_features=max_features, max_depth=max_D,n_jobs=num_jobs) #n_jobs=-1?

        forest.fit(training, np.ravel(labels_train))

        end_training_time = time.time()
        total_training_time += end_training_time - start_training_time

        start_testing_time = time.time()
",src/random_forest_classify.py,DPRL/MathSymbolRecognizer,1
"
def writeSensAndSpec(fpr, tpr, thresh, filename):
    specificity = 1 - fpr
    a = numpy.vstack([specificity, tpr, thresh])
    b = numpy.transpose(a)
    numpy.savetxt(filename, b, fmt='%.5f', delimiter=',')


def doRUSRFC(analysisDict):
    print('{0} Started'.format(analysisDict['analysisName']))
    RUSRFC = RUSRandomForestClassifier.RUSRandomForestClassifier(n_Forests=100, n_TreesInForest=300)
    predClasses, classProb, featureImp, featureImpSD = RUSRFC.CVJungle(analysisDict['X_train'], analysisDict['Y_train'],
                                                                       shuffle=True, print_v=True, k=10)
    cm = confusion_matrix(analysisDict['Y_train'], predClasses)
    print('Analysis - {0}  - Validation - \n{1}'.format(analysisDict['analysisName'], cm))

    #### For test set
    predClasses_test = RUSRFC.predict(analysisDict['X_test'])
    cm_test = confusion_matrix(analysisDict['Y_test'], predClasses_test)
    print('Test set results - \n{0}'.format(cm_test))",Python/RUSRandomForest/runClassificationHAI2016.py,sulantha2006/Conversion,1
"
    features_train = pca.transform(features_train)
    features_test = pca.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))
",nytimes/step4_analysis_supervised_4(pca).py,dikien/Machine-Learning-Newspaper,1
"
X = v[:,0:target_index]
y = v[:,target_index]

random = 99 # pick reproducible pseudo-random sequence

n_estimators = int(sys.argv[1])
min_samples_leaf = int(sys.argv[2])

start = time.clock()
clf = RandomForestClassifier(n_estimators=n_estimators, oob_score=False,
                             max_features=""sqrt"", bootstrap=True,
                             min_samples_leaf=min_samples_leaf, criterion=""entropy"",
                             random_state=random)
clf = clf.fit(X, y)
stop = time.clock()

print ""Fitting %d estimators %d min leaf size %f seconds\n"" % (n_estimators,min_samples_leaf,stop-start)",python/connect4_timing.py,parrt/AniML,1
"                data2[k] = data2[k].apply(hash)

	ids = data2.AuctionId
	return ids, np.array(data2[used_keys2]), data2.ProbabilityOfDefault

def getListOfLoansToInvest(filename=None, verbose = True):
	global ypred
	ids, Xtest, pf = loadXtest(filename)
	AppliedAmountTest = Xtest[:,applied_amount_idx]			
	InterestTest = Xtest[:,interest_idx]
	est= ensemble.RandomForestClassifier(n_estimators = 100, n_jobs = -1)
	est.fit(X,y)
	probs = est.predict_proba(Xtest)[:,0]
#	probs = pf
	ypred = (np.min(zip(probs, 0.999*np.ones_like(probs)), axis=1)*(1.0+InterestTest)-1.0)/InterestTest
	action = np.logical_and( (ypred>=threshold), (AppliedAmountTest>minimum_amount))
	depth = AppliedAmountTest[action].min() * action.sum()
	if depth<money_to_invest:
		raise Exception(""Not enough depth to invest ""+str(money_to_invest))
",main.py,ghgr/P2P-lending,1
"            X_test_boolean.append(features['boolean'].values())
            X_test_numeric.append(features['numeric'].values())
            unlabeled.append((t, features))
    return X_test_boolean, X_test_numeric


classifiers = {
    'Nearest Neighbors': KNeighborsClassifier(3),
    'RBF SVM': SVC(gamma=2, C=1),
    'Decision Tree': DecisionTreeClassifier(max_depth=5),
    'Random Forest': RandomForestClassifier(max_depth=5,
                                            n_estimators=10, max_features=1),
    'AdaBoost': AdaBoostClassifier(),
    'Gaussian Naive Bayes': GaussianNB(),
    'Bernoulli Naive Bayes': BernoulliNB(),
    'LDA': LDA()
}
#val_ratio_datasets
ratio_datasets = GetLabeledData()
",python/mlAlgorithms/classifier.py,veksev/cydi,1
"# keylist = []
# for key, value in df.iteritems():
#     temp = value
#     tempk = key
#     dflist.append(temp)
#     keylist.append(tempk)
# Y = dflist[0]
# X = dflist[2:]

#clf = tree.DecisionTreeClassifier()
clf = RandomForestClassifier(n_estimators=30)
clf = clf.fit(train_samps.values, train_labels.values)
# print test_samps
# print test_samps.values
anses = clf.predict(test_samps.values)

# print anses
# print test_labels.values
# print sum(anses)/len(anses)
# print sum(test_labels.values)/len(test_labels.values)",learning/randomforest2.py,ucsd-progsys/ml2,1
"    seed = 7
    scoring = 'accuracy'
    # Spot Check Algorithms
    models = []
    models.append(('LR', LogisticRegression()))
    models.append(('LDA', LinearDiscriminantAnalysis()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('NB', GaussianNB()))
    models.append(('SVM', SVC()))
    models.append(('RF', RandomForestClassifier(n_estimators=50)))
    # evaluate each model in turn
    results = []
    names = []
    for name, model in models:
        kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)
        cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
        results.append(cv_results)
        names.append(name)
        msg = ""%s: %f (%f)"" % (name, cv_results.mean(), cv_results.std())",core/main.py,god99me/RandomRoughForest,1
"        Xtrain = Xall[len(Xtest.index):]
        Xtest = Xall[:len(Xtest.index)]

        Xtest['test'] = 1
        Xtrain['test'] = 0
        Xall = pd.concat([Xtest, Xtrain], ignore_index=True)
        ytemp = Xall['test']
        Xall.drop(['test'],axis=1,inplace=True)

        #model = LogisticRegression(C=1.0,penalty='l2')
        model = RandomForestClassifier(n_estimators=100)
        #model = XgboostClassifier(n_estimators=100,learning_rate=0.01,max_depth=2, NA=0,subsample=.5,colsample_bytree=1.0,min_child_weight=5,n_jobs=4,objective='binary:logistic',eval_metric='logloss',booster='gbtree',silent=1,eval_size=0.0)
        #Xall_train = Xall.iloc[ np.random.permutation(len( Xall )) ]
        #buildModel(model,Xall,ytemp,cv=StratifiedShuffleSplit(ytemp,n_iter=5,test_size=0.2), scoring='accuracy', n_jobs=1,trainFull=False,verbose=True)

        model.fit(Xall,ytemp)
        Xall['sim'] = model.predict_proba(Xall)[:,1]

        Xtrain = Xall[len(Xtest.index):]
        Xtest = Xall[:len(Xtest.index)]",numerai/numerai.py,chrissly31415/amimanera,1
"    f = open(path)
    csv_f = csv.reader(f)
    for row in csv_f:
        labels.append(convertToFloat(row[0]))
        data.append(convertToFloat(row[1:]))
    f.close()
    return np.array(data), np.array(labels)

# Random Forest Classifier
def runForest(X_train, y_train):
    forest = RandomForestClassifier(n_estimators=90, random_state=42)
    forest.fit(X_train, y_train)
    return forest

# Stochastic Gradient Descent Classifier
def runSGD(X_train, y_train):
    sgd = SGDClassifier(n_iter=500, loss='modified_huber', penalty='elasticnet', random_state=42)
    sgd.fit(X_train, y_train)
    return sgd
 ",xeneta_qualifier/run.py,xeneta/LeadQualifier,1
"        counter += 1

    test_centroids = np.zeros((test[""review""].size, num_clusters), dtype=""float32"")
    counter = 0

    for review in clean_test_reviews:
        test_centroids[counter] = create_bag_centroids(review, word_centroid_map)
        counter += 1


    forest = RandomForestClassifier(n_estimators=100)
    print (""Fitting a random forest to labelled train data..."")
    forest = forest.fit(train_centroids, train[""sentiment""])
    result = forest.predict(test_centroids)

    output = pd.DataFrame(data={""id"": test[""id""], ""sentiment"": result})",src/word2vec_clustering.py,switchkiller/ml_imdb,1
"#regression = linear_model.LinearRegression()
#regression.fit(x,y)

# fit a curve
# curve = np.polyfit(x,y,10)
# xfit = np.linspace(-10, 120, 100)
# yfit = np.log(xfit)

# Training
x = np.reshape(x,(1000,1))
rf = RandomForestClassifier()
rf.fit(x, y)

# Testing
xfit = np.linspace(-10, 120, 1000)
a, = xfit.shape
xfit = np.reshape(xfit,(a,1))
yfit = rf.predict(xfit)

plt.scatter(xfit,yfit)",example/random_forest.py,Valay/cifar-10,1
"from sklearn.tree import *
from sklearn.calibration import CalibratedClassifierCV
from sklearn.lda import LDA
from gdbn import activationFunctions

normalise = False
select = False

#Trees are very slow, but naive baynes seems hopeless
def random_forest(X, Y):
    trainer = RandomForestClassifier(n_jobs=-1, n_estimators=400, max_features=None)
    return build_classifier(X, Y, trainer)

def extra_random_trees(X, Y):
    trainer = CalibratedClassifierCV(ExtraTreesClassifier(n_jobs=-1, n_estimators=400, max_features=None))
    return build_classifier(X, Y, trainer)

def decision_tree_classifier(X, Y):
    trainer = DecisionTreeClassifier(max_features=None, max_depth=None)
    return build_classifier(X, Y, trainer)",project3/methods.py,ethz-nus/lis-2015,1
"	test_data = test_data[1:]
	for instance in test_data:
		instance[3] = int(instance[3])

	#start train with SVM
	if model_name == 'SVM':
		clf = svm.SVC()
	if model_name == 'decision_tree':
		clf = tree.DecisionTreeClassifier()
	if model_name == 'random_forest':
		clf = RandomForestClassifier()
	if model_name == 'logistic_regression':
		clf = LogisticRegression()
	if model_name == 'linear_regression' or model_name == 'neural_net':
		if model_name == 'linear_regression':
			clf = LinearRegression()
		elif model_name == 'neural_net':
			clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)
		for instance in train_data:
			for feature_idx in range(len(train_data[0])):",code/train_and_test.py,samfu1994/cs838webpage,1
"        if not isinstance(self.export, str):
            mname = 'default'
        else:
            mname = self.export

        df = self.__loadData(data, dropna)
        features = df.columns[:-1]
        X = df[features]
        y = df.iloc[:, -1].values

        # clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, n_jobs=2)
        clf = RandomForestClassifier(n_estimators=int(settings[""n_estimators""]), criterion=settings[""criterion""],
                                     max_features=settings[""max_features""], max_depth=max_depth,
                                     min_samples_split=int(settings[""min_sample_split""]),
                                     min_samples_leaf=int(settings[""min_sample_leaf""]),
                                     min_weight_fraction_leaf=int(settings[""min_weight_faction_leaf""]),
                                     bootstrap=settings[""bootstrap""],
                                     n_jobs=int(settings[""n_jobs""]),
                                     random_state=settings[""random_state""], verbose=int(settings[""verbose""]))
",dmonscikit/dmonscilearnclassification.py,igabriel85/dmon-adp,1
"                hasgreek = 'HASGREEK'
                return 1
        except ValueError:
            return 0
    return 0


class EnsembleNER(object):
    def __init__(self, path, goldset, base_model, features=None, types=None):
        self.ensemble_pipeline = Pipeline([
            ('clf', ensemble.RandomForestClassifier(criterion=""gini"", n_estimators=1000))
            ])
        self.base_model = base_model
        self.path = path
        self.predicted = []
        self.res = None
        self.ids, self.data, self.labels = [], [], []
        self.goldset = goldset
        if types: # features is a list of classifier names
            self.types = types",src/postprocessing/ensemble_ner.py,AndreLamurias/IBEnt,1
"                           converters={0:lambda s: ord(s.split(""\"""")[1])})
    trainDataResponse = trainData[:,1]
    trainDataFeatures = trainData[:,0]

    # Train H2O GBM Model:
    #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")
    rf_h2o = h2o.random_forest(x=alphabet[['X']], y=alphabet[""y""], ntrees=1, max_depth=1, nbins=100)

    # Train scikit GBM Model:
    # Log.info(""scikit GBM with same parameters:"")
    rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
    rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

    # h2o
    rf_perf = rf_h2o.model_performance(alphabet)
    auc_h2o = rf_perf.auc()

    # scikit
    auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_smallcatRF.py,ChristosChristofidis/h2o-3,1
"
param_range = range(1, 50)
training_scores, validation_scores = validation_curve(DecisionTreeClassifier(), digits.data, digits.target,
                                                      param_name=""max_depth"",
                                                      param_range=param_range,
                                                      cv=5)
plt.figure()
plot_validation_curve(param_range, training_scores, validation_scores)

param_range = range(1, 20, 1)
training_scores, validation_scores = validation_curve(RandomForestClassifier(n_estimators=100),
                                                      digits.data, digits.target,
                                                      param_name=""max_features"",
                                                      param_range=param_range,
                                                      cv=5)
plt.figure()
plot_validation_curve(param_range, training_scores, validation_scores)",solutions/forests.py,amueller/advanced_training,1
"graph = pydot.graph_from_dot_data(dot_data.getvalue())
graph.write_png(""figures/decision_tree_rating_2.png"")


# Now for the random forest
# Prepare and split data to training/test set
df_train, df_test, train_label, test_label = train_test_split(data, ratings, test_size=0.2, random_state=42)

# Use cross-validation to get the best number of trees to use
# parameters = {'n_estimators': [10, 50, 100, 150, 200]}
# rf_model = RandomForestClassifier(max_depth=None, min_samples_split=1, random_state=42)
# rf = grid_search.GridSearchCV(rf_model, parameters, cv=5, error_score=0, verbose=1)

# Sticking to 100 trees rather than cross-validating
rf = RandomForestClassifier(max_depth=None, min_samples_split=1, random_state=42, n_estimators=100)

# Fit and predict model
rf.fit(df_train, train_label)
# print('Best parameters: {}'.format(rf.best_params_))
prediction = rf.predict(df_test)",tree_model.py,dr-rodriguez/Exploring-Goodreads,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",autocause/autocause_settings.py,diogo149/autocause,1
"test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1) 


# The data is now ready to go. So lets fit to the train, then predict to the test!
# Convert back to a numpy array
train_data = train_df.values
test_data = test_df.values


print 'Training...'
forest = RandomForestClassifier(n_estimators=100)
forest = forest.fit( train_data[0::,1::], train_data[0::,0] )

print 'Predicting...'
output = forest.predict(test_data).astype(int)


predictions_file = open(""myfirstforest.csv"", ""wb"")
open_file_object = csv.writer(predictions_file)
open_file_object.writerow([""PassengerId"",""Survived""])",myfirstforest.py,portaloffreedom/data-mininig-titanic-group08,1
"                                                   ids_valid, tasks)

    transformers = []
    classification_metric = dc.metrics.Metric(
        dc.metrics.matthews_corrcoef, np.mean, mode=""classification"")
    params_dict = {""n_estimators"": [1, 10]}

    def multitask_model_builder(model_params, model_dir):

      def model_builder(model_dir):
        sklearn_model = RandomForestClassifier(**model_params)
        return dc.models.SklearnModel(sklearn_model, model_dir)

      return dc.models.SingletaskToMultitask(tasks, model_builder, model_dir)

    optimizer = dc.hyper.HyperparamOpt(multitask_model_builder)
    best_model, best_hyperparams, all_results = optimizer.hyperparam_search(
        params_dict,
        train_dataset,
        valid_dataset,",deepchem/hyper/tests/test_hyperparam_opt.py,lilleswing/deepchem,1
"print(""WORD2VEC"")

# extract w2v probabilities on training and test set
word2vecModel = trainWord2Vec(200, 12, corpusFile)
print(""\nCalculating training set w2v vectors..."")
w2v_train = getAvgFeatureVecs(X_train_w2v, word2vecModel, 200)
print(""Calculating test set w2v vectors..."")
w2v_test = getAvgFeatureVecs(X_test_w2v, word2vecModel, 200)

# Fit a random forest to the training data, using 1000 trees
forest_w2v = RandomForestClassifier( n_estimators = 1000 , oob_score=True , random_state=1)
print (""Fitting a random forest to labeled training data..."")
forest_w2v = forest_w2v.fit( w2v_train, y_train )
 
print (""Oob Score:"" + str(forest_w2v.oob_score_) ) 

# get out-of-the-bag predictions
oob_probabilities_w2v = forest_w2v.oob_decision_function_

# variable to access index of classes",OobFusion_2D.py,MKLab-ITI/category-based-classification,1
"
print ""Creating average feature vecs for test reviews""
clean_test_reviews = []
for review in test[""review""]:
    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))

testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)

# Fit a random forest to the training data, using 100 trees
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100)

print ""Fitting a random forest to labeled training data...""
forest = forest.fit(trainDataVecs, train[""sentiment""])

# Text & extract results
result = forest.predict(testDataVecs)

# Write the test results
output = pd.DataFrame(data={""id"":test[""id""], ""sentiment"":result})",popcorn/src/word2vec.py,hkhpub/kagglepy,1
"
        NN = GridSearchCV(NN, param_grid, refit=True, verbose=True, scoring='roc_auc', n_jobs=1, cv=5)
        ## Fit the Classifier
        np.random.seed(1)
        NN.fit(np.asarray(X_), np.asarray(y_, dtype=np.int8))
        ## Best Fit Estimator
        Best_Model = NN.best_estimator_
    
    elif classifier_type == ""RF"":
        ## Random Forest
        rf = RandomForestClassifier(random_state=0, verbose=1, n_estimators=1000)
        ## Meta Parameters Grid Search with Cross Validation
        param_grid = {'max_features': [""auto"", ""log2"", np.int(np.shape(X_)[1]/2)],
                      'n_estimators': [100,500,1000]}    
        rf = GridSearchCV(rf, param_grid, refit=True, verbose=True, scoring='roc_auc', n_jobs=1, cv=5)
        ## Fit the Classifier
        np.random.seed(1)
        rf.fit(np.asarray(X_), np.asarray(y_, dtype=np.int8))
        ## Best Fit Estimator
        Best_Model = rf.best_estimator_",R/Neural_Network/program/Helper.py,IQSS/gentb-site,1
"        self.predict(clf, X, Y)
        #  feature importance
        self.plot(clf, self.features)

    def get_classifier(self, X, Y):
        """""" 
        :param X: 
        :param Y: 
        :return: 
        """"""
        clf = RandomForestClassifier(n_estimators=10)
        clf.fit(X, Y)
        return clf

    def predict(self, clf, X, Y):
        """""" 
        :param clf: 
        :param X: 
        :param Y: 
        :return: ",machinelearning/randomforest/model.py,ideaplat/Tback,1
"import pickle, json, requests, base64
from sklearn import datasets 
from sklearn.ensemble import *

# get a dataset 
iris = datasets.load_iris()# or whatever dataset
# train a model on the dataset
clf = RandomForestClassifier(n_estimators=12, random_state = 1960).fit(iris.data, iris.target)

# stringify the model
b64_data = base64.b64encode(pickle.dumps(clf))
# send the model th the web service
json_data={""Name"":""model1"", ""PickleData"":b64_data , ""SQLDialect"":""oracle""}
r = requests.post(""https://sklearn2sql.herokuapp.com/model"", json=json_data)
content = r.json()
lSQL = content[""model""][""SQLGenrationResult""][0][""SQL""]
print(lSQL);",tests/demo.py,antoinecarme/sklearn2sql_heroku,1
"    with open(output_filename, 'wb') as outputFile:
        writer = csv.writer(outputFile)
        for i in data:
            writer.writerow(i)


def classify(featureMatrix):
    n_estimators = 150 # This value changes to 80 when we are classifying 3 letters
    a,b = featureMatrix.shape
    X, y = featureMatrix[:, :b-1], featureMatrix[:, b-1]
    clf1 = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=n_estimators, random_state=42))
    print(np.mean(cross_val_score(clf1, X, y, cv=10)))

def plot_learning_curve(X, y):
    from sklearn.model_selection import learning_curve

    pipe = Pipeline([('clf', RandomForestClassifier(n_estimators=150, random_state=42))])

    train_sizes, train_scores, test_scores = \
                        learning_curve(estimator=pipe,",libras_myo/classification.py,pernambucano/myo_libras,1
"        ('vectorizer', CountVectorizer(
            min_df=0,
            ngram_range=(1,3),
            analyzer='word',
            # stop_words='english',
            strip_accents='unicode',
        )),
        ('tfidf', TfidfTransformer()),
        # ('clf', OneVsOneClassifier(LinearSVC()))
        ('clf', OneVsRestClassifier(clf))
        # ('clf', RandomForestClassifier())
        # ('clf', MultinomialNB()) # Does not work
        # ('clf', BernoulliNB(binarize=0.0)) # Does not work
        ])

    classifier.fit(x, Y)
    return classifier

def score_classifier(x,y,lb,classifier):
    Y = lb.fit_transform(y)",src/classifier/classifier.py,Glavin001/IssueBot,1
"feat = FeatureUnion([('words', words),
	                 ('char', char)
])

# Construct transformation pipeline
text_clf = Pipeline([('feat', feat),
	                 # ('select', select),
                     # ('clf', MultinomialNB()),
                     #('clf', SGDClassifier(penalty='l2'))
                     ('clf', LinearSVC(penalty='l2',C=0.5))
                     #('clf',RandomForestClassifier(n_estimators=300))
                     
])

# Set the parameters to be optimized in the Grid Search
parameters = {'feat__words__ngram_range': [(1,5), (1,6)],
			  # 'feat__words__stop_words': (""english"", None),
              'feat__words__min_df': (2,3),
              'feat__words__use_idf': (True, False),
              'feat__char__use_idf': (True, False),",SAM/model1.py,tanayz/Kaggle,1
"#                out_file='tree.dot',
#                feature_names=['petal length', 'petal width'])


#############################################################################
print(50 * '=')
print('Section: Combining weak to strong learners via random forests')
print(50 * '-')


forest = RandomForestClassifier(criterion='entropy',
                                n_estimators=10,
                                random_state=1,
                                n_jobs=2)
forest.fit(X_train, y_train)

plot_decision_regions(X_combined, y_combined,
                      classifier=forest, test_idx=range(105, 150))

plt.xlabel('petal length [cm]')",code/optional-py-scripts/ch03.py,wei-Z/Python-Machine-Learning,1
"import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier

timer = time.time()

classifiers = [LogisticRegression(), LinearSVC(), KNeighborsClassifier(), 
               RandomForestClassifier(), SVC()]

XandY = np.loadtxt('train1000.csv', delimiter=',', skiprows=1)
np.random.shuffle(XandY)
X = XandY[:,1:]
Y = XandY[:,0]
nX = np.shape(X)[0]

XTrain = X[:0.7*nX,:]
YTrain = Y[:0.7*nX]",scikit-learn/classifiersTest.py,SAGridOps/SoftwareTests,1
"for i in range(cf.num_rep):
    print ""repetition"", i

    # training 
    train_indices_act = set(range(num_actives)).difference(test_indices_act[i])
    train_indices_inact = set(range(num_inactives)).difference(test_indices_inact[i])
    train_fps = [fps_act[j] for j in train_indices_act]
    train_fps += [fps_inact[j] for j in train_indices_inact]
    ys_fit = [1]*len(train_indices_act) + [0]*len(train_indices_inact)
    # train the model
    ml = RandomForestClassifier(n_estimators=100, max_depth=100, min_samples_split=2, min_samples_leaf=1)
    ml.fit(train_fps, ys_fit)

    # chemical similarity
    simil = cPickle.load(infile)

    # ranking
    test_fps = [fps_act[j] for j in test_indices_act[i]]
    test_fps += [fps_inact[j] for j in test_indices_inact[i]]
    scores = [[pp[1], s[0], s[1]] for pp,s in zip(ml.predict_proba(test_fps), simil)]",evaluation/train_evaluate_RF_model.py,sriniker/TDT-tutorial-2014,1
"    xTrain = numpy.append(xTrain, xTrainTemp, axis=0)
    xTest = numpy.append(xTest, xTestTemp, axis=0)
    yTrain = numpy.append(yTrain, yTrainTemp, axis=0)
    yTest = numpy.append(yTest, yTestTemp, axis=0)

missCLassError = []
nTreeList = range(50, 2000, 50)
depth = 1
for iTrees in nTreeList:
    maxFeat = 4  # try tweaking
    glassRFModel = ensemble.RandomForestClassifier(n_estimators=iTrees,
                                                   max_depth=depth, max_features=maxFeat,
                                                   oob_score=False, random_state=531)

    glassRFModel.fit(xTrain, yTrain)
    # Accumulate auc on test set
    prediction = glassRFModel.predict(xTest)
    correct = accuracy_score(yTest, prediction)

    missCLassError.append(1.0 - correct)",semester2/task10/exercise1.py,SvichkarevAnatoly/Course-Python-Bioinformatics,1
"    smooth_idf = True,
    sublinear_tf=True,
    max_df=0.5,
    stop_words='english',
    tokenizer = get_tokens
)

clf_pipe = pipeline.Pipeline([
    ('vect', tfidf),
    ('densify', DenseTransformer()),
    ('clf', RandomForestClassifier(n_estimators = 10, criterion='gini'))
])



rf_model = clf_pipe.fit(data_train[0:1000], y_train[0:1000])

pred_rf = rf_model.predict(data_test[0:1000])

pred_ef = np.vstack((df_test.loc[0:999, 'id'], pred_rf)).T",code/classify_RandomForest.py,mirjalil/IMDB-reviews,1
"

def apply_algorithm(paras, X, y):

    if paras['clf'] == 'svm':
        clf = svm.SVC(kernel=paras['svm'][1], C=paras['svm'][0], probability=True)
    elif paras['clf'] == 'knn':
        clf = neighbors.KNeighborsClassifier(paras['knn'][0],\
                                             weights=paras['knn'][1])
    elif paras['clf'] == 'rf':
        clf = RandomForestClassifier(max_depth=paras['rf'][0], \
                                     n_estimators=paras['rf'][1],\
                                     max_features=paras['rf'][2])
    else:
        print str(""unknown classifier"") 
        sys.exit(2)


    return clf
",python/backup/methods.py,Healthcast/RSV,1
"

dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

clfs = [
	('ab', ab, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('dt', dt,  {'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('gb', gb, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],'max_depth':[5,10,25,50,75,100]}),
	('rf', rf, {'n_estimators':[10,25,50,75,100],'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('svcl', svcl, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('lr', lr, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]})",scripts/histograms/normalised-ml-search.py,jmrozanec/white-bkg-classification,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/default_no_meta.py,diogo149/CauseEffectPairsPaper,1
"

X_train, X_cross_validate, y_train, y_cross_validate = train_test_split(train_data, labels, test_size=0.5, random_state=42)

iris = load_iris()

# features = [6, 5, 32, 33, 7, 10, 1, 3, 15]
num_features = X_train.shape[1]

#create the random forest model
forest = RandomForestClassifier(n_estimators = 100, n_jobs=10)

#fit the RandomForestClassififer on the training data
forest.fit(X_train, y_train)

#predict the labels for the cross validation set
preds = forest.predict(X_cross_validate)

print(pd.crosstab(y_cross_validate, preds, rownames=['actual'], colnames=['preds']))
print(""Cross Validation Accuracy : "",accuracy_score(y_cross_validate,preds))",predict.py,rob2910/dd_water_pumps,1
"
def accumulate_scores(X, y, clf, paths):
    target = []
    pred = []
    for mask in generate_mask_for_mats(paths):
        pred.append(clf.predict_proba(X[mask])[:, 1])
        target.append(y[mask][0])
    return np.array(pred), np.array(target)

def scores_for_post(X, y):
    clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)
    skf = StratifiedKFold(y, n_folds=2)
    for train_index, test_index in skf:
        print(""Detailed classification report:"")
        print()
        print(""The model is trained on the full development set."")
        print(""The scores are computed on the full evaluation set."")
        print()
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]",gingivere/postprocess.py,schae234/gingivere,1
"### http://scikit-learn.org/stable/modules/pipeline.html

### Convert pandas datafram to dictionary
tmp_df = enrondf.T
data_dict = tmp_df.to_dict()

### Construct a simple classifiers to validate features set. 
clf_cv = {}
clf_cv['Naive Bayes']   = GaussianNB()
clf_cv['Decision Tree'] = DecisionTreeClassifier()
clf_cv['Random Forest'] = RandomForestClassifier()
clf_cv['AdaBoost']      = AdaBoostClassifier()

### validate features selection
validate_features = fin_features + email_features + new_features_list
### TODO : uncomment to check log
'''
fcv_score = validateFeaturesSelection(data_dict, validate_features, clf_cv)
'''
'''",P5-Identify Fraud from Enron Email/poi_id.py,slimn/Data-Analyst,1
"             ""Naive Bayes"", \
             ""LDA"", \
             ""QDA"" \
            ]

    classifiers = [\
                   KNeighborsClassifier(3), \
                   SVC(kernel='linear', C=0.025), \
                   SVC(gamma=2, C=1), \
                   DecisionTreeClassifier(max_depth=5), \
                   RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), \
                   AdaBoostClassifier(), \
                   GaussianNB(), \
                   LDA(), \
                   QDA()
                  ]

    return classifiers, names

def loadData():",examples/scikit-learn/examples/general/classifier_comparison.py,KellyChan/Python,1
"    print ""repetition"", i
    # indices of training molecules
    train_indices_act = set(range(num_actives)).difference(test_indices_act[i])
    train_indices_inact = set(range(num_inactives)).difference(test_indices_inact[i])
    ys_fit = [1]*len(train_indices_act) + [0]*len(train_indices_inact)

    ### Models with RDK5 
    train_fps = [fps_act_rdk5[j] for j in train_indices_act]
    train_fps += [fps_inact_rdk5[j] for j in train_indices_inact]
    # train the RF
    rf_rdk5 = RandomForestClassifier(n_estimators=100, max_depth=100, min_samples_split=2, min_samples_leaf=1)
    rf_rdk5.fit(train_fps, ys_fit)
    # train the LR
    lr_rdk5 = LogisticRegression()
    lr_rdk5.fit(train_fps, ys_fit)
    # chemical similarity
    simil = cPickle.load(infile1)
    # ranking
    test_fps = [fps_act_rdk5[j] for j in test_indices_act[i]] 
    test_fps += [fps_inact_rdk5[j] for j in test_indices_inact[i]]",evaluation/train_evaluate_fusion.py,sriniker/TDT-tutorial-2014,1
"			removeList.sort(reverse=True)


			arrays = [X2,spam_test2,ham_test2]
			# for array in arrays:
			# 	for seg in array:
			# 		seg.pop(9)
			# 		seg.pop(1)


			clf = RandomForestClassifier(n_estimators=150, random_state=42)

			#clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.1, max_depth=1, random_state=42)


			clf = clf.fit(X2, labels)
			predict_spam = clf.predict(spam_test2)
			predict_ham = clf.predict(ham_test2)

			I = 0",src/old_pipeline/elimination.py,cudbg/Dialectic,1
"    #
    ##################################################################################
    print ""Build Model(s)""

    from sklearn.ensemble import RandomForestClassifier

    model_minutes = [5,15,30]
    h = {}
    for i in model_minutes:
        # Model to Predict i minutes ahead
        h[i] = RandomForestClassifier(n_estimators=100)
        h[i].fit(Z[0:-i],Y[i:])   

    ##################################################################################
    #
    # 5. Dump to Disk
    #
    ##################################################################################
    print ""Dump to Disk""
",pyfiles/prediction/run_build_models.py,aalto-trafficsense/regular-routes-server,1
"test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1) 

# The data is now ready to go. So lets fit to the train, then predict to the test!
# Convert back to a numpy array
train_data = train_df.values
test_data = test_df.values

print train_df.columns
print 'Training...'
#from sklearn.ensemble import RandomForestClassifier
#clf = RandomForestClassifier(n_estimators=100)#0.79

#from sklearn.neighbors import KNeighborsClassifier  
#clf=KNeighborsClassifier(n_neighbors=7)#0.7037

#from sklearn import svm   
#clf=svm.SVC(C=10,gamma=0.0029)#0.7878
#clf=GridSearchCV(svm.SVC(), param_grid={""C"":np.logspace(-2, 10, 13),""gamma"":np.logspace(-9, 3, 13)})
#output = clf.fit( train_data[0::,1::], train_data[0::,0] ).predict(test_data).astype(int)
#print(""The best parameters are %s with a score of %0.2f""% (knnClf.best_params_, knnClf.best_score_))",myfirstforest.py,zlykan/my_test,1
"                val = self.__DEFAULT_CLF_CFG.get(key) if val is None else val
            except KeyError:
                logger.error(str(val) + "" not an implemented classifier."")
                raise

            temp_bagging = val.pop('bagging', bagging)
            bagging = temp_bagging if bagging is None else bagging

            if key == 'rdf':
                config_clf = dict(val)  # possible multi-threading arguments
                clf = SklearnClassifier(RandomForestClassifier(**config_clf))
            elif key == 'erf':
                config_clf = dict(val)  # possible multi-threading arguments
                clf = SklearnClassifier(ExtraTreesClassifier(**config_clf))
            elif key == 'nn':
                config_clf = dict(val)  # possible multi-threading arguments
                clf = TheanetsClassifier(**config_clf)
            elif key == 'ada':
                config_clf = dict(val)  # possible multi-threading arguments
                clf = SklearnClassifier(AdaBoostClassifier(**config_clf))",raredecay/tools/estimator.py,mayou36/raredecay,1
"    clf = neighbors.KNeighborsClassifier(algorithm='kd_tree')
    clf.fit(train_x, train_y)
    print ""start*****predict""
    test_label=clf.predict(test_data)
    result = pd.DataFrame(test_label)
    print ""save****result""
    result.to_csv(""F:/datacastle/sklearn_knn_result.csv"")
    '''
def random_forest_class(train_x,train_y,test_data):
    print ""start*****trainning""
    clf = RandomForestClassifier(n_estimators=10)
    clf = clf.fit(train_x, train_y)
    print ""start*****predict""
    test_label = clf.predict(test_data)
    result = pd.DataFrame(test_label)
    print ""save****result""
    result.to_csv(""F:/datacastle/sklearn_random_forest_result.csv"")

def statistic_category(train_data, features_type_data):
    category_index = [sample[0] for sample in features_type_data if sample[-1] == 'category']",tools/category.py,likeucode/SHU-DataScience,1
"        mnb = MultinomialNB(**kwargs)
        return mnb

    elif modelType == 'bernoulliNB':
        from sklearn.naive_bayes import BernoulliNB
        bnb = BernoulliNB(**kwargs)
        return bnb

    elif modelType == 'randomForest':
        from sklearn.ensemble import RandomForestClassifier
        rfc = RandomForestClassifier(random_state=234, **kwargs)
        return rfc

    elif modelType == 'svm':
        from sklearn.svm import SVC
        svc = SVC(random_state=0, probability=True, **kwargs)
        return svc

    elif modelType == 'LinearRegression':
        #assert column, ""Column name required for building a linear model""",datascienceutils/utils.py,greytip/data-science-utils,1
"from sklearn.qda import QDA
clf = QDA(priors=None, reg_param=0.001).fit(X_cropped, np.ravel(y_cropped[:]))
y_validation_predicted = clf.predict(X_validation)
print ""Error rate for QDA (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)



# Start Random Forest Classification
print ""Performing Random Classification:""
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=500)
forest = forest.fit(X_cropped, np.ravel(y_cropped[:]))
y_validation_predicted = forest.predict(X_validation)
print ""Error rate for Random Forest (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)
# ml_aux.plot_confusion_matrix(y_validation, y_validation_predicted, ""CM Random Forest (t1)"")
# plt.show()


# Start k nearest neighbor Classification
print ""Performing kNN Classification:""",Code/Machine_Learning_Algos/training_fullset.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"    X_test = X[n_train:]
    y_test = y[n_train:]

    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    ""dummy"": DummyClassifier(),
    'CART': DecisionTreeClassifier(),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),
    'RandomForest': RandomForestClassifier(n_estimators=100),
    'Nystroem-SVM': make_pipeline(
        Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'SampledRBF-SVM': make_pipeline(
        RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'LinearRegression-SAG': LogisticRegression(solver='sag', tol=1e-1, C=1e4),
    'MultilayerPerceptron': MLPClassifier(
        hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
        algorithm='sgd', learning_rate_init=0.2, momentum=0.9, verbose=1,
        tol=1e-4, random_state=1),",projects/scikit-learn-master/benchmarks/bench_mnist.py,DailyActie/Surrogate-Model,1
"    ----------
    classes_ : array-like, shape = [n_predictions]

    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.naive_bayes import GaussianNB
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> y = np.array([1, 1, 1, 2, 2, 2])
    >>> eclf1 = VotingClassifier(estimators=[
    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
    >>> eclf1 = eclf1.fit(X, y)
    >>> print(eclf1.predict(X))
    [1 1 1 2 2 2]
    >>> eclf2 = VotingClassifier(estimators=[",projects/scikit-learn-master/sklearn/ensemble/voting_classifier.py,DailyActie/Surrogate-Model,1
"    clf.fit(X_train, y_train)
    print(clf.best_params_)
    bestScore = clf.best_score_
    testScore = clf.score(X_test, y_test)
    writeData(outputFile, 'decision_tree', bestScore, testScore)

def doRandomForest(X_train, X_test, y_train, y_test, outputFile):
    print('------ Random forest ------')
    tuned_parameters = {'max_depth': range(1, 51), 
                        'min_samples_split': range(2, 11)}
    clf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv = 5, n_jobs = 8)   
    clf.fit(X_train, y_train)
    print(clf.best_params_)
    bestScore = clf.best_score_
    testScore = clf.score(X_test, y_test)
    writeData(outputFile, 'random_forest', bestScore, testScore)

def writeData(outName, method, trainScore, testScore):
    with open(outName, 'a') as outFile:
        outFile.write('{0},{1:10.4f},{2:10.4f}\n'.format(method, trainScore, testScore))",Exercises/Exercise_3/part_3/problem3_3.py,Stocastico/AI_MOOC,1
"            Use e.g. ""small"", ""medium"", ""large"" or integer-values. Defaults to ""large"".

        text_fontsize (string or int, optional): Matplotlib-style fontsizes. 
            Use e.g. ""small"", ""medium"", ""large"" or integer-values. Defaults to ""medium"".
            
    Returns:
        ax (:class:`matplotlib.axes.Axes`): The axes on which the plot was drawn.

    Example:
        >>> import scikitplot.plotters as skplt
        >>> rf = RandomForestClassifier()
        >>> rf = rf.fit(X_train, y_train)
        >>> y_pred = rf.predict(X_test)
        >>> skplt.plot_confusion_matrix(y_test, y_pred, normalize=True)
        <matplotlib.axes._subplots.AxesSubplot object at 0x7fe967d64490>
        >>> plt.show()

        .. image:: _static/examples/plot_confusion_matrix.png
           :align: center
           :alt: Confusion matrix",scikitplot/plotters.py,reiinakano/scikit-plot,1
"    # (""PCA_2"", PCA(2)),
    # (""PCA_10"", PCA(2)),
    # (""PCA_100"", PCA(2)),
    # (""PCA_400"", PCA(2)),
    #
    # (""TruncatedSVD_2"", TruncatedSVD(2)),
    # (""TruncatedSVD_10"", TruncatedSVD(2)),
    # (""TruncatedSVD_100"", TruncatedSVD(2)),
    # (""TruncatedSVD_400"", TruncatedSVD(2)),
    # (""LogisticRegression"", SelectFromModel(LogisticRegression(penalty=""l1""))),
    # (""RandomForestClassifier_20"", SelectFromModel(RandomForestClassifier(n_estimators=20, max_depth=3))),
    # (""RandomForestClassifier_100"", SelectFromModel(RandomForestClassifier(n_estimators=100, max_depth=None))),
]

estimators = [
    (""SVC_linear"", SVC(kernel='linear')),
    # (""LinearSVC"", LinearSVC(penalty='l1', dual=False)),
    # (""SVC_rbf"", SVC(kernel='rbf')),
    # (""RandomForestClassifier_20"", RandomForestClassifier(n_estimators=20, max_depth=3)),
    # (""RandomForestClassifier_100"", RandomForestClassifier(n_estimators=100, max_depth=5)),",scripts/quick_feature_selection_and_pipeline_evaluation.py,Rostlab/LocText,1
"    assert ((num_true_seeds > 1) and (num_false_seeds > 1))  # can't handle this otherwise, yet
    if pm.verbose:
        print(""\n%d total seeds (%d positive, %d negative)"" % (num_true_seeds + num_false_seeds, num_true_seeds, num_false_seeds))

    # construct classifier
    if (pm.classifier == 'logreg'):
        clf = LogisticRegression()
    elif (pm.classifier == 'naive_bayes'):
        clf = GaussianNB()
    elif (pm.classifier == 'randfor'):
        clf = RandomForestClassifier(n_estimators = pm.num_trees, n_jobs = pm.n_jobs)
    elif (pm.classifier == 'boost'):
        clf = AdaBoostClassifier(n_estimators = pm.num_trees)
    elif (pm.classifier == 'kde'):
        clf = TwoClassKDE()
        train_in = mat[training]
        train_out = ind[training]
        if pm.verbose:
            print(""\nCross-validating to optimize KDE bandwidth..."")
        timeit(clf.fit_with_optimal_bandwidth)(train_in, train_out, gridsize = pm.kde_cv_gridsize, dynamic_range = pm.kde_cv_dynamic_range, cv = pm.kde_cv_folds, verbose = int(pm.verbose), n_jobs = pm.n_jobs)",nominate.py,jeremander/AttrVN,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.mllib.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = sqlContext.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = sqlContext.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0
    >>> numpy.argmax(result.probability)
    0",python/pyspark/ml/classification.py,yantaiv/Trans-spark-release-HDP-2.6.0.3-8,1
"def test_RF():
    
    iris = datasets.load_iris()
    iris_X = iris.data[:,:2]
    iris_y = iris.target
    X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=.5, \
                                                        random_state=0)
    #test max_depth: the max depth of the tree
    #test n_estimators: how many trees used
    #test max_features: how many good features used in each split
    clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)
    clf.fit(X_train, y_train)
    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1

    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, Z)",python/backup/skeleton.py,Healthcast/RSV,1
"
    Examples
    --------

    >>> import numpy as np
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.naive_bayes import GaussianNB
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from mlxtend.sklearn import EnsembleVoteClassifier
    >>> clf1 = LogisticRegression(random_seed=1)
    >>> clf2 = RandomForestClassifier(random_seed=1)
    >>> clf3 = GaussianNB()
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> y = np.array([1, 1, 1, 2, 2, 2])
    >>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3],
    ... voting='hard', verbose=1)
    >>> eclf1 = eclf1.fit(X, y)
    >>> print(eclf1.predict(X))
    [1 1 1 2 2 2]
    >>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')",mlxtend/classifier/ensemble_vote.py,rasbt/mlxtend,1
"def calculate_result(actual,pred):  
    m_precision = metrics.precision_score(actual,pred);  
    m_recall = metrics.recall_score(actual,pred);  
    print 'predict info:'  
    print 'accuracy:{0:.3f}'.format(metrics.accuracy_score(actual,pred))  
    print 'precision:{0:.3f}'.format(m_precision)  
    print 'recall:{0:0.3f}'.format(m_recall);  
    print 'f1-score:{0:.3f}'.format(metrics.f1_score(actual,pred));  

def calculate_n(x,y):
    clf = RandomForestClassifier(n_estimators=3)
    max_metric=cross_val_score(clf,x,y,cv=5,scoring='f1').mean()
    n=3
    for i in range(4,100):
      clf = RandomForestClassifier(n_estimators=i)
      metric = cross_val_score(clf,x,y,cv=5,scoring='f1').mean()   
      if max_metric < metric :
        max_metric=metric
        n=i
    return n",src/randomforest.py,zhangxyz/MLFSdel,1
"    ensemble = sys.argv[1]
    n_estimators = int(sys.argv[2])
    criterion = sys.argv[3]
    interp = sys.argv[4]
    
    print('Loading data')
    X_train, y_train, X_test = mnist.load_kaggle()
    X_train = mnist.shrink_data_dims(X_train, 14, interp=interp)

    if ensemble == 'rf':
        model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion)
    elif ensemble == 'et':
        model = ExtraTreesClassifier(n_estimators=n_estimators, criterion=criterion)
    else:
        print(""Bad model"", ensemble)
        return 1
    
    print('Training')
    model.fit(X_train, y_train)
",train_tree.py,notkarol/kaggle_mnist,1
"        for miss_col in miss_cols_uniq:
            # edatatract valid observations given current column missing data
            valid_obs = [n for n in xrange(data.shape[0])
                         if data[n, miss_col] != '?']

            # prepare independent and dependent variables, valid obs only
            data_train = data_factorized[:, valid_cols][valid_obs]
            y_train = data_factorized[valid_obs, miss_col]

            # train random forest classifier
            rf_clf = RandomForestClassifier(n_estimators=100)
            rf_clf.fit(data_train, y_train)

            # given current feature, find obs with missing vals
            miss_obs_iddata = miss_rows[miss_cols == miss_col]

            # predict missing values
            y_hat = rf_clf.predict(data_factorized[:, valid_cols][miss_obs_iddata])

            # replace missing data with prediction",code/missing_data_imputation.py,jvpoulos/cs289-project,1
"
def createFamilySize(data):
    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1
    return data

def createDecisionTree():
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createRandomForest():
    clf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createExtraTree():
    clf = ExtraTreesClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createAdaBoost():
    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    clf = AdaBoostClassifier(dt, n_estimators=300)",kaggle-titanic-machine-learning-from-disaster/src/classify.py,KellyChan/Kaggle,1
"    #              'max_features': ['auto'],
    #              'criterion': ['gini', 'entropy'],
    #              'min_samples_split': [15, 16, 17, 18, 19, 20, 21, 22, 23],
    #              'min_samples_leaf': [5, 6, 7, 8],
    #              'max_depth': [12, 13, 14, 15, 16, 17],
    #              'bootstrap': [True]}
    # paramgrid = {'kernel': ['rbf'],
    #              'gamma': [.01, 'auto', 1.0, 5.0, 10.0, 11, 12, 13],
    #              'C': [.001, .01, .1, 1, 5]}
    # model = SVC(probability=True)
    model = RandomForestClassifier(n_jobs=-1)
    # model = GradientBoostingClassifier()
    # model, gridsearch = gridsearch(paramgrid, model, X_train_bw, y_train_b)
    model = evaluate_model(model, X_train_bw, y_train_b)
    print(""\nthis is the model performance on the training data\n"")
    view_classification_report(model, X_train_b, y_train_b)
    confusion_matrix(y_train_b, model.predict(X_train_b))
    print(""this is the model performance on the test data\n"")
    view_classification_report(model, X_test_b, y_test_b)
    confusion_matrix(y_test_b, model.predict(X_test_b))",src/classification_model.py,brityboy/BotBoosted,1
"        """"""

        models = (
            LinearRegression(),
            LogisticRegression(),
            KMeans(),
            LSHForest(),
            PCA(),
            RidgeCV(),
            LassoCV(),
            RandomForestClassifier(),
        )

        for model in models:
            self.assertTrue(isestimator(model))

    def test_pipeline_instance(self):
        """"""
        Test that isestimator works for pipelines
        """"""",tests/test_utils/test_types.py,pdamodaran/yellowbrick,1
"            return(probs[0][1]) # first element is prob of low quality, second element is prob of high quality

    #########################################################################
    def featureImportances(self):
        return self.rfclf.feature_importances_

    #########################################################################
    # learning pipeline setup
    def setUpLearningPipeline(self):
        # create random forest classifier pipeline
        self.rfclf = RandomForestClassifier(n_estimators=10) 
        self.randfor = Pipeline([(""scaler"", preprocessing.StandardScaler()), (""clf"", self.rfclf)])
        # build learning pipeline for logistic regression classifier
        self.logres_clf = sklearn.linear_model.LogisticRegression(C=0.35) 
        self.logres = Pipeline([(""scaler"", preprocessing.StandardScaler()), (""clf"", self.logres_clf)])
        # build learning pipeline for extra trees classifier
        self.xtrees_clf = sklearn.ensemble.ExtraTreesClassifier(n_estimators=10)
        self.xtrees = Pipeline([(""scaler"", preprocessing.StandardScaler()), (""clf"", self.xtrees_clf)])

    #########################################################################",app/qualPred.py,ahna/wikiscore,1
"

if __name__ == ""__main__"":
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn import datasets
    iris = datasets.load_digits()
    X = iris.data
    y = iris.target
    clf = StackingClassifier([LogisticRegression(), RandomForestClassifier(n_estimators=30, random_state=32), KNeighborsClassifier()], LogisticRegression(), 2, False, True)
    clf.fit(X, y)
    #clf.predict(X)

    from sklearn.cross_validation import cross_val_score
    print np.mean(cross_val_score(LogisticRegression(), X, y))
    print np.mean(cross_val_score(RandomForestClassifier(n_estimators=30), X, y))
    print np.mean(cross_val_score(KNeighborsClassifier(), X, y))

    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix",src/enEnsemble/stackingclassifier.py,reiinakano/ensemble_helper,1
"    for strict in [True, False]:
        strictStr = 'strict' if strict else ''

        X = cpe.buildConceptPairList(d, inputDs[0], strict)
        y = len(X) * ['isParent']

        negSample = cpe.shuffledConceptPairList(X)
        X += negSample
        y += len(negSample) * ['notParent']

        for clfModel in [RandomForestClassifier(n_estimators=11, max_depth=7), KNeighborsClassifier()]:
            for feature in [cppf.subCarth, cppf.subPolar, cppf.subAngular,
                            cppf.concatCarth, cppf.concatPolar, cppf.concatAngular]:
                for post in [pf.noPost, pf.postNormalize]:
                    expStr = '_'.join([inputDs[1], strictStr, str(clfModel.__class__.__name__), str(feature.__name__), str(post.__name__)])
                    savePath = '../data/learnedModel/taxo/' + expStr + '.dill'
                    print '&&&&&&&&&&'
                    print expStr

                    clf = cppf.ConceptPairClf(clfModel, post(feature))",toolbox/experiment/trainAll_taxoClf.py,pelodelfuego/word2vec-toolbox,1
"
skf = cross_validation.StratifiedKFold(target, n_folds=10, random_state=42)

params = {'n_estimators': 1000,
          'n_jobs': -1}

ind = 1
for train_index, test_index in skf:
    X_train, X_test = training[train_index], target[train_index]

    clf = RandomForestClassifier(**params)
    fit = clf.fit(X_train, X_test)
    prediction_1 = fit.predict_proba(training[test_index])
    print log_loss(target[test_index], prediction_1)
    prediction_2 = fit.predict_proba(testing.values)
    submission = pd.DataFrame(prediction_2)
    submission.columns = [""Class_"" + str(i) for i in range(1, 10)]
    submission[""id""] = test[""id""]
    submission.to_csv(""rf_1000_cv10_ind{ind}.csv"".format(ind=ind), index=False)
    ind += 1",src/cross_fold_submission.py,ternaus/kaggle_otto,1
"            valid_dataset, [classification_metric], transformers)
    
  if model == 'rf':
    # Initialize model folder

    # Loading hyper parameters
    n_estimators = hyper_parameters['n_estimators']

    # Building scikit random forest model
    def model_builder(model_dir_rf):
      sklearn_model = RandomForestClassifier(
        class_weight=""balanced"", n_estimators=n_estimators,n_jobs=-1)
      return dc.models.sklearn_models.SklearnModel(sklearn_model, model_dir_rf)
    model_rf = dc.models.multitask.SingletaskToMultitask(
        tasks, model_builder)
    
    print('-------------------------------------')
    print('Start fitting by random forest')
    model_rf.fit(train_dataset)
    ",examples/benchmark.py,rbharath/deepchem,1
"        # X = dft[['d1-0']]
        # X = dft[['d1-0','d1-1','d1-2','d1-3','dist']]
        # X = dft[['1-0','1-1','1-2','1-3','d1-0','d1-1','d1-2','d1-3','1-B','2-0','2-1','2-2','2-3','d2-0','d2-1','d2-2','d2-3','2-B','3-0','3-1','3-2','3-3','d3-0','d3-1','d3-2','d3-3','3-B','dist']]
        # X = dft[['d1-0','d1-1','d1-2','d1-3','d2-0','d2-1','d2-2','d2-3','d3-0','d3-1','d3-2','d3-3','dist']]
        # X = dft[['d1-0','d1-1','d1-2','d1-3']]
        X = dft[['1-0','1-1','d1-0','d1-1']]
        y = dft['act']

        # ref score
        clf = LogisticRegression(C=1,penalty='l2', class_weight={0:1,1:4})
        # clf = RandomForestClassifier(n_estimators=10, max_features=.5)
        scores = sl.cross_validation.cross_val_score(clf, X, y, scoring='roc_auc')
        mscores = np.mean(scores)
        print(k + ': ref xval scores=' + str(scores) + ' | mean=' + str(mscores))
        if mscores > best_auc:
            best_auc = mscores

        # pca = ProbabilisticPCA(n_components='mle')
        # X = pca.fit_transform(X)
",cnct_graph_analyze.py,ecodan/kaggle-connectomix,1
"    return (data, result)

def cross_validation_test():
    data = get_analyze_data()
    target = data[""hand""]
    train = data.drop([""id""], axis = 1)
    kfold = 5
    cross_val_test = {}

    print ""Cross validation test...""
    model_rfc = RandomForestClassifier(n_estimators = 100)
    model_knc = KNeighborsClassifier(n_neighbors = 15)
    model_lr = LogisticRegression(penalty='l1', tol=0.01)

    scores = cross_validation.cross_val_score(model_rfc, train, target, cv = kfold)
    cross_val_test['RFC'] = scores.mean()

    scores = cross_validation.cross_val_score(model_knc, train, target, cv = kfold)
    cross_val_test['KNC'] = scores.mean()
",poker.py,zhzhussupovkz/poker-rule-induction,1
"             ""adaBoost""
            ]

    clfs = [
            MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1,max_iter=400),
            GaussianNB(),
            KNeighborsClassifier(3),
            SVC(kernel=""linear"", C=0.025,probability=True),
            SVC(gamma=2, C=1,probability=True),
            DecisionTreeClassifier(max_depth=5),
            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            AdaBoostClassifier()
           ]

    if SUBMODELS and SUBSET_FEATURES:
        #feats = [0,2,3]
        feats = [0,2,3,4,5,6,7]
        clfs = [clfs[i] for i in feats]
        names = [names[i] for i in feats]
    elif SUBMODELS:",UFCML/ufcml.py,LittleLebowskiUrbanAchievers/octagon-api,1
"    createVerticalFeatures = False
    logtransform = None
    useActivity = True
    diagnosis = True

    Xtest, Xtrain, ytrain, idx,  sample_weight, Xval, yval = prepareDataset(quickload=quickload,seed=seed, nsamples=nsamples, holdout=holdout,keepFeatures = keepFeatures, dropFeatures= dropFeatures, dummy_encoding=dummy_encoding, labelEncode=labelEncode, oneHotenc= oneHotenc, removeRare_freq=removeRare_freq,  logtransform=logtransform, useActivity= useActivity, diagnosis = diagnosis)

    #interact_analysis(Xtrain)
    #model = sf.RandomForest(n_estimators=120,mtry=5,node_size=5,max_depth=6,n_jobs=2,verbose_level=0)
    #model = Pipeline([('scaler', StandardScaler()), ('model',ross1)])
    #model = RandomForestClassifier(n_estimators=100,max_depth=None,min_samples_leaf=5,n_jobs=2, max_features=Xtrain.shape[1]/3,oob_score=False)
    #model = XgboostClassifier(n_estimators=800,learning_rate=0.025,max_depth=10, NA=0,subsample=.9,colsample_bytree=0.7,min_child_weight=5,n_jobs=2,objective='binary:logistic',eval_metric='logloss',booster='gbtree',silent=1,eval_size=0.0)
    model = XgboostClassifier(n_estimators=200,learning_rate=0.2,max_depth=10, NA=0,subsample=.9,colsample_bytree=0.7,min_child_weight=5,n_jobs=2,objective='binary:logistic',eval_metric='logloss',booster='gbtree',silent=1,eval_size=0.0)
    #model = ExtraTreesClassifier(n_estimators=250,max_depth=None,min_samples_leaf=1,n_jobs=2, max_features=3*Xtrain.shape[1]/3)
    #model = KNeighborsClassifier(n_neighbors=20)
    #model = LogisticRegression()

    #model = KerasNN(dims=Xtrain.shape[1],nb_classes=2,nb_epoch=3,learning_rate=0.02,validation_split=0.0,batch_size=128,verbose=1,layers=[32,32], dropout=[0.2,0.2])
    #model = Pipeline([('scaler', StandardScaler()), ('nn',model)])
    cv = StratifiedKFold(ytrain,2,shuffle=True)",competition_scripts/genentech.py,chrissly31415/amimanera,1
"print('Training accuracy:', knn.score(X_train_std[:, k5], y_train))
print('Test accuracy:', knn.score(X_test_std[:, k5], y_test))

#############################################################################
print(50 * '=')
print('Section: Assessing Feature Importances with Random Forests')
print(50 * '-')

feat_labels = df_wine.columns[1:]

forest = RandomForestClassifier(n_estimators=10000,
                                random_state=0,
                                n_jobs=-1)

forest.fit(X_train, y_train)
importances = forest.feature_importances_

indices = np.argsort(importances)[::-1]

for f in range(X_train.shape[1]):",code/optional-py-scripts/ch04.py,wei-Z/Python-Machine-Learning,1
"from collections import Counter
import datetime

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import shuffle

from models.features.topics import topics_similarity

clf = RandomForestClassifier()


def generate_features(data, val=None):
    features = []
    for raw in data:
        features.extend(topics_similarity(raw))

    if val is None:
        return features",models/nmf_model.py,xenx/recommendation_system,1
"tfidf = feature_extraction.text.TfidfTransformer(smooth_idf=False)
train = tfidf.fit_transform(train).toarray()
test = tfidf.transform(test).toarray()

# encode labels
label_encode = preprocessing.LabelEncoder()
labels = label_encode.fit_transform(labels)

# train classifier
if algorithm == 'Random Forest':
    clf = ensemble.RandomForestClassifier(n_estimators=1000, verbose=2)
    clf.fit(train, labels)
    predictions = clf.predict_proba(test)

elif algorithm == 'Extra Tree':
    clf = ensemble.ExtraTreesClassifier(n_estimators=1000, verbose=2)
    clf.fit(train, labels)
    predictions = clf.predict_proba(test)

elif algorithm == 'Gradient Boosting':",BLG454E/TermProject/main.py,emreozdil/ITU-Computer-Engineering,1
"import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier

train_set = pd.read_csv(""train_parsed.csv"")
test_set = pd.read_csv(""test_parsed.csv"")


target = train_set[""Survived""].values
train_feat = train_set[[""Pclass"", ""Sex"", ""Embarked"", ""AgeGroup"", ""Title"", ""Family""]].values
forest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)
prediction = forest.fit(train_feat, target)

print(prediction.score(train_feat, target))

test_feat = test_set[[""Pclass"", ""Sex"", ""Embarked"", ""AgeGroup"", ""Title"", ""Family""]].values
solution = prediction.predict(test_feat)

print(prediction.feature_importances_)
",Cloudera/Code/Titanic_Dataset/random_forest_pred.py,cybercomgroup/Big_Data,1
"    
    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    sklearn_model = RandomForestClassifier()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9",deepchem/models/tests/test_overfit.py,bowenliu16/deepchem,1
"    global behavior_defects_regr
    data = np.array(data).reshape(1, 3)
    predicted_test = behavior_defects_regr.predict(data)
    acceleration = data.item((0, 0))
    print ""Predicted %d"" % predicted_test[0]
    return predicted_test[0]

def init_behavior_defects_module(values, trees, data, labels):
    # Fit regression model
    global behavior_defects_regr
    behavior_defects_regr = RandomForestClassifier(n_estimators=trees)
    behavior_defects_regr.fit(data[:, [0,1,2]], labels)
    print ""init_behavior_defects_module"", behavior_defects_regr.feature_importances_
    return

def predicted(data):
   return behavior_defects_regr.predict(data)",behavior_defects.py,dkdbProjects/server-result-sender,1
"sss = StratifiedShuffleSplit(labels, test_size=0.05, random_state=1234)
for train_index, test_index in sss:
    break

train_x, train_y = train.values[train_index], labels.values[train_index]
test_x, test_y = train.values[test_index], labels.values[test_index]

### building the classifiers
clfs = []

rfc = RandomForestClassifier(n_estimators=50, random_state=4141, n_jobs=-1)
rfc.fit(train_x, train_y)
print('RFC LogLoss {score}'.format(score=log_loss(test_y, rfc.predict_proba(test_x))))
clfs.append(rfc)

### usually you'd use xgboost and neural nets here

logreg = LogisticRegression()
logreg.fit(train_x, train_y)
print('LogisticRegression LogLoss {score}'.format(score=log_loss(test_y, logreg.predict_proba(test_x))))",7 - Ensemble (sample).py,eugeneyan/Otto,1
"import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

from vk_text_likeness.logs import log_method_begin, log_method_end


class PredictActionModel:
    def __init__(self, action_data):
        self.action_data = action_data
        self.like_model = RandomForestClassifier(n_jobs=-1)
        self.repost_model = RandomForestClassifier(n_jobs=-1)
        self.is_fitted = False

    def fit(self, post_subset=None):
        df = self.action_data.get_all()
        if post_subset is not None:
            df = df[df['post_id'].isin(post_subset)]
        log_method_begin()
        x_df = df.drop(['user_id', 'post_id', 'is_member', 'is_liked', 'is_reposted'], axis=1)",vk_text_likeness/predict_model.py,reo7sp/vk-text-likeness,1
"        Test that isestimator works for instances
        """"""

        models = (
            LinearRegression(),
            LogisticRegression(),
            KMeans(),
            LSHForest(),
            PCA(),
            LassoCV(),
            RandomForestClassifier(),
        )

        for model in models:
            self.assertTrue(isestimator(model))

    def test_pipeline_instance(self):
        """"""
        Test that isestimator works for pipelines
        """"""",tests/test_utils.py,jkeung/yellowbrick,1
"			return x_.astype(np.float32)
		else:
			return x.astype(np.float32)

	def addABC(self, preproc=None, params={}):
		name = 'ABC'
		self.getEstimatorList().append((name, preproc, AdaBoostClassifier(**params)))

	def addRFC(self, preproc=None, params={}):
		name = 'RFC'
		self.getEstimatorList().append((name, preproc, RandomForestClassifier(**params)))

	def addETC(self, preproc=None, params={}):
		name = 'ETC'
		self.getEstimatorList().append((name, preproc, ExtraTreesClassifier(**params)))

	def addLR(self, preproc=None, params={}):
		name = 'LR'
		self.getEstimatorList().append((name, preproc, LogisticRegression(**params)))
",MetaClassifier.py,uzbit/mlutils,1
"        v_len = i
        data,target = get_train_sample('Drosophila',v_len)
        X = np.array(data) 
        y = np.array(target)
        print('------------V_LEN: %d'%v_len)
        clf = DecisionTreeClassifier(max_depth=None, min_samples_split=100, random_state=0)
        scores = cross_val_score(clf,X,y)
        print('AUC for Decision Tree: %f'%scores.mean())


        clf = RandomForestClassifier(n_estimators=100, max_depth=None,
            min_samples_split=1, random_state=0)
        scores = cross_val_score(clf,X,y) 
        print('AUC for Random Forest: %f'%scores.mean())
        

        clf = ExtraTreesClassifier(n_estimators=100, max_depth=None,
            min_samples_split=7, random_state=0)
        scores = cross_val_score(clf, X, y)
        print('AUC for ExtraTreesClassifier: %f'%scores.mean())",Experiment/Sklearn/EssembleLearningRF.py,mushroom-x/piRNA,1
"    mean[10:] = 0.0
    std[10:] = 1.0
    X_train = (X_train - mean) / std
    X_test = (X_test - mean) / std
    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    'GBRT': GradientBoostingClassifier(n_estimators=250),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=20),
    'RandomForest': RandomForestClassifier(n_estimators=20),
    'CART': DecisionTreeClassifier(min_samples_split=5),
    'SGD': SGDClassifier(alpha=0.001, n_iter=2),
    'GaussianNB': GaussianNB(),
    'liblinear': LinearSVC(loss=""l2"", penalty=""l2"", C=1000, dual=False,
                           tol=1e-3),
    'SAG': LogisticRegression(solver='sag', max_iter=2, C=1000)
}

if __name__ == ""__main__"":",projects/scikit-learn-master/benchmarks/bench_covertype.py,DailyActie/Surrogate-Model,1
"logger = logging.getLogger(__name__)


class Brain(object):

    def __init__(self):
        self._lobes = {'svc': svm.SVC(gamma=0.001),
                       'sgd': SGDClassifier(),
                       'lsvc': svm.LinearSVC(),
                       'kn': KNeighborsClassifier(n_neighbors=3),
                       'rf': RandomForestClassifier(n_estimators=10, random_state=123),
                       'kmeans': KMeans(n_clusters=3, random_state=1)
                       }
        self.votinglobe = VotingClassifier(
            estimators=[(lobe, self._lobes[lobe]) for lobe in self._lobes],
            voting='hard',
            n_jobs=4,
        )

    def splitData(self, df, size=1):",marconi/tools/brain/__init__.py,s4w3d0ff/marconibot,1
"        training_label = [arr[idx_imb] for idx_arr, arr in enumerate(label_bal)
                         if idx_arr != idx_lopo_cv]
        # Concatenate the data
        training_data = np.vstack(training_data)
        training_label = label_binarize(np.hstack(training_label).astype(int),
                                        [0, 255])
        print 'Create the training set ...'

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_bal.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/balancing/pipeline_classifier_t2w.py,I2Cvb/mp-mri-prostate,1
"
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    MLPClassifier(alpha=1),
    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    SetCoveringMachineClassifier(max_rules=4, model_type=""conjunction"", p=2.0),
    SetCoveringMachineClassifier(max_rules=4, model_type=""disjunction"", p=1.0)]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)",examples/decision_boundary.py,aldro61/pyscm,1
"
    return X_train, y_train, X_test, y_test, scaler


def modelselect(input_filename, num_test_examples, block_size, n_estimators=100):
    # Perform some model selection to determine good parameters
    # Load data
    X_train, y_train, X_test, y_test, scaler = loaddata(input_filename, num_test_examples, block_size)

    # Feature generation using random forests
    forest = RandomForestClassifier(n_estimators=n_estimators, n_jobs=-1)
    forest.fit(X_train, y_train)
    encoder = OneHotEncoder()
    encoder.fit(forest.apply(X_train))
    X_train = encoder.transform(forest.apply(X_train))
    learner = SGDClassifier(loss=""hinge"", penalty=""l2"", learning_rate=""invscaling"", alpha=0.001, average=10**4, eta0=0.5, class_weight=""balanced"")

    metric = ""f1""
    losses = [""log"", ""hinge"", ""modified_huber"", ""squared_hinge"", ""perceptron""]
    penalties = [""l2"", ""l1"", ""elasticnet""]",tyrehug/exp/outofcore.py,charanpald/tyre-hug,1
"            from sklearn import ensemble
        except ImportError as error:
            logging.debug(error)
            return 

        classes = numpy.unique(y)
        if classes.shape[0] == 2: 
            self.worstResponse = classes[classes!=self.bestResponse][0]
        
        if self.type == ""class"": 
            self.learner = ensemble.RandomForestClassifier(n_estimators=self.numTrees, max_features=self.maxFeatures, criterion=self.criterion, max_depth=self.maxDepth, min_samples_split=self.minSplit, random_state=21)
        else: 
            self.learner = ensemble.RandomForestRegressor(n_estimators=self.numTrees, max_features=self.maxFeatures, criterion=self.criterion, max_depth=self.maxDepth, min_samples_split=self.minSplit, random_state=21)          
            
        self.learner = self.learner.fit(X, y)

    def getLearner(self):
        return self.learner

    def getClassifier(self): ",sandbox/predictors/RandomForest.py,charanpald/sandbox,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/no_test_instances )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X,Y, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 20, n_jobs = 8)
rf.fit(X,Y)
result_rf = rf.predict(Z)
#print('X', len(X),len(Y),len(X1[train_size:dataset_size]))
#print('RF prediction : ---> ',result_rf )
#print('actual ans: -->',test_class)
CM = confusion_matrix(test_class,result_rf) 
print(""Confusion Matrix : "")
print(CM)
for i in range(0,no_test_instances):",sandbox/petsc/solvers/scripts/scikit_learn_classifiers_all_features.py,LighthouseHPC/lighthouse,1
"    config = {}
    execfile(""params.conf"", config)
    inputfile = config[""histogram_dataset""]    
    trainingSamples = config[""trainingSamples""]
    testingSamples = config[""testingSamples""]

    #selectedFeatures = ""all""
    selectedFeatures = [20, 21, 22, 23, 24]
    features, labels = sc.Data_Preparation(inputfile, selectedFeatures)

    Scikit_RandomForest_Model = ensemble.RandomForestClassifier(n_estimators=510, criterion='gini', max_depth=None,
                                                                 min_samples_split=2, min_samples_leaf=1, max_features='sqrt',
                                                                 bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, verbose=0,
                                                                 min_density=None, compute_importances=None)
    Scikit_SVM_Model = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)
    
    Scikit_GradientBoostingClassifier_Model = GradientBoostingClassifier(n_estimators=500, learning_rate=1.0, max_depth=1, random_state=0)

    # ANOVA SVM-C
    anova_filter = SelectKBest(f_regression, k=5)",RandomForest_test.py,DistributedSystemsGroup/YELP-DS,1
"    # defualt to gradient boosting if nothing is provided
    # classifierType = sys.argv[1] if (len(sys.argv) > 1) else 'gb'

    # do not include last 5 columns because
    # variables = train.columns[0:-5]

    if classifierType == 'gb':
        baseline = GradientBoostingClassifier(n_estimators=50, learning_rate=0.005, subsample=0.7,
                                          min_samples_leaf=10, max_depth=7, random_state=11)
    elif classifierType == 'rf':
        baseline = RandomForestClassifier(n_estimators=25)


    baseline.fit(train[variables], train['signal'])
    # sort importances
    indices = np.argsort(baseline.feature_importances_)
    # plot as bar chart
    plt.barh(np.arange(len(variables)), baseline.feature_importances_[indices])
    plt.yticks(np.arange(len(variables)) + 0.25, np.array(variables)[indices])
    _ = plt.xlabel('Relative importance: ' + classifierType)",py/features.py,ZahidP/tasty-physics,1
"            if info['is_sparse']==True:
                self.name = ""BaggingRidgeRegressor""
                self.model = BaggingRegressor(base_estimator=Ridge(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            else:
                self.name = ""GradientBoostingRegressor""
                self.model = GradientBoostingRegressor(n_estimators=1, verbose=verbose, warm_start = True)
            self.predict_method = self.model.predict # Always predict probabilities
        else:
            if info['has_categorical']: # Out of lazziness, we do not convert categorical variables...
                self.name = ""RandomForestClassifier""
                self.model = RandomForestClassifier(n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            elif info['is_sparse']:                
                self.name = ""BaggingNBClassifier""
                self.model = BaggingClassifier(base_estimator=BernoulliNB(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...                          
            else:
                self.name = ""GradientBoostingClassifier""
                self.model = eval(self.name + ""(n_estimators=1, verbose="" + str(verbose) + "", min_samples_split=10, random_state=1, warm_start = True)"")
            if info['task']=='multilabel.classification':
                self.model = MultiLabelEnsemble(self.model)
            self.predict_method = self.model.predict_proba  ",automl_lib/models.py,jamesrobertlloyd/automl-phase-1,1
"#################
### Gridsearch ##
#################



X_train_feat = vect.fit_transform(X_train, y_train)
X_train_feat = X_train_feat.toarray()


clf_2 = RandomForestClassifier(n_estimators=50)


tuned_parameters = [
  {'criterion': ['gini', 'entropy'], 
   'max_features': ['auto', 'log2', 'sqrt'],
    },
 ]

",code/webapp-lyricsonly_py27/classify_lyrics/random_forests_scripts/run_gridsearch_1.py,rasbt/musicmood,1
"test_data_paths[""Patient_6""] = ""/Users/dryu/Documents/DataScience/Seizures/data/preprocessed/test_Patient_6_preprocessed.pkl""
test_data_paths[""Patient_7""] = ""/Users/dryu/Documents/DataScience/Seizures/data/preprocessed/test_Patient_7_preprocessed.pkl""
test_data_paths[""Patient_5""] = ""/Users/dryu/Documents/DataScience/Seizures/data/preprocessed/test_Patient_5_preprocessed.pkl""

# Train a classifier.
def TrainRandomForest(p_subject, p_save):
	print ""Welcome to TrainRandomForest("" + p_subject + "", "" + str(p_save) + "")""
	training_data = pd.read_pickle(input_data_paths[p_subject])

	# Ictal vs interictal
	forest_seizure = RandomForestClassifier(n_estimators = 500, n_jobs = 1, max_features=""sqrt"", max_depth=None, min_samples_split=1)
	y_seizure = [1 * (x > 0) for x in training_data.T[""classification""]]
	forest_seizure.fit(training_data[:-2].T, y_seizure)

	# IctalA vs IctalB
	forest_early = RandomForestClassifier(n_estimators = 500, n_jobs = 1, max_features=""sqrt"", max_depth=None, min_samples_split=1)
	y_early = [1 * (x == 2) for x in training_data.T[""classification""]]
	forest_early.fit(training_data[:-2].T, y_early)

	## Print feature importance",code/main.py,DryRun/seizures,1
"                            each matrix features[i] of class i is [numOfSamples x numOfDimensions]
        - n_estimators:     number of trees in the forest
    RETURNS:
        - svm:              the trained SVM variable

    NOTE:
        This function trains a linear-kernel SVM for a given C value. For a different kernel, other types of parameters should be provided.
    '''

    [X, Y] = listOfFeatures2Matrix(features)
    rf = sklearn.ensemble.RandomForestClassifier(n_estimators = n_estimators)
    rf.fit(X,Y)

    return rf

def trainGradientBoosting(features, n_estimators):
    '''
    Train a gradient boosting classifier
    Note:     This function is simply a wrapper to the sklearn functionality for SVM training
              See function trainSVM_feature() to use a wrapper on both the feature extraction and the SVM training (and parameter tuning) processes.",src/pyAudioAnalysis/audioTrainTest.py,belkinsky/SFXbot,1
"survival[survival < meanSurvival] = 0
survival[survival >= meanSurvival] = 1

survival = survival.astype(bool)

# Split data into test and train datasets
#exp_train,exp_test,surv_train,surv_test = train_test_split(expression.values[1:,:],
#                                                           survival,
#                                                           train_size=0.8)

model = RandomForestClassifier(n_estimators = 100)

exp_vals = expression.values[1:,:]
print ""EXPRESSION VALS SHAPE: "",exp_vals.shape
#print ""EXPRESSION VALS: \n"",exp_vals
print ""SURVIVAL SHAPE: "",survival.ravel().shape
scores = cross_validation.cross_val_score(model,exp_vals,survival.ravel(),cv=5,scoring='roc_auc')
print ""RAW ROC AUC VALUES: "",scores
print(""ROC AUC: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))",python/scikitlearn/survivaltest/scripts/kaggleinspired_nowmitCV.py,jdurbin/sandbox,1
"
names = [
    ""Nearest Neighbors"", ""Decision Tree"",
    ""Random Forest"", ""AdaBoost"",
    ""Linear SVM"", ""RBF SVM"",
    ""Naive Bayes""
]
classifiers = [
    KNeighborsClassifier(3),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianNB()
]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)",semester2/task13/exercise2.py,SvichkarevAnatoly/Course-Python-Bioinformatics,1
"
def train_and_score(_tr, _vv, _vp, model_sizes, colors=None):
	all_venues, train_pairs, valid_pairs = generate_interaction(_tr, _vv, _vp)
	
	print ""Creating models""

	# plt.figure(figsize=(10,10)); lw = 2
	roc_aucs = []
	for size in model_sizes:
		extractor = FeatureHasher(n_features=2**size)
		model = RandomForestClassifier(n_estimators=200, 
			criterion='entropy', 
			max_depth=25, 
			min_samples_split=2, 
			oob_score=True, 
			n_jobs=-1)

		all_labels, all_paris = [], []
		print ""Training""
		for i, (user, yay_venues) in enumerate(train_pairs.iteritems()):",ml_models/rf_train.py,fanshi118/Time-Out-NY-with-ML-Revisited,1
"# print len(heart.columns)

dim = data.shape[1]
target_index = dim-1

X = v[:,0:target_index]
y = v[:,target_index]

random = 99 # pick reproducible pseudo-random sequence

clf = RandomForestClassifier(n_estimators=50, oob_score=True,
                             max_features=""sqrt"", bootstrap=True,
                             min_samples_leaf=20, criterion=""entropy"",
                             random_state=random)
clf = clf.fit(X, y)
oob_error = 1 - clf.oob_score_
tree.export_graphviz(clf.estimators_[0], out_file=""/tmp/t0.dot"", feature_names=colnames)

kfold = KFold(n_splits=5, shuffle=True, random_state=random)
",python/heart.py,parrt/AniML,1
"test_set_tfidf = tfidf.transform(test_data)

end = time.time()
print ""Time Taken For Feature Extraction : "", end-start

# Initializing Classifiers 

perceptron = Perceptron()
mnb = MultinomialNB()
bnb = BernoulliNB()
rf = RandomForestClassifier(n_estimators=91)
knn = KNeighborsClassifier(n_neighbors=3)

# Listing Features and Classifiers

test_feature_list = [test_set_bag,test_set_tfidf]
train_feature_list = [train_set_bag,train_set_tfidf]
feature_name_list = [""Bag of Words"",""Tf-Idf""]
classifier_name_list = [""Perceptron"",""Multinomial NB"",""Bernoulli NB"",""Random Forest"",""KNN(K=5)""]
classifier_list = [perceptron,mnb,bnb,knn,rf]",Bag of Popcorn.py,rupakc/Kaggle-Bag-of-Words-Meets-Bag-of-Popcorns,1
"            ""Embarked""
        ]
        return translation[index]

    def evaluate_ind(self, ind):
        no_of_inputs = ind.count(1)
        # Uses fold=0 for feature selection
        expected_outputs = get_training_data().Survived.values
        train_data = self.massage_data_with_outputs(get_training_data(), ind)

        forest = RandomForestClassifier(n_estimators=1000,
                                        n_jobs=-1,
                                        criterion=""entropy"")

        forest.fit(train_data, expected_outputs)

        # Uses fold=0 for feature selection
        expected_eval_outputs = get_evaluation_data().Survived.values
        eval_data = self.massage_data_with_outputs(get_evaluation_data(), ind)
",src/ga_for_rfc.py,JakeCowton/titanic,1
"import pandas as pd

# ANIMALS
INPUT_FILE = ""../../data/animals/label_scores.txt""
# PLANTS
# INPUT_FILE = ""../../data/plants/label_scores.txt""
FEATURES_FILE = '../../data/aaindex/aaindex_used.txt'


def rand_forest(labels, features):
    rf = RandomForestClassifier()
    rf.fit(features, labels)
    # rf.apply()

    return rf


def rand_forest_pca(labels, features, n_folds):
    sd = SplitData(labels, features, n_splits=n_folds)
    Ytr, Xtr = sd.Ytr, sd.Xtr",src/ml/random_forest.py,seokjunbing/cs75,1
"                    cnt_targets, nfeatures)); t = time.time()

            # train a classifier
            if self.classifier == 'lda':
                self.clf = LinearDiscriminantAnalysis(solver='svd', store_covariance=False, priors=self.priors)
                #self.clf = LinearDiscriminantAnalysis(solver='eigen', store_covariance=True, priors=self.priors)
            elif self.classifier == 'qda':
                self.clf = QuadraticDiscriminantAnalysis(priors=self.priors)
            elif self.classifier == 'rf':
                # the gala parameters
                #self.clf = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=20,
                #    bootstrap=False, random_state=None)
                #self.clf = RandomForestClassifier(n_estimators=5*nfeatures,n_jobs=self.nthreads,max_depth=10)
                self.clf = RandomForestClassifier(n_estimators=256,n_jobs=self.nthreads,max_depth=16)
            elif self.classifier == 'svm':
                self.clf = SVC(kernel='rbf',probability=True,cache_size=2000)
            elif self.classifier == 'nb':
                self.clf = GaussianNB()
            elif self.classifier == 'kn':
                self.clf = KNeighborsClassifier(n_neighbors=10,n_jobs=self.nthreads)",recon/python/dpSupervoxelClassifier.py,elhuhdron/emdrp,1
"
from sklearn.cross_validation import train_test_split
from sklearn import ensemble

ID=int(sys.argv[1])

NUM_TREES=1000
MAX_FEATS=40

def fit(train_feats, train_labels):
    model = ensemble.RandomForestClassifier(n_estimators=NUM_TREES,
                                            max_features=MAX_FEATS,
                                            random_state=123,
                                            verbose=100)
    model.fit(train_feats, train_labels)
    return model

train_feats  = np.loadtxt(""DATA/train_feats.raw.split.mat.gz"")
train_labels = np.loadtxt(""DATA/train_labels.split.mat.gz"")
val_feats    = np.loadtxt(""DATA/val_feats.raw.split.mat.gz"")",scripts/TRAIN/random_forests.py,pakozm/otto-kaggle,1
"    '''
    graphing
        dendrograms
        auc roc curves
        boundary curves for SVM
        mesh grids classification
    '''
    def __init__(self):
        params_ = params = {

                RandomForestClassifier():
            {
                'n_estimators':[200],
                'max_features': [8,9,10,11],
                'random_state': [1]
             },
                LogisticRegression():
            { 
                'penalty': ['l2'],
                'C' : [.01, 0.1, 1, 10],",models.py,joseburaschi/QEDA,1
"vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
                                 stop_words='english')


X2 = vectorizer.fit_transform(data_text)
clf2 = SGDClassifier(loss='log', penalty='l2',alpha=1e-5, n_iter=25, random_state=42,shuffle=True)
clf2.fit(X2, labels)
vocab = my_dict2 = {y:x for x,y in vectorizer.vocabulary_.iteritems()}


#clf = RandomForestClassifier(n_estimators=750)

allTextInOrders = data_text + data_text + unhelpful_exp_text + helpful_exp_text
trainingDistributions = []

# for reviewT in data_text:
# 	trainingDistributions.append(lda[corpus.dictionary.doc2bow(corpus.proc(reviewT))])


",src/old_pipeline/treesCellReviews.py,cudbg/Dialectic,1
"clf_DT = tree.DecisionTreeClassifier()
#do k=5 fold crossvalidation
scores_DT = cross_val_score(clf_DT, X,Y, cv = nfolds)#cv = number of folds (how to choose?)
# I was thinking i want to maximize the number of folds since our dataset is so small
print(""Decision Tree scores: "",scores_DT)
print(""Accuracy: %0.2f (+/- %0.2f)"" %(scores_DT.mean(), scores_DT.std()*2))
myscores['DT'] = scores_DT.mean()

#Random Forest
#using the same training data
clf_RF = RandomForestClassifier(n_estimators = 10)#how to choose n_estimators?
#cv
scores_RF = cross_val_score(clf_RF, X,Y, cv = nfolds)
print(""Random Forest scores: "",scores_RF)
print(""Accuracy: %0.2f (+/- %0.2f)"" %(scores_RF.mean(), scores_RF.std()*2))
myscores['RF'] = scores_RF.mean()

#Multi-layer perceptron
#MLP is sensitive to feature scaling on the data so lets standardize 
clf_MLP = make_pipeline(preprocessing.StandardScaler(), MLPClassifier(solver= 'lbfgs', alpha = 1e-5, hidden_layer_sizes=(5,2), random_state=1))",sklearn/classifiers1.py,gnublet/py_explorations,1
"            
        
        def fit_transform(self, X, y, **fit_params):
            self.fit(X,y)
            return self.transform(X)
    
    # Initialize the standard scaler 
    scl = StandardScaler()
    
    # We will use SVM here..
    rf = OneVsRestClassifier(RandomForestClassifier(n_estimators=300,max_depth=16))
    
    # Create the pipeline 
    model = pipeline.Pipeline([('UnionInput', FeatureUnion([('svd', svd), ('dense_features', FeatureInserter())])),('scl', scl),('rf', rf)])

    # Fit Model
    model.fit(X, y)

    preds = model.predict(X_test)
###################################Model performance ############    ",Search_result/srr71.py,tanayz/Kaggle,1
"
# Load MUV dataset
muv_tasks, muv_datasets, transformers = load_muv()
(train_dataset, valid_dataset, test_dataset) = muv_datasets

# Fit models
metric = dc.metrics.Metric(
    dc.metrics.roc_auc_score, np.mean, mode=""classification"")

def model_builder(model_dir):
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=500)
  return dc.models.SklearnModel(sklearn_model, model_dir)
model = dc.models.SingletaskToMultitask(muv_tasks, model_builder)

# Fit trained model
model.fit(train_dataset)
model.save()

# Evaluate train/test scores",examples/muv/muv_sklearn.py,peastman/deepchem,1
"    trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )

    print(""Creating average feature vecs for test reviews"")

    testDataVecs = getAvgFeatureVecs( getCleanReviews(test), model, num_features )


    # ****** Fit a random forest to the training set, then make predictions
    #
    # Fit a random forest to the training data, using 100 trees
    forest = RandomForestClassifier( n_estimators = 100 )

    print(""Fitting a random forest to labeled training data..."")
    forest = forest.fit(trainDataVecs, train[""sentiment""])

    # Test & extract results
    result = forest.predict(testDataVecs)

    # Write the test results
    output = pd.DataFrame( data={""id"":test[""id""], ""sentiment"":result})",doc_ref/NLP/word2vec-nlp-tutorial/DeepLearningMovies/Word2Vec_AverageVectors.py,gtesei/fast-furious,1
"sys.path.append(os.path.abspath(scriptpath))
import utils

parameter_str = '_'.join(['top', str(utils.k), 'cw', str(utils.click_weight), 'year', utils.train_year])
train = joblib.load(utils.processed_data_path + 'train_is_booking_all_' + parameter_str +'.pkl')

X_train = train.ix[:,2:]
y_train = train['hotel_cluster'].astype(int)

print ""train RandomForest Classifier...""
cforest = RandomForestClassifier(n_estimators=32, max_depth=50, min_samples_split=50, min_samples_leaf=5, 
	random_state=0, verbose=1, n_jobs=-1)

cforest.fit(X_train, y_train)
joblib.dump(cforest, utils.model_path + 'rf_all_'+ parameter_str + '.pkl')


",src/model/train_rf_model.py,parkerzf/kaggle-expedia,1
"                ""Neural Net"",
                ""AdaBoost"",
                ""Naive Bayes"",
                ""QDA""]
            classifiers = [
                KNeighborsClassifier(3),
                SVC(kernel=""linear"", C=0.025),
                SVC(gamma=2, C=1),
                GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
                DecisionTreeClassifier(max_depth=5),
                RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
                MLPClassifier(alpha=1),
                AdaBoostClassifier(),
                GaussianNB(),
                QuadraticDiscriminantAnalysis()]
            for name, clf in zip(names, classifiers):
                try:
                    clf.fit(self.trainX, self.trainY)
                    score = clf.score(self.testX, self.testY)
                    print(name, score)",rf.py,schollz/find,1
"
            if len(relevant_test_case_names) == 0:
                continue
                
            
            #### FIT PIPELINE ####
            print(""Fitting pipeline for %s events...""%nr_events)
            sys.stdout.flush()
            
            if classifier == ""rf"":
                cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=best_params[dataset_name][method_name][nr_events]['rf_max_features'], random_state=random_state)

            elif classifier == ""gbm"":
                cls = GradientBoostingClassifier(n_estimators=best_params[dataset_name][method_name][nr_events]['gbm_n_estimators'], max_features=best_params[dataset_name][method_name][nr_events]['gbm_max_features'], learning_rate=best_params[dataset_name][method_name][nr_events]['gbm_learning_rate'], random_state=random_state)

            else:
                print(""Classifier unknown"")
                break             
            
            feature_combiner = FeatureUnion([(method, init_encoder(method)) for method in methods])",experiments_final/run_index_lossy.py,irhete/predictive-monitoring-benchmark,1
"
		shuffle_idx = np.random.permutation(y.size)

		X = X[shuffle_idx]
		y = y[shuffle_idx]

		# hold out 20 percent of data for testing accuracy
		n_train = round(X.shape[0]*.8)

		# define base models
		base_models = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
		               RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
		               ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini')]

		# define blending model
		blending_model = LogisticRegression()

		# initialize multi-stage model
		sg = StackedGeneralizer(base_models, blending_model, 
			                    n_folds=N_FOLDS, verbose=VERBOSE)",stacked_generalizer.py,dustinstansbury/stacked_generalization,1
"dot_data = StringIO()  
tree.export_graphviz(clf, out_file=dot_data,  
                         feature_names=features)  
graph = pydot.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())  

#The code below this point would be ran in a seperate file, with this one imported to it. It is placed in this file for display


#Test the validity of the tree using the ""Random Forest"" methood with a factor of 25 seperate tests 
clf = RandomForestClassifier(n_estimators=25)
clf = clf.fit(X, y)

#Predict employment of an employed 10-year veteran with an AS Degree
print clf.predict([[10, 1, 4, 0, 0, 0, 0]])
#...and an unemployed 10-year veteran with an AS Degree
print clf.predict([[10, 0, 4, 0, 0, 0, 0]])",Data-Science-Tools/application_filter.py,Tlinne2/Basic-Python-Projects-,1
"    def train(self, X_train, labels):
        """"""
        Train model with given data corpus
        Arguments:
            X_train: the train data [n_samples, n_features]
            labels: the GT labels [n_samples, n_classes]
        Return:
            return trained model
        """"""
        # train estimator
        clf = RandomForestClassifier(n_estimators = self.n_estimators, random_state = RANDOM_STATE, n_jobs = -1)
        self.model = clf.fit(X_train, labels)
        
        train_score = self.model.score(X_train, labels)
        print(""RandomForest:\ntrain score = %.3f, n_estimators = %d"" % (train_score, self.n_estimators))
        
        return self.model


",src/random_forest_model.py,yaricom/english-article-correction,1
"for tag, count in zip(vocab, dist):
 print count, tag


# ******* Train a random forest using the bag of words
#
print ""Training the random forest (this may take a while)...""
from sklearn.ensemble import RandomForestClassifier

# Initialize a Random Forest classifier with 100 trees
forest = RandomForestClassifier(n_estimators = 100) 

# Fit the forest to the training set, using the bag of words as 
# features and the sentiment labels as the response variable
#
# This may take a few minutes to run
forest = forest.fit( train_data_features, train[""sentiment""] )


# ******* Read the test data, clean it, and make predictions",TutorialCode_Final/BagOfWords.py,angelachapman/Kaggle-DeepLearning-Tutorial,1
"    posiResult = {}
    trainFeature, testFeature, trainIdList, testIdList, trainTarList = merge_feature()
    tmp = [t < 32 for t in trainTarList]
    tmp = np.array(tmp)
    trainFeature = trainFeature[tmp]
    target = np.array(trainTarList)
    target = target[tmp]
    trainIdList = np.array(trainIdList)
    trainIdList = trainIdList[tmp]
    Cfeature = trainFeature.columns[:]
    clf = RandomForestClassifier(n_estimators=200, min_samples_split=17)
    clf.fit(trainFeature[Cfeature], target)
    rfPreds = clf.predict(testFeature)
    rfProb = clf.predict_proba(testFeature)
    gbdt = GradientBoostingClassifier(n_estimators=150, min_samples_split=17)
    gbdt.fit(trainFeature[Cfeature], target)
    gbdtPreds = gbdt.predict(testFeature)
    gbdtProb = gbdt.predict_proba(testFeature)
    allProb = rfProb + gbdtProb
    allPreds = []",position_predict/predict_code/posi_predict.py,yinzhao0312/Position-predict,1
"
random = 99 # pick reproducible pseudo-random sequence

kfold = KFold(n_splits=k, shuffle=True, random_state=random)

avg_err = 0.0
for train_index, test_index in kfold.split(X):
    # print(""TRAIN:"", train_index, ""TEST:"", test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    clf = RandomForestClassifier(n_estimators=n_estimators, oob_score=False,
                                 max_features=""sqrt"", bootstrap=True,
                                 min_samples_leaf=min_samples_leaf, criterion=""entropy"",
                                 random_state=random)
    clf = clf.fit(X_train, y_train)
    # print ""oob error"", oob_error,
    cats = clf.predict(X_test)
    counts = collections.Counter(y_test==cats)
    err = counts[False] / float(len(y_test))
    avg_err += err",python/rf_error.py,parrt/AniML,1
"                [10, 50, 100, 500, 1000],
                np.arange(0., 0.51, 0.05),
                [0.1, 0.25, 0.5, 0.75, 'sqrt', 'log2', None],
                ['gini', 'entropy']):
        features = input_data.drop('class', axis=1).values.astype(float)
        labels = input_data['class'].values

        try:
            # Create the pipeline for the model
            clf = make_pipeline(preprocessor,
                                RandomForestClassifier(n_estimators=n_estimators,
                                                       min_weight_fraction_leaf=min_weight_fraction_leaf,
                                                       max_features=max_features,
                                                       criterion=criterion,
                                                       random_state=324089))
            # 10-fold CV score for the pipeline
            cv_predictions = cross_val_predict(estimator=clf, X=features, y=labels, cv=10)
            accuracy = accuracy_score(labels, cv_predictions)
            macro_f1 = f1_score(labels, cv_predictions, average='macro')
            balanced_accuracy = balanced_accuracy_score(labels, cv_predictions)",preprocessor_model_code/RandomForestClassifier.py,rhiever/sklearn-benchmarks,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/default_numerical_only.py,diogo149/CauseEffectPairsPaper,1
"    fake_features_z = [([.1, .2], [.1,.1], 10)]#, ([.2, .1], [.1,.1], 10)]
    clean_train, dirty_train, clean_test = corrupt_dataset(fake_features_z, train_data, train_labels, test_data, test_labels)
    vectorizer = CountVectorizer(lowercase=False, binary=True) 
    dirty_train_vectors = vectorizer.fit_transform(dirty_train)
    clean_train_vectors = vectorizer.transform(clean_train)
    test_vectors = vectorizer.transform(clean_test)
    terms = np.array(list(vectorizer.vocabulary_.keys()))
    indices = np.array(list(vectorizer.vocabulary_.values()))
    inverse_vocabulary = terms[np.argsort(indices)]
    tokenizer = vectorizer.build_tokenizer()  
    c1 = ensemble.RandomForestClassifier(n_estimators=30, max_depth=5)
    c2 = ensemble.RandomForestClassifier(n_estimators=30, max_depth=5)
    untrustworthy = [i for i, x in enumerate(inverse_vocabulary) if x.startswith('FAKE')]
    train_idx, test_idx = tuple(cross_validation.ShuffleSplit(dirty_train_vectors.shape[0], 1, 0.2))[0]
    train_acc1 = train_acc2 = test_acc1 = test_acc2 = 0
    print 'Trying to find trees:'
    sys.stdout.flush()
    iteration = 0
    found_tree = True
    while np.abs(train_acc1 - train_acc2) > 0.001 or np.abs(test_acc1 - test_acc2) < 0.05: ",generate_data_for_compare_classifiers.py,marcotcr/lime-experiments,1
"import random
import pickle
import sys

# This is the file that is used for getting classifier, given the 
# specific GO term the user input

# The GO term as the only parameter that is passed in
GO = sys.argv[1]

rf = RandomForestClassifier(criterion=""entropy"", \
	 n_estimators = 300, max_depth = 100, class_weight={0:100, 1:1})

#####################
# Get Train samples #
#####################
with open(""negative.fa"") as F:
	neg = F.readlines()
	nneg = len(neg)/2
",pipeline/trainModel.py,Data-Integration-Practicum/Data-Integration-Practicum-Terminator-Prediction,1
"# Import the random forest package
from sklearn.ensemble import RandomForestClassifier 
from sklearn import cross_validation

dataset = np.loadtxt('training_data', delimiter="","")

# Create the random forest object which will include all the parameters
# for the fit
forest = RandomForestClassifier(n_estimators = 100)

# Fit the training data to the Survived labels and create the decision trees
forest_fit = forest.fit(dataset[0::,1::],dataset[0::,0])

importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)

# Take the same decision trees and run it on the test data",data_generation/classify.py,alexcritschristoph/CircHMP,1
"
testDataVecs = getAvgFeatureVecs(clean_test_tweets, model, num_features)

imp = Imputer(missing_values='NaN', strategy='median', axis=0)
trainDataVecs = imp.fit_transform(trainDataVecs)
testDataVecs = imp.fit_transform(testDataVecs)


# Random Forest Classification

forest = RandomForestClassifier( n_estimators = 100 )

print ""Fitting a random forest to labeled training data...""
forest = forest.fit( trainDataVecs, corpus[""Sentiment""] )
result = forest.predict( testDataVecs )

# Write the test results 
output = pd.DataFrame( data={""id"":test[""id""], 
                ""id"":test[""id""],
                ""created_at"":test[""created_at""],",Sentiment_Model.py,georgetown-analytics/auto-sentiment,1
"            from sklearn.neighbors import KNeighborsClassifier
            estimator = KNeighborsClassifier(
                **self.kwargs)
        elif self.estimator == 'decision-tree':
            from sklearn.tree import DecisionTreeClassifier
            estimator = DecisionTreeClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'random-forest':
            from sklearn.ensemble import RandomForestClassifier
            estimator = RandomForestClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'adaboost':
            from sklearn.ensemble import AdaBoostClassifier
            estimator = AdaBoostClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'gradient-boosting':
            from sklearn.ensemble import GradientBoostingClassifier",imblearn/under_sampling/instance_hardness_threshold.py,dvro/imbalanced-learn,1
"            dt = data.loc[data[0] == i]
            dataCropped = pd.concat([dataCropped, dt])
data = pd.DataFrame.transpose(dataCropped)
# Removing oligo names from dataset
data = data.drop(data.index[0])


estimators = input('How many trees to build?: ')
estimators = int(estimators)

forest = RandomForestClassifier(n_estimators = estimators)
forest = forest.fit(data, classifier)

importances = forest.feature_importances_

importancesList = pd.DataFrame(importances, index=oligosList, columns=['Importance'])
joblib.dump(forest, './model/model.pkl')
    
importancesList.to_csv(""importances.txt"", sep='\t')
",scleroderma-prediction/RF_train_with_oligos.py,Karl-Marka/data-mining,1
"        x_orig_train, x_orig_test = df_orig_x.ix[train_idx, :].dropna(), df_orig_x.ix[test_idx, :].dropna()
        y_orig_train, y_orig_test = df_orig_y.ix[train_idx].dropna(), df_orig_y.ix[test_idx].dropna()
        x_orig_train, x_orig_valid, y_orig_train, y_orig_valid = train_test_split(x_orig_train, y_orig_train, test_size=0.33, random_state=rand)

        # standarization
        x_orig_train_std = scaler.fit_transform(x_orig_train, y_orig_train)
        x_orig_valid_std = scaler.transform(x_orig_valid)
        x_orig_test_std = scaler.transform(x_orig_test)

        # random forest
        clf = RandomForestClassifier(max_features=.33, n_jobs=-1, random_state=rand)
        clf.fit(x_orig_train_std, y_orig_train)
        score = clf.score(x_orig_test_std, y_orig_test)
        print('{}:\tFold: {} Score: {}'.format(time.strftime(""%d/%m/%Y %H:%M:%S""), f, score))
        scores.append(score)

        f += 1

    score_mean = np.mean(scores)
    score_std = np.std(scores)",main.py,pbozzi/fraud-paper,1
"        for imp in imps:
            for k,(train,test) in enumerate(shuffle_split):
                X_train,X_test = X[imp][train],X[imp][test]
                Y_train,Y_test = Y[imp][train,j],Y['missing'][test,j]
                clf_args_ = {key:(value if type(value) is not dict \
                             else value[prop])\
                             for key,value in clf_args.items()}
                if clf_args_['max_features'] not in [None, 'auto']:
                   clf_args_['max_features'] = min(X_train.shape[1],
                                                   clf_args_['max_features'])
                rfc = RandomForestClassifier(**clf_args_)
                #if Y_train.shape[1] == 1:
                #    Y_train = Y_train.ravel()
                rfc.fit(X_train,Y_train)
                Y_predict = rfc.predict(X_test)#.reshape(-1,n_props)
                probs = rfc.predict_proba(X_test)
                if probs.shape[1]<2 and probs.mean()==1.0:
                    n_test_samples = len(probs)
                    ps[imp][n_prop,k,:n_test_samples] = 0.0
                else:",scratch.py,rgerkin/upsit,1
"from sklearn.ensemble import RandomForestClassifier


class RFClassificationModel(object):
    def __init__(self,reg_model=None):
        self.reg_model = RandomForestClassifier(n_estimators=100,n_jobs=2)

    def Train(self,x,y):
        self.reg_model.fit(x,y)

    def GetClassifier(self):
        return self.reg_model

    def GetFeatureImportances(self):
        return self.reg_model.feature_importances_",FraudDetection.ML/AI/classifican_models.py,imironica/Fraud-Detection-System,1
"    IND_PART = 'Individual', 'Individual', 'Part'

    
class Clf(object):
    SVC_RBF = SVC(kernel='rbf', class_weight=None, random_state=0)
    SVC_RBF_CW = SVC(kernel='rbf', class_weight='auto', random_state=0)
    LINEAR_L1 = LinearSVC(loss='l1', random_state=0, class_weight=None)
    LINEAR_L1_CW = LinearSVC(loss='l1', random_state=0, class_weight='auto')
    LINEAR_SVC = SVC(kernel='linear', random_state=0, class_weight='auto')
    TREE = DecisionTreeClassifier(random_state=0)
    RF = RandomForestClassifier(random_state=0)
    MAJORITY = DummyClassifier(strategy='most_frequent')
    RANDOM = DummyClassifier(strategy='stratified')
    ADABOOST = AdaBoostClassifier(random_state=0)
    LR = LogisticRegression()

    
def limit_to_task_data(data, task: Task):
    parent_type, child_type, relation_type = task
    task_data = data[data['parent_type'] == parent_type][data['child_type'] == child_type].copy()",dswont/relation_type.py,anonymous-ijcai/dsw-ont-ijcai,1
"import scipy.sparse as sp

from skml.problem_transformation import ClassifierChain
from skml.datasets import load_dataset

X, y = load_dataset('yeast')


class TestCC(Chai):
    def test_cc_fit_predict(self):
        chain = ClassifierChain(RandomForestClassifier())
        chain.fit(X, y)
        y_pred = chain.predict(X)
        hamming_loss(y, y_pred)

    def test_cc_fit_predict_proba(self):
        chain = ClassifierChain(RandomForestClassifier())
        chain.fit(X, y)

        y_pred_proba = chain.predict_proba(X)",test/test_cc.py,ChristianSch/skml,1
"    def _get_model_object(self, model, parameters, n_cores):
        """"""This method takes the requested model type and
        hyperparameters and produces the relevant classifier object.

        Returns:
            object with the fit() and predict_proba() methods
            implemented on it
        """"""

        if self.model_type == ""RandomForest"":
            return ensemble.RandomForestClassifier(
                n_estimators=self.hyperparameters['n_estimators'],
                max_features=self.hyperparameters['max_features'],
                criterion=self.hyperparameters['criterion'],
                max_depth=self.hyperparameters['max_depth'],
                min_samples_split=self.hyperparameters['min_samples_split'],
                n_jobs=self.n_cores)

        elif self.model_type == ""RandomForestBagging"":
            return ensemble.BaggingClassifier(",fpsd/classify.py,freedomofpress/fingerprint-securedrop,1
"                         'col_name': 'f0'},
                        {'func': 7, 'vals': (1, 2), 'col_name' :
                         'f1'}]
        self.assertRaises(ValueError, utils.check_arguments, row_invalid1,
                          row_reqs)
        row_invalid2 = [{'func': modify.row_val_eq, 
                         'vals': 8, 'col_name': 'f0'},
                        {'func': modify.row_val_between, 'vals': (1, 2)}]
        self.assertRaises(ValueError, utils.check_arguments, row_invalid2,
                          row_reqs)
        clfs_invalid1 = [{'clf': RandomForestClassifier(), 
                          'n_estimators': [1, 10],
                          'max_features': [10, 100, 1000]}]
        self.assertRaises(ValueError, utils.check_arguments, clfs_invalid1,
                          clfs_reqs, optional_keys_take_lists=True)
        clfs_invalid2 = [{'clf': RandomForestClassifier, 
                          'n_estimators': 1,
                          'max_features': [10, 100, 1000]}]
        self.assertRaises(ValueError, utils.check_arguments, clfs_invalid2,
                          clfs_reqs, optional_keys_take_lists=True)",tests/test_utils.py,jamestwhedbee/diogenes,1
"
    counter = 0
    for review in clean_test_reviews:
        test_centroids[counter] = create_bag_of_centroids( review, \
            word_centroid_map )
        counter += 1


    # ****** Fit a random forest and extract predictions
    #
    forest = RandomForestClassifier(n_estimators = 100)

    # Fitting the forest may take a few minutes
    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit(train_centroids,train[""sentiment""])
    result = forest.predict(test_centroids)

    # Write the test results
    output = pd.DataFrame(data={""id"":test[""id""], ""sentiment"":result})
    output.to_csv(""../result/BagOfCentroids.csv"", index=False, quoting=3)",pycode/Word2Vec_BagOfCentroids.py,CharLLCH/Word2Vec,1
"        self.bags = []
        for i in range(n_trees):
            cols = sample(range(self.n_features), tree_features)
            cols.sort()
            self.col_list[i, :] = cols
            data_temp = data[:, cols]
            n_nans = np.sum(np.isnan(data_temp), 1)
            data_temp = data_temp[n_nans == 0, :]
            classes_temp = classes[n_nans == 0]
            #bag = BaggingClassifier(n_estimators=1, max_features=tree_features)
            bag = RandomForestClassifier(n_estimators=1, max_features=tree_features)
            bag.fit(data_temp, classes_temp)
            self.bags.append(bag)
            print(np.shape(data_temp))
        
    def classify(self, data):
        nan_cols = np.arange(self.n_features)[np.isnan(data)]
        decisions = []
        s1 = set(nan_cols)
        for i in range(self.n_trees):",half_random_forest/code/half_random_forest.py,Niederb/python_machine_learning,1
"X_Test = test.ix[:, 2:len(test.columns) - 1]
Y_Test = test.ix[:, len(test.columns) - 1: len(test.columns)]

X_Test_standard = normalize(X_Test)
print X_Test_standard.shape

print ""Training Data Set Size : "", str(len(X))
print ""Testing Data Set Size : "", str(len(X_Test))

# tune parameters here.
rf = RandomForestClassifier(n_estimators=150, max_features=20)

rf.fit(X_standard, Y)
# predict
Y_Result = rf.predict(X_Test_standard)

print precision_recall_fscore_support(Y_Test, Y_Result, average='micro')


",src/train_test/random_forests.py,mohitreddy1996/Gender-Detection-from-Signature,1
"            other = classes[0] if prediction == classes[1] else classes[1]
            return [prediction, other]
        else:
            result = zip(self.classes_, self.model.decision_function(vector)[0])
            result.sort(key=itemgetter(1))
            return map(itemgetter(0), result)


class wrapped_randomforest(predictor):
    def __init__(self, null_value):
        self.model = RandomForestClassifier()
        self.null_value = null_value

    def fit(self, data, classes):
        self.classes_ = classes
        #data = stripMagnets(data)
        return self.model.fit(data, classes)

    def predict(self, vector):
        classes = self.classes_",FingerprintsREST/views/MatchLocation/predictors.py,alexf101/indoor-tracking,1
"
  Xtrain = Train['X']
  Ytrain = Train['Y']
  assert Xtrain.shape[0] == Ytrain.size
  assert Xtrain.ndim == 2
  assert Ytrain.ndim == 1
  assert len(np.unique(Ytrain)) == 2

  cname = cname.lower()
  if cname == 'randomforest':
    C = RandomForestClassifier(n_estimators=10, max_depth=None,
                             min_samples_leaf=10, random_state=0)
  elif cname == 'logistic':
    C = LogisticRegression(penalty='l2', C=1.0)
  elif cname == 'nearestneighbor1':
    C = KNeighborsClassifier(n_neighbors=1, algorithm='brute')
  elif cname == 'nearestneighbor3':
    C = KNeighborsClassifier(n_neighbors=3, algorithm='brute')
  elif cname == 'svm-rbf':
    C = SVC(C=1.0, kernel='rbf', probability=True)",satdetect/detect/Classifier.py,michaelchughes/satdetect,1
"df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75
df['species'] = pd.Factor(iris.target, iris.target_names)
df.head()
pd.factorize(

train = df[df['is_train']==True]
test  = df[df['is_train']==False]


features = df.columns[:4]
clf = RandomForestClassifier(n_jobs=2)
y, _ = pd.factorize(train['species'])
clf.fit(train[features], y)

preds = iris.target_names[clf.predict(test[features])]
pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds'])",study/main_random_forest.py,leandroohf/Public_Liberty_Mutual_Group_Property_Inspection_Prediction,1
"    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis(),
    MLPClassifier(solver='lbfgs',alpha=1e-4,hidden_layer_sizes=(10,2),random_state=1,verbose=True),
    LassoCV(),
    LinearRegression(),
    LogisticRegression(),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    MLPClassifier(solver='lbfgs',alpha=1e-5,hidden_layer_sizes=(len(names)-1,2),random_state=1,max_iter=500)
    #VotingClassifier()
]
metric = ['accuracy','f1']
dataset = 0 # all datasets (0), dataset 1-2 (1), dataset 3 (2)
download = 1 # Download pre-computed (1) data or compute them anew (0)

########################################## Download necessary files ################################################",MLslippagesrc/MLslippage/mltraining.py,jagrio/MachineLearningSlippage,1
"df['ingredients_stem'] = df.ingredients_all.apply(lambda x: "" "".join([stemmer.stem(word.lower()) for word in x.split("" "")]))
# set the X, y
X = df['ingredients_stem']
y = df['cuisine_idx']
#
predDf = []
# models
models = [
    ('rf',
        Pipeline([('vect', TfidfVectorizer(strip_accents='unicode')),
                  ('clf', RandomForestClassifier(n_estimators=1000, random_state=0))
                 ])
    ),
]
# evaluate them
rf = cross_val_models(models, X, y, 5)
predDf = predDf.append(rf, ignore_index=True)
predDf

",project2/code/rf.py,bluegrapes/DAT8Coursework,1
"        df = concat([data[k], data['class']], axis=1, keys=[title[k], 'class'])
        f = plt.figure(figsize=(8, 6))
        p = df.boxplot(by='class', ax = f.gca())
        f.savefig('./%s/box_%s_class.png' % (dirs[0], title[k]))

def roc_plot():
    data, class_names = get_analyze_data()
    target = data[6]
    train = data.drop(['id', 6], axis = 1)

    model_rfc = RandomForestClassifier(n_estimators = 100, criterion = ""entropy"", n_jobs = -1)
    model_knc = KNeighborsClassifier(n_neighbors = 10, algorithm = 'brute')
    model_gbc = GradientBoostingClassifier(n_estimators = 100)
    model_lr = LogisticRegression(penalty='l1', tol=0.01)

    ROCtrainTRN, ROCtestTRN, ROCtrainTRG, ROCtestTRG = cross_validation.train_test_split(train, target, test_size=0.25)

    print 'ROC...'

    pl.clf()",liver_disorders.py,zhzhussupovkz/liver-disorders,1
"# argv[1] - training set file
# argv[2] - testing set file
# argv[3] - output file

import sys
from prediction_model import predict
from sklearn.ensemble import RandomForestClassifier

predict(sys.argv[1], sys.argv[2], sys.argv[3], lambda:  RandomForestClassifier(max_depth=6, n_estimators=10, max_features=5))",Assignment-1/2B/vu-dm-16-model-randomforest.py,Ignotus/DataMining,1
"
    # TEST DATA
    test_df = pd.read_csv('../data/test.csv', header=0)        # Load the test file into a dataframe

    # Collect the test data's PassengerIds before dropping it
    ids = test_df['PassengerId'].values

    test_data = clean_data(test_df)

    print 'Training...'
    forest = RandomForestClassifier(n_estimators=1000, n_jobs=-1)
    forest = forest.fit( train_data[0::,1::], train_data[0::,0] )

    print 'Predicting...'
    output = forest.predict(test_data).astype(int)


    prediction_filepath = ""%s/prediction/myfirstforest.csv"" % os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
    prediction_dir = os.path.dirname(prediction_filepath)
    if not os.path.exists(prediction_dir):",titanic/code/first_attempt.py,ryanbahneman/kaggle,1
"
    pos_idx = [fet_list.index(i) for i in pos_list if i in fet_set]

    y = np.zeros(features.shape[0])
    y[pos_idx] = 1

    print 'n_pos', np.sum(y), 'n_neg', np.sum(1 - y)

    params = {'n_estimators':[2, 4, 5, 6, 8, 10, 30]}
    #params = {'n_estimators':[50, 70, 100, 120, 150, 200]}
    clf = grid_search.GridSearchCV(RandomForestClassifier(n_estimators = 2, n_jobs = 4), params, scoring = metrics.make_scorer(lambda yt, yp: metrics.f1_score(yt, yp, pos_label = 0)), cv = 5)
    clf.fit(features, y)
    print clf.best_score_
    print clf.best_estimator_
    cPickle.dump(clf.best_estimator_, open(osp.join(data_dir, 'c3d-models-rfc.pkl'), 'w'))

if __name__ == ""__main__"":
    main()

# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4",code/gifs-filter/c3d-models/train.py,raingo/TGIF-Release,1
"    featureMatrix['is_train'] = np.random.uniform(0, 1, len(featureMatrix)) <= .75

    #split out the train and test df's into separate objects
    train, test = featureMatrix[featureMatrix['is_train']==True], featureMatrix[featureMatrix['is_train']==False]

    #drop the is_train column, we don't need it anymore
    train = train.drop('is_train', axis=1)
    test = test.drop('is_train', axis=1)

    #create the random forest class and factorize the class column
    clf = RandomForestClassifier(n_jobs=4, n_estimators=opts.numtrees, oob_score=True)
    y, _ = pd.factorize(train['class'])

    #train the random forest on the training set, dropping the class column (since the trainer takes that as a separate argument)
    print('\nTraining')
    clf.fit(train.drop('class', axis=1), y)

    #remove the 'answers' from the test set
    testnoclass = test.drop('class', axis=1)
",train_flows_rf.py,DKarev/isolation-forest,1
"
def main():
  options = get_options()
  stars = load_stars(options.input)
  classifiers = [
    ('Gaussian Naive Bayes',
     GaussianNB()),
    ('Logistic Regression',
     LogisticRegression()),
    ('Random Forest',
     GridSearchCV(RandomForestClassifier(n_jobs=options.n_jobs),
                  {n_estimators: [10, 50, 100, 500]}, scoring=options.scoring,
                  n_jobs=options.n_jobs, cv=options.cv)),
    ('Extremely Randomized Trees',
     GridSearchCV(ExtraTreesClassifier(n_jobs=options.n_jobs),
                  {n_estimators: [10, 50, 100, 500]}, scoring=options.scoring,
                  n_jobs=options.n_jobs, cv=options.cv)),
    ('Linear SVC',
     SVC(kernel='linear', C=1., probability=True, random_state=0))]
  for name, classifier in classifiers:",src/learn.py,astroswego/supervised-classification,1
"# In[11]:

label_percent(Y_test)
Y_test.size


# In[12]:

# Create the random forest object which will include all the parameters
# for the fit
forest = RandomForestClassifier(n_estimators = 100, max_depth=10)

# Fit the training data to the Survived labels and create the decision trees
forest = forest.fit(X_train, Y_train)

# Take the same decision trees and run it on the test data
output = forest.predict(X_test)

classification_report1 = classification_report(y_true=Y_test, y_pred=output)
print(classification_report1)",lobpredictrst/Test_Random_Forest_Shuffle.py,doutib/lobpredict,1
"
# In[21]:

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.model_selection import GridSearchCV


# In[23]:

rf = RandomForestClassifier(n_estimators=20, n_jobs=2)
param_grid = {
                 'n_estimators': [5, 10, 15, 20],
                 'max_depth': [2, 5, 7, 9]
             }


# In[24]:

grid_rf = GridSearchCV(rf, param_grid, cv=10)",scripts/train.py,vibhutiM/Production-Failures,1
"
        if not kwargs:
            kwargs = {""n_jobs"": 8,
                      ""max_depth"": 12,
                      ""min_samples_leaf"": 50,
                      ""n_estimators"": 50,
                      ""max_features"": None,
                      ""verbose"": 10,
                      }

        self.clf = RandomForestClassifier(**kwargs)

        self.clf.fit(allrows[columns].values, allrows[""tag""].values)

        # add audit trail into the model output:
        self.clf.columns = columns


    def classify(self, instances, columns, *args, **kwargs):
        """""" Classify a set of instances after training",src/python/scoringModelTraining/germline/lib/evs/strelka_rf.py,Illumina/strelka,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/only_fit.py,diogo149/CauseEffectPairsPaper,1
"
    assert np.array_equal(result['guess'].values, dtc.predict(testing_features))

def test_random_forest():
    """"""Ensure that the TPOT random forest method outputs the same as the sklearn random forest""""""

    tpot_obj = TPOT()
    result = tpot_obj._random_forest(training_testing_data, 100, 0)
    result = result[result['group'] == 'testing']

    rfc = RandomForestClassifier(n_estimators=100, max_features='auto', random_state=42, n_jobs=-1)
    rfc.fit(training_features, training_classes)

    assert np.array_equal(result['guess'].values, rfc.predict(testing_features))

def test_xgboost():
    """"""Ensure that the TPOT xgboost method outputs the same as the xgboost classfier method""""""

    tpot_obj = TPOT()
    result = tpot_obj._xgradient_boosting(training_testing_data, n_estimators=100, learning_rate=0, max_depth=3)",tests.py,pronojitsaha/tpot,1
"        logging.getLogger('').addHandler(console)
        logging.info(""START CLASS {class_name}."".format(class_name = CreateRandomForestModel.__name__))

        try:
            self.con = MySQLdb.connect(host='localhost', user='root', passwd='931209', charset='utf8')
            logging.info(""Success in connecting MySQL."")
        except MySQLdb.Error, e:
            logging.error(""Fail in connecting MySQL."")
            logging.error(""MySQL Error {error_num}: {error_info}."".format(error_num = e.args[0], error_info = e.args[1]))

        self.model = RandomForestClassifier()



    @Decorator.log_of_function
    def __del__(self):
        try:
            self.con.close()
            logging.info(""Success in quiting MySQL."")
        except MySQLdb.Error, e:",Titanic/class_create_model_of_random_forest.py,ysh329/Titanic-Machine-Learning-from-Disaster,1
"
    data_set = pd.read_csv('data/train.csv')
    train = data_set.sample(frac=0.66)
    test = data_set.drop(train.index.values.tolist())

    X_train = train.iloc[:, 1:].values
    Y_train = train[[0]].values.ravel()
    X_test = test.iloc[:, 1:].values
    Y_test = test[[0]].values.ravel()

    rf = RandomForestClassifier(n_estimators=100)
    rf.fit(X_train, Y_train)
    rf.predict(X_test)

    # Roughly 96% accuracy
    print('Overall Accuracy: {0:3f}%'.format(rf.score(X_test, Y_test) * 100))",rf.py,timothymiko/DigitRecognizer,1
"path = '../Data/'
print(""read training data"")
train = pd.read_csv(path+""train_tfidf.csv"")
label = train['target']
trainID = train['id']
del train['id'] 
del train['target']
tsne = pd.read_csv(path+'tfidf_train_tsne.csv')
train = train.join(tsne)

clf = RandomForestClassifier(n_jobs=-1, n_estimators=300, verbose=3, random_state=131)
iso_clf = CalibratedClassifierCV(clf, method='isotonic', cv=10)
iso_clf.fit(train.values, label)

print(""read test data"")
test  = pd.read_csv(path+""test_tfidf.csv"")
ID = test['id']
del test['id']
tsne = pd.read_csv(path+'tfidf_test_tsne.csv')
test = test.join(tsne)",new/src/1st_level/rf_tfidf_retrain.py,puyokw/kaggle_Otto,1
"            models = []
            if 'LogisticRegression' in tuning_ranges:
                models.append(LogisticRegression(penalty='l1', class_weight='auto'))
            if 'DecisionTreeClassifier' in tuning_ranges:
                models.append(DecisionTreeClassifier())
            if 'LinearSVC' in tuning_ranges:
                models.append(LinearSVC(penalty='l1', loss='l2', dual=False, class_weight='auto'))
            if 'SVC' in tuning_ranges:
                models.append(SVC(class_weight='auto'))
            if 'RandomForestClassifier' in tuning_ranges:
                models.append(RandomForestClassifier(n_estimators=500, oob_score=True, n_jobs=njobs))
            if 'GbcAutoNtrees' in tuning_ranges:
                models.append(GbcAutoNtrees(subsample=0.75, n_estimators=500, learning_rate=0.01))

        super(ClassificationSuite, self).__init__(tuning_ranges=tuning_ranges, models=models, cv=cv, njobs=njobs,
                                                  pre_dispatch=pre_dispatch, stack=stack, verbose=verbose)

        self.scorer = make_scorer(accuracy_score)
        self.nfeatures = n_features
        self.classes = None",bck_stats/sklearn_estimator_suite.py,brandonckelly/bck_stats,1
"        # If the name is given, then pop it
        name = kwargs.pop('name', None)
        if name is None:
            # If the name of the matcher is give, then create one.
            # Currently, we use a constant string + a random number.
            self.name = 'RandomForest'+ '_' + get_ts()
        else:
            # Set the name of the matcher, with the given name.
            self.name = name
        # Set the classifier to the scikit-learn classifier.
        self.clf = RandomForestClassifier(*args, **kwargs)",py_entitymatching/matcher/rfmatcher.py,anhaidgroup/py_entitymatching,1
"indices = np.random.permutation(len(X))
X_train = X[indices[:-10]]
y_train = y[indices[:-10]]
X_test  = X[indices[-10:]]
y_test  = y[indices[-10:]]


from sklearn.ensemble import RandomForestClassifier

#training model using Random Forest Classification algorithm
rfc = RandomForestClassifier(n_estimators=10)
rfc.fit(X_train, y_train)

#Printing the score of our model which is close 98% is good
print rfc.score(X_train, y_train)

#predict the test data which we got from line 17
print rfc.predict(X_test)

#try and get a predication for below academic marks in an order",student_stream_predict.py,bikashsharmabks/stream_prediction_machine_learning,1
"from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import label_binarize
from sklearn.cross_validation import train_test_split
from sklearn.cross_validation import KFold
from sklearn.cross_validation import StratifiedKFold
from sklearn.cross_validation import cross_val_score
from sklearn.multiclass import OneVsRestClassifier

rdf = RandomForestClassifier(n_estimators=100)
ada = OneVsRestClassifier(AdaBoostClassifier())
lsvc = SVC(kernel=""linear"", C=4, probability=True)
rsvc = SVC(probability=True, C=32, gamma=0.125) # tuned HP for HELA
log = LogisticRegression()
lda = LDA()
knn = KNeighborsClassifier()

# estimators=[('lr', rdf), ('rf', log), ('lda', lda)]
estimators=[('l',rdf),('lda', lda),('svc',rsvc),('lsvc',lsvc)]",sklearn-server/app.py,daviddao/luminosity,1
"linear_acc = accuracy_score(Y_test, pred_linear_svc)

KNN = OneVsRestClassifier(KNeighborsClassifier(n_neighbors = 3)).fit(X_train, Y_train)
pred_KNN = KNN.predict(X_test)
KNN_acc = accuracy_score(Y_test, pred_KNN)

Decision_tree = OneVsRestClassifier(DecisionTreeClassifier()).fit(X_train, Y_train)
pred_decision_tree = Decision_tree.predict(X_test)
decision_tree_acc = accuracy_score(Y_test, pred_decision_tree)

random_forest = OneVsRestClassifier(RandomForestClassifier(n_estimators=100)).fit(X_train, Y_train)
pred_random_forest = random_forest.predict(X_test)
random_forest_acc = accuracy_score(Y_test, pred_random_forest)

#Neural Network (Fully Connected)
#Inputs need to be numpy arrays
train = X.values
classes = Y
#Making fully connected neural network. Could implement regularization with dropout, but seems to not be needed
#   as acc and val_acc are similar",Twitter_Hashtags/tweet_analysis.py,MichaelMKKang/Projects,1
"    labelIndexer = StringIndexer(inputCol=""label"", outputCol=""indexedLabel"").fit(data)
    # Automatically identify categorical features, and index them.
    # Set maxCategories so features with > 4 distinct values are treated as continuous.
    featureIndexer =\
        VectorIndexer(inputCol=""features"", outputCol=""indexedFeatures"", maxCategories=4).fit(data)

    # Split the data into training and test sets (30% held out for testing)
    (trainingData, testData) = data.randomSplit([0.7, 0.3])

    # Train a RandomForest model.
    rf = RandomForestClassifier(labelCol=""indexedLabel"", featuresCol=""indexedFeatures"")

    # Chain indexers and forest in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])

    # Train model.  This also runs the indexers.
    model = pipeline.fit(trainingData)

    # Make predictions.
    predictions = model.transform(testData)",NodeLocalSpark/examples/src/main/python/ml/random_forest_classifier_example.py,medicalinformaticscorp/nodeLocalRDD,1
"
# Split Train / Test
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.20, random_state=36)

# First, we will train and apply a Random Forest WITHOUT calibration
# we use a BaggingClassifier to make 5 predictions, and average
# because that's what CalibratedClassifierCV do behind the scene,
# and we want to compare things fairly, i.e. be sure that averaging several models 
# is not what explains a performance difference between no calibration, and calibration.

clf = RandomForestClassifier(n_estimators=50, n_jobs=-1)
clfbag = BaggingClassifier(clf, n_estimators=5)
clfbag.fit(Xtrain, ytrain)
ypreds = clfbag.predict_proba(Xtest)
print(""loss WITHOUT calibration : "", log_loss(ytest, ypreds, eps=1e-15, normalize=True))

# Now, we train and apply a Random Forest WITH calibration
# In our case, 'isotonic' worked better than default 'sigmoid'
# This is not always the case. Depending of the case, you have to test the two possibilities
",calibration.py,AurelienGalicher/DStoolkit,1
"    bounds = [[1, 25]]
    xmax = 10  # FIXME - should this not be optional?

    @staticmethod
    def _f(x):
        # iris = load_iris()
        X, y = X, y = make_hastie_10_2(random_state=0)
        x = np.ravel(x)
        f = np.zeros(x.shape)
        for i in range(f.size):
            clf = RandomForestClassifier(n_estimators=1, min_samples_leaf=int(np.round(x[i])), random_state=0)
            # scores = cross_val_score(clf, iris.data, iris.target)
            scores = cross_val_score(clf, X, y, cv=5)
            f[i] = -scores.mean()
        return f.ravel()


from multiprocessing import Process, Queue, Manager, Array
from Queue import Empty as q_Empty
",sandpit.py,jamesrobertlloyd/automl-phase-2,1
"        self.SetStartDate(2013, 10, 11)  #Set End Date
        self.AddEquity(""SPY"", Resolution.Daily)

        # numpy test
        print ""numpy test >>> print numpy.pi: "" , numpy.pi
        
        # scipy test: 
        print ""scipy test >>> print mean of 1 2 3 4 5:"", scipy.mean(numpy.array([1, 2, 3, 4, 5]))

        #sklearn test
        print ""sklearn test >>> default RandomForestClassifier:"", RandomForestClassifier()
        
        # cvxopt matrix test
        print ""cvxopt >>>"", cvxopt.matrix([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], (2,3))

        # blaze test
        blaze_test()
        
        # cvxpy test
        cvxpy_test()",Algorithm.Python/PythonPackageTestAlgorithm.py,Jay-Jay-D/LeanSTP,1
"
# TASK 3: split the data into training and testing sets
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# TASK 4: fit a logistic regression model and examine the coefficients
from sklearn.ensemble import RandomForestClassifier



rf = RandomForestClassifier()
rf.fit(X_train, y_train)
rfProp =zip(X.columns, rf.feature_importances_)
pd.DataFrame(rfProp).to_clipboard()


from sklearn import metrics 
# predict and calculate AUC 
y_pred_prob = rf.predict_proba(X_test)[:, 1]
",code/logRegrRF.py,andretadeu/jhu-immuno,1
"#                               Models
#==============================================================================

def xgBoost():
    clf = xgb.XGBClassifier(max_depth = 8,n_estimators=300,nthread=8,seed=123,
                            silent=1,objective= 'multi:softmax',learning_rate=0.1,
                            subsample=0.9)
    return clf

def randomForest():
    clf = RandomForestClassifier(max_depth=8, n_estimators=500,n_jobs=8,
                                 random_state=123)
    return clf

def extraTrees():
    clf = ExtraTreesClassifier(max_depth=8, n_estimators=500,n_jobs=8,random_state=123)
    return clf

def kNN():
    clf = KNeighborsClassifier(n_neighbors=3,n_jobs=8)",backend.py,Waffleboy/Predict-Me-Now,1
"test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1) 


# The data is now ready to go. So lets fit to the train, then predict to the test!
# Convert back to a numpy array
train_data = train_df.values
test_data = test_df.values


print 'Training...'
forest = RandomForestClassifier(n_estimators=100)
forest = forest.fit( train_data[0::,1::], train_data[0::,0] )

print 'Predicting...'
output = forest.predict(test_data).astype(int)


predictions_file = open(""myfirstforest.csv"", ""wb"")
open_file_object = csv.writer(predictions_file)
open_file_object.writerow([""PassengerId"",""Survived""])",data/myfirstforest.py,liuslevis/Titanic,1
"print(""\nclassification report : \n\n"",metrics.classification_report(Test_Data_labels,Predicted_Train,labels=[0,1]))
print(""**********************************************************************"")

accuracy.append(metrics.accuracy_score(Test_Data_labels,Predicted_Train))
precision.append(metrics.precision_score(Test_Data_labels,Predicted_Train))
recall.append(metrics.recall_score(Test_Data_labels,Predicted_Train))
f1.append(metrics.f1_score(Test_Data_labels,Predicted_Train,labels=[0,1]))

#################################################

Model = RandomForestClassifier(random_state=30).fit(Train_Data,Train_Data_labels)

print(""Result on PCA with 25 components :"")
print(""Random Forest:"")
Predicted_Train = Model.predict(Train_Data)
print(""Accuracy on Training Data :"",metrics.accuracy_score(Train_Data_labels,Predicted_Train))

Predicted_Train = Model.predict(Test_Data)
print(""Accuracy on Testing Data :"",metrics.accuracy_score(Test_Data_labels,Predicted_Train))
print(""\nconfusion_matrix : \n"",metrics.confusion_matrix(Test_Data_labels,Predicted_Train,labels=[0,1]))",IDS/Algorithms(4).py,danialjahed/IDS-KDDcup,1
"        folded_dataset = self.create_folded_dataset(window_size)
        indim = 21 * (2 * window_size + 1)
        mean_AUC = 0
        mean_decision_value = 0
        mean_mcc = 0
        sample_size_over_thousand_flag = False
        for test_fold in xrange(self.fold):
            test_labels, test_dataset, train_labels, train_dataset = folded_dataset.get_test_and_training_dataset(test_fold)
            if len(test_labels) + len(train_labels) > 1000:
                sample_size_over_thousand_flag = True
            clf = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features)
            clf.fit(train_dataset, train_labels)
            probas = clf.predict_proba(test_dataset)
            decision_values = map(lambda x: x[1], probas) # Probability of being binding residue
            AUC, decision_value_and_max_mcc = validate_performance.calculate_AUC(decision_values, test_labels)
            mean_AUC += AUC
            mean_decision_value += decision_value_and_max_mcc[0]
            mean_mcc += decision_value_and_max_mcc[1]
            if sample_size_over_thousand_flag:
                break",cross_validation.py,clclcocoro/MLwithGA,1
"                            vprint(""{} data type, using a Classifier"".format(data.dtypes[k]), file=fp_log)
                            if classifier == 'LR':
                                if len(np.unique(y.astype(""int64""))) == 1:
                                    vprint(""Only 1 class in training set, will use majority class"", file=fp_log)
                                    h = dummy.DummyClassifier(strategy=""most_frequent"")
                                else:
                                    h = LogisticRegression(n_jobs=-1)
                            elif classifier == 'DT':
                                h = DecisionTreeClassifier(max_depth=5)
                            elif classifier == 'RF':
                                h = RandomForestClassifier(max_depth=4,
                                                           n_estimators=100,
                                                           random_state=1,
                                                           n_jobs=-1)
                            elif classifier == 'ET':
                                h = ExtraTreesClassifier(n_estimators=100,
                                                         max_features=""auto"",
                                                         criterion='entropy',
                                                         min_samples_split=4,
                                                         max_depth=35,",code/cascade.py,jacobmontiel/cim,1
"if options.TRAINING_INPUT_DIRECTORY is not None:
    TRAINING_INPUT_DIRECTORY = options.TRAINING_INPUT_DIRECTORY

# From demonstration , we only used  4 of 5 histrical data for training
train_mat = genfromtxt(TRAINING_INPUT_DIRECTORY+'/traing_matrix.csv', delimiter=',')
# First column is the lable column
y = train_mat[:, 0]
X = train_mat[:, 1:]

# build a classifier, in this demo we use Random Forest, you can switch to any other classifier
clf = RandomForestClassifier(n_estimators=50)

# Utility function to report best scores


def report(grid_scores, n_top=3):
    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]
    for i, score in enumerate(top_scores):
        print(""Model with rank: {0}"".format(i + 1))
        print(""Mean validation score: {0:.3f} (std: {1:.3f})"".format(",model/random-forest-model.py,BrakeValve/brake-valve-server,1
"    fp = cf.getNumpyFP(line[3], fpname, 'float')
    if fp is not None:
        fps_inact.append(fp)
num_inactives = len(fps_inact)
print ""inactives read and fingerprints calculated:"", num_inactives

# training 
train_fps = fps_act + fps_inact
ys_fit = [1]*num_actives + [0]*num_inactives
# train the model
ml = RandomForestClassifier(n_estimators=100, max_depth=100, min_samples_split=2, min_samples_leaf=1)
ml.fit(train_fps, ys_fit)

# write the model to file
outfile = gzip.open(path+'rf_'+fpname+'_model.pkl.gz', 'wb')
cPickle.dump(ml, outfile, 2)
outfile.close()",final_models/train_RF_model.py,sriniker/TDT-tutorial-2014,1
"minmaxscale=preprocessing.MinMaxScaler(feature_range=(0,1))
train_data=minmaxscale.fit_transform(train_data)
test_data=minmaxscale.fit_transform(test_df.values)

print np.shape(train_data)
print np.shape(test_data)


print 'Training...'
#from sklearn.ensemble import RandomForestClassifier
#clf = RandomForestClassifier(n_estimators=100)#0.8035

#from sklearn.neighbors import KNeighborsClassifier  
#clf=KNeighborsClassifier(n_neighbors=7)#0.7800

from sklearn import svm   
clf=svm.SVC(C=10,gamma=0.0029)#0.7867
#clf=GridSearchCV(svm.SVC(), param_grid={""C"":np.logspace(-2, 10, 13),""gamma"":np.logspace(-9, 3, 13)})
#output = clf.fit( train_data[0::,1::], train_data[0::,0] ).predict(test_data).astype(int)
#print(""The best parameters are %s with a score of %0.2f""% (knnClf.best_params_, knnClf.best_score_))",my_1.py,zlykan/my_test,1
"                    Additional arguments to
                    `sklearn.ensemble.RandomForestClassifier`
            """"""
            
            logger.info('Extracting features...')
            features, classes =  cls._prepare_observations(feature_extractor,
                                                           train_set,
                                                           assessments)
            
            logger.info('Constructing an RF model.')
            rf_model = RandomForestClassifier(criterion=criterion,
                                              **kwargs)
            
            
            logger.info(""Training the model..."")
            rf_model.fit(features, classes['class'])
            logger.info(""Model training complete."")
            
            return cls(rf_model, feature_extractor, assessments)
        ",wikiclass/models/rf_text.py,wikimedia/Wiki-Class,1
"# then transforms the strings into their corresponding integer classes
# target now contains list of integers, one for each data point
# where integer maps to flower species name
target = le.fit_transform(target)

(trainData, testData, trainTarget, testTarget) = train_test_split(data,
                                                                  target,
                                                                  test_size=0.3,
                                                                  random_state=42)

model = RandomForestClassifier(n_estimators=25, random_state=84)
model.fit(trainData, trainTarget)

print(classification_report(testTarget, model.predict(testData),
                            target_names=targetNames))",plantClassification/classify.py,johnshiver/imageLab,1
"	true 	= sum(scores)
	acc 	= sum(scores)/len(scores)'''

	cv 		= cross_validation.LeaveOneOut(n=len(y))
	y_pred 	= [0] * len(y)
	for train, test in cv:
		X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]
		clf 	= naive_bayes.GaussianNB()
		#clf 	= svm.LinearSVC()
		#clf = tree.DecisionTreeClassifier()
		#clf = RandomForestClassifier(n_estimators=10)
		#clf = NearestCentroid()

		clf.fit(X_train, y_train)
		pred 	= clf.predict(X_test)[0]
		ind 	= test[0]
		y_pred[ind] = pred
	y 		= y.tolist()
	true 	= len([1 for i in range(len(y)) if y[i]==y_pred[i]])
	true_r 	= len([1 for i in range(len(y)) if y[i]==1 and y[i]==y_pred[i]])",scripts/predict_rivalry_net/src/predict_rivalry.py,nbir/gambit-scripts,1
"    return parameters

def setup_opencv_boost(n_trees, max_depth):
    parameters = {""boost_type"": BOOST_REAL,
                  ""weak_count"": n_trees,
                  ""weight_trim_rate"": 0,
                  ""max_depth"": max_depth}
    return parameters

def setup_sklearn_randomForest(n_trees, n_features, max_depth, min_samples_at_leaf):
    forest = RandomForestClassifier(n_estimators=n_trees,
                                    criterion='gini',
                                    max_features=n_features,
                                    max_depth=max_depth,
                                    min_samples_leaf=min_samples_at_leaf,
                                    min_samples_split=2*min_samples_at_leaf,
                                    bootstrap=False)
    return forest

def setup_sklearn_adaBoost(n_trees, n_features, max_depth, min_samples_at_leaf, algorithm):",examples/python/compare_boosting_implementations.py,classner/fertilized-forests,1
"
  classifiers = [
      (""Dummy"", DummyClassifier(strategy='stratified')), # see http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html
      # (""Linear SVM"", SVC(kernel=""linear"", C=0.025)),
      # (""RBF SVM"", SVC(gamma=2, C=1)),
      #(""Decision Tree"", DecisionTreeClassifier(max_depth=5)),
      #(""Nearest Neighbors"", KNeighborsClassifier(3)),
      # (""AdaBoost"", AdaBoostClassifier()),
      (""Naive Bayes"", GaussianNB()),
      (""LogisticRegression"", LogisticRegression()),
      (""Random Forest"", RandomForestClassifier(n_estimators=100)),
      #(""XGBoost"", xgb.XGBClassifier())
      ] # TODO: xgboost

  ###############################################################
  # Initial configuration
  np.random.seed(SEED)
  logg.configure_logging() # For more details use logg.configure_logging(console_level=logging.DEBUG)

  ################################################################",scripts/pipeline/model_building.py,prikhodkop/ECG_project,1
"    num_estimators=10000
    for formula_name, formula in formula_map.iteritems():

        print ""name=%s formula=%s"" % (formula_name,formula)

        y_train,X_train = dmatrices('Survived ~ ' + formula, 
                                    train_df_filled,return_type='dataframe')
        print ""Running RandomForestClassifier with formula : %s"" % formula
        print ""X_train cols=%s "" % X_train.columns
        y_train = np.ravel(y_train)
        model = RandomForestClassifier(n_estimators=num_estimators, random_state=0)
        print ""About to fit...""
        rf_model = model.fit(X_train, y_train)
        print ""Training score:%s"" % rf_model.score(X_train,y_train)
        X_test=dmatrix(formula,test_df_filled)
        predicted=rf_model.predict(X_test)
        print ""predicted:%s"" % predicted[:5]
        assert len(predicted)==len(test_df)
        pred_results=pd.Series(predicted,name='Survived')
        rf_results=pd.concat([test_df['PassengerId'],pred_results],axis=1)",MasteringPandas/2060_11_Code/run_rf_titanic.py,moonbury/pythonanywhere,1
"	# Get data
	data = pd.read_csv(data_path)
	x = data.ix[:, :-1].as_matrix()
	y = data.ix[:, -1].as_matrix()
	x, y = convert_data_to_int(x, y)
	
	# Run random forest in parallel
	sss = StratifiedShuffleSplit(y, n_iter=nsplits, train_size=pct_train,
		random_state=seed)
	results = Parallel(n_jobs=-1)(delayed(train_score_clf)(
		RandomForestClassifier(random_state=i), x[tr], x[te], y[tr], y[te])
		for i, (tr, te) in enumerate(sss))
	print 'Random Forest: {0:.3f} %'.format(np.median(results))
	
	# Run SVM in parallel
	sss = StratifiedShuffleSplit(y, n_iter=nsplits, train_size=pct_train,
		random_state=seed)
	results = Parallel(n_jobs=-1)(delayed(train_score_clf)(
		LinearSVC(random_state=i), x[tr], x[te], y[tr], y[te])
		for i, (tr, te) in enumerate(sss))",dev/car_evaluation/car_eval.py,tehtechguy/mHTM,1
"	test_data = test_data[1:]
	for instance in test_data:
		instance[3] = int(instance[3])
#========================================================================
	#start train with SVM
	if model_name == 'SVM':
		clf = svm.SVC()
	if model_name == 'decision_tree':
		clf = tree.DecisionTreeClassifier()
	if model_name == 'random_forest':
		clf = RandomForestClassifier()
	if model_name == 'logistic_regression':
		clf = LogisticRegression()
	if model_name == 'linear_regression':
		clf = LinearRegression()
		for instance in train_data:
			for feature_idx in range(len(train_data[0])):
				instance[feature_idx] = int(instance[feature_idx])

	#calc ratio between traning instances and test instances",train_and_test.py,samfu1994/cs838webpage,1
"from sklearn.ensemble import RandomForestClassifier

def RF_predict(rf, data):
  predictions = rf.predict_proba(data)
  return [ypred.argmax() for index, ypred in enumerate(predictions)]
  
def RF_learn(Xtrain, ytrain, nr_of_trees=10, n_jobs=-1):
  rf = RandomForestClassifier(n_estimators=nr_of_trees, n_jobs=n_jobs)
  rf.fit(Xtrain, ytrain)
  return rf
",rf.py,misza222/kaggle-digit-recognizer,1
"
names = [""Nearest Neighbors"", ""RBF SVM"",
         ""Decision Tree"", ""Random Forest"", ""AdaBoost"",
         ]

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)

classifiers = [
    KNeighborsClassifier(3),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    RandomForestClassifier(random_state=1),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()
	]
",scripts/05-histogram-random-forest.py,jmrozanec/white-bkg-classification,1
"import numpy as np
import docopt
import h5py
import utils


class Classifier(object):

    def __init__(self, n_features=-1):
        self.classifiers = [
            ens.RandomForestClassifier(n_estimators=400, n_jobs=-1),
#            ens.BaggingClassifier(ens.RandomForestClassifier(n_estimators=150, n_jobs=-1), max_samples=0.5, max_features=0.5),
#            neighbors.KNeighborsClassifier(400)
#            ens.BaggingClassifier(neighbors.KNeighborsClassifier(400), max_samples=0.5, max_features=0.5, n_jobs=-1)
        ]
        self.n_features = n_features

    def fit(self, X, y):
        print(""Train classifier"")
        print(""Shape:"", X.shape)",ndsbkaggle/train.py,EmanuelaBoros/kaggle-national-data-science-bowl-,1
"    svc = SVC(kernel=""poly"", degree=3)
    svc.fit(x_train, y_train)
    y_test = svc.predict(x_test)
    train_score = svc.score(x_train, y_train)
    scores = cross_val_score(svc, x_train, y_train, cv=5)

    return (y_test, scores.mean(), scores.std(), train_score)

def run_random_forest(x_train, y_train, x_test):
    global max_features, n_estimators
    rfc = RandomForestClassifier(n_estimators=34, max_features=max_features, criterion='gini', max_depth=5)
    rfc.fit(x_train, y_train)
    y_test = rfc.predict(x_test)
    train_score = rfc.score(x_train, y_train)
    scores = cross_val_score(rfc, x_train, y_train, cv=5)

    return (y_test, scores.mean(), scores.std(), train_score)

def write_result(y_test):
    submission = pd.DataFrame({",titanic/titanic.py,shawpan/kaggle,1
"import os, sys, codecs, re
import cPickle
import lemmatizer

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn import tree, svm
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

classifiers = {
				'rf': RandomForestClassifier(n_estimators = 150, min_samples_split = 2, n_jobs = -1),
				'dt': tree.DecisionTreeClassifier(), 
				'svm': svm.SVC(),
				'part': ExtraTreesClassifier(n_estimators = 30, min_samples_split = 2, n_jobs = -1),
				'featureselection': Pipeline([
					('feature_selection', LinearSVC(penalty=""l1"", dual = False)),
					('classification', RandomForestClassifier())
					])
			}
",anaphoramllib.py,max-ionov/russian-anaphora,1
"t0 = time()
pred = clf.predict(features_test)
print ""predicting time:"", round(time()-t0, 3), ""s""
acc = accuracy_score(pred, labels_test)
print acc


# Random Forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
clf = RandomForestClassifier(n_estimators=10)

t0 = time()
clf.fit(features_train, labels_train)
print ""training time:"", round(time()-t0, 3), ""s""

t0 = time()
pred = clf.predict(features_test)
print ""predicting time:"", round(time()-t0, 3), ""s""
acc = accuracy_score(pred, labels_test)",intro_to_machine_learning/lesson/lesson_4_choose_your_own_algorithm/your_algorithm.py,tuanvu216/udacity-course,1
"print ""SVC log loss: "", ll_tst

conf_svm = plot_confusion(sl)

#Neural net
trndata, tstdata = createDataSets(sl.X_train_scaled, Y_train, sl.X_test_scaled, Y_test)
fnn = train(trndata, tstdata, epochs = 1000, test_error = 0.025, momentum = 0.15, weight_decay = 0.0001)

sl_ccrf = SKSupervisedLearning(CalibratedClassifierCV, X, Y_train, Xt, Y_test)
sl_ccrf.train_params = \
    {'base_estimator': RandomForestClassifier(**{'n_estimators' : 7500, 'max_depth' : 200}), 'cv': 10}
sl_ccrf.fit_standard_scaler()
ll_ccrf_trn, ll_ccrf_tst = sl_ccrf.fit_and_validate()

print ""Calibrated log loss: "", ll_ccrf_tst
conf_ccrf = plot_confusion(sl_ccrf)

#predicted = cross_val_predict(SVC(**sl.train_params), sl.X_train_scaled, n_jobs = -1, y = Y_train, cv=10)

#fig,ax = plt.subplots()",Learning/svm_experiments.py,fierval/KaggleMalware,1
"import numpy as np
from sklearn.metrics import log_loss
import pandas as pd

def getModel():
    return joblib.load('10_trees_rf')

if __name__ == '__main__':
    train = Validation.getData(1)

    model = RandomForestClassifier(n_estimators = 10, max_depth=16, oob_score=True, n_jobs=3)
    
    hours, days, districts, seasons = ModelTrainer.getFeatureDummies(train)
    train_data = pd.concat([hours, days, districts, seasons], axis=1)
    
    model = ModelTrainer.trainModel(model, train)
    
#     scores = cross_validation.cross_val_score(model, train_data[features], train['crime'], cv=5)
    
    joblib.dump(model, '10_trees_rf')",sf_crime/RF_10_trees.py,KenerChang/kaggle,1
"            np.hstack(training_label).astype(int), [0, 255]))
        print 'Create the training set ...'

        # Learn the PCA projection
        pca = SparsePCA(n_components=sp)
        training_data = pca.fit_transform(training_data)
        testing_data = pca.transform(testing_data)

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_sp.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/selection-extraction/sparse-pca/pipeline_classifier_dce.py,I2Cvb/mp-mri-prostate,1
"                        if scale == 'minmax':
                            X = MinMaxScaler().fit_transform(X)
                        elif scale == 'standard':
                            X = StandardScaler().fit_transform(X)

                    B = {}
                    classifier = qry['classifier']
                    if classifier == 'forest':
                        n_estimators = 100
                        max_features = None
                        B = {y: RandomForestClassifier(n_estimators=n_estimators,
                                        max_features=max_features, n_jobs=-1,
                                        random_state=0) for y in labels}
                    elif classifier == 'logreg':
                        B = {y: LogisticRegression(class_weight='auto',
                                        random_state=0) for y in labels}
                    elif classifier == 'tree':
                        B = {y: DecisionTreeClassifier(max_features=None,
                                        random_state=0) for y in labels}
",vizdom_demo/demo.py,feiyanzhandui/tware,1
"    syncer_obj.add_tag(x_test, ""testing data"")
    syncer_obj.add_tag(x_train, ""training data"")
    # First, we will train and apply a Random Forest WITHOUT calibration
    # we use a BaggingClassifier to make 5 predictions, and average
    # because that's what CalibratedClassifierCV do behind the scene,
    # and we want to compare things fairly, i.e. be sure that averaging several
    # models
    # is not what explains a performance difference between no calibration,
    # and calibration.

    clf = RandomForestClassifier(n_estimators=50, n_jobs=-1)

    clfbag = BaggingClassifier(clf, n_estimators=5)
    clfbag.fit_sync(x_train, y_train)

    y_preds = clfbag.predict_proba_sync(x_test)

    SyncableMetrics.compute_metrics(
        clfbag, log_loss, y_test, y_preds, x_test, """", """", eps=1e-15,
        normalize=True)",client/python/samples/sklearn/OttoGroup-Calibration.py,mitdbg/modeldb,1
"        k_neighboors = KNeighborsClassifier()
        n_neighbors = [3, 5, 11, 21, 31]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(k_neighboors, dict(n_neighbors=n_neighbors), X, y)
        f = open('output/religiousity.knn.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates Random Forest classifier
        random_forest = RandomForestClassifier()
        n_estimators = [2, 3, 5, 10, 20, 40, 60]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(random_forest, dict(n_estimators=n_estimators), X, y)
        f = open('output/religiousity.randomforest.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates MLP classifier",tests/test_religiousity.py,fberanizo/author-profiling,1
"X.drop(labels=['user', 'class'], axis=1, inplace=True)
print(X.describe())

#
# INFO: An easy way to show which rows have nans in them
print(X[pd.isnull(X).any(axis=1)])

#
# TODO: Create an RForest classifier 'model' and set n_estimators=30,
# the max_depth to 10, and oob_score=True, and random_state=0
model = RandomForestClassifier(n_estimators=30, max_depth=10, oob_score=True, random_state=0)

# 
# TODO: Split your data into test / train sets
# Your test size can be 30% with random_state 7
# Use variable names: X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)

print('Fitting...')
s = time.time()",Module6/assignment6.py,Wittlich/DAT210x-Python,1
"#     #           multi_class='ovr', fit_intercept=True, intercept_scaling=1,
#     #           class_weight=None, verbose=VERBOSE, random_state=None, max_iter=1000),
#     # SVC(kernel=""linear"", C=0.025, verbose=VERBOSE), # ~47% acc, takes about 5 min to run on 20000 inputs
#     # SVC(gamma=2, C=1, verbose=VERBOSE), # takes too long on 20000 inputs
#     SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1, eta0=0.0,    # ~0.41 acc, 3 sec
#                   fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge',
#                   n_iter=5, n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
#                   shuffle=True, verbose=VERBOSE, warm_start=False),
#     # GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),     # takes too long
#     DecisionTreeClassifier(max_depth=None),     # 0.41 acc, 20 sec
#     RandomForestClassifier(n_estimators=10, criterion='gini', max_features='auto',      # ~0.49 acc, 12 sec
#                            min_samples_split=2, verbose=VERBOSE),
#     MLPClassifier(alpha=1, verbose=VERBOSE),        # 0.48 acc, 29 sec
#     # AdaBoostClassifier(),       # 0.30 acc, 72 sec
#     # GaussianNB(),       # 0.09 acc, 1 sec
#     # QuadraticDiscriminantAnalysis(),     # 0.18 acc, 2 sec
#     LinearDiscriminantAnalysis()        # 0.46 acc, 2 sec
# ]

NAMES = [",src/ml/voting_classifier.py,seokjunbing/cs75,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans10_lr_only.py,diogo149/CauseEffectPairsPaper,1
"
    train_navigation_classifier()


classifier = None

def train_navigation_classifier():
    global classifier

    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier()

    rf.fit(navigation_examples.keys(), navigation_examples.values())
    
    classifier = rf


def classify_navigation(frames):
    if classifier is None:
        return _classify_navigation_fallback(frames)",pdf_decanter/decomposer.py,hmeine/pdfdecanter,1
"names = [""Nearest Neighbors"", ""Linear SVM"", ""Gaussian SVM"", ""Polynomial SVM"", ""Decision Tree"",
		 ""Random Forest"", ""AdaBoost Classifier"", ""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	KNeighborsClassifier(n_neighbors=15, weights='distance'),
	SVC(kernel=""linear"", C=3.4),
	SVC(kernel=""rbf"", C=3.4, gamma=0.1),
	SVC(kernel=""poly"", C=3.4, degree=2, gamma=0.1),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]



def main():
	#ignore all warnings
	#warnings.filterwarnings(""ignore"")",UMKL/classifiers.py,akhilpm/Masters-Project,1
"  alignImages=False
  
  #checkSubmission('/home/loschen/programs/cxxnet/example/kaggle_bowl/cxx_standard2.csv')
  #sys.exit(1)
  
  Xtrain, ytrain, class_names,Xtest = prepareData(subset=subset,loadTempData=loadTempData,extractFeatures=extractFeatures,maxPixel = maxPixel,doSVD=doSVD,subsample=subsample,nudgeData=nudgeData,dilation=dilation,kmeans=dokmeans,randomRotate=randomRotate,useOnlyFeats=useOnlyFeats,stripFeats=stripFeats,createExtraFeatures=createExtraFeatures,convolution=convolution,alignImages=alignImages,standardize=standardize,location=location)
  Xtrain = scaleData(Xtrain)
  print ""Xtrain dtype:"",Xtrain.dtypes[0]
  print ""ytrain dtype:"",ytrain.dtype
  
  model =  RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=5,n_jobs=5,criterion='gini', max_features='auto',oob_score=False)
  #model = SGDClassifier(loss=""log"", eta0=1.0, learning_rate=""constant"",n_iter=5, n_jobs=4, penalty=None, shuffle=False)#~percetpron
  
  #model = DBN([Xtrain.shape[1], 500, -1],learn_rates = 0.3,learn_rate_decays = 0.9,epochs = 30,verbose = 1)#2.15
  #model = GradientBoostingClassifier(loss='deviance',n_estimators=100, learning_rate=0.1, max_depth=2,subsample=.5,verbose=False)
  #model = LogisticRegression(C=1.0)#3.02
  #print Xtrain.describe()
  #model = SGDClassifier(alpha=0.01,n_iter=250,shuffle=True,loss='log',penalty='l2',n_jobs=4,verbose=False)#mll=3.0
  #model = Pipeline(steps=[('rbm', BernoulliRBM(n_components =300,learning_rate = 0.1,n_iter=15, random_state=0, verose=True)), ('lr', LogisticRegression())])
  #model = LDA()#4.84",competition_scripts/plankton.py,chrissly31415/amimanera,1
"        target[nqso:nqso+nstar]=0

    #----------------------------------------------------------
    #   Start the training
    #----------------------------------------------------------

        print('training over ',nqso,' qsos and ',nstar,' stars')

        print('with random Forest')
        np.random.seed(0)
        rf = RandomForestClassifier(200)
        rf.fit(data, target)
        joblib.dump(rf, modelDir+'rf_model_dr3.pkl.gz',compress=9)
        np.random.seed(0)
        rf.fit(data[:,0:9], target)
        joblib.dump(rf, modelDir+'rf_model_normag_dr3.pkl.gz',compress=9)

        print('with adaBoost')
        ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=8),
                         algorithm=""SAMME.R"",",py/desitarget/train/train_mva_decals.py,desihub/desitarget,1
"        :param model_field:
        :param n_estimators:
        :param max_depth:
        :param _id:
        :param min_samples_split:
        :param max_features:
        :param window_size:
        :return:
        """"""
        if model_field is None:
            model = RandomForestClassifier(n_estimators=n_estimators,
                                           max_depth=max_depth,
                                           min_samples_split=min_samples_split,
                                           max_features=max_features, )
        else:
            model = model_field
        self.window_size = window_size
        self.labels = labels
        self.trained = False
        super(RandomForestModel, self).__init__(model_type=""RandomForestClassifier"",",DB/classification/RandomForestModel.py,seadsystem/Backend,1
"        training_label = [arr[idx_imb] for idx_arr, arr in enumerate(label_bal)
                         if idx_arr != idx_lopo_cv]
        # Concatenate the data
        training_data = np.vstack(training_data)
        training_label = label_binarize(np.hstack(training_label).astype(int),
                                        [0, 255])
        print 'Create the training set ...'

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_bal.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/balancing/pipeline_classifier_aggreagation.py,I2Cvb/mp-mri-prostate,1
"Perceptron(fit_intercept=True, n_iter=5, shuffle=False),
#linear_model.Ridge(alpha = .5),
svm.LinearSVC(),
svm.SVR(),
SGDClassifier(loss=""hinge"", penalty=""l2""),
SGDClassifier(loss=""log""),
KNeighborsClassifier(n_neighbors=2),
KNeighborsClassifier(n_neighbors=6),
KNeighborsClassifier(n_neighbors=10),
NearestCentroid(), 
RandomForestClassifier(n_estimators=2), 
RandomForestClassifier(n_estimators=10), 
RandomForestClassifier(n_estimators=18), 
RandomForestClassifier(criterion=""entropy"", n_estimators=2), 
RandomForestClassifier(criterion=""entropy"", n_estimators=10), 
RandomForestClassifier(criterion=""entropy"", n_estimators=18), 
AdaBoostClassifier(n_estimators=50), 
AdaBoostClassifier(n_estimators=100), 
AdaBoostClassifier(learning_rate= 0.5, n_estimators=50), 
AdaBoostClassifier(learning_rate= 0.5, n_estimators=100), ",predictor.py,DGaffney/stop_sign_project,1
"    *args, **kwargs configure the classifier.

    Output is the prediction.""""""

    assert data.shape[:-1] == segmentation.shape
    shape = data.shape[:-1]
    n_instances = np.product(shape)
    n_features = data.shape[-1]
    n_labels = len(np.unique(segmentation))

    rf = RandomForestClassifier(**kwargs)
    rf = rf.fit(data.reshape(n_instances, n_features),
                segmentation.reshape(n_instances))

    probs = rf.predict_proba(data.reshape(n_instances, n_features))
    probs = probs.reshape(*(list(shape) + [n_labels]))

    return probs

",SegmentationQuality/RandomForestReproduce.py,jenspetersen/Experiments,1
"		print depth
		dt = DecisionTreeClassifier(max_depth=depth)
		dt.fit(x_train, y_train)
		predictions = dt.predict(x_test)
		print metrics.accuracy_score(y_test, predictions)

# # random forests
# for depth in [2, 5, 10]:
# 	for n_trees in [10, 20, 50, 100, 200, 500, 1000]:
# 		print depth, n_trees
# 		rf = RandomForestClassifier(n_estimators=n_trees, max_depth=depth)
# 		rf.fit(x_train, y_train)
# 		predictions = rf.predict(x_test)
# 		print metrics.accuracy_score(y_test, predictions)

# for depth in [10, 20]:
# 	for n_trees in [1000, 5000]:
# 		print depth, n_trees
# 		xgb = xgboost.XGBClassifier(max_depth=depth, n_estimators=n_trees)
# 		xgb.fit(x_train, y_train)",BoW.py,HristoBuyukliev/fakenews,1
"    'generated', 'df_full_features.pickle'))

# Training is first two debates, testing is last
df_train = df.query('Date != ""10/19/2016""')
df_test = df.query('Date == ""10/19/2016""')

feature_columns_to_ignore = [
    'Line', 'Speaker', 'Text', 'Date', 'TextClean']


clf = RandomForestClassifier(
    n_estimators=100, max_depth=10, max_features=500, random_state=451)
clf_fit = clf.fit(
    df_train.drop(feature_columns_to_ignore, axis=1), df_train.Speaker)

# print(df_test.Speaker[:10])
# print(clf_fit.predict(df_test.drop(
#     feature_columns_to_ignore, axis=1))[:10])
# print(clf_fit.predict_proba(df_test.drop(
#     feature_columns_to_ignore, axis=1))[:10])",model.py,user01/PresidentialDebates,1
"# load data
Ztr = np.loadtxt('./train.csv', delimiter=',')
Zte = np.loadtxt('./test.csv', delimiter=',')
Xtr = Ztr[:, :-1]
ytr = Ztr[:, -1]
Xte = Zte[:, :-1]
yte = Zte[:, -1]

# train tree ensemble
forest = GradientBoostingClassifier(min_samples_leaf=10)
#forest = RandomForestClassifier(min_samples_leaf=10)
#forest = ExtraTreesClassifier(min_samples_leaf=10)
#forest = AdaBoostClassifier()
forest.fit(Xtr, ytr)

# fit simplified model
Kmax = 10
splitter = DefragModel.parseSLtrees(forest) # parse sklearn tree ensembles into the array of (feature index, threshold)
mdl = DefragModel(modeltype='classification', maxitr=100, qitr=0, tol=1e-6, restart=20, verbose=0)
mdl.fit(Xtr, ytr, splitter, Kmax, fittype='FAB')",example/example_sklearn.py,sato9hara/defragTrees,1
"
def writeSensAndSpec(fpr, tpr, thresh, filename):
    specificity = 1 - fpr
    a = numpy.vstack([specificity, tpr, thresh])
    b = numpy.transpose(a)
    numpy.savetxt(filename, b, fmt='%.5f', delimiter=',')


def doRUSRFC(analysisDict):
    print('{0} Started'.format(analysisDict['analysisName']))
    RUSRFC = RUSRandomForestClassifier.RUSRandomForestClassifier(n_Forests=200, n_TreesInForest=500)
    predClasses, classProb, featureImp, featureImpSD = RUSRFC.CVJungle(analysisDict['X'], analysisDict['Y'],
                                                                       shuffle=True, print_v=True)
    cm = confusion_matrix(analysisDict['Y'], predClasses)
    print('Analysis - {0} - {1}'.format(analysisDict['analysisName'], cm))
    featureImpScale = [featureImp[analysisDict['analysis_cols'].index(i)] if i in analysisDict['analysis_cols'] else 0
                       for i in analysisDict['all_list']]
    featureImpScaleSD = [
        featureImpSD[analysisDict['analysis_cols'].index(i)] if i in analysisDict['analysis_cols'] else 0 for i in
        analysisDict['all_list']]",Python/RUSRandomForest/runClassificationParellel.py,sulantha2006/Conversion,1
"earth_classifier = Pipeline([('earth', Earth(max_degree=3, penalty=1.5)),
                             ('logistic', LogisticRegression())])

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""Naive Bayes"", ""LDA"", ""QDA"", ""Earth""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025, probability=True),
    SVC(gamma=2, C=1, probability=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    GaussianNB(),
    LDA(),
    QDA(),
    earth_classifier]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",examples/plot_classifier_comp.py,scikit-learn-contrib/py-earth,1
"    training_label = [arr for idx_arr, arr in enumerate(label)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = label_binarize(np.hstack(training_label).astype(int),
                                    [0, 255])
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    pred_prob = crf.fit(training_data, np.ravel(training_label)).predict_proba(
        testing_data)

    result_cv.append([pred_prob, crf.classes_])

# Save the information
path_store = '/data/prostate/results/mp-mri-prostate/exp-1/t2w'
if not os.path.exists(path_store):
    os.makedirs(path_store)",pipeline/feature-classification/exp-1/pipeline_classifier_t2w.py,I2Cvb/mp-mri-prostate,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans10_none.py,diogo149/CauseEffectPairsPaper,1
"    if not tunings:
      train = SMOTE(train, resample=True)
    else:
      train = SMOTE(train, a, b, resample=True)
    # except: set_trace()

  if not tunings:
    if regress:
      clf = RandomForestRegressor(n_estimators=100, random_state=1, warm_start=True,n_jobs=-1)
    else:
      clf = RandomForestClassifier(n_estimators=100, random_state=1, warm_start=True,n_jobs=-1)
  else:
    if regress:
      clf = RandomForestRegressor(n_estimators=int(tunings[0]),
                                   max_features=tunings[1] / 100,
                                   min_samples_leaf=int(tunings[2]),
                                   min_samples_split=int(tunings[3]),
                                   warm_start=True,n_jobs=-1)
    else:
      clf = RandomForestClassifier(n_estimators=int(tunings[0]),",src/tools/oracle.py,rahlk/RAAT,1
"    #                        LogregHDF5Storer),
    #
    # 'logreg4': ModelConfig('logreg4',
    #                        'logreg#penalty=l1#C=1E-4',
    #                        LogisticRegression(penalty='l1', C=1E-4, random_state=0),
    #                        LogregHDF5Storer)
    #

    'rfc1':  ModelConfig('rfc1',
                         'rfc#n_estimators=200',
                         RandomForestClassifier(n_estimators=200, random_state=0),
                         RFHDF5Storer),
}


# --- Features specs


def extract_ecfps(dset, no_dupes=True, binary=True):
    molids, X, y = dset.ecfps_molidsXY(no_dupes=no_dupes)",manysources/experiments.py,sdvillal/manysources,1
"        LogisticRegression(
            penalty='l1',
            C = c,
            fit_intercept=intercept,
            class_weight=class_weight)
        for c in  Cs
    ]

    n_estimators = args.num_trees
    rf_models = [
        RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            criterion=c)
        for c in [""gini"", ""entropy""]
        for max_depth in [None, 10, 20]
    ]

    models = lr_models  + rf_models
",feature_selection.py,iskandr/mhcpred,1
"            from sklearn.neighbors import KNeighborsClassifier
            estimator = KNeighborsClassifier(
                **self.kwargs)
        elif self.estimator == 'decision-tree':
            from sklearn.tree import DecisionTreeClassifier
            estimator = DecisionTreeClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'random-forest':
            from sklearn.ensemble import RandomForestClassifier
            estimator = RandomForestClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'adaboost':
            from sklearn.ensemble import AdaBoostClassifier
            estimator = AdaBoostClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'gradient-boosting':
            from sklearn.ensemble import GradientBoostingClassifier",imblearn/under_sampling/instance_hardness_threshold.py,dvro/UnbalancedDataset,1
"              ""min_samples_split"": sp_randint(1, 25),
              ""min_samples_leaf"": sp_randint(1, 10),
              ""bootstrap"": [True, False],
              ""criterion"": [""gini"", ""entropy""],
              ""n_estimators"": sp_randint(5, 100)}
best_score = 0.0
for i in range(2):
    # print(""# Tuning hyper-parameters for %s"" % score)
    # print()

    # clf = GridSearchCV(RandomForestClassifier(),
    #    param_grid=param_grid,
    #    cv=5, scoring='%s_weighted' % score, n_jobs=-1)

    # perform a randomized grid search for the best possible
    n_iter_search = 50
    clf = RandomizedSearchCV(RandomForestClassifier(),
                             param_distributions=param_grid,
                             n_iter=n_iter_search,
                             cv=3,",data/boada/analysis_all/MLmethods/mkObservations.py,boada/vpCluster,1
"def main():
    #create the training & test sets, skipping the header row with [1:]
    #dataset = genfromtxt(open('bls_test_data.csv','r'), delimiter=',', dtype='f8')[1:]
    df = pd.read_csv(('bls_data.csv','r'), delimiter=',', dtype='f8')[1:]    
    target = [x[0] for x in df]
    train = [x[1:] for x in df]
    #test = genfromtxt(open('bls_test_data.csv','r'), delimiter=',', dtype='f8')[1:]
    test = pd.read_csv(('bls_data.csv','r'), delimiter=',', dtype='f8')[1:]
    
    #create and train the random forest
    #multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)
    rf = RandomForestClassifier(n_estimators=100)
    rf.fit(train, target)

    savetxt('bls_result.csv', rf.predict(test), delimiter=',', fmt='%f')

###################################
# Run script
###################################
",scripts/random_forest.py,georgetown-analytics/higher-ed-investors,1
"
    def learn(self, clf=None):
        trainFeatures   = []
        trainClasses    = []
        for cl in self.Features.keys():
            for ev in self.Features[cl]:
                trainFeatures += ev
                trainClasses  += [cl]*len(ev)
        
        if clf == None:
            clf = RandomForestClassifier(n_estimators=40, max_depth=None,min_samples_split=1, random_state=0)
        clf.fit(trainFeatures, trainClasses)
        self.clf = clf


    def save(self, path):
        from sklearn.externals import joblib
        joblib.dump(self.clf, path) 
        
    def load(self, path):",datapipe/classifiers/EventClassifier.py,jdhp-sap/data-pipeline-standalone-scripts,1
"				negindicators += 1
	
	print ""pos "" + str(posindicators)
	print ""neg ""+ str(negindicators)
			
	train_data = np.array(train_data)

	#print 'Training '
	#print

	forest = RandomForestClassifier(n_estimators=100)

	try:

		forest = forest.fit(train_data[0::,1::], train_data[0::,0])

		#print ""estimators""
		#print forest.estimators_

		#print ""n classes""",rforestDS.py,ucsf-ckm/CodeJam-RandomForest,1
"# -*- coding: utf-8 -*-

from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = RandomForestClassifier(n_estimators=15, max_depth=None,
                             min_samples_split=2, random_state=0)
clf.fit(X, y)

output = Porter(clf, language='c').export()
print(output)

""""""
#include <stdlib.h>
#include <stdio.h>",examples/classifier/RandomForestClassifier/c/basics.py,nok/sklearn-porter,1
"    test_card_array)



# Solve the problem with my 52 element representation
# --
X0 = train_data[:,:-1]
y0 = train_data[:,-1]
    
# Make a clasifier and train it
rf413 = RandomForestClassifier(n_estimators=52, max_depth=None)
rf413.fit(X0, y0)
rf413_score = cross_val_score(rf413, X0, y0)
print(""rf413_score.mean() = {0}"".format(rf413_score.mean()))



# Solve the problem with my 52 element representation
# --
X = training_card_array",poker/script1.py,wgm2111/wgm-kaggle,1
"        df = pd.read_csv(path)
    X = df.values
    X_test, ids = X[:, 1:], X[:, 0]
    return X_test.astype(float), ids.astype(str)


def train():
    X_train, X_valid, y_train, y_valid = load_train_data()
    # Number of trees, increase this to beat the benchmark ;)
    n_estimators = 10
    clf = RandomForestClassifier(n_estimators=n_estimators)
    print("" -- Start training Random Forest Classifier."")
    clf.fit(X_train, y_train)
    y_prob = clf.predict_proba(X_valid)
    print("" -- Finished training."")

    encoder = LabelEncoder()
    y_true = encoder.fit_transform(y_valid)
    assert (encoder.classes_ == clf.classes_).all()
",benchmark.py,ottogroup/kaggle,1
"
# ---------------------------------------------------------------------

## STEP 4: GENERATE FEATURES - write a function to discretize a continuous variable
## and a function that can take a categorical variable and create binary variables.

def find_features(df, features, variable):
	'''Uses random forest algorithm to determine the relative importance of each
	potential feature. Takes dataframe, a numpy array of features, and the dependent
	variable. Outputs dataframe, sorting features by importance'''
	clf = RandomForestClassifier()
	clf.fit(df[features], df[variable])
	importances = clf.feature_importances_
	sort_importances = np.argsort(importances)
	rv = pd.DataFrame(data={'variable':features[sort_importances],
							'importance':importances[sort_importances]})
	return rv

def adjust_outliers(x, cap):
	'''Takes series and creates upperbound cap to adjust for outliers'''",pipeline/pipeline.py,BridgitD/school-dropout-predictions,1
"        BaggingClassifier(
            base_estimator=tree.DecisionTreeClassifier(
                criterion='gini',
                max_depth=10)
        ),
        AdaBoostClassifier(
            base_estimator=tree.DecisionTreeClassifier(
                criterion='gini',
                max_depth=10)
        ),
        RandomForestClassifier(
            criterion='gini',
            max_depth=10
        )
    ]

    for name, em_clf in zip(names, models):
        logger.info(""###################---"" + name + ""---###################"")

        em_clf.fit(data_features_train, data_targets_train)",algorithm_examples/ensemble_methods_example.py,NZubia/DataMiningUACH,1
"#trainRes = Results_Data_Training.as_matrix(colsRes) #training results
trainRes_1 = Data_Training['raterScale'].values
trainArr


# # III. Random Forest

# In[21]:

#Initialize
forest = RandomForestClassifier(n_estimators = 100)

# Fit the training data and create the decision trees
forest = forest.fit(trainArr,trainRes_1)

# Take the same decision trees and run it on the test data
Data_Testing = Data_Simple5.sample(frac=0.2)
Input_Data_Testing = Data_Testing.drop(exclude, axis=1)
testArr = Input_Data_Testing.as_matrix()
results = forest.predict(testArr)",Homework/04 - Applied ML/Homework_04_Referees_teamawesome-Q1 + Bonus.py,Merinorus/adaisawesome,1
"    ## 3  - 0.666666666667 0.666666666667 0.666666666667 1.0 1.0
    #
    # s1 = featureMatrix.iloc[0:i,:]
    # s2 = featureMatrix.iloc[i:i*2,:]
    # s3 = featureMatrix.iloc[i*2:i*3,:]
    # s4 = featureMatrix.iloc[i*3:i*4,:]
    # s5 = featureMatrix.iloc[i*4:i*5,:]
    #
    # ### W/o S1
    # trainingwos1 = s2.append(s3).append(s4).append(s5)
    # clf = RandomForestClassifier(n_estimators=n_estimators, random_state=30)
    # clf.fit(trainingwos1.iloc[:,:24], trainingwos1.iloc[:,24])
    # scorewos1 = clf.score(s1.iloc[:,:24], s1.iloc[:,24])
    # ### W/o S2
    # trainingwos2 = s1.append(s3).append(s4).append(s5)
    # clf = RandomForestClassifier(n_estimators=n_estimators, random_state=30)
    # clf.fit(trainingwos2.iloc[:,:24], trainingwos2.iloc[:,24])
    # scorewos2 = clf.score(s2.iloc[:,:24], s2.iloc[:,24])
    # ### W/o S3
    # trainingwos3 = s1.append(s2).append(s4).append(s5)",libras_myo/plotpandas.py,pernambucano/myo_libras,1
"
        if not self.get_input('load_model'):
            params = []
            vars = ['maxDepth', 'maxBins', 'minInstancesPerNode', 'minInfoGain', 'maxMemoryInMB', 'cacheNodeIds', 'checkpointInterval', 'impurity', 'numTrees', 'featureSubsetStrategy', 'seed', 'labelCol', 'featuresCol']
            for var in vars:
                if (var in 'labelCol') or (var in 'featuresCol') or (var in 'impurity') or (var in 'featureSubsetStrategy'):
                    params.append('{0}=""{1}""'.format(var, str(self.get_input(var))))
                else:
                    params.append('{0}={1}'.format(var, str(self.get_input(var))))

            cmds.append('rf = RandomForestClassifier( ' + ', '.join(params) + ')\n')
            cmds.append(""df_ = {0}\n"".format(self.get_input('dataframe')))
            cmds.append(""labelCol_ = '{0}'\n"".format(self.get_input('labelCol')))
            cmds.append(""train_perc_ = {0}\n"".format(self.get_input('train_perc')))
            cmds.append(""labels = df_.select(labelCol_).distinct().collect()\n"")
            cmds.append(""""""
train = None
test = None
for label in labels:
    if train == None:",spark/init.py,ftoliveira/provtracker,1
"        
        # make subset and return
        new_object = cp.deepcopy(self)
        new_object.X = self.X[indices,:]
        new_object.y = self.y[indices]
        new_object.strata = self.strata[indices]
        return new_object
        
    def RF_cv_by_strata(self):
        """""" Train on own data, test on other data """"""
        rf = RandomForestClassifier(random_state = self.ran_stat, n_estimators = self.n_trees, n_jobs=self.n_jobs) 
        strata = set(self.strata) 
        res_dict = {}
        for i in itertools.permutations(strata,r=2):
            o1 = self.select_strata(i[0])
            o2 = self.select_strata(i[1])
            rf.fit(o1.X,o1.y)
            y_probability = rf.predict_proba(o2.X)
            y_predicted = rf.predict(o2.X)
            res_dict[i] = zip(y_predicted,y_probability)",src/feature_sommelier/ROIseries_feature_sommelier.py,RLPAgroScience/ROIseries,1
"
    ###############################################################################
    # Classification and ROC analysis

    # Run classifier with cross-validation and plot ROC curves
    cv = StratifiedKFold(y, n_folds=13,shuffle=True) #Maybe try shuffle=True ?


    'Best hyper param  classifiers For Thermophiles:'
    # svm=SVC(C=50, cache_size=1500, class_weight='auto', gamma=0.0, kernel='rbf', probability=True)
    # rf = RandomForestClassifier(n_jobs=-1,min_samples_split= 3, max_features= 0.4, min_samples_leaf= 2, n_estimators= 200, max_depth= 8)

    'Best hyper param  classifiers For NP:'
    svm =SVC(C=50, cache_size=1800, class_weight='auto',gamma=0.0, kernel='rbf', max_iter=-1, probability=True )
    rf = RandomForestClassifier(max_depth= None, min_samples_split= 3, min_samples_leaf= 1, n_estimators= 250,  n_jobs= -1, max_features= ""auto"")

    classifier = svm

    mean_tpr = 0.0
    mean_fpr = np.linspace(0, 1, 100)",ProFET/feat_extract/plot_roc.py,ddofer/ProFET,1
"    fts = []
    voc = instance.split("","")
    #index starts off from 1 if the account name is included
    # 0 otherwise
    for index in range(1,len(voc)):
        if index in features:
            fts.append(voc[index].strip())
    XTest = np.empty([1, len(fts)])
    for item in range(0, len(fts)):
        XTest[0, item] = fts[item]
    rf = RandomForestClassifier(n_estimators=100, random_state=0)
    rf.fit(XTrain, yTrain)
    predRF = rf.predict(XTest)
    #print(predRF)
    if predRF[0]==0: 
	#print(voc[0], ""bot"", predRF)
	return voc[0], ""bot"", predRF
    elif predRF[0]==1:
	#print(voc[0], ""human"", predRF)
	return voc[0], ""human"", predRF",lib/rfclassifier/classify.py,zafargilani/stcs,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.datasets import load_digits
import numpy as np

mnist = load_digits()
clf = RandomForestClassifier(n_estimators=100)
scores = cross_val_score(clf, mnist.data, mnist.target)
print(scores.mean())

clf = ExtraTreesClassifier(n_estimators=100)
scores = cross_val_score(clf, mnist.data, mnist.target)
print(scores.mean())",random-forest-example.py,seymour1/ML_Toolkit,1
"# predictions['sampler_id'] = predictions.sampler.map({
#     'RandomOverSampler': 0,
#     'SMOTE': 1,
#     'DummySampler': 2,
#     'SMOTEENN': 3,
#     'SMOTETomek': 4,
#     'ADASYN': 5}).astype(int)

classifier_list = [
    # ['GradientBoosting', GradientBoostingClassifier()],
    ['RandomForest', RandomForestClassifier()],
    # ['GaussianNB', GaussianNB()],
    # ['DecisionTree', DecisionTreeClassifier()],
    # ['LogisticRegression', LogisticRegression()],
    # ['MLP', MLPClassifier()],
    # ['AdaBoost', AdaBoostClassifier()],
    # ['KNN', KNeighborsClassifier()]
    #    ['SVC', SVC(probability=True)],
    #    ['QDA', QuadraticDiscriminantAnalysis()],
]",ml_files/perform_sklearn.py,patemotter/trilinos-prediction,1
"
fraud = df[df['Class'] == 0]
normal = df[df['Class'] == 1]

cc_data = fraud.append(normal)
print (""Percentage of normal transactions                 :"", len(normal) / float(len(cc_data)))
print (""Percentage of fraudulent trasactions                :"", len(fraud) / float(len(cc_data)))
print (""Total number of transactions in our new dataset :"", len(cc_data))

#Random forest
reg_model = RandomForestClassifier()
train_data,test_data = train_test_split(cc_data,test_size=0.2)
y = train_data['Class']
x = train_data.drop(['Class'],axis=1)

reg_model.fit(x,y)

test_x = test_data.drop(['Class'],axis=1)
test_y = test_data['Class']
",FraudDetection.ML/TransactionEngine/test.py,imironica/Fraud-Detection-System,1
"# The model for the Tornado Web Application
class Model():


    # Dictionary with supported sklearn models (and more in the future)

    ##############
    ### Models ###
    ##############

    rdf = RandomForestClassifier(n_estimators=100)
    ada = OneVsRestClassifier(AdaBoostClassifier())
    lsvc = LinearSVC()
    rsvc = SVC(probability=True, C=32, gamma=0.125) # tuned HP for HELA
    log = LogisticRegression()
    lda = LDA()
    knn = KNeighborsClassifier()
    grad = GradientBoostingClassifier()

    models = {",tornado-server/app.py,daviddao/luminosity,1
"    #t = np.hstack((t,extra_test))

    print x.shape

    label = np.array(label)

    clf = LogisticRegression(penalty='l2',dual=True,fit_intercept=True,C=20,tol=1e-9,class_weight=None, random_state=None, intercept_scaling=1.0)
    #clf = AdaBoostClassifier(n_estimators=100)
    #clf = SGDClassifier(loss=""log"",n_iter=300, penalty=""l2"",alpha=0.00005,fit_intercept=Tr1ue)#sgd 
    #clf = svm.SVC(C=1,degree=9,gamma=10,probability=True)
    #clf = RandomForestClassifier(n_estimators=2000, criterion =""gini"",max_depth=20,min_samples_split=1, random_state=0,n_jobs=3)
    #clf = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0,max_depth=1, random_state=0)
    print ""cross validation""
    print np.mean(cross_validation.cross_val_score(clf,x,label,cv=20,scoring='roc_auc'))

    clf.fit(x,label)
    #
    print """",clf.score(x,label)
    answer =  clf.predict_proba(t)[:,1]
    ",lda.py,ezhouyang/class,1
"# print(""Best estimator found by grid search:"")
# print(clf.best_estimator_)




#==============================================================================
# RandomForest (Classifier Variant) Model Fitting parameters
#==============================================================================

estimator = Pipeline([(""forest"", RandomForestClassifier(random_state=0, n_estimators=100))])
estimator.fit(predictors, outcomes)

predicted = estimator.predict(predictors)
prediction_scores  = accuracy_score(outcomes, predicted) #This code will change, fi we cross validate

# print (type(predicted[2]))
# print (predicted[2])
lst_predicted = []
for i in predicted:",Visual_Game/CreatingBaseDF.py,kizzen/Baller-Shot-Caller,1
"targets_tr = traindf['cuisine']

predictors_ts = tfidfts


classifier = LinearSVC(C=1, penalty=""l2"", dual=False)
# parameters = {'C':[1, 10]}
# parameters = {""max_depth"": [3, 5,7]}
# clf = LinearSVC()
# clf = LogisticRegression()
# clf = RandomForestClassifier(n_estimators=100, max_features=""auto"",random_state=50)

# classifier = grid_search.GridSearchCV(clf, parameters)
# classifier = GridSearchCV(clf, parameters)
# classifier = RandomForestClassifier(n_estimators=200)

classifier=classifier.fit(predictors_tr,targets_tr)

predictions=classifier.predict(predictors_ts)
testdf['cuisine'] = predictions",deep_learn/whatiscooking/ReadCookingPlusPlus.py,zhDai/CToFun,1
"import numpy as np
import pandas as pd

# create the training & test sets, skipping the header row with [1:]
dataset = pd.read_csv(""train.csv"")
target = dataset[[0]].values.ravel()
train = dataset.iloc[:, 1:].values
test = pd.read_csv(""test.csv"").values

# create and train the random forest
# multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)
rf = RandomForestClassifier(n_estimators=100)
rf.fit(train, target)
pred = rf.predict(test)

np.savetxt('submission_rand_forest.csv', np.c_[range(1, len(test) + 1), pred], delimiter=',', header='ImageId,Label',
           comments='', fmt='%d')",DigitRecogniser/Main.py,kraktos/Data_Science_Analytics,1
"                                           svm.SVC(probability=True, kernel=""sigmoid"", C=0.1, coef0=0.01, gamma=0.01),
                                           True, True, False)
    available_clfs[""svm_gs2""] = Classifier(""svm"", ""Co-best SVM according to Skll Grid Search"",
                                           svm.SVC(probability=True, kernel=""sigmoid"", C=0.01, coef0=0.01, gamma=0.0),
                                           True, True, False)
    available_clfs[""mnb""] = Classifier(""mnb"", ""Multinomial Naive Bayes"", naive_bayes.MultinomialNB(), False, True,
                                       False)  # MNB can't do default scaling: ValueError: Input X must be non-negative
    available_clfs[""knn""] = Classifier(""knn"", ""k Nearest Neighbour"", neighbors.KNeighborsClassifier(), True, True,
                                       False)  # knn can do feature scaling
    available_clfs[""raf""] = Classifier(""raf"", ""Random Forest"",
                                       ensemble.RandomForestClassifier(n_estimators=15, max_depth=5, oob_score=True),
                                       True, True, False)
    return available_clfs


def __load_arg_parser():
    arg_parser = argparse.ArgumentParser(description=""Perform data manipulation to extract features and finally run ""
                                                     ""classification."")
    group_get_model = arg_parser.add_mutually_exclusive_group(required=True)
    group_get_model.add_argument(""--training"", help=""The base path to the training set"")",safeness/glad/glad-main.py,pasmod/obfuscation,1
"
import pickle
import requests, json

iris = datasets.load_iris()
# print iris.DESCR
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
rfc = RandomForestClassifier(n_estimators=10)
rfc.fit(X_train,y_train)
print(y)
pickle.dump(rfc,open(""python2pickle141.pkl"",""wb""))

with open(""python2pickle141.pkl"", 'rb') as f:
    d = pickle.load(f)
    print(classification_report(y_test,d.predict(X_test)))

",predict2016/garbage/test.py,znagler/znagler.github.io,1
"from collections import Counter
import datetime

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import shuffle

from models.features.topics import topics_similarity
from models.features.cosine import cosine_similarity_features

clf = RandomForestClassifier()
clf_one = RandomForestClassifier()
clf_two = RandomForestClassifier()

def generate_features(data, val=None):
    features = []
    for raw in data:
        features.extend(topics_similarity(raw))
    features.extend(cosine_similarity_features(data[:-1], data[-1]))
",models/hybrid_model.py,xenx/recommendation_system,1
"
    # display the plot
    # plt.show()
    # save the plot as an image
    save_plot_img(""{}-learning-curve.png"".format(classifier))


weaks = [
    BaggingClassifier(5, lambda: SVMClassifier(6)),
    BaggingClassifier(5, lambda: SVMClassifier(9, kernel=""poly"", degree=3)),
    BaggingClassifier(10, lambda: RandomForestClassifier(128))]
ensemble = VoteClassifier(*weaks)
classifiers = weaks + [ensemble]

for c in classifiers:",analysis/report.py,kosigz/DiabetesPatientReadmissionClassifier,1
"def test_max_features(max_features):
    if (max_features not in ['sqrt', 'auto', 'log2', None]):
        try:
            max_features = int(max_features)
        except ValueError:
            print(""max_features has to be an integer or one of 'sqrt', 'auto', 'log2' or None."")
            raise
    return max_features

def learn(X,y, n_trees = 10, criterion = 'entropy', max_features = ""sqrt"", max_depth = None, min_samples_split = 2, min_samples_leaf = 1, min_weight_fraction_leaf = 0, max_leaf_nodes = None, min_impurity_split = 1e-7, bootstrap = False, oob_score = False, n_jobs = 10, random_state = None, warm_start = False, class_weight = 'balanced_subsample'):
    rf = sklearn.ensemble.RandomForestClassifier(n_estimators = n_trees, \
                                                criterion = criterion, \
                                                max_features = max_features, \
                                                max_depth = max_depth, \
                                                min_samples_split = min_samples_split, \
                                                min_samples_leaf = min_samples_leaf, \
                                                min_weight_fraction_leaf = min_weight_fraction_leaf, \
                                                max_leaf_nodes = max_leaf_nodes, \
                                                min_impurity_split = min_impurity_split, \
                                                bootstrap = bootstrap, \",dingo/random_forest.py,andersgs/dingo,1
"                          DeprecationWarning)
            # Define the classifier to use
            if self.estimator == 'knn':
                self.estimator_ = KNeighborsClassifier(**self.kwargs)
            elif self.estimator == 'decision-tree':
                from sklearn.tree import DecisionTreeClassifier
                self.estimator_ = DecisionTreeClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'random-forest':
                from sklearn.ensemble import RandomForestClassifier
                self.estimator_ = RandomForestClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'adaboost':
                from sklearn.ensemble import AdaBoostClassifier
                self.estimator_ = AdaBoostClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'gradient-boosting':
                from sklearn.ensemble import GradientBoostingClassifier
                self.estimator_ = GradientBoostingClassifier(
                    random_state=self.random_state, **self.kwargs)",imblearn/ensemble/balance_cascade.py,chkoar/imbalanced-learn,1
"from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np


from skml.ensemble import EnsembleClassifierChain
from skml.datasets import load_dataset

X, y = load_dataset('yeast')
ensemble = EnsembleClassifierChain(RandomForestClassifier(),
                                   threshold=.5,
                                   max_features=1.0)
ensemble.fit(X, y)
y_pred = ensemble.predict(X)

print(""hamming loss: "")
print(hamming_loss(y, y_pred))

print(""accuracy:"")",examples/example_ecc.py,ChristianSch/skml,1
"
    n_features = X_train.shape[1]   
    print ""XX:"", n_features

    g =   1.0/float((3*n_features))
    print g

    print ""Training.""

 
    clf = RandomForestClassifier(n_estimators=850, max_depth=None, max_features=int(math.sqrt(n_features)), min_samples_split=100, random_state=144, n_jobs=4);
    clf.fit(X_train, y_train)
    print ""Validation set score: RF "" , clf.score(X_val, y_val)
 
    clf_etree = ExtraTreesClassifier(n_estimators=1000, max_depth=None, max_features=int(math.sqrt(n_features)), min_samples_split=100, random_state=144, n_jobs=4);
    clf_etree.fit(X_train, y_train)
    print ""Validation set score: ERF "" , clf_etree.score(X_val, y_val)

    clf_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=""SAMME"", n_estimators=500, random_state=74494, learning_rate=0.8) 
    clf_boost.fit(X_train, y_train)",python_scripts_from_net/forest.py,sankar-mukherjee/DecMeg2014,1
"
def createKNN():
    clf = KNeighborsClassifier(n_neighbors=13,algorithm='kd_tree',weights='uniform',p=1)
    return clf

def createDecisionTree():
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createRandomForest():
    clf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createExtraTree():
    clf = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createAdaBoost():
    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    clf = AdaBoostClassifier(dt, n_estimators=300)",kaggle-digit-recognizer/src/classify.py,KellyChan/Kaggle,1
"  # set_trace()
  return _Abcd(before=actual, after=preds, show=False)[-1]


def rforest(train, test, tunings=None, smoteit=True, duplicate=True):
  ""RF ""
  # Apply random forest Classifier to predict the number of bugs.
  if smoteit:
    train = SMOTE(train, atleast=50, atmost=101, resample=duplicate)
  if not tunings:
    clf = RandomForestClassifier(n_estimators=100, random_state=1)
  else:
    clf = RandomForestClassifier(n_estimators=int(tunings[0]),
                                 max_features=tunings[1] / 100,
                                 min_samples_leaf=int(tunings[2]),
                                 min_samples_split=int(tunings[3])
                                 )
  train_DF = formatData(train)
  test_DF = formatData(test)
  features = train_DF.columns[:-2]",old/Modeller/Prediction.py,ai-se/Transfer-Learning,1
"    training_label = [arr for idx_arr, arr in enumerate(label)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = label_binarize(np.hstack(training_label).astype(int),
                                    [0, 255])
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    pred_prob = crf.fit(training_data, np.ravel(training_label)).predict_proba(
        testing_data)

    result_cv.append([pred_prob, crf.classes_])

# Save the information
path_store = '/data/prostate/results/mp-mri-prostate/exp-1/adc'
if not os.path.exists(path_store):
    os.makedirs(path_store)",pipeline/feature-classification/exp-1/pipeline_classifier_adc.py,I2Cvb/mp-mri-prostate,1
"from nonconformist.evaluation import ClassIcpCvHelper, RegIcpCvHelper
from nonconformist.evaluation import class_avg_c, class_mean_errors
from nonconformist.evaluation import reg_mean_errors, reg_median_size


# -----------------------------------------------------------------------------
# Classification
# -----------------------------------------------------------------------------
data = load_iris()

icp = OobCpClassifier(ClassifierNc(OobClassifierAdapter(RandomForestClassifier(n_estimators=100, oob_score=True))))
icp_cv = ClassIcpCvHelper(icp)

scores = cross_val_score(icp_cv,
                         data.data,
                         data.target,
                         iterations=5,
                         folds=5,
                         scoring_funcs=[class_mean_errors, class_avg_c],
                         significance_levels=[0.05, 0.1, 0.2])",examples/oob_calibration.py,donlnz/nonconformist,1
"from my_fun import *
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.externals import joblib


train_path = 'datasetASL/ds9/'
train_path = 'datasetECCV/train/'
test_path = 'datasetECCV/test/'

clfRandom = RandomForestClassifier(n_estimators=100)
clfRandomAdd = joblib.load('tree_Random_class_4.pkl')


clf = clfRandom

#validation = test_obj(train_path, clf)
#validation = cross_test_obj(train_path, clf)

validation = conf_test_obj(train_path, test_path, clf)",prensilia/validation.py,parloma/Prensilia,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn.externals import joblib

from classify_seg import getseg

def sort_by_count(d):
    d = collections.OrderedDict(sorted(d.items(), key = lambda t: -t[1]))  
    return d
def classify_model(train_seg,train_y,vect):	
	train_X = vect.transform(train_seg)
	clf_RFC = RandomForestClassifier(n_estimators=550,min_samples_split=1)
	clf_RFC.fit(train_X, train_y)
	return clf_RFC
def model_test(vect,clf_RFC,test_doc):    
    test_seg = test_doc['seg_word']
    test_X = vect.transform(test_seg)
    pre_RFC = clf_RFC.predict(test_X)
    test_doc['pre_sen'] = pre_RFC
    return test_doc
rootdir = os.getcwd()",classify_march.py,huaijun-lee/classify,1
"vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
                                 stop_words='english')


X2 = vectorizer.fit_transform(data_text)
clf2 = SGDClassifier(loss='log', penalty='l2',alpha=1e-5, n_iter=25, random_state=42,shuffle=True)
clf2.fit(X2, labels)
vocab = my_dict2 = {y:x for x,y in vectorizer.vocabulary_.iteritems()}


#clf = RandomForestClassifier(n_estimators=750)

allTextInOrders = data_text + data_text + unhelpful_exp_text + helpful_exp_text
trainingDistributions = []

# for reviewT in data_text:
# 	trainingDistributions.append(lda[corpus.dictionary.doc2bow(corpus.proc(reviewT))])


",src/old_pipeline/treesReviews.py,cudbg/Dialectic,1
"            X = selector.fit_transform(X)
        except ValueError as e:
            return []
        indeces = selector.get_support(indices = True)
        return indeces

    def random_forest_classifier(self, X, y, threshold = 0.1):
        """"""
        Use the RandomForestClassifier to score the features by their importance
        """"""
        clf = ensemble.RandomForestClassifier()
        clf.fit(X, y)
        features_importance = clf.feature_importances_
        indeces = []
        for i, f in enumerate(features_importance):
            if f > threshold:
                indeces.append(i)
        return indeces

    def indeces_to_mask(self, indeces, X):",app/models/feature_filtering.py,jpbonson/VisualizationToolForMLData,1
"# Load the fina data set.
data   = pd.read_csv('final-data.csv', header=0)

# Since final-data.csv data contains the whole
# shuffled titanic data, we used 2/3 of 
# the data for training and 1/3 for testing
train  = data[0:872]
test   = data[872::]

# Train the random forest and used the parameters found using GridSearchCV
model  = RandomForestClassifier(n_estimators=65, criterion='gini', max_depth=5, max_features=0.5,random_state=600)
model.fit(train[attrib],train['Survived'])

# Predict data and store on final .csv file.\
test['Predicted'] = model.predict(test[attrib])
test.to_csv('survival-prediction.csv', sep=',', index=False)

# Output result: 0.828
print model.score(test[attrib], test['Survived']) 
",predict.py,cadrev/Titanic-Prediction,1
"import matplotlib.pyplot as plt
from scikitplot import classifier_factory

X, y = load_data(return_X_y=True)
rf = classifier_factory(RandomForestClassifier())
rf.plot_confusion_matrix(X, y, normalize=True)
plt.show()

# Using the more flexible functions API
from scikitplot import plotters as skplt
rf = RandomForestClassifier()
rf = rf.fit(X, y)
preds = rf.predict(X)
skplt.plot_confusion_matrix(y_true=y, y_pred=preds)
plt.show()",examples/plot_confusion_matrix.py,reiinakano/scikit-plot,1
"    print 'calculating features...'
    ff = FeatureFactory()
    X_scaled = ff.getFeatures(opens, highs, lows, closes, volumes)
    # print X_scaled

    # set rewards
    print 'calculating rewards...'
    rewards = ff.getRewards(closes)

    # fitting regressor
    # rfc = RandomForestClassifier(n_estimators=30)
    rfc = ExtraTreesClassifier(
        # n_estimators=30,
        # max_features='sqrt'
    )

    # predict
    rfc.fit(X_scaled, rewards)
    # y_predict = rfc.predict(X_train)
",06_randomforests/classifier-1d.py,Tjorriemorrie/trading,1
"                 min_samples_split=0.3, min_samples_leaf=0.2):
        self.problem = problem
        self.n_estimators = int(n_estimators)
        self.max_features = max_features
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.name = 'RF'

    def eval(self):
        if self.problem == 'binary':
            mod = RandomForestClassifier(n_estimators=self.n_estimators,
                                         max_features=self.max_features,
                                         min_samples_split=self.min_samples_split,
                                         min_samples_leaf=self.min_samples_leaf,
                                         n_jobs=-1,
                                         random_state=20)
        else:
            mod = RandomForestRegressor(n_estimators=self.n_estimators,
                                        max_features=self.max_features,
                                        min_samples_split=self.min_samples_split,",testing/modaux.py,hawk31/pyGPGO,1
"print 'imputing with feature summarization (mode)'
summ_func = lambda x: mode(x)[0]
data_mode = imp.summarize(x, summ_func, missing_data_cond)

# replace categorical features with one hot row
print 'imputing with one-hot'
data_onehot = imp.binarize_data(x, cat_cols)

# replace missing data with predictions using random forest
print 'imputing with predicted values from random forest'
clf = RandomForestClassifier(n_estimators=100, criterion='gini')
data_rf = imp.predict(x, cat_cols, missing_data_cond, clf)

# replace missing data with predictions using SVM
print 'imputing with predicted values usng SVM'
clf = SVM(
    penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', 
    fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, 
    random_state=None, max_iter=1000)
data_svm = imp.predict(x, cat_cols, missing_data_cond, clf)",example_adult.py,rafaelvalle/MDI,1
"
order = np.random.permutation(60000)
train_set = [mnist.data[order[:50000],:], mnist.target[order[:50000]]]
valid_set = [mnist.data[order[50000:60000],:], mnist.target[order[50000:60000]]]

for i in range(20):
    # Get suggested new experiment
    job = scientist.suggest()

    # Perform experiment
    learner = RandomForestClassifier(**job)
    learner.fit(*train_set)
    accuracy = learner.score(*valid_set)

    # Inform scientist about the outcome
    scientist.update(job,accuracy)
    scientist.report()
",examples/mnist_random_forest.py,schevalier/Whetlab-Python-Client,1
"df_sub = df_file[df_file['3COURSEID']=='TU154']
df_sub = df_sub.iloc[np.random.permutation(len(df_sub))]

A = df_sub.as_matrix()
X = A[:,6:]
X = X.astype(np.int64, copy=False)
y = A[:,5]
y = y.astype(np.int64, copy=False)

#Training data
clf_rf = RandomForestClassifier(n_estimators=10, max_depth=None, 
         min_samples_split=1, random_state=None, max_features=None)
scores = cross_val_score(clf_rf, X, y, cv=5)
clf = clf_rf.fit(X,y)
print scores
print ""Random Forest Cross Validation: %s""%scores.mean()
print ""-----------------------------------""

#i=0
#actual=np.array(y)",pae/forcast/plotgraph_cs213.py,wasit7/book_pae,1
"print(__doc__)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB

clf1 = LogisticRegression(random_state=123)
clf2 = RandomForestClassifier(random_state=123)
clf3 = GaussianNB()
X = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])
y = np.array([1, 1, 2, 2])

eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
                        voting='soft',
                        weights=[1, 1, 5])

# predict class probabilities for all classifiers",projects/scikit-learn-master/examples/ensemble/plot_voting_probas.py,DailyActie/Surrogate-Model,1
"    return (tp+tn)/len(ans)


def train_and_predict(feat_df,url_df,predict_save_name,stats_save_name):
    TrainX= feat_df.values[0:300,:]
    TrainY=url_df.answer.values[0:300]
    TestX= feat_df.values[300:,:]
    TestY= url_df.answer.values[300:]
    TestUrls= url_df.URL.values[300:]

    RF = RandomForestClassifier(n_estimators=100, max_depth=None,
                                         min_samples_split=2, random_state=0,
                                        compute_importances=True)
    RF.fit(TrainX,TrainY)
    predict= RF.predict(TestX)
    
    #save predictions to file
    results_df= DataFrame()
    results_df[""url""]=TestUrls
    results_df[""answer""]=TestY",machine_learn/blob_hog_predict_common_url_set/predict_with_blob.py,kaylanb/SkinApp,1
"from __future__ import unicode_literals
from __future__ import with_statement

import sklearn.ensemble

import data
import process.bag_of_words
import submissions

if __name__ == '__main__':
    rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, random_state=process.seed)
    rf.fit(process.bag_of_words.train, data.target)
    pred = rf.predict(process.bag_of_words.test)

    submissions.save_csv(pred, '{file_name}.csv'.format(file_name=__file__[:-3]))",word2vec_nlp_tutorial/random_forest.py,wjfwzzc/Kaggle_Script,1
"            # quick classifier
            X = dft[cols]
            y = dft['act']

            scaler = pre.StandardScaler()
            # scaled = scaler.fit_transform(X)
            # X = pd.DataFrame(scaled, columns=X.columns)

            # this is for a baseline
            clf = LogisticRegression(C=1,penalty='l1', class_weight={0:1,1:8})
            # clf = RandomForestClassifier(n_estimators=10, max_features=.5)
            scores = sl.cross_validation.cross_val_score(clf, X, y, scoring='roc_auc')
            mscores = np.mean(scores)
            print(k + ': xval scores=' + str(scores) + ' | mean=' + str(mscores))
            if mscores > best_auc:
                best_auc = mscores

            # neural net
            print('NN')
            X_train,X_test,y_train,y_test = train_test_split(X, y, train_size=0.2, test_size=0.1)",cnct_graph2_analyze_nn.py,ecodan/kaggle-connectomix,1
"    #RandomForest: Random Forest, we set the max_depth equal to 50 because of prior tests
    #NB: LDA representation using an ngram range of (1,1)
    #train_features: Train reviews that have been transformed into the relevant features
    #train_labels: Labels for the training reviews, transformed into a binary variable
    #Output: A fitted model object

    if svm_clf == True:
        clf = svm.LinearSVC()
        clf.fit(train_features, train_labels)
    elif RandomForest == True:
        clf = RandomForestClassifier(max_depth = 100, max_leaf_nodes=50, criterion='entropy')
        clf.fit(train_features, train_labels)
    elif nb == True:
        clf = GaussianNB()
        clf.fit(train_features, train_labels)
    elif lasso == True:
        clf = Lasso()
        clf.fit(train_features, train_labels)
    elif kneighbor == True:
        clf = KNeighborsClassifier()",machine_learning/yelp_ml_p3.py,georgetown-analytics/yelp-classification,1
"    # Numpy arrays are easy to work with, so convert the result to an
    # array
    train_data_features = train_data_features.toarray()

    # ******* Train a random forest using the bag of words
    #
    print ""Training the random forest (this may take a while)...""


    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators = 100)

    # Fit the forest to the training set, using the bag of words as
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit( train_data_features, train[""sentiment""] )


",codeFromGithub/BagOfWords.py,prajitdas/DeepLearningMovies,1
"
	print('\n{}: {}'.format(icp_name, ds_name))
	scores = scores.drop(['fold', 'iter'], axis=1)
	print(scores.groupby(['significance']).mean())

# -----------------------------------------------------------------------------
# Classification
# -----------------------------------------------------------------------------
data = load_iris()

nc = NcFactory.create_nc(RandomForestClassifier(n_estimators=100))
icp = IcpClassifier(nc)
icp_cv = ClassIcpCvHelper(icp)
score_model(icp_cv,
            'IcpClassifier',
            data,
            'iris',
            [class_mean_errors, class_avg_c])

# -----------------------------------------------------------------------------",examples/nc_factory.py,donlnz/nonconformist,1
"	y = input_res[:,0:cut_pt].transpose().ravel()

	##############################################################################
	# Split the dataset in two equal parts
	X_train, X_test, y_train, y_test = cval.train_test_split(x, y, test_size=0.5, random_state=0)

	##############################################################################
	# Train classifiers
	print(""Training classifiers..."")
	# Intialize Classifier
	clf = RandomForestClassifier(n_jobs=-1, verbose=1) #verbose: 1 to print details, 0 to disable
	bclf = AdaBoostClassifier(base_estimator=clf)

	# Initialize grid_search range and other parameters
	n_estimators_range = np.array([100,1000,10000])
	param_grid = dict(n_estimators=n_estimators_range)
	cv = cval.StratifiedShuffleSplit(y_train, n_iter=2, test_size=0.25, random_state=28)

	# Run grid search
	start = time.clock()",model/random_forest/grid_search_with_randomforest.py,flyingpoops/kaggle-digit-recognizer-team-learning,1
"trainDataVecs = getAvgFeaturesVecs(clean_train_reviews, model, num_features)

print (""Creating average feature vecs for the test reviews"")
clean_test_reviews = []
for review in test[""review""]:
    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))

testDataVecs = getAvgFeaturesVecs(clean_test_reviews, model, num_features)

from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100)

print (""Fitting a random forest to labelled training data..."")
forest = forest.fit(trainDataVecs, train[""sentiment""])

# test and extract results
result = forest.predict(testDataVecs)

# Write and test results
output = pd.DataFrame(data={'id':test['id'], 'sentiment':result})",src/word2vec_vectorAveraging.py,switchkiller/ml_imdb,1
"        labels_test = labels_test + ([subject] * XX.shape[0])
        ids_test.append(ids)

    X_test = np.vstack(X_test)
    ids_test = np.concatenate(ids_test)
    print ""Testset:"", X_test.shape

    # Define our two-layer classifier

    clf1 = LogisticRegression(C = C, penalty = 'l2')
    clf2 = RandomForestClassifier(n_estimators = numTrees, n_jobs = 1)
    
    baseClf = LrCollection(clf1 = clf1, 
                           clf2 = clf2,     # useCols and useRows can be used
                           useCols = True,  # for predicting with only time 
                           useRows = True)  # or sensor dimension

    # Wrap the classifier inside our iterative scheme.
    # The clf argument accepts any sklearn classifier, as well.
                        ",train.py,mahehu/decmeg,1
"        assert y is None
        X = phi(data)
        patches = self._get_patches(X)
        comps = self.train_from_samples(patches)

        # Now, train random forests to do the coding
        from sklearn.ensemble import RandomForestClassifier

        patches = patches.reshape((patches.shape[0], -1))

        clf = RandomForestClassifier(n_estimators=self._settings.get('trees', 10), max_depth=self._settings.get('max_depth', 10))
        import gv
        with gv.Timer('Forest training'):
            clf = clf.fit(patches, comps)

        self._random_forest = clf


    def train_from_samples(self, patches):
        from pnet.bernoulli_mm import BernoulliMM",pnet/old_layers/random_forest_parts_layer.py,amitgroup/parts-net,1
"        prefilter_train1 = xgb.DMatrix( prefilter_train, label=train_label_new)
        evallist  = [(prefilter_train1, 'train')]
        num_round = 10
        clf = xgb.train( plst, prefilter_train1, num_round, evallist )
        prefilter_test1 = xgb.DMatrix( prefilter_test)
        ae_y_pred_prob = clf.predict(prefilter_test1)
        '''
	'''
        tmp_aver = [0] * len(real_labels)
        print 'deep autoencoder'
        clf = RandomForestClassifier(n_estimators=50)
        clf.fit(prefilter_train_bef, train_label_new)
        ae_y_pred_prob = clf.predict_proba(prefilter_test_bef)[:,1]
        all_prob[class_index] = all_prob[class_index] + [val for val in ae_y_pred_prob]
        tmp_aver = [val1 + val2/3 for val1, val2 in zip(ae_y_pred_prob, tmp_aver)]
        proba = transfer_label_from_prob(ae_y_pred_prob)
        #pdb.set_trace()            
        acc, precision, sensitivity, specificity, MCC = calculate_performace(len(real_labels), proba,  real_labels)
	fpr, tpr, auc_thresholds = roc_curve(real_labels, ae_y_pred_prob)
	auc_score = auc(fpr, tpr)",DeepInteract_new.py,sperfu/DeepInteract-for-CircRNA-Disease-Inference-,1
"
        self.assertRaises(exceptions.UserError,
                          functions.verify_dataset,
                          [[[1, 2, 2]], [[2, 3, 5]]],
                          [1, 2, 3])


class TestIsValidJSON(unittest.TestCase):
    def test_is_valid_json(self):
        assert functions.is_valid_json({'x': ['i am serializable', 0.1]})
        assert not functions.is_valid_json({'x': RandomForestClassifier()})


class TestMakeSerializable(unittest.TestCase):
    def test_make_serializable(self):
        assert functions.is_valid_json({'x': ['i am serializable', 0.1]})
        assert not functions.is_valid_json({'x': RandomForestClassifier()})
        assert functions.make_serializable(
            {
                'x': ['i am serializable', 0.1],",xcessiv/tests/test_functions.py,reiinakano/xcessiv,1
"# RandomForestFilename = ""randomForest500Model""
#
# train_dataset = pd.read_csv(""../../Data/act_train_features_reduced.csv"")
# test_dataset = pd.read_csv(""../../Data/act_test_features_reduced.csv"")
# train_output = pd.read_csv(""../../Data/act_train_output.csv"")
#
# train_dataset = pd.merge(train_dataset, train_output, on=""activity_id"", how='inner')
# print(""--- %s seconds ---"" % (time.time() - start_time))
#
# randomForestModel = Utility.loadModel(""randomForestModel_OHE"")
# # randomForestModel = RandomForestClassifier(n_estimators=500)
# #
# # randomForestModel.fit(train_dataset[columns], train_dataset[[""outcome""]].values.ravel())
#
# prob_train = randomForestModel.predict_proba(train_dataset[columns])
# prob_test = randomForestModel.predict_proba(test_dataset[columns])
# # Utility.saveModel(randomForestModel, ""randomForestModel_OHE"")
#
# train_dataset[""Random_Forest_1""] = prob_train[:,1]
#",Initial_Classification_Models/Ensemble/RandomForest500XGBoost.py,BhavyaLight/kaggle-predicting-Red-Hat-Business-Value,1
"        'st': Stacking and then training on all data to predict test using save final model with cross-validation
              -- Use this for train / test splits of data setup like time series
        'cv': Only cross validation without saving the prediction.
              -- Use this for the final level ensembler to get a feel for the loss.
        :param feval: The evaluation function e.g sklearn.metrics.log_loss.
                      This function is expected to have the following inputs feval(y_prob, y_true)
        """"""
        # Check that the estimator is a dictionary of estimators and names.
        if not isinstance(base_estimators_dict, dict):
            raise ValueError(""\nbase_estimators_dict must be a dictionary of estimator and name e.g\n""
                             ""estimators = {RandomForestClassifier(n_estimators=123, random_state=42): 'RFC1',\n""
                             ""              RandomForestClassifier(n_estimators=321, random_state=56), 'RFC2'}"")
        # Check that the estimator type has been set.
        if not (estimator_type in ('classification', 'regression')):
            raise ValueError(""estimator_type must be either 'classification', 'regression'"")
        # Check that the stack type has been set.
        if not (stack_type in ('s', 't', 'st', 'cv')):
            raise ValueError(""stack_type must be either 's', 't', 'st', or 'cv'"")

        self.base_estimators = list(base_estimators_dict.keys())",gestalt/stackers/stacking.py,mpearmain/gestalt,1
"        chunks[ui == cvcode] += np.int(k + 1)   ## +1 to adj for -1 init
        
    # cv = LeavePLabelOut(chunks, p=len(uncvcode)/n_fold) ## int div
    cv = StratifiedKFold(chunks, n_folds=n_fold, indices=True)

    assert X.shape[0] == chunks.shape[0], ""X and chunks length mismatch""       
    assert np.sum(chunks == -1) == 0, ""Chunks is malformed""  

# Clf
if args.clf == ""RandomForestClassifier"":
    clf = RandomForestClassifier(n_estimators=500, max_features=None)
elif args.clf == ""GradientBoostingClassifier"":   
    clf = GradientBoostingClassifier(
            n_estimators=100, learning_rate=1.0, 
            max_depth=1, random_state=prng
            )
else:
    raise ValueError(""--clf not understood"")

# Go",meta/within.py,parenthetical-e/wheelerexp,1
"        y_pred2 = rfe.fit(X, y).predict(X)
    assert_array_equal(y_pred, y_pred2)


def test_rfe_features_importance():
    generator = check_random_state(0)
    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target

    clf = RandomForestClassifier(n_estimators=20,
                                 random_state=generator, max_depth=2)
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    rfe.fit(X, y)
    assert_equal(len(rfe.ranking_), X.shape[1])

    clf_svc = SVC(kernel=""linear"")
    rfe_svc = RFE(estimator=clf_svc, n_features_to_select=4, step=0.1)
    rfe_svc.fit(X, y)
",scikit-learn-0.17.1-1/sklearn/feature_selection/tests/test_rfe.py,RPGOne/Skynet,1
"        training_file = '%s/../database/%s.json' % (
            os.path.dirname(os.path.realpath(inspect.getfile(self.__class__))),
            kwargs.get('training_file_for_query')
            )
        training_database = json.load(open(training_file))['data']
        training_data = [(data,int(classe)) for classe in [""0"", ""1""] for data in training_database[classe]]
        self.classifier = NaiveBayesClassifier(training_data)

        self.partners = Database('partners_fake', parse_db=True) #default fixed to partners
        self.fields = Database('fields') #lexical fields of different features in db
        self.clf = self.train_feature_finder(self.fields.db, RandomForestClassifier(n_estimators=20))

    def process(self, statement):
        
        confidence = self.classifier.classify(statement.text.lower())
        entry, query = self.build_query(statement.text.lower())
        if entry is None:
            if query is None:
                response = Statement('Je sais que tu cherches a savoir quelquechose sur les partners LV, mais je vais avoir besoin que tu me clarifies tout a !')
            else:",chatterbot_fork/logic/query_adapter.py,arnauddelaunay/BotValue-public,1
"from sklearn import datasets
from sklearn.cross_validation import train_test_split
import matplotlib.pyplot as pl
from matplotlib.colors import ListedColormap
from sklearn.ensemble import RandomForestClassifier

iris = datasets.load_iris()
X = iris.data[:, [2, 3]]
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=4)
forest.fit(X_train, y_train)
y_pred = forest.predict(X_test)

def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])
",final/code/sk_learn/decision_forest.py,Mageluer/computational_physics_N2014301040052,1
"        #                                    steps=20000,
        #                                    learning_rate=0.001)),
        ('SVM, adj.', SVC(probability=False,
                          kernel=""rbf"",
                          C=2.8,
                          gamma=.0073,
                          cache_size=200)),
        ('SVM, linear', SVC(kernel=""linear"", C=0.025, cache_size=200)),
        ('k nn', KNeighborsClassifier(3)),
        ('Decision Tree', DecisionTreeClassifier(max_depth=5)),
        ('Random Forest', RandomForestClassifier(n_estimators=50, n_jobs=10)),
        ('Random Forest 2', RandomForestClassifier(max_depth=5,
                                                   n_estimators=10,
                                                   max_features=1,
                                                   n_jobs=10)),
        ('AdaBoost', AdaBoostClassifier()),
        ('Naive Bayes', GaussianNB()),
        ('Gradient Boosting', GradientBoostingClassifier()),
        ('LDA', LinearDiscriminantAnalysis()),
        ('QDA', QuadraticDiscriminantAnalysis())",ML/mnist/many-classifiers/python.py,MartinThoma/algorithms,1
"
        # train and fit pipeline for each cluster
        for cl in range(n_clusters):
            print(""Fitting pipeline for cluster %s...""%cl)
            relevant_cases = data_freqs[cluster_assignments == cl].index

            if len(relevant_cases) == 0:
                continue
            
            if classifier == ""rf"":
                cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=best_params[dataset_name][method_name]['rf_max_features'], random_state=random_state)
                
            elif classifier == ""gbm"":
                cls = GradientBoostingClassifier(n_estimators=best_params[dataset_name][method_name]['gbm_n_estimators'], max_features=best_params[dataset_name][method_name]['gbm_max_features'], learning_rate=best_params[dataset_name][method_name]['gbm_learning_rate'], random_state=random_state)
                
            else:
                print(""Classifier unknown"")
                break
                
            feature_combiner = FeatureUnion([(method, init_encoder(method)) for method in methods])",experiments_final/run_all_cluster.py,irhete/predictive-monitoring-benchmark,1
"print 'accuracy: %0.5f' % metrics.zero_one_score(rf_benchmark_targets, predicted_targets)
print 'loss: %d' % metrics.zero_one(rf_benchmark_targets, predicted_targets)

submitFile(predicted_targets, 'knn')

# ==============================================================================
# Random forest classifier
# ==============================================================================
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, verbose=1)
rf.fit(features_to_train, targets_to_train)

predicted_targets = rf.predict(features_to_test)

# Just print out the precision and f1 scores
print 'precision: %0.5f' % metrics.precision_score(rf_benchmark_targets, predicted_targets)
print 'f1 score: %0.5f' % metrics.f1_score(rf_benchmark_targets, predicted_targets)

# The following scores are used for classification models",digit.py,jhprks/digitrecognizer,1
"        for d in districts:
            district_X_train = X_train[X_train[:, district_idx] == d]
            district_X_train = np.delete(district_X_train, district_idx, 1)
            district_y_train = y_train[X_train[:, district_idx] == d]
            district_X_test = X_test[X_test[:, district_idx] == d]
            district_X_test = np.delete(district_X_test, district_idx, 1)
            print ""Growing forest for"", d

            # Not saving output in Git so make this deterministic 
            # with random_state
            rf = RandomForestClassifier(n_estimators=self.n_trees, n_jobs=-1,
                                        random_state=782629)
            rf.fit(district_X_train, district_y_train)

            district_ys[d] = list(rf.predict(district_X_test))
            print ""Finished"", d

        print ""All predictions made""

        y_hat = []",kaggle/sf-crime/random_forest.py,noelevans/sandpit,1
"    """"""
    #Scale data
    #X = StandardScaler().fit_transform(X)
    #split data to train and test
    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)


    # print X_train
    # print y_train
    # create rfc object
    forest = RandomForestClassifier(n_estimators=n_estimators)
    #fit training data
    prob = forest.fit(X_train, y_train, ).predict_proba(X_test)

    #compute ROC
    fpr, tpr, thresholds = roc_curve(y_test, prob[:, 1])
    roc_auc = auc(fpr, tpr)
    #print fpr, tpr, thresholds
    print ""AUC Random Forest:  "" + str(roc_auc)
",StatisticalModelling.py,bdolenc/Zemanta-challenge,1
"compustat = pd.read_csv('compustat_filenames.csv', sep=',')
de = compustat['delta_sale']
dt = compustat['delta_at']
ds = pd.merge(df, compustat, left_on='Filename', right_on='Filename')


# We split the global matrix ""result"" into a training and a testing set
train, test = validator.split(ds,0.5)

# We fit a Random Forest model 	 (n_estimators default=10, min_samples_leaf default=1)
rf = RandomForestClassifier(n_estimators=100)
rf.fit(train[3].reshape(-1, 1), train[2].reshape(-1, 1))
predictions = rf.predict(test[3].reshape(-1, 1))
test[5] = pd.Series(predictions, index=test.index)

test.columns = ['MD&A_Text','Filename','Actual','MatchDico','WordCrisis','Predicted'] 
print(test)

tab = pd.crosstab(test['Actual'], test['Predicted'], rownames=['Actual'], colnames=['Predicted'], margins=True) # Print confusion matrix
print(tab)",FinancialAnalystV2/main.py,CedricVallee/pythonFinancialAnalyst,1
"
################################
# Initialize classifiers
################################

np.random.seed(1)
print(""Ensemble: LR - linear SVC"")
clf1 = LogisticRegression(random_state=1, C=7)
clf2 = LinearSVC(random_state=1, C=0.4, penalty=""l2"", dual=False)
nb = BernoulliNB()
rfc = RandomForestClassifier(random_state=1, criterion = 'gini', n_estimators=500)
sgd = SGDClassifier(random_state=1, alpha=0.00001, penalty='l2', n_iter=50)


eclf = EnsembleClassifier(clfs=[clf1, clf2,nb, rfc, sgd], weights=[2, 2, 1, 1,2])
np.random.seed(1)
for clf, label in zip([eclf], 
    ['Ensemble']):
    scores = cross_validation.cross_val_score(clf, predictors_tr,targets_tr, cv=2, scoring='accuracy')
    print(""Accuracy: %0.4f (+/- %0.5f) [%s]"" % (scores.mean(), scores.std(), label))",finalModel.py,ahadmushir/whatsCooking,1
"
X = v[:,0:target_index]
y = v[:,target_index]

random = 99 # pick reproducible pseudo-random sequence

n_estimators = int(sys.argv[1])
min_samples_leaf = int(sys.argv[2])

start = time.clock()
clf = RandomForestClassifier(n_estimators=n_estimators, oob_score=False,
                             max_features=""sqrt"", bootstrap=True,
                             min_samples_leaf=min_samples_leaf, criterion=""entropy"",
                             random_state=random)
clf = clf.fit(X, y)
stop = time.clock()

print ""Fitting %d estimators %d min leaf size %f seconds\n"" % (n_estimators,min_samples_leaf,stop-start)",python/shuttle_timing.py,parrt/AniML,1
"        desc = TextProcessing.process(data[10])
        friend = data[11]
        location = TextProcessing.process(data[12])
        geo_enabled = FeatureMapping.mapping_other('geo_enabled', data[13])
        
        y.append(label)
        x.append([text, source, re_tweet, geo, place, hash_tag, media, verified, follower, statues, desc, friend, location, geo_enabled])
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import f1_score, accuracy_score
    clf = RandomForestClassifier()
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    fsc = f1_score(y_test, y_pred)
    acc = accuracy_score(y_test, y_pred)
    print 'f1-score : ',fsc
    print 'accuracy : ',acc
    print y_pred
    print y_test
    ",demo_filter.py,chaluemwut/smcdemo,1
"        # Pipeline(gen_ictal=False, pipeline=[FreqCorrelation(1, 48, 'us', with_corr=True, with_eigen=True)]),
        # Pipeline(gen_ictal=False, pipeline=[FreqCorrelation(1, 48, 'us', with_corr=True, with_eigen=False)]),
        # Pipeline(gen_ictal=False, pipeline=[FreqCorrelation(1, 48, 'us', with_corr=False, with_eigen=True)]),
        # Pipeline(gen_ictal=False, pipeline=[FreqCorrelation(1, 48, 'none', with_corr=True, with_eigen=True)]),
        # Pipeline(gen_ictal=False, pipeline=[TimeFreqCorrelation(1, 48, 400, 'us')]),
        # Pipeline(gen_ictal=False, pipeline=[TimeFreqCorrelation(1, 48, 400, 'usf')]),
        # Pipeline(gen_ictal=False, pipeline=[TimeFreqCorrelation(1, 48, 400, 'none')]),
    ]
    classifiers = [
        # NOTE(mike): you can enable multiple classifiers to run them all and compare results
        # (RandomForestClassifier(n_estimators=50, min_samples_split=1, bootstrap=False, n_jobs=4, random_state=0), 'rf50mss1Bfrs0'),
        # (RandomForestClassifier(n_estimators=150, min_samples_split=1, bootstrap=False, n_jobs=4, random_state=0), 'rf150mss1Bfrs0'),
        # (RandomForestClassifier(n_estimators=300, min_samples_split=1, bootstrap=False, n_jobs=4, random_state=0), 'rf300mss1Bfrs0'),
        (RandomForestClassifier(n_estimators=3000, min_samples_split=1, bootstrap=False, n_jobs=4, random_state=0), 'rf3000mss1Bfrs0'),
    ]
    cv_ratio = 0.5

    def should_normalize(classifier):
        clazzes = [LogisticRegression]
        return np.any(np.array([isinstance(classifier, clazz) for clazz in clazzes]) == True)",seizure_detection.py,MichaelHills/seizure-detection,1
"    param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
                  'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]}
    clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)
    clf.fit(X_train, Y_train)
    return clf


def classify_rf(X_train, Y_train):
    param_grid = {'n_estimators': [50, 200, 700],
                  'max_features': ['auto', 'sqrt', 'log2']}
    clf = GridSearchCV(RandomForestClassifier(n_estimators=500, oob_score=True), param_grid)
    clf.fit(X_train, Y_train)
    return clf


def classify_gp(X, Y):
    # Using same lengthscale for all features
    kernel = 1.0 * RBF([1.0])
    gpc_rbf = GaussianProcessClassifier(kernel=kernel).fit(X, Y)
    return gpc_rbf",selam/prepdata.py,jinified/selam,1
"# n_training['isInteracted'] = np.array([0] * len(n_training['isInteracted']))
# n_testing['isInteracted'] = np.array([0] * len(n_testing['isInteracted']))

X_train, y_train = get_params(df=p_training.append(n_training))
X_test, y_test = get_params(df=p_testing.append(n_testing))

print len(y_train), len(y_test)
lr = LogisticRegression()
gnb = GaussianNB()
svc = LinearSVC(C=1.0)
rfc = RandomForestClassifier(n_estimators=100)

# plt.figure(figsize=(10, 10))
# ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
# ax2 = plt.subplot2grid((3, 1), (2, 0))
#
# ax1.plot([0, 1], [0, 1], ""k:"", label=""Perfectly calibrated"")
for clf, name in [(lr, 'Logistic'),
                  (gnb, 'Naive Bayes'),
                  (svc, 'Support Vector Classification'),",old/experimentos/m_svm.py,jblupus/PyLoyaltyProject,1
"train_data = vectorizer.fit_transform(text_data[:20000])

train_data_features = train_data.toarray()

print len(train_data_features)
print len(train_data_features[0])

'''
vocab = vectorizer.get_feature_names()

forest = RandomForestClassifier(n_estimators=100)
df = train
train = df.sample(frac=0.8, random_state=200)
test = df.drop(train.index)
forest = forest.fit(train_data_features, train['sentiment'])

num_of_reviews = len(test['review'])
clean_test_review = []

for i in xrange(0, num_of_reviews):",machine-learning-algorithms/NLP/main.py,TRBaldim/Projeto-BigData,1
"model = SGDClassifier()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)
acc_8 = round(accuracy_score(Y_test, Y_pred)*100, 2)            #14.46% accuracy

model = DecisionTreeClassifier()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)
acc_9 = round(accuracy_score(Y_test, Y_pred)*100, 2)            #99.99% accuracy

model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)
acc_10 = round(accuracy_score(Y_test, Y_pred)*100, 2)

models = pd.DataFrame({
    'Model' : ['BernoulliNB', 'SVC', 'KNeighbors', 'Gaussian', 'Perceptron', 'Linear SVC', 'Stochastic Gradient Decent', 'Decision Tree', 'Random Forest'],
    'Accuracy Score' : [acc_1, acc_3, acc_4, acc_5, acc_6, acc_7, acc_8, acc_9, acc_10]
    })  #Try adding in logistic regression much later (since it takes so long to run)
models.sort_values(by='Score', ascending=False)",Kaggle_Projects/PokemonGO predictions.py,MichaelMKKang/Projects,1
"                                                                         y_train)
    print ""Accuracy with selected f in testset: %f"" % classifier_.score(x_test_std[:, k5],
                                                                        y_test)

    features_list = df_wine.columns[1:]
    print ""Top five features are: %s"" % features_list[k5]

    # Feature reduction using random forest
    """"""
    features_list = df_wine.columns[1:]
    forest = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)
    forest.fit(x_train, y_train)
    feature_importance = forest.feature_importances_
    indices = np.argsort(feature_importance)[::-1]

    for f in range(x_train.shape[1]):
        print ""%d) %s -- %f"" % (f, features_list[f], feature_importance[f])

    plt.bar(range(x_train.shape[1]),
            feature_importance[indices],",Datasets/UCI_Wine/WineClassification.py,nitish-tripathi/Simplery,1
"import unittest
from Boruta import BorutaPy
from sklearn.ensemble import RandomForestClassifier


class BorutaTestCases(unittest.TestCase):

    def test_get_tree_num(self):
        rfc = RandomForestClassifier(max_depth=10)
        bt = BorutaPy(rfc)
        self.assertEqual(bt._get_tree_num(10),44,""Tree Est. Math Fail"")
        self.assertEqual(bt._get_tree_num(100),141,""Tree Est. Math Fail"")



if __name__ == '__main__':
    unittest.main()
",bcml/Boruta/test/unit_tests.py,sandialabs/BioCompoundML,1
"            self.classifier = classifier
        else:
            self.classifier = self._build_classifier()
            print(""Using {} classifier"".format(self.classifier))

        self.classes = classes

    def _build_classifier(self):
        classifiers = {
            ""SVM"": SVC(),
            ""RF"": RandomForestClassifier(),
            ""LDA"": LinearDiscriminantAnalysis(),
        }
        clf = classifiers[self.config[""classifier_name""]]
        clf.set_params(**self.config[""classifier_params""])
        return clf

    def classification_probas(self, X_train, y_train, X_test):
        self.classifier.fit(X_train, y_train)
        return self.classifier.predict_proba(X_test)",ml/classification.py,pbrusco/ml-eeg,1
"        self.estimator = None
        self.best_params = best_params
        self.model_type = model_type

    def build_best_estimator(self):
        if self.model_type == ""Decision_Tree"":
            model = tree.DecisionTreeClassifier(max_depth = 6, min_samples_leaf = 30)
            self.estimator = model.fit(self.x_train, self.y_train)
            tree.export_graphviz(self.estimator, out_file= output_dir + ""/tree.dot"")
        else:
            model = ensemble.RandomForestClassifier(**self.best_params)
            self.estimator = model.fit(self.x_train, self.y_train)
        return model, self.estimator

    def save_estimator(self, model_target):
        print (""Saving model..."")
        model, estimator = self.build_best_estimator()
        joblib.dump(model, model_target + '/' + self.model_type + '_model.pkl')

    def score_estimator_train(self):",love_matcher/refactored/training/training.py,xebia-france/luigi-airflow,1
"               ('/disk/data1/s1145806/cached_hlf_train_data_raw_ranged.pkl'     , '/disk/data1/s1145806/cached_hlf_test_data_raw_ranged.pkl'     )
               ]

n_trees = 500
max_depth = 10
n_jobs = 24

print ""{} trees, {} deep, (n_jobs={})"".format(n_trees,max_depth,n_jobs)

settings = neukrill_net.utils.Settings('settings.json')
clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_trees, max_depth=max_depth, min_samples_leaf=3, n_jobs=n_jobs, random_state=42)

for pathpair in cache_paths:
    print pathpair
    out_fname = pathpair[0][:-4] + ""{}trees_{}deep"".format(n_trees,max_depth) + '_predictions.csv'
    predict(pathpair, out_fname, clf, settings)



cache_paths = [",test_hlf_cache.py,Neuroglycerin/neukrill-net-work,1
"""""""
criterion = [""gini"", ""entropy""]
trees = [10,20,50]
samples = [20,50,100,500]
rounds = 10

for c in criterion:
    for t in trees:
        for s in samples:
            print(""===== Criterion: %s, Trees: %d, Samples/Leaf: %d =====""%(c, t, s))
            rf = RandomForestClassifier(criterion=c, n_estimators=t, min_samples_split=s)
            fit = rf.fit(data, labels)
            score = rf.score(data, labels)
            print(""Training Score: %f""%(score))
            print(""Cross Validation Score: %f""%(cross_val(data, labels, 10, rounds, rf)))
""""""

rf = RandomForestClassifier(criterion=""gini"", n_estimators=50, min_samples_split=50)
score = cross_val_topX(data, labels, 10, 5, rf, 10)",project/code/analysis/reviews_rf.py,cycomachead/info290,1
"
    #transform Y_nn and Y_nn_test 
    Y_train[Y_train < 5] = 0
    Y_train[Y_train >= 5] = 1

    Y_test[Y_test < 5] = 0
    Y_test[Y_test >= 5] = 1

    #max_features values: sqrt(n_features)/2, sqrt(n_features), 2*sqrt(n_features)
    #n_features == sqrt(10) ~ 3.16
    clf = RandomForestClassifier(n_estimators=10, max_features=5, oob_score=True)
    clf.fit(X_train, Y_train)

    importances = clf.feature_importances_
    std = np.std([tree.feature_importances_ for tree in clf.estimators_],
             axis=0)

    indices = np.argsort(importances)[::-1]

    # Print the feature ranking",src/ensemble_models.py,c-m/Licenta,1
"
def loadData(datafile):
    return pd.read_csv(datafile)


def createDecisionTree():
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createRandomForest():
    clf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createExtraTree():
    clf = ExtraTreesClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createAdaBoost():
    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    clf = AdaBoostClassifier(dt, n_estimators=300)",kaggle-forest-cover-type-prediction/src/classification-master.py,KellyChan/Kaggle,1
"graph.write_png('tree.png') 
from IPython.core.display import Image 
Image(filename='tree.png')

##
score1=cross_validation.cross_val_score(clf, x, y,cv=10)
print score1


##
clf2=RandomForestClassifier(n_estimators=1000,criterion='entropy',min_samples_leaf=8,random_state=1,n_jobs=5)
#print (clf2)
print clf2.fit(x_train,y_train)

##
print(""{:.16f}"".format(clf2.score(x_test,y_test)))
print(""{:.16f}"".format(clf2.score(x_train,y_train)))

#,#
score2=cross_validation.cross_val_score(clf2,x,y,cv=10)",randomforest-/f.py,YISION/yision.github.io,1
"
from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

from skopt.gp_opt import gp_minimize


digits = load_digits()
X, y = digits.data, digits.target
rfc = RandomForestClassifier(random_state=10)

def compute_mean_validation_score(forest_params):
    max_depth, max_features, mss, msl = forest_params

    rfc.set_params(
        max_depth=max_depth, max_features=max_features,
        min_samples_split=mss, min_samples_leaf=msl)

    return -np.mean(cross_val_score(rfc, X, y, cv=3, n_jobs=-1))",examples/plot_random_vs_gp.py,betatim/BlackBox,1
"        sub_model_file = join(dirname(__file__),
            '../classifier_models/sub_model.p')
        training_file = open(sub_model_file)
        training_data = pickle.load(training_file)
        training_file.close()
        sub_target_file = join(dirname(__file__),
            '../classifier_models/sub_targets.p')
        targets_file = open(sub_target_file)
        targets = pickle.load(targets_file)
        targets_file.close()
        self.clf = RandomForestClassifier()
        self.clf.fit(training_data, targets)

    def predict(self, feature_vector):",classifiers/lexent_classifier_sub.py,gavinmh/entailment-api,1
"
class Classifier(BiPlot):
    '''
    To hold methods and data to support classification of measurements in a STOQS database.
    See http://scikit-learn.org/stable/auto_examples/plot_classifier_comparison.html
    '''
    classifiers = { 'Nearest_Neighbors': KNeighborsClassifier(3),
                    'Linear_SVM': SVC(kernel=""linear"", C=0.025),
                    'RBF_SVM': SVC(gamma=2, C=1),
                    'Decision_Tree': DecisionTreeClassifier(max_depth=5),
                    'Random_Forest': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
                    'AdaBoost': AdaBoostClassifier(),
                    'Naive_Bayes': GaussianNB(),
                    'LDA': LDA(),
                    'QDA': QDA()
                  }
    def getActivity(self, mpx, mpy):
        '''
        Return activity object which MeasuredParameters mpx and mpy belong to
        '''",contrib/analysis/classify.py,mikemccann/stoqs,1
"        #if 'noncoding_mutation_rate' in df.columns:
            #tmpdf['noncoding_mutation_rate'] = df['noncoding_mutation_rate']
        #if 'expression' in df.columns:
            #tmpdf['expression'] = df['expression']
        #if 'replication_time' in df.columns:
            #tmpdf['replication_time'] = df['replication_time']

        self.x, self.y = features.randomize(df)

        # setup classifier
        self.clf = RandomForestClassifier(n_estimators=ntrees)

    def _update_metrics(self, y_true, y_pred, onco_prob, tsg_prob):
        super(RandomForest, self)._update_metrics(y_true, y_pred, onco_prob, tsg_prob)

        # evaluate feature importance for random forest
        self.feature_importance.append(self.clf.feature_importances_)

    def _update_onco_metrics(self, y_true, y_pred, prob):
        super(RandomForest, self)._update_onco_metrics(y_true, y_pred, prob)",src/classify/python/random_forest_clf.py,KarchinLab/2020plus,1
"    if test_data[i,7] == '':
        test_data[i,7] = np.median(test_data[(test_data[0::,7] != '') &\
                                             (test_data[0::,0] == test_data[i,0])\
            ,7].astype(np.float))

test_data = np.delete(test_data,[1,6,8],1) #remove the name data, cabin and ticket

#The data is now ready to go. So lets train then test!

print 'Training '
forest = RandomForestClassifier(n_estimators=100)

forest = forest.fit(train_data[0::,1::],\
                    train_data[0::,0])

print 'Predicting'
output = forest.predict(test_data)

open_file_object = csv.writer(open(""myfirstforest.csv"", ""wb""))
open_file_object.writerow([""PassengerId"",""Survived""])",src/models/myfirstforest.py,queirozfcom/titanic,1
"    if M.startswith('RFC'):
        return get_RFC()
    elif M.startswith('ETC'):
        return get_ETC()
    elif M.startswith('GBC'):
        return get_GBC()
    elif M.startswith('ADD'):
        return get_ADD()

def get_RFC(n_estimators=500,max_depth=None,n_jobs=-1,criterion='entropy'):
    return RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=2, min_samples_leaf=1
                ,max_features='auto', bootstrap=False, oob_score=False, n_jobs=n_jobs, min_density=None)

def get_ETC(n_estimators=500,max_depth=10,n_jobs=-1,criterion='gini'):
    return ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth, criterion=criterion,min_samples_split=2, min_samples_leaf=1, max_features=None
     , bootstrap=False, oob_score=False, n_jobs=n_jobs)

def get_GBC(n_estimators=70,learning_rate=0.07,max_depth=5):
    return GradientBoostingClassifier(loss='deviance', learning_rate=learning_rate, n_estimators=n_estimators, subsample=1.0, min_samples_split=2
     , min_samples_leaf=1, max_depth=max_depth, max_features=None)",ensemble.py,euclides5414/kaggle-allstate-purchase,1
"    return self.labels[num]

class Classifier(object):
  def __init__(self):
    self.classifier = self._NewClassifier()
    self.labels_enum = StrEnumerator()
    self.features_enum = StrEnumerator()

  def _NewClassifier(self):
    if args.classifier == ""RandomForest"":
      return ensemble.RandomForestClassifier(n_estimators=400)
    elif args.classifier == ""SVM"":
      return svm.SVC(kernel='rbf', gamma=0.0001, C=1000000.0)
    elif args.classifier == ""GBRT"":
      return ensemble.GradientBoostingClassifier(
          n_estimators=100, learning_rate=0.2, max_depth=2, random_state=0)
    else: 
      assert False, (""Unknown classifier:"", args.classifier)

  def FromSparse(self, Xsparse):",src/classify.py,ytsvetko/adjective_supersense_classifier,1
"# def experiments_100():
# 	names = [""3 Nearest Neighbors"",  ""Decision Tree"",
# 	         ""Random Forest"", ""AdaBoost"",
# 	         ""Naive Bayes"", ""Logistic Regression""
# 	        ]  ## ""Linear SVM"", ""RBF SVM"", ""Linear Discriminant Analysis"", ""Quadratic Discriminant Analysis""
# 	classifiers = [
# 	    KNeighborsClassifier(3),
# 	#     SVC(kernel=""linear"", C=0.025),  ## very slow (can't wait)  
# 	#     SVC(gamma=2, C=1), ## very slow (can't wait)
# 	    DecisionTreeClassifier(max_depth=5),
# 	    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
# 	    AdaBoostClassifier(),
# 	    GaussianNB(),
# 	#     LinearDiscriminantAnalysis(),
# 	    LogisticRegression(penalty='l2', class_weight='balanced', solver='liblinear')
#     	]


# 	bigram_vectorizer = CountVectorizer(min_df=1, token_pattern='\w+\-*\w+', ngram_range=(1, 2))
# 	results_dict = dict()",code/code/predict.py,Seondong/MSRA_proposal_wifi_log,1
"
        if options.debug: print(""Fitting..."")

        if options.method==""SVM"":
            clf = svm.SVC()
        elif options.method==""nuSVM"":
            clf = svm.NuSVC()
        elif options.method=='NN':
            clf = neighbors.KNeighborsClassifier(options.n)
        elif options.method=='RanForest':
            clf = ensemble.RandomForestClassifier(n_estimators=options.n,random_state=options.random)
        elif options.method=='AdaBoost':
            clf = ensemble.AdaBoostClassifier(n_estimators=options.n,random_state=options.random)
        elif options.method=='tree':
            clf = tree.DecisionTreeClassifier(random_state=options.random)
        else:
            clf = svm.LinearSVC()
        
        #scores = cross_validation.cross_val_score(clf, training_X, training_Y)
        #print scores",tools/error_correct.py,BIC-MNI/pyezminc,1
"                           converters={0:lambda s: ord(s.split(""\"""")[1])})
    trainDataResponse = trainData[:,1]
    trainDataFeatures = trainData[:,0]

    # Train H2O GBM Model:
    #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")
    rf_h2o = h2o.random_forest(x=alphabet[['X']], y=alphabet[""y""], ntrees=1, max_depth=1, nbins=100)

    # Train scikit GBM Model:
    # Log.info(""scikit GBM with same parameters:"")
    rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
    rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

    # h2o
    rf_perf = rf_h2o.model_performance(alphabet)
    auc_h2o = rf_perf.auc()

    # scikit
    auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_smallcatRF.py,YzPaul3/h2o-3,1
"import scipy
import scipy.io as sio
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import train_test_split
from sklearn.externals import joblib
import pickle

class RandomForest():
    def __init__(self):
        self.rf = RandomForestClassifier(n_estimators=150, min_samples_split=2, n_jobs=-1)
        self.accuracy = 0
        self.y_out = []

    def train(self, X_train, y_train):
        self.rf.fit(X_train, y_train.ravel())

    def test(self, X_test):
        self.y_out = self.rf.predict(X_test)
",classifiers/randomForest.py,gupta-abhay/Statistical-Review-Models,1
"        ('LR', LogisticRegression()),
        ('LDA', LinearDiscriminantAnalysis()),
        ('CART', DecisionTreeClassifier()),
        ('KNN', KNeighborsClassifier()),
        ('NB', GaussianNB()),
        ('K Neighbors', KNeighborsClassifier(3)),
        ('SVM Linear', SVC(kernel=""linear"", C=0.025)),
        ('SVM Gamma', SVC(gamma=2, C=1)),
        ('GaussianProcessClassifier', GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True)),
        ('Decision Tree Classifier', DecisionTreeClassifier(max_depth=5)),
        ('Random Forest Classifier', RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),
        ('MLP Classifier', MLPClassifier(alpha=1)),
    ])

    @classmethod
    def score(cls, filename):
        names = ['SLOC', 'COMMENTS', 'UDFS', 'LEVEL', 'SUBMISSION', 'GRADE']
        dataframe = pandas.read_csv(filename, names=names)
        array = dataframe.values
        X = array[:, 0:5]",bricklayer/doctor/classifier.py,cbrentharris/bricklayer,1
"    ids_valid = [""C""] * n_valid
    valid_dataset = dc.data.DiskDataset.from_numpy(
        X_valid, y_valid, w_valid, ids_valid, tasks)

    transformers = []
    classification_metric = dc.metrics.Metric(
        dc.metrics.matthews_corrcoef, np.mean, mode=""classification"")
    params_dict = {""n_estimators"": [1, 10]}
    def multitask_model_builder(model_params, model_dir):
      def model_builder(model_dir):
        sklearn_model = RandomForestClassifier(**model_params)
        return dc.models.SklearnModel(sklearn_model, model_dir)
      return dc.models.SingletaskToMultitask(tasks, model_builder, model_dir)

    optimizer = dc.hyper.HyperparamOpt(multitask_model_builder)
    best_model, best_hyperparams, all_results = optimizer.hyperparam_search(
      params_dict, train_dataset, valid_dataset, transformers,
      classification_metric, logdir=None)

  def test_multitask_tf_mlp_ECFP_classification_hyperparam_opt(self):",deepchem/hyper/tests/test_hyperparam_opt.py,bowenliu16/deepchem,1
"
# Random Forest implementation
def predict_rf(name, dataset):
    if os.path.isfile(""./data/pipe_rf.pkl"") and dataset is None:         
        file_rf = open('./data/pipe_rf.pkl', 'rb')
        pipe_rf = pickle.load(file_rf)
    else:
        file_rf = open('./data/pipe_rf.pkl', 'wb')
        pipe_rf = Pipeline([('vect', CountVectorizer(analyzer = 'char_wb', ngram_range=(2,6))),
                            ('tfidf', TfidfTransformer()),
                            ('clf', RandomForestClassifier(n_estimators=10, n_jobs=-1))])        
        dataset = load_data(dataset)
        pipe_rf = pipe_rf.fit(dataset[0].ravel(), dataset[2].ravel())
        pickle.dump(pipe_rf, file_rf)

        #Akurasi
        predicted = pipe_rf.predict(dataset[1].ravel())
        Akurasi = np.mean(predicted == dataset[3].ravel())*100
        print(""Akurasi :"", Akurasi, ""%"")
    ",jenis-kelamin.py,irfani/Jenis-Kelamin,1
"
validation_size = 0.20
seed = 7
X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size = validation_size, random_state = seed)

scoring = 'accuracy'

kfold = model_selection.KFold(n_splits = 10, random_state = seed)
cv_log_result = model_selection.cross_val_score(LogisticRegression(), X_train, Y_train, cv = kfold, scoring = scoring)
cv_svc_result = model_selection.cross_val_score(LinearSVC(), X_train, Y_train, cv = kfold, scoring = scoring)
cv_rfc_result = model_selection.cross_val_score(RandomForestClassifier(), X_train, Y_train, cv = kfold, scoring = scoring)
cv_knn_result = model_selection.cross_val_score(KNeighborsClassifier(), X_train, Y_train, cv = kfold, scoring = scoring)
cv_gnb_result = model_selection.cross_val_score(GaussianNB(), X_train, Y_train, cv = kfold, scoring = scoring)

print('Logistic Regression: %f (%f)' % (cv_log_result.mean(), cv_log_result.std()))
print('Linear SVC: %f (%f)' % (cv_svc_result.mean(), cv_svc_result.std()))
print('Random Forest Classifier: %f (%f)' % (cv_rfc_result.mean(), cv_rfc_result.std()))
print('KNN Classifier: %f (%f)' % (cv_knn_result.mean(), cv_knn_result.std()))
print('Gaussian Naive Bayes: %f (%f)' % (cv_gnb_result.mean(), cv_gnb_result.std()))",factors_model_comparison.py,czhroailsky/csv_table_ngram,1
"test_dataset = fold_datasets[-1]

# Get supports on test-set
support_generator = dc.data.SupportGenerator(
    test_dataset, n_pos, n_neg, n_trials)

# Compute accuracies
task_scores = {task: [] for task in range(len(test_dataset.get_task_names()))}
for (task, support) in support_generator:
  # Train model on support
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=100)
  model = dc.models.SklearnModel(sklearn_model)
  model.fit(support)

  # Test model
  task_dataset = dc.data.get_task_dataset_minus_support(
      test_dataset, support, task)
  y_pred = model.predict_proba(task_dataset)
  score = metric.compute_metric(",examples/low_data/muv_rf_one_fold.py,rbharath/deepchem,1
"
def getScores(clf, X, y):
    predictions = clf.predict(X)
    scores = precision_recall_fscore_support(y, predictions, average='binary')
    return scores

def transform(data, transformer):
    return transformer.transform(data).toarray()

def runForest(X_train, y_train):
    forest = RandomForestClassifier(n_estimators=90, random_state=42)
    forest.fit(X_train, y_train)
    return forest

# def runGaussianNB(X_train, X_test, y_train, y_test):
#    clf=GaussianNB()
#    clf.fit(X_train,y_train)
#    return clf

# def runKNN(X_train, X_test, y_train, y_test):",train_algorithm/run.py,xeneta/LeadQualifier,1
"	
	#bayes= GaussianNB()
	#parameters={'priors':[None]}
	#test_clasif('naive bayes',bayes,X_test,X_train,Y_test,Y_train,parameters,['objective','subjective'])
	
	#sgd = SGDClassifier()
	#parameters={'loss':['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 
	#'huber','epsilon_insensitive','squared_epsilon_insensitive'],'alpha':[0.0001,0.001,0.01,0.1,1],'epsilon':[0.01,0.1,1],'random_state':[0]}
	#test_clasif('sgd',sgd,X_test,X_train,Y_test,Y_train,parameters,['objective','subjective'])
	
	#random= RandomForestClassifier()
	#parameters={'n_estimators':[5,10,15,20,25,30],'criterion':['gini','entropy'],'max_features':['sqrt','log2',None],'random_state':[0]}
	#test_clasif('random forest',random,X_test,X_train,Y_test,Y_train,parameters,['objective','subjective'])
	

def init_clasif():
	fileName  = 'Results/featList1.txt'
	data,target,featList=readData(fileName)
	clf = ExtraTreesClassifier(n_estimators=10,random_state=0)
	clf = clf.fit(data,target)",main.py,The-Blitz/BSDT,1
"                                           != '',5].astype(np.float))
#All missing ebmbarks just make them embark from most common place
train_data[train_data[0::,11] == '',11] = np.round(np.mean(train_data[train_data[0::,11]\
                                                   != '',11].astype(np.float)))
train_data = np.delete(train_data,[0,3,8,10],1) #remove the name data, cabin and ticket
#I need to do the same with the test data now so that the columns are in the same
#as the training data

# Create the random forest object which will include all the parameters
# for the fit
Forest = RandomForestClassifier(n_estimators = 100)
print ""Training ....""
# Fit the training data to the training output and create the decision
# trees
Forest = Forest.fit(train_data[0::,1::],train_data[0::,0])

#Open the test file object
test_file_obect = csv.reader(open('data/test.csv', 'rb'))
header = test_file_obect.next()
test_data = []",Titanic/random_forest.py,saran87/machine-learning,1
"    Examples
    --------
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.models import BayesMinimumRiskClassifier
    >>> from costcla.metrics import savings_score
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> f = RandomForestClassifier(random_state=0).fit(X_train, y_train)
    >>> y_prob_test = f.predict_proba(X_test)
    >>> y_pred_test_rf = f.predict(X_test)
    >>> f_bmr = BayesMinimumRiskClassifier()
    >>> f_bmr.fit(y_test, y_prob_test)
    >>> y_pred_test_bmr = f_bmr.predict(y_prob_test, cost_mat_test)
    >>> # Savings using only RandomForest
    >>> print savings_score(y_test, y_pred_test_rf, cost_mat_test)
    0.12454256594
    >>> # Savings using RandomForest and Bayes Minimum Risk",costcla/models/directcost.py,madjelan/CostSensitiveClassification,1
"

if __name__ == '__main__':
    project_name = 'MLBootCamp'
    project = Project(project_name)
    project.load('/mnt/projects/Algomost/corgi/tests/MLB_project/')
    dataset = Dataset().load(project, dataset_id=1, flatten_y=True)

    models = [
        [
            RandomForestClassifier(random_state=5, n_jobs=-1, n_estimators=100),
            KNeighborsClassifier(n_jobs=-1),
            XGBClassifier(objective='multi:softmax', silent=True, learning_rate=0.1),
            LGBMClassifier(objective='multiclass', learning_rate=0.1),
            LogisticRegression(penalty='l2'),
        ],
        [
            Ridge(),
            RandomForestRegressor(random_state=5, n_jobs=-1, n_estimators=100),
            XGBRegressor(objective='reg:linear', learning_rate=0.1),",corgi/model.py,JunkieStyle/corgi,1
"    print(cv.best_params_)
    """"""
    """"""
    factor = cv.best_estimator_.named_steps['clf'].coef_.tolist()
    result = zip(factor, categorical_columns(d))
    result.sort(key=itemgetter(0), reverse=True)
    for a,b in result[:20]:
            print(a,b)
    """"""
    """"""
    p = RandomForestClassifier()
    params = {
            ""max_depth"": [3, None],
            ""max_features"": sp_randint(1, 11),
            ""min_samples_split"": sp_randint(1, 11),
            ""min_samples_leaf"": sp_randint(1, 11),
            ""bootstrap"": [True, False],
            ""criterion"": [""gini"", ""entropy""]
    }
    cv = RandomizedSearchCV(p, params, n_iter=100, cv=best_cv_num(_n), n_jobs=-1, verbose=1)",adult/ml.py,arosh/ml,1
"
num_classes = len(encoder.classes_)
num_features = X.shape[1]

layers0 = [('input', InputLayer),
           ('dense0', DenseLayer),
           ('dropout', DropoutLayer),
           ('dense1', DenseLayer),
           ('output', DenseLayer)]

clf1 = RandomForestClassifier(n_estimators=800, n_jobs=-1)
clf2 = GradientBoostingClassifier(n_estimators=1000)
clf3 = OneVsRestClassifier(SVC(C=5, cache_size=2048), n_jobs=-1)
clf4 = NeuralNet(layers=layers0,

                 input_shape=(None, num_features),
                 dense0_num_units=512,
                 dropout_p=0.5,
                 dense1_num_units=512,
                 output_num_units=num_classes,",src/ensemble_merging.py,ternaus/kaggle_otto,1
"
    if imp_method == 'RandomReplace':
        imp_data = imputer.replace(data, params_dict['miss_data_cond'])
    elif imp_method == 'Drop':
        imp_data = imputer.drop(data, params_dict['miss_data_cond'])
    elif imp_method == 'Summary':
        imp_data = imputer.summarize(data,
                                     params_dict['summary_func'],
                                     params_dict['miss_data_cond'])
    elif imp_method == 'RandomForest':
        clf = RandomForestClassifier(n_estimators=100, criterion='gini')
        imp_data = imputer.predict(data,
                                   params_dict['cat_cols'],
                                   params_dict['miss_data_cond'],
                                   clf)

    elif imp_method == 'SVM':
        clf = SVM()
        imp_data = imputer.predict(data,
                                   params_dict['cat_cols'],",processing.py,rafaelvalle/MDI,1
"    column_transforms = {""default"": default_transform,
                         ""Name"": name_transform,
                         ""SibSp"": sibsp_transform}#,
                         #""Ticket"": ticket_transform}
    # X_train, y_train, X_val, y_val, X_test = load_data(
    #    column_transforms, val_size=.20, drop_columns=columns_to_drop)

    X_train, y_train, X_test = load_data(column_transforms,
                                         drop_columns=columns_to_drop)

    clf = RandomForestClassifier(n_estimators=20)
    clf.fit(X_train, y_train)

    '''
    y_pred = clf.predict(X_val)
    print (classification_report(y_val, y_pred))
    '''
    # '''
    y_pred = clf.predict(X_test)
    test_df = pd.DataFrame(y_pred, columns=[""Survived""])",Titanic/RandomForest.py,nickmarton/kaggle,1
"    labelBase = [sample[target] for sample in data]

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(featData_MinMax, labelBase, test_size=0.1, random_state=0)
    
    classifiers = [
#        ('ada-10', AdaBoostClassifier(n_estimators=10)),
#        ('ada-50', AdaBoostClassifier(n_estimators=50)),
 #       ('ada-100', AdaBoostClassifier(n_estimators=100)),
#        ('svm-1', svm.SVC(cache_size=1000,C=1.0)),
#        ('svm-05', svm.SVC(cache_size=1000,C=0.5)),
 #       ('forest-10', RandomForestClassifier(n_estimators=10)),
 #       ('forest-20', RandomForestClassifier(n_estimators=20)),
        ('forest-50', RandomForestClassifier(n_estimators=50)),
        ('forest-50-min5', RandomForestClassifier(n_estimators=50,min_samples_leaf=5)),
        ('forest-50-min10', RandomForestClassifier(n_estimators=50,min_samples_leaf=10)),
 #       ('forest-5010', RandomForestClassifier(n_estimators=50,max_features=10)),
 #       ('forest-5020', RandomForestClassifier(n_estimators=50,max_features=20)),
    ]    
    
    for name,clf in classifiers:    ",laurent/coFeeClassif.py,sankar-mukherjee/CoFee,1
"    intrusions['attack_cat'] = intrusions['attack'].astype('category')
    del intrusions['attack']
    dummy = pd.get_dummies(intrusions, prefix=['Protocol', 'Service', 'Flag'], prefix_sep='_', columns=['protocol_type', 'service', 'flag'])
    # joblib.dump(dummy, 'models/rf_dummy_df.pkl')

    # # Train model
    X = dummy.ix[:,(dummy.columns != 'attack_cat')]
    y = dummy['attack_cat']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=10)

    # rf = RandomForestClassifier()
    # rf.fit(X_train, y_train)
    # joblib.dump(rf, 'models/rf_model.pkl')

    rf = joblib.load('models/rf_model.pkl')
    y_pred = rf.predict(X_test)
    # joblib.dump(y_test, 'models/rf_y_test.pkl')
    # joblib.dump(y_pred, 'models/rf_y_pred.pkl')

    generate_confusion_matrix(y_test, y_pred)",build_model.py,joshpeng/Network-Intrusions-Flask,1
"    data['y_test'] = min_max_scaler.fit_transform(data['y_test'])
    return data

def DrawTree(clfDT):
   with open(""StackOverflow.dot"", 'w') as f:
      f = tree.export_graphviz(clfDT, out_file=f)

def getOfflineClassifiers(param):
    batch_classifiers = {
        'SVM': svm.SVC(gamma=0.001, C=10., class_weight=""auto""),
        #'Random Forest': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
        'LR': LogisticRegression(class_weight=""auto""),
        'KNN': KNeighborsClassifier(n_neighbors=3),
        'DT': tree.DecisionTreeClassifier(),
        'DBN': DBN([param, 300, 2],learn_rates = 0.3,learn_rate_decays = 0.9,epochs = 2,verbose = 1),
    }
    return batch_classifiers

def initClsStats(classifiers):
    cls_stats = {}",ML/OfflineLearning.py,Nik0l/UTemPro,1
"            aux.iloc[:,c]=(aux.iloc[:,c]-ecocDF.loc[i,'meanNorm'][c])/ecocDF.loc[i,'stdNorm'][c]
        _SVC=cPickle.load(open(resultsPath+'/'+ecocDF.loc[i,'svmID']+'.p','rb'))
        if probability==True and ecocDF.loc[i,'_probability']==True:
            predictDF.loc[:,ecocDF.loc[i,'svmID']]=_SVC.predict_proba(aux)[:,0]
        else:
            predictDF.loc[:,ecocDF.loc[i,'svmID']]=_SVC.predict(aux)
    predictDF.loc[:,'y']=y
    return predictDF

def classifyRFC(predictDF):
    _RFC=ensemble.RandomForestClassifier(100,oob_score=True)
    _RFC.fit(predictDF.drop('y',1),predictDF.y)
    return _RFC.oob_score_
    
def classifyETC(predictDF, n_trees):
    _ETC=ensemble.ExtraTreesClassifier(n_trees,oob_score=True, bootstrap=True)
    _ETC.fit(predictDF.drop('y',1),predictDF.y)",bin/helpers.py,gabrielusvicente/MC886,1
"        elif (model_type == ""linear_regression""):
            self.classification = False
            self.model = eval(""LinearRegression("" + model_params + "")"")
        elif (model_type == ""perceptron""):
            self.model = eval(""Perceptron("" + model_params + "")"")
        elif (model_type == ""extra_trees""):
            self.feat_transformer = DictVectorizer(sparse=False)
            self.model = eval(""ExtraTreesClassifier("" + model_params + "")"")
        elif (model_type == ""random_forest""):
            self.feat_transformer = DictVectorizer(sparse=False)
            self.model = eval(""RandomForestClassifier("" + model_params + "")"")
        elif (model_type == ""ada_boost""):
#            self.feat_transformer = DictVectorizer(sparse=False)
            self.model = eval(""AdaBoostClassifier("" + model_params + "")"")
        elif (model_type == ""sgd_classifier""):
            self.model = eval(""SGDClassifier("" + model_params + "")"")
        elif (model_type == ""baseline""):
            self.model = eval(""DummyClassifier("" + model_params + "")"")
        else:
            print >> sys.stderr, ""Model of type "" + model_type + "" is not supported.""",mlfix/scripts/model.py,varisd/school,1
"
class Train(object):
    """"""docstring for TrainModel""""""
    def preprocess_model(self):
        '''This allows preprocessing using logistic regression'''
        X_train, X_train_lr, y_train, y_train_lr = train_test_split(self.train,
                                                                    self.predictors,
                                                                    test_size=0.5)
        encode = OneHotEncoder()
        logistic = LogisticRegression()
        self.clf = RandomForestClassifier(n_estimators=512,
                                          oob_score=True, n_jobs=-1)
        self.clf.fit(X_train, y_train)
        encode.fit(self.clf.apply(X_train))
        self.predmodel = logistic.fit(encode.transform(self.clf.apply(X_train_lr)), y_train_lr)

    def train_model(self):
        '''This is standard model training'''
        '''For RandomForestClassifier to work their must be no nan values, one
        way of handling this is to use the --impute option. This uses mean",bcml/Train/train_model.py,sandialabs/BioCompoundML,1
"names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA""]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",learn-python-for-data-science-1/classifiers.py,jlcanela/learn-data,1
"        X_train, X_test, y_train, y_test = cross_validation.train_test_split(
            features,
            rewards,
            test_size=0.40,
            # random_state=shuffle,
        )
        logging.info('Data splitted')

        # create classifier
        logging.info('Classifier: training...')
        # rfc = RandomForestClassifier(n_estimators=30)
        rfc = ExtraTreesClassifier(n_estimators=20, oob_score=True, bootstrap=True)
        rfc.fit(X_train, y_train)

        # saving
        logging.info('Classifier: saving...')
        externals.joblib.dump(rfc, 'models/' + currency + '.pkl', compress=9)

        # score
        logging.info('Classifier: scoring...')",06_randomforests/minmax/classifier.py,Tjorriemorrie/trading,1
"    elif sclf == 'lg2':
        clf = linear_model.LogisticRegression(penalty='l2', C=C, class_weight=class_weight)
    elif sclf == 'lgCV':
        clf = linear_model.LogisticRegressionCV(penalty='l2', class_weight=class_weight)
    elif sclf == 'ridgeCV':
        clf = linear_model.RidgeCV()
    elif sclf == 'rf':
        from sklearn.ensemble import RandomForestClassifier
        if not hasattr(RandomForestClassifier,'coef_'):
            RandomForestClassifier.coef_ = property(lambda self:self.feature_importances_)
        clf = RandomForestClassifier(n_estimators=100, max_depth=2, min_samples_leaf=2,
            class_weight=class_weight)
    elif sclf == 'gnb':
        clf = naive_bayes.GaussianNB()
    elif sclf == 'knc':
        clf = neighbors.KNeighborsClassifier(n_neighbors=5)
    else:
        raise ValueError(""bad sclf: {}"".format(sclf))
    return clf
",kgml/predictive_analysis.py,orazaro/kgml,1
"        from sklearn.ensemble import RandomForestClassifier
        if rf_test is False:
            [train_X, train_Y] = pd.read_pickle(util.features_prefix + name + '_XY.pkl')
            [X_train, X_validate, X_test, y_train, y_validate, y_test] = pd.read_pickle(
                util.features_prefix + name + '_XXXYYY.pkl')
            x = np.concatenate([X_train, X_validate], axis=0)
            y = np.concatenate([y_train, y_validate], axis=0)
            print 'rf'
            n_estimator = range(100, 301, 100)
            max_depth = range(5, 26, 1)
            clf = RandomForestClassifier(n_jobs=4)
            parameters = {'n_estimators': n_estimator, 'max_depth': max_depth}
            grid_clf = grid_search.GridSearchCV(clf, parameters)
            grid_clf.fit(np.array(train_X), np.array(train_Y))
            score = grid_clf.grid_scores_
            l1 = [1 - x[1] for x in score if x[0]['n_estimators'] == n_estimator[0]]
            l2 = [1 - x[1] for x in score if x[0]['n_estimators'] == n_estimator[1]]
            l3 = [1 - x[1] for x in score if x[0]['n_estimators'] == n_estimator[2]]
            plt.plot(range(5, 26, 1), l1,
                     'b--')",Step7_basal_classifier_with_parameter_search.py,lyoshiwo/resume_job_matching,1
"
                                fout.write(""%s;%s;%s;%s;%s;%s;%s;%s;%s\n""%(part, dataset_name, method_name, rf_max_features, hmm_n_iter, hmm_n_states, nr_events, ""nrow_train_X"", train_X.shape[0]))
                                fout.write(""%s;%s;%s;%s;%s;%s;%s;%s;%s\n""%(part, dataset_name, method_name, rf_max_features, hmm_n_iter, hmm_n_states, nr_events, ""ncol_train_X"", train_X.shape[1]))
                                fout.write(""%s;%s;%s;%s;%s;%s;%s;%s;%s\n""%(part, dataset_name, method_name, rf_max_features, hmm_n_iter, hmm_n_states, nr_events, ""nrow_test_X"", test_X.shape[0]))
                                fout.write(""%s;%s;%s;%s;%s;%s;%s;%s;%s\n""%(part, dataset_name, method_name, rf_max_features, hmm_n_iter, hmm_n_states, nr_events, ""ncol_test_X"", test_X.shape[1]))

                                # fit classifier
                                print(""Fitting classifier..."")
                                sys.stdout.flush()
                                start = time()
                                cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=rf_max_features, random_state=random_state)
                                cls.fit(train_X, train_y)
                                cls_fit_time = time() - start
                                preds_pos_label_idx = np.where(cls.classes_ == pos_label)[0][0] 
                                del train_X

                                # test
                                print(""Testing..."")
                                sys.stdout.flush()
                                start = time()",experiments_param_optim_cv/run_index_lossless_optim_cv.py,irhete/predictive-monitoring-benchmark,1
"#####################################################################
test_data_features = vectorizer.transform(clean_test_reviews)
test_data_features = test_data_features.toarray()





print ""Training the random forest wait it will take time to train ...""

forest = RandomForestClassifier( n_estimators = 100, n_jobs = -1, verbose = 1 )

forest = forest.fit( train_data_features, train[""sentiment""] )

##############################################################
# Use the random forest to make sentiment label predictions  #
##############################################################

print ""Predicting test labels...\n""
rf_p = forest.predict_proba( test_data_features )",MovieReviewSentimentAnalysis/MovieReveiw/sgd_model.py,anilcs13m/Projects,1
"
classifiers = []
predictions = []
Ncolors = np.arange(1, X.shape[1] + 1)
depths = [5, 20]

for depth in depths:
    classifiers.append([])
    predictions.append([])
    for nc in Ncolors:
        clf = RandomForestClassifier(random_state=0, max_depth=depth,
                                     criterion='entropy')
        clf.fit(X_train[:, :nc], y_train)
        y_pred = clf.predict(X_test[:, :nc])

        classifiers[-1].append(clf)
        predictions[-1].append(y_pred)

predictions = np.array(predictions)
",book_figures/chapter9/fig_rrlyrae_forest.py,eramirem/astroML,1
"#
pred_dir = ""pred_files""
if not os.path.exists(pred_dir):
  os.mkdir(pred_dir)


def train_pred(target, train_pos_multiply=0):
  # 
  d = ci.Dataset(target, train_pos_multiply=train_pos_multiply)
  # random forest clf
  clf = RandomForestClassifier(n_estimators=100, max_features=1.0/3, n_jobs=10, max_depth=None, min_samples_split=5, random_state=0)
  # fit model
  clf.fit(d.train_features, d.train_labels)
  # save model
  joblib.dump(clf, model_dir + ""/rf_%s.m"" % target)
  # predict class probabilities
  #train_pred_proba = clf.predict_proba(d.train_features)[:, 1]
  test_pred_proba = clf.predict_proba(d.test_features)[:, 1]
  # save pred
  test_pred_file = open(pred_dir + ""/test_%s.pred"" % target, ""w"")",rf_model/chembl_rf.py,xiaotaw/chembl,1
"      Fitted clones of the input classifiers

    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.naive_bayes import GaussianNB
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from mlxtend.sklearn import EnsembleClassifier
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> y = np.array([1, 1, 1, 2, 2, 2])
    >>> eclf1 = EnsembleClassifier(clfs=[clf1, clf2, clf3],
    ... voting='hard', verbose=1)
    >>> eclf1 = eclf1.fit(X, y)
    >>> print(eclf1.predict(X))
    [1 1 1 2 2 2]
    >>> eclf2 = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')",mlxtend/classifier/ensemble.py,YoungKwonJo/mlxtend,1
"
# calculate roc and auc
prediction_p = sgd.predict_proba(X_test)
print(""Prediction Probabilities"")
print(prediction_p[:,1])

plot_data(y_test, prediction_p[:,1], ""Linear Classifier"")


# Random Forest
rf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1)
rf1.fit(X_train, y_train)
print('Random Forest classifier: ', rf1)

# calculate predictions with probabilities
prediction_p = rf1.predict_proba(X_test)
plot_data(y_test, prediction_p[:,1], ""Random Forest"")


# SVC",examples/titanic-febi-test.py,remigius42/code_camp_2017_machine_learning,1
"class SelectiveRegressor(object):

    def __init__(self, cutoff, n_trees = 100):
        self.n_trees = n_trees
        self.cutoff = cutoff 
    
    def fit(self,X,Y,W=None):
        n = len(X)
        self.filters = []
        mask = Y <= self.cutoff 
        self.gate = sklearn.ensemble.RandomForestClassifier(n_estimators = self.n_trees)
        self.gate.fit(X, mask)
        self.regressor = sklearn.ensemble.RandomForestRegressor(n_estimators = self.n_trees)
        self.regressor.fit(X[mask], Y[mask])
        return self

    def predict(self, X):
        n = len(X)
        indices = np.arange(n)
        mask = self.gate.predict(X)",selective_regressor.py,iskandr/mhcpred,1
"    bow_rdd = sm.RDD.map(lambda (key, (bow, meta)): (key, bow))
    bow_rdd = sm.RDD.join(sm.target).map(lambda (key, (bow, label)): (label, bow))

    remover = StopWordsRemover(inputCol=""raw"", outputCol=""words"")
    hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=""word_counts"",
                numFeatures=10000)
    tfidf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=""features"",
                minDocFreq=20)
    indexer = StringIndexer(inputCol=""string_label"", outputCol=""label"")

    for model in [GBTClassifier(), RandomForestClassifier(), MultilayerPerceptronClassifier()]:

        if type(model) == MultilayerPerceptronClassifier:
            layers = [10000, 100, 2]
            model = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128)

        pipeline = Pipeline(stages=[remover, hashingTF, tfidf, # scaler,
                                    indexer, model])
        scores = cross_val_score(pipeline, bow_rdd)
        grid_search.debug('Model: %s\nscores: %s\nAverage: %s' \",code/grid_search/grid_search.py,Nathx/parental_advisory_ml,1
"
cache_paths = ['/disk/data1/s1145806/cached_hlf_train_data_raw.pkl',
               '/disk/data1/s1145806/cached_hlf_train3_data_raw.pkl',
               '/disk/data1/s1145806/cached_hlf_train6_data_raw.pkl',
               '/disk/data1/s1145806/cached_hlf_train10_data_raw.pkl',
               '/disk/data1/s1145806/cached_hlf_train15_data_raw.pkl',
               '/disk/data1/s1145806/cached_hlf_train15alt_data_raw.pkl',
               '/disk/data1/s1145806/cached_hlf_train30_data_raw.pkl']

settings = neukrill_net.utils.Settings('settings.json')
clf = sklearn.ensemble.RandomForestClassifier(n_estimators=1000, max_depth=25, min_samples_leaf=3, n_jobs=16, random_state=42)

for path in cache_paths:
    print path
    nll = check_score(path, clf, settings)
    print ""scored {}"".format(nll)
",crossval_numAug.py,Neuroglycerin/neukrill-net-work,1
"
def get_benign_summary_data():
  return map(np.mean, (benign_mse_sum, benign_ssim_sum, benign_border_sum, benign_color_sum))

def build_random_forest_classfier():
  X, y = build_feature_vector()
  X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size = 0.1,
                                     random_state = 0)

  #print len(X_test)
  clf_tmp = RandomForestClassifier(n_estimators = 30)
  clf_tmp = clf_tmp.fit(X_train, y_train)
  print clf_tmp.score(X_test, y_test)
  y_pred = clf_tmp.predict(X_test)
  #print 
  print precision_score(y_test, y_pred)

  #scores = cross_validation.cross_val_score(clf_tmp, X, y, cv = 8)
  #print scores.mean()
",build_model.py,agadiraju/SeniorDesign,1
"def estimate_weights_random_forests(X_s, X_t, X_w):

    X_all, all_labels = prepare_data_for_weights_estimation(X_s, X_t)
    # train logistic regression
    kf = KFold(X_all.shape[0], 10, shuffle=True)
    param_grid_rf = [
      {""n_estimators"": np.array([500]),
       ""max_depth"": np.array([6]),
       # ""max_features"": np.array([1, 2, 4, 8, 16]),
       ""min_samples_leaf"": np.array([100])}]
    rf = GridSearchCV(RandomForestClassifier(50, max_depth=10,
                                             class_weight=""auto"", n_jobs=-1),
              param_grid_rf, cv=kf, n_jobs=-1)
    rf = RandomForestClassifier(100, max_depth=6, min_samples_leaf=200,
                                class_weight=""auto"", n_jobs=-1)
    rf.fit(X_all, all_labels)
    # print ""best parameters for rf weights determination: "", rf.best_estimator_
    probas = rf.predict_proba(X_w)
    weights = probas[:, 1] / probas[:, 0]
    return weights",Modules/Biophotonics/python/iMC/regression/domain_adaptation.py,RabadanLab/MITKats,1
"
class RandomForest:

    model = None
    model_type = None
    column_names = None
    params = None
    trained = None

    def __init__(self, codes, feature_names=None, **kwargs):
        self.model = RandomForestClassifier()
        self.model_type = 'random_forest'
        self.params = kwargs
        self.trained = None

    # X = sparse matrix
    # all_y = DataFrame(?)
    def fit(self, X, all_y):
        y = all_y.values
        self.model.fit(X, y)",core/models/random_forest.py,dallascard/guac,1
"import numpy as np 
import pandas as pd 
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

def randomForestCrossVal(X,Y,n_estimators=10,max_depth=None):
	clf = RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth)
	scores = cross_val_score(clf,X,Y)
	print scores.mean()
	return scores 
	

def randomForest(Xtrain,Xtest,Ytrain,Ytest,n_estimators=10,max_depth=None):
	clf = RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth)
	clf = clf.fit(Xtrain,Ytrain)",py-modules/randomForest.py,jvahala/brew-thing,1
"TRAIN_SPLITS_WINDOW =[[0,1],[1,2],[2,3]]
TEST_SPLITS_WINDOW = [2,3,4]
TRAIN_SPLITS_TACK =[[0,1],[0,1,2],[0,1,2,3]]
TEST_SPLITS_TACK = [2,3,4]

TO_DROP = ['crid', 'officer_id', 'index', 'Unnamed: 0']
FILL_WITH_MEAN = ['officers_age', 'transit_time', 'car_time','agesqrd','complainant_age']

#estimators
CLFS = {
        'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),
        'AB': AdaBoostClassifier(
                    DecisionTreeClassifier(max_depth=1),
                    algorithm=""SAMME"",
                    n_estimators=200),
        'LR': LogisticRegression(penalty='l1', C=1e5),
        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),
        'GB': GradientBoostingClassifier(
                    learning_rate=0.05,
                    subsample=0.5,",pipeline/pipeline.py,ladyson/police-complaints,1
"    'sklearn_adaboost_regressor_source',
    'xgboost_regressor_source',

    # Transformers
    'identity_transformer_source'
]

sklearn_random_forest_classifier_source = \
    """"""from sklearn.ensemble import RandomForestClassifier

base_learner = RandomForestClassifier(random_state=8)
""""""

sklearn_extra_trees_classifier_source = \
    """"""from sklearn.ensemble import ExtraTreesClassifier

base_learner = ExtraTreesClassifier(random_state=8)
""""""

sklearn_logistic_regression_source = \",xcessiv/presets/learnersource.py,reiinakano/xcessiv,1
"    for ftr in sorted(features,reverse=1)[0:top_k_ftrs]:
        print ftr
    
    
    
    
    print(""\n----------------------------"")
    print(""Random Forest"")
    print(""----------------------------"")
    
    forest = RandomForestClassifier(n_estimators=100, random_state=0,
                                  class_weight=""balanced"")
    
    vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.50, lowercase=True)
    
    classifier = Pipeline([
                           ('vect', vectorizer),
                           ('forest', forest)])
    classifier.fit(X_train,y_train)
    y_pred = classifier.predict(X_test)",featurize.py,jessilyn/Redaptor_Psych,1
"        filter_shape_l1=2, step_shape_l1=1, n_l1_output=4,
        filter_shape_l2=2, step_shape_l2=1, n_l2_output=4,
        block_shape=2
    )
    pcanet.validate_structure()

    pcanet.fit(images_train)
    X_train = pcanet.transform(images_train)
    X_test = pcanet.transform(images_test)

    model = RandomForestClassifier(n_estimators=100, random_state=1234, n_jobs=-1)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(""accuracy: "" + str(accuracy))",example.py,IshitaTakeshi/PCANet,1
"        k_neighboors = KNeighborsClassifier()
        n_neighbors = [3, 5, 11, 21, 31]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(k_neighboors, dict(n_neighbors=n_neighbors), X, y)
        f = open('output/gender.knn.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates Random Forest classifier
        random_forest = RandomForestClassifier()
        n_estimators = [2, 3, 5, 10, 20, 40, 60]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(random_forest, dict(n_estimators=n_estimators), X, y)
        f = open('output/gender.randomforest.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates MLP classifier",tests/test_gender.py,fberanizo/author-profiling,1
"#clf = SGDClassifier(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'linearsvm accuracy ', accuracy_score(test_gt,pred)

#clf = LogisticRegression(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'logistic accuracy ', accuracy_score(test_gt,pred)

#clf = RandomForestClassifier(min_samples_leaf=20,n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'rfc accuracy ', accuracy_score(test_gt,pred)
#pred = clf.predict(raw_orig)
#with open('rfc.txt','w') as otf:
#    for p in pred:
#        otf.write(str(int(p)) + '\n')
from keras.utils.np_utils import to_categorical
",learning/1dcnn-nin-basic.py,leonidk/centest,1
"    # Convert the list to array
    t2w_training_data = np.concatenate(t2w_training_data, axis=0)
    t2w_validation_data = np.concatenate(t2w_validation_data, axis=0)
    t2w_testing_data = np.array(t2w_testing_data)
    # Select subset
    t2w_training_data = t2w_training_data[:, t2w_feat_sel_idx]
    t2w_validation_data = t2w_validation_data[:, t2w_feat_sel_idx]
    t2w_testing_data = t2w_testing_data[:, t2w_feat_sel_idx]

    # Train an rf
    crf_t2w = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    crf_t2w.fit(t2w_training_data, training_label)

    # ADC
    # Load data
    adc_training_data = [arr for idx_arr, arr in enumerate(data[1])
                         if idx_arr in idx_patient_training]
    adc_validation_data = [arr for idx_arr, arr in enumerate(data[1])
                         if idx_arr in idx_patient_validation]
    adc_testing_data = data[1][idx_lopo_cv]",pipeline/feature-classification/exp-5/pipeline_classifier_stacking.py,I2Cvb/mp-mri-prostate,1
"    for _ in xrange(n):
        data = raw_input().split()
        testName.append(data[0])
        data.pop(0)
        testData.append({int(x.split(':')[0]) : float(x.split(':')[1]) for x in data})
    testData = transformer.fit_transform(testData).toarray()
    return testData, testName
    
def main():    
    trainData, trainLabel = read_train_data()
    svm = RandomForestClassifier()
    svm.fit(trainData, trainLabel)
    testData, testName = read_test_data()
    testLabel = svm.predict(testData)
    for x, y in zip(testName, testLabel): 
        print x + ' %+d'%(y)
    
if __name__ == ""__main__"":",practice/ai/machine-learning/quora-answer-classifier/quora-answer-classifier.py,EdisonCodeKeeper/hacker-rank,1
"    test  = data[data[""is_test""] == True]

    classes_list = list(set(train[target_name]))
    classes_map  = dict(zip(classes_list, range(len(classes_list))))
    x_train = train[[c for c in train.keys() if c != target_name and c != ""is_test""]]
    y_train = [classes_map[target] for target in train[target_name]]
    x_test  = test [[c for c in test.keys()  if c != target_name and c != ""is_test""]]

    if model_name==""Random Forest"":
        from sklearn.ensemble import RandomForestClassifier
        model = RandomForestClassifier(n_estimators=100)
    elif model_name==""Decision Tree"":
        from sklearn.tree import DecisionTreeClassifier
        model = DecisionTreeClassifier()

    model.fit(x_train, y_train)
    y_hat = model.predict(x_test)
    y_hat = [classes_list[y] for y in y_hat]

    f = open(output_file, ""w"")",accuracy_benchmark/classification.py,benhamner/MachineLearning.jl,1
"test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1) 


# The data is now ready to go. So lets fit to the train, then predict to the test!
# Convert back to a numpy array
train_data = train_df.values
test_data = test_df.values


print 'Training...'
forest = RandomForestClassifier(n_estimators=300)
forest = forest.fit( train_data[0::,1::], train_data[0::,0] )

print 'Predicting...'
output = forest.predict(test_data).astype(int)


predictions_file = open(""myfirstforest.csv"", ""wb"")
open_file_object = csv.writer(predictions_file)
open_file_object.writerow([""PassengerId"",""Survived""])",problems/Kaggle/TitanicMachineLearningfromDisaster/GetStarted/myfirstforest.py,nesterione/problem-solving-and-algorithms,1
"
        # Parameter tuning on RF

        if self.regression:
            models = [MeanRegressor(),
                      GradientBoostingRegressor(), 
                      RandomForestRegressor(n_estimators=300)]
            
        else:
            models = [ModeClassifier(),
                      RandomForestClassifier(n_estimators=300)]

        for ticker in tickers:
            if ticker in self.labels.keys():
                print(ticker)
                print(""distribution of labels:"")
                for i, count in enumerate(np.bincount(self.labels[ticker]['binary'].values)):
                    print(""%d: %d"" % (i, count))
                if self.regression:
                    # min = self.labels[ticker]['pct-delta'].values.min()",src/Fab.py,souljourner/fab,1
"    X_test, ids = X[:, 1:], X[:, 0]
    return X_test.astype(float), ids.astype(str)

def validate_model(model, validation_x, validation_y):
    y_prob = model.predict_proba(validation_x)
    score = logloss_mc(validation_y, y_prob)
    print("" -- {} Multiclass logloss on validation set: {:.4f}."".format(type(model).__name__, score))
    return score

def train_rf(training_x, training_y, n_est=10, max_d=5, max_f='auto'):
    clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, max_depth=max_d, max_features=max_f)
    clf.fit(training_x, training_y)
    return clf
    
def train_ex(training_x, training_y, n_est=10, max_d=5, max_f='auto'):
    clf = ExtraTreesClassifier(n_jobs=-1, n_estimators=n_est, max_depth=max_d, max_features=max_f)
    clf.fit(training_x, training_y)
    return clf
    
#def train_ada(training_x, training_y, n_est=10):",otto-model-explore.py,ryanswanstrom/kaggle-otto,1
"    if not tunings:
      train = SMOTE(train, resample=True)
    else:
      train = SMOTE(train, a, b, resample=True)
    # except: set_trace()

  if not tunings:
    if regress:
      clf = RandomForestRegressor(n_estimators=100, random_state=1, warm_start=True,n_jobs=-1)
    else:
      clf = RandomForestClassifier(n_estimators=100, random_state=1, warm_start=True,n_jobs=-1)
  else:
    if regress:
      clf = RandomForestRegressor(n_estimators=int(tunings[0]),
                                   max_features=tunings[1] / 100,
                                   min_samples_leaf=int(tunings[2]),
                                   min_samples_split=int(tunings[3]),
                                   warm_start=True,n_jobs=-1)
    else:
      clf = RandomForestClassifier(n_estimators=int(tunings[0]),",src/tools/oracle.py,ai-se/XTREE,1
"        self._confidences = []
        self._merged = []
        self.baseline_clf = None
        self.baseline_transforms = []
        self.config = config
        self.results = {}

    #trains a baseline classifier on the baseline feature set
    def trainBaseline(self,ids,classes):
        def getTreeClf():
            return RandomForestClassifier(n_estimators=self.config.hid_layer_units_baseline, n_jobs=-1, random_state=42, oob_score=True, verbose=1)
        print 'Using',self.config.baselineClassifier,'as baseline classifier'
       
        if self.config.baselineClassifier.lower() == 'none':
            return

        X = loadBaselineData(ids)
        y = np.array(classes,dtype=np.int32)

        scaler = StandardScaler(copy=False, with_mean=True, with_std=True).fit(X) #mean_substract.MeanNormalize(copy=False).fit(X)",sequence_cnn.py,bmilde/deepschmatzing,1
"def getTunings(fname):
    raw = pd.read_csv(root + '/old/tunings.csv').transpose().values.tolist()
    formatd = pd.DataFrame(raw[1:], columns=raw[0])
    try:
        return formatd[fname].values.tolist()
    except KeyError:
        return None


def rforest(train, target):
    clf = RandomForestClassifier(n_estimators=100, random_state=1)
    source = list2dataframe(train)
    features = source.columns[:-1]
    klass = source[source.columns[-1]]
    clf.fit(source[features], klass)
    preds = clf.predict(target[target.columns[:-1]])
    distr = clf.predict_proba(target[target.columns[:-1]])[:, 1]

    # Find a threshold for cutoff
    try:",src/oracle/model.py,ai-se/XTREE,1
"
def main():

    # create the training & test sets, skipping the header row with [1:]
    dataset = genfromtxt(open('Data/train.csv', 'r'), delimiter=',', dtype='f8')[1:]
    target = [x[0] for x in dataset]
    train = [x[1:] for x in dataset]
    test = genfromtxt(open('Data/test.csv', 'r'), delimiter=',', dtype='f8')[1:]

    # create and train the random forest
    # multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)
    rf = RandomForestClassifier(n_estimators=3000)
    rf.fit(train, target)
    predicted_probs = [[index + 1, x[1]] for index, x in enumerate(rf.predict_proba(test))]

    savetxt('Data/submission.csv', predicted_probs, delimiter=',', fmt='%d,%f',
            header='MoleculeId,PredictedProbability', comments='')

if __name__ == ""__main__"":
    main()",BioResponse/makeSubmission.py,kraktos/Kaggling,1
"
            return KerasClassifier(nn=model,**self.params)

PARAMS_V3 = {
             'n_estimators':500, 'criterion':'gini', 'n_jobs':8, 'verbose':0,
             'random_state':407, 'oob_score':True,
             }

class ModelV3(BaseModel):
        def build_model(self):
            return RandomForestClassifier(**self.params)

PARAMS_V4 = {
             'n_estimators':550, 'criterion':'gini', 'n_jobs':8, 'verbose':0,
             'random_state':407,
             }

class ModelV4(BaseModel):
        def build_model(self):
            return ExtraTreesClassifier(**self.params)",examples/multi_class/scripts/multiclass.py,ikki407/stacking,1
"  score0 = model.score(test0X, test0y)
  print(""Interactions         "" + str(score1))
  print(""Non-interactions     "" + str(score0))
  print(""Accuracy             "" + str((score0 * test0Count + score1 * test1Count) / testCount))
  a, b, c, d = score1 * test1Count, score0 * test0Count, (1.0 - score1) * test1Count, (1.0 - score0) * test0Count
  tss = (a * b - c * d) / ((a + c) * (b + d))
  print(""TSS                  "" + str(tss))

# Train random forests with the best parameters found by randomized search

rf = RandomForestClassifier(n_estimators = 100, bootstrap = True, min_samples_leaf = 10, criterion = 'entropy', max_depth = None)
rf.fit(X, y)
score_and_show(rf, ""\nRandom Forest"")",1_Netflix/Supervised Learning/rf_nonbinaries.py,PhDP/Articles,1
"            if info['is_sparse']==True:
                self.name = ""BaggingRidgeRegressor""
                self.model = BaggingRegressor(base_estimator=Ridge(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            else:
                self.name = ""GradientBoostingRegressor""
                self.model = GradientBoostingRegressor(n_estimators=1,  max_depth=4, min_samples_split=14, verbose=verbose, warm_start = True)
            self.predict_method = self.model.predict # Always predict probabilities
        else:
            if info['has_categorical']: # Out of lazziness, we do not convert categorical variables...
                self.name = ""RandomForestClassifier""
                self.model = RandomForestClassifier(n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            elif info['is_sparse']:                
                self.name = ""BaggingNBClassifier""
                self.model = BaggingClassifier(base_estimator=BernoulliNB(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...                          
            else:
                self.name = ""GradientBoostingClassifier""
                self.model = eval(self.name + ""(n_estimators=1, verbose="" + str(verbose) + "", random_state=1, warm_start = True)"")
            if info['task']=='multilabel.classification':
                self.model = MultiLabelEnsemble(self.model)
            self.predict_method = self.model.predict_proba  ",lib/models.py,djajetic/AutoML2,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans_gap.py,diogo149/CauseEffectPairsPaper,1
"# looking at https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests

from sklearn.ensemble import RandomForestClassifier
import numpy as np

forest = RandomForestClassifier(n_estimators = 100)

# let's get some data. So called ""iris"" data
# http://www.math.uah.edu/stat/data/Fisher.csv
data = np.loadtxt(""iris.csv"", delimiter="","", skiprows=1)

X = data[0::,1::]	# features
Y = data[0::,0]	    # prediction class

# get first 70% as training data, 30% as testing data",python/play/test_random_forest.py,antlr/codebuff,1
"
# Unlike the first step, we now need to parse the reviews as a whole, not as individual sentences
clean_train_reviews = []
for review in train[""review""]:
    clean_train_reviews.append(review_to_wordlist(review,remove_stopwords=True))

trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features)

# Fit a simple classifier such as logreg or RF 
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 100)

print ""Fitting a random forest to labeled training data...""
forest = forest.fit(trainDataVecs,train[""sentiment""])

print ""Creating average feature vecs for test reviews""
clean_test_reviews = []
for review in test[""review""]:
    clean_test_reviews.append(review_to_wordlist(review,remove_stopwords=True))
",Word2Vec_TutorialCode.py,angelachapman/Kaggle-DeepLearning-Tutorial,1
"        'params': [
            {
                'activation': ['relu'],
                'hidden_layer_sizes': [(20, 10)],
                'random_state': [77]
            }
        ]
    },
    {
        'name': 'Random Forest',
        'model': RandomForestClassifier(),
        'params': [
            {
                'max_depth': [8],
                'n_estimators': [20],
                'min_samples_split': [5],
                'criterion': ['gini'],
                'max_features': ['auto'],
                'class_weight': ['balanced'],
                'random_state': [77]",scripts/classifier.py,yohanesgultom/id-openie,1
"
def main():

    # testing
    iris = datasets.load_iris()
    train_x, test_x, train_y, test_y = cross_validation.train_test_split(
        iris.data, iris.target)

    print ""Start training own random forest""
    t0 = time.time()
    rf = RandomForestClassifier(minInstances=10)
    rf.train(train_x, train_y)
    print ""Finished training after {} seconds"".format(time.time() - t0)

    print ""Start predicting""
    t0 = time.time()
    pred_y = rf.predict(test_x)
    print ""Finished predicting after {} seconds"".format(time.time() - t0)

    accuracy = np.sum(pred_y == test_y) / float(len(test_y))",classifier/tree.py,jenspetersen/misc,1
"    node = selectedSamples[0].node
    if selectedFeatures is None:
        selectedFeatures = nodeTraxelMap[node].Features.keys()
        forbidden = ['JaccardScores', 'id', 'filename', 'Polygon', 'detProb', 'divProb', 'com']
        forbidden += [f for f in selectedFeatures if f.count('_') > 0]
        for f in forbidden:
            if f in selectedFeatures:
                selectedFeatures.remove(f)
        getLogger().info(""No list of selected features was specified, using {}"".format(selectedFeatures))

    rf = RandomForestClassifier(selectedFeatures=selectedFeatures)
    features = rf.extractFeatureVector(nodeTraxelMap[node].Features, singleObject=True)
    featureMatrix = np.zeros([len(selectedSamples), features.shape[1]])
    featureMatrix[0, :] = features
    for idx, candidate in enumerate(selectedSamples[1:]):
        features = rf.extractFeatureVector(nodeTraxelMap[candidate.node].Features, singleObject=True)
        featureMatrix[idx + 1, :] = features

    rf.train(featureMatrix, labels)
",hytra/jst/classifiertrainingexampleextractor.py,chaubold/hytra,1
"    # set rewards
    print 'calculating rewards...'
    rewards = ff.getRewards(closes)  # print rewards
    # print rewards

    # train split
    # print '\nsplitting training set'
    X_train, X_test, y_train, y_test = ff.getSplit(X_scaled, rewards, 0)

    # fitting regressor
    # rfc = RandomForestClassifier(n_estimators=30)
    rfc = ExtraTreesClassifier(n_estimators=30, max_features='sqrt')

    # scores
    scores = cross_val_score(
        estimator=rfc,
        X=X_test,
        y=y_test,
        verbose=0,
        cv=2,",06_randomforests/classifier.py,Tjorriemorrie/trading,1
"    # Index labels, adding metadata to the label column.
    # Fit on whole dataset to include all labels in index.
    labelIndexer = StringIndexer(inputCol=""label"", outputCol=""indexedLabel"").fit(data)

    # Automatically identify categorical features, and index them.
    # Set maxCategories so features with > 4 distinct values are treated as continuous.
    featureIndexer =\
        VectorIndexer(inputCol=""features"", outputCol=""indexedFeatures"", maxCategories=4).fit(data)

    # Train a RandomForest model.
    rf = RandomForestClassifier(labelCol=""indexedLabel"", featuresCol=""indexedFeatures"", numTrees=10)

    # Convert indexed labels back to original labels.
    labelConverter = IndexToString(inputCol=""prediction"", outputCol=""predictedLabel"",
                                   labels=labelIndexer.labels)

    # Chain indexers and forest in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])
    
    # Split the data into training and test sets (30% held out for testing)",Spark/spark2.10/python/ml/random_forest_classifier_example.py,trhongbinwang/data_science_journey,1
"##n_classes = y.shape[1]
n_classes=len(set(y))
target_names=list(le.classes_)
print (""n_classes"",n_classes,""target_names"",target_names)
# shuffle and split training and test sets
##X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
##                                                    random_state=0)

'Best hyper param  classifiers For Thermophiles:'
# svm=SVC(C=50, cache_size=1900, class_weight='auto', gamma=0.0, kernel='rbf', probability=True)
rf = RandomForestClassifier(n_jobs=-1,min_samples_split= 2, max_features= 0.4, min_samples_leaf= 2, n_estimators= 20, max_depth= 8)

'Best hyper param  classifiers For NP:'
##svm =SVC(C=50, cache_size=1500, class_weight='auto',gamma=0.0, kernel='rbf', max_iter=-1, probability=True )
##rf = RandomForestClassifier(max_depth= None, min_samples_split= 3, min_samples_leaf= 1, n_estimators= 250,  n_jobs= -1, max_features= auto)


# Learn to predict each class against the other
##classifier = OneVsRestClassifier(svm.SVC(kernel='rbf', probability=True,
##                                  cache_size=1200,random_state=random_state))",ProFET/feat_extract/SciKit_plot_roc.py,ddofer/ProFET,1
"# train_log_scale = scale(train_log.values.astype(float), axis=0)

id = testing[""id""]

test = testing.drop([""id""], axis=1)

# testing_log = testing.applymap(fun)

# testing_log_scale = scale(testing_log.values.astype(float), axis=0)

clf = RandomForestClassifier(n_estimators=1000, n_jobs=3)
# forest = GradientBoostingClassifier(n_estimators=1000, subsample=0.5)
# forest = SGDClassifier(n_jobs=3, loss=""log"")

print ""Estimating""
clf = RandomForestClassifier(n_estimators=10, n_jobs=3)
scores = cross_validation.cross_val_score(clf, train.values, target.values, cv=5, scoring=""log_loss"")
print ""Estimated log_loss = "", -np.mean(scores)

",src/create_submission.py,ternaus/kaggle_otto,1
"    logging.basicConfig(format=('%(asctime)s|%(name)s|%(levelname)s| ' +
                        '%(message)s'), level=logging.INFO)
    log.info(""Starting ROC comparison example with FACT data"")

    X, y = make_classification(n_samples=10000, n_features=20,
                               n_informative=2, n_redundant=10,
                               random_state=42)
    sample_weight = None
    alpha = 0.05

    clf = RandomForestClassifier(n_jobs=40, n_estimators=200)

    log.info(""Real classifiaction"")
    clf, y_pred_a, cv_step = disteval.cv_test_ref_classification(
        clf, X, y, sample_weight, cv_steps=10, return_all_models=False)

    log.info(""Classifiaction with random label (Guessing clf)"")
    y_fake = np.random.randint(0, 2, size=X.shape[0])
    clf, y_pred_guess, cv_step = disteval.cv_test_ref_classification(
        clf, X, y_fake, sample_weight, cv_steps=10, return_all_models=False)",examples/roc_curve_equivalence.py,tudo-astroparticlephysics/pydisteval,1
"   # return singlelayer_perceptron(data, weights, biases)
    return two_layer_perceptron(data, weights, biases, weights2, drop)
  #  return three_layer_perceptron(data, weights, biases, weights2, biases2, weights3, drop)

def train_predict_model(model_data, label_name, feature_set, num_labels, use_regression):
    label_names, train_dataset, valid_dataset, test_dataset = model_data
    train_labels, train_dataset = prepare_train_data(train_dataset, label_names, label_name, feature_set, num_labels, use_regression)
    valid_labels, valid_dataset = prepare_train_data(valid_dataset, label_names, label_name, feature_set, num_labels, use_regression)
    test_labels, test_dataset = prepare_train_data(test_dataset, label_names, label_name, feature_set, num_labels, use_regression)
    num_features = len(train_dataset[0])
    #clf = sklearn.ensemble.RandomForestClassifier(verbose=1, n_estimators=500)
    #clf.fit(train_dataset, train_num_labels)
    #print(clf.predict(test_dataset))
    #print(accuracy_score(test_num_labels,clf.predict(test_dataset)))
    #return
#sum(test_num_labels == clf.predict(test_labels))
    
    print('Processing: ', label_name, num_features, num_labels)
    graph = tf.Graph()
    with graph.as_default():",newalgo/tf_train_py2.py,eshavlyugin/Preferans,1
"    clf.fit(x_train, y_train)

    print 'Accuracy in training set: %f' % clf.score(x_train, y_train)
    print 'Accuracy in cv set: %f' % clf.score(x_cv, y_cv)
    return clf

def random_forest(x_train, y_train, x_cv, y_cv):
    """""" Using Random Forest to classify the data. """"""

    print 'Training with RF...'
    clf = RandomForestClassifier(n_estimators = 2000, max_features=2)
    clf.fit(x_train, y_train)

    print 'Predicting...'
    print 'Accuracy in training set: %f' % clf.score(x_train, y_train)
    if y_cv != None:
        print 'Accuracy in cv set: %f' % clf.score(x_cv, y_cv)
    return clf

def logistic_regression(x_train, y_train, x_cv, y_cv):",PhotoQualityPrediction/classification.py,bharcode/Kaggle,1
"			'gini' (default) for Gini impurity
			'entropy' for the information gain
		* max_features: number of features to select at each split
		
		 """"""
		if self._rf_param == [n_estimators, criterion, max_features]:
			return pd.DataFrame({'Predictors': self.predictors, 'RF': self._rf_imp})
		else:
			self._rf_param = [n_estimators, criterion, max_features]
			if max_features == 'auto':
				model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_features=int(np.sqrt(len(self.dataframe.columns))), bootstrap=True).fit(self.dataframe, self.response)
			else:
				model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_features=max_features, bootstrap=True).fit(self.dataframe, self.response)

			self._rf_imp = model.feature_importances_/model.feature_importances_.max()
			return pd.DataFrame({'Predictors': self.predictors, 'RF': self._rf_imp})
		
	def boosting(self, n_estimators=2000, learning_rate=.1, max_depth=1):
		"""""" Returns the importance calculated by a gradient boosting classifier.
",decam/feature_importance.py,ericfourrier/decam,1
"    count_enrollment = df_sub['3COURSEID'].value_counts()
    #print ""Number of %s enrollment: %s""%(subject,count_enrollment)

    A = df_sub.as_matrix()
    X = A[:,4:]
    X = X.astype(np.int64, copy=False)
    y = A[:,2]
    y = y.astype(np.int64, copy=False)

    #Training data
    forest = RandomForestClassifier(n_estimators=10, max_depth=None, 
            min_samples_split=1, random_state=None, max_features=None)
    clf = forest.fit(X, y)
    scores = cross_val_score(clf, X, y, cv=5)
    print scores
    print ""Random Forest Cross Validation of %s: %s""%(subject,scores.mean())
    precision_rf[subject] = scores.mean()
    df_precision.loc[subject]=precision_rf[subject]
    print ""-----------------------------------""
    ",pae/final_code/src/train_scikit.py,wasit7/book_pae,1
"############Gross Fitting: Primaries only.
#==============================================================================
# Modelling class (Classifier Variant) Model Fitting parameters
#==============================================================================

class MultModels:
    def __init__(self, df):
        self.predictors, self.outcomes = gen_predictors_outcomes(df)
      
    def rf(self):
        self.estimator = Pipeline([(""forest"", RandomForestClassifier(random_state=0, n_estimators=50))])
        #add type
        self.scor_vis()
        
    def adaboost(self):
        self.estimator = Pipeline([(""AdaBoost"", AdaBoostClassifier())])
        self.scor_vis()
                
    def svm(self):
        #train_test_split",ModelFitting/master_pipeline_python.py,kizzen/Baller-Shot-Caller,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/len(X1) )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X1,target, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 20, n_jobs = 8)
result_rf = cross_val_predict(rf,X1,target, cv = 10)
#print('X', len(X),len(Y),len(X1[train_size:dataset_size]))
#print('RF prediction : ---> ',result_rf )
#print('actual ans: -->',test_class)
CM = confusion_matrix(target,result_rf) 
print(""Confusion Matrix : "")
print(CM)
for i in range(0,len(X1)):
	if(target[i]== result_rf[i]):",sandbox/petsc/solvers/scripts/scikit_learn_classifiers_all_featuresCV.py,LighthouseHPC/lighthouse,1
"        cur = get_feature_vector(img_path)
        feature_vectors.append(cur)

    assert len(feature_vectors) > 0, 'Must have at least one feature vector to train on'

    # good estimator number for classification tasks
    sqrt_feat_num = int(np.sqrt(len(feature_vectors[0])))
    print(' ', sqrt_feat_num, 'estimator%s' % ('' if sqrt_feat_num == 1 else 's'))

    # create forest
    clf = RandomForestClassifier(
        n_estimators=sqrt_feat_num, n_jobs=-1,
        max_depth=None, min_samples_split=1
    )
    clf = clf.fit(feature_vectors, image_classes)

    return clf

def train_model(cache_file='cache_dir/model.pkl'):
    """""" Train model or use cached version if available",mlearner.py,kpj/PyClass,1
"from time import time
import numpy as np

def define_hyper_params():
    """"""
        Esta funcin devuelve un diccionario con
        los clasificadores que vamos a utilizar y
        una rejilla de hiperparmetros
    """"""
    clfs = {
        'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),
        'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),
        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=""SAMME"", n_estimators=200),
        'LR': LogisticRegression(penalty='l1', C=1e5),
        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),
        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),
        'NB': GaussianNB(),
        'DT': DecisionTreeClassifier(),
        'SGD': SGDClassifier(loss=""hinge"", penalty=""l2""),
        'KNN': KNeighborsClassifier(n_neighbors=3) ",Tareas/tarea_3/temp.py,rsanchezavalos/compranet,1
"    #model = AdaBoostClassifier(n_estimators=100,learning_rate=0.1)    
    #analyzeLearningCurve(model,Xtrain,ytrain,wtrain)
    #odel = ExtraTreesClassifier(n_estimators=250,max_depth=None,min_samples_leaf=5,n_jobs=4,criterion='entropy', max_features=5,oob_score=False)#opt
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=15)), ('model', model)])
    #model = amsGridsearch(model,Xtrain,yt    #smoothWeights=Falserain,wtrain,fitWithWeights=fitWithWeights,nfolds=nfolds,useProba=useProba,cutoff=cutoff)
    model = GradientBoostingClassifier(loss='deviance',n_estimators=150, learning_rate=0.1, max_depth=6,subsample=1.0,verbose=False) #opt weight =500 AMS=3.548
    #model = GradientBoostingClassifier(loss='deviance',n_estimators=500, learning_rate=0.05, max_depth=6,subsample=1.0,max_features=8,min_samples_leaf=100,verbose=0) #opt weight =500 AMS=3.548
    #model = GradientBoostingClassifier(loss='deviance',n_estimators=800, learning_rate=0.02, max_depth=6,subsample=.5,max_features=8,min_samples_leaf=100,verbose=False) 
    #model = GradientBoostingClassifier(loss='deviance',n_estimators=2000, learning_rate=0.01, max_depth=7,subsample=0.5,max_features=10,min_samples_leaf=50,verbose=0)#3.72  | 3.69
    #model = XgboostClassifier(n_estimators=120,learning_rate=0.1,max_depth=6,n_jobs=1,NA=-999.0)
    #model =  RandomForestClassifier(n_estimators=250,max_depth=None,min_samples_leaf=5,n_jobs=1,criterion='entropy', max_features=5,oob_score=False)#SW-proba=False ams=3.42
    #model = AdaBoostClassifier(base_estimator=basemodel,n_estimators=10,learning_rate=0.5)   
    #model = GradientBoostingClassifier(loss='deviance',n_estimators=200, learning_rate=0.08, max_depth=7,subsample=1.0,max_features=10,min_samples_leaf=20,verbose=False)
    #model = GradientBoostingClassifier(loss='deviance',n_estimators=300, learning_rate=0.08, max_depth=6,subsample=1.0,max_features=10,min_samples_leaf=50,verbose=False)#AMS=3.678
    #model = GradientBoostingClassifier(loss='deviance',n_estimators=500, learning_rate=0.05, max_depth=6,subsample=1.0,max_features=6,min_samples_leaf=100,verbose=False)#new opt
    #model = BaggingClassifier(base_estimator=basemodel,n_estimators=10,n_jobs=1,verbose=1)
    model=buildAMSModel(model,Xtrain,ytrain,wtrain,nfolds=nfolds,fitWithWeights=fitWithWeights,useProba=useProba,cutoff=cutoff,scale_wt=scale_wt,n_jobs=8,smoothWeights=smoothWeights,normalizeWeights=normalizeWeights,verbose=verbose) 
    #divideAndConquer(model,Xtrain,ytrain,Xtest,wtrain,n_clusters=3)#did not work
    #model = amsXvalidation(model,Xtrain,ytrain,wtrain,nfolds=nfolds,cutoff=cutoff,useProba=useProba,fitWithWeights=fitWithWeights,useRegressor=useRegressor,scale_wt=scale_wt,buildModel=True)    
    #iterativeFeatureSelection(model,Xtrain,Xtest,ytrain,1,1)    ",competition_scripts/higgs.py,chrissly31415/amimanera,1
"                          cache_size=200)),
        # ('SVM, linear', SVC(probability=True,
        #                     kernel=""linear"",
        #                     C=0.025,
        #                     cache_size=200)),
        ('k nn (k=3)', KNeighborsClassifier(3)),
        ('k nn (k=5)', KNeighborsClassifier(5)),
        ('k nn (k=7)', KNeighborsClassifier(7)),
        ('k nn (k=21)', KNeighborsClassifier(21)),
        ('Decision Tree', DecisionTreeClassifier(max_depth=5)),
        ('Random Forest', RandomForestClassifier(n_estimators=50, n_jobs=10)),
        ('Random Forest 2', RandomForestClassifier(max_depth=5,
                                                   n_estimators=10,
                                                   max_features=1,
                                                   n_jobs=10)),
        ('AdaBoost', AdaBoostClassifier()),
        ('Naive Bayes', GaussianNB()),
        ('Gradient Boosting', GradientBoostingClassifier()),
        ('LDA', LinearDiscriminantAnalysis()),
        ('QDA', QuadraticDiscriminantAnalysis())",ML/data-driven/blood-donation/main.py,MartinThoma/algorithms,1
"
# VALIDATE CLASSIFIER
print('Validating neural net classifier')

y_score = model.predict_proba(x_test)
roc(to_categorical(y_test),y_score,name=""../results/kdd98_propagation_classifier_roc.pdf"")


# TRAIN RANDOM FOREST
print('Training random forest classifier')
clf = RandomForestClassifier(n_estimators=100)
clf = clf.fit(x_train, y_train.ravel())

# VALIDATE RANDOM FOREST
print('Validating random forest classifier')
y_score = clf.predict_proba(x_test)

# SAVE ROC CURVE PLOT
roc(to_categorical(y_test),y_score,name=""../results/kdd98_propagation_classifier_roc_rf.pdf"")
",src/kdd98_propagate_classifier.py,sisl/CustomerSim,1
"    assert_array_equal(X_transformed_sparse.toarray(), X_transformed.toarray())


def test_parallel_train():
    rng = check_random_state(12321)
    n_samples, n_features = 80, 30
    X_train = rng.randn(n_samples, n_features)
    y_train = rng.randint(0, 2, n_samples)

    clfs = [
        RandomForestClassifier(n_estimators=20, n_jobs=n_jobs,
                               random_state=12345).fit(X_train, y_train)
        for n_jobs in [1, 2, 3, 8, 16, 32]
    ]

    X_test = rng.randn(n_samples, n_features)
    probas = [clf.predict_proba(X_test) for clf in clfs]
    for proba1, proba2 in zip(probas, probas[1:]):
        assert_array_almost_equal(proba1, proba2)
",venv/lib/python3.4/site-packages/sklearn/ensemble/tests/test_forest.py,valexandersaulys/airbnb_kaggle_contest,1
"	#df_test = pd.read_csv(test_file, skiprows = 0)
	#print df_test	
	
	#print df_test
	

	
	
	
	global forest
	forest = RandomForestClassifier(n_estimators = 100) 
	forest = forest.fit(X_train, y_train)

	
	joblib.dump(forest, 'my_model.pkl')


def compute(url):
	
	forest = joblib.load('my_model.pkl')",ml/svm.py,srujant/MLNews,1
"def train(samples, vocabulary):
    logger.debug(""Extracting features"")
    
    X = []
    for s in samples:
        X.append(extract_features(s[0], vocabulary))
    X = sp.vstack(X, format='csr')

    y = np.array([s[1] for s in samples])

    clf = RandomForestClassifier(n_estimators=30)
    
    if args[""-c""]:
        logger.debug(""Performing N-fold cross-validation (N=%s)"" % args[""-f""])
        scores = cross_validation.cross_val_score(clf, X.toarray(), y,
                                                    n_jobs=int(args[""-j""]),
                                                    cv=int(args[""-f""]),
                                                    scoring=args[""-s""])
        print(""F1: %0.4f (+/- %0.4f)"" % (scores.mean(), scores.std() * 2))
    ",user_agent_ml/train.py,904labs/user-agent-ml,1
"

class ForestPredictor(PredictorBase):
    """"""""
    A simple application of RandomForestClassifier

    @author: Shaun
    """"""

    def __init__(self, n_estimators=100, max_features='auto'):
        self.clf = RandomForestClassifier(n_estimators=n_estimators, 
                max_features=max_features)

    @abstractmethod
    def fit(self, X, y):
        """"""
        Method to fit the model.

        Parameters:
        X - 2d numpy array of training data. X.shape = [n_samples, d_features]",code/python/seizures/prediction/ForestPredictor.py,vincentadam87/gatsby-hackathon-seizure,1
"
for item in train_data_list:
	train_data.append([item[0], item[1]])
	train_data_label.append(item[2])
for item in test_data_list:
	test_data.append([item[0], item[1]])
	test_data_label.append(item[2])

#print len(train_data), len(train_data_label), len(train_data_list)

#rf = RandomForestClassifier(n_estimators=150, min_samples_split=2, n_jobs=-1)
rf = RandomForestClassifier()
rf.fit(train_data, train_data_label)
#print rf.predict(test_data)
print ""Random forest""
#print rf.score(test_data,test_data_label)

y_rf_predict = rf.predict(test_data)
print ""After predict""
#print y_rf_predict",randomforest_1.py,siddharthhparikh/INFM750-project,1
"                ]

def main():
    print(""Reading the data"")
    data = cu.get_dataframe(train_file)

    print(""Extracting features"")
    fea = features.extract_features(feature_names, data)

    print(""Training the model"")
    rf = RandomForestClassifier(n_estimators=50, verbose=2, compute_importances=True, n_jobs=-1)
    rf.fit(fea, data[""OpenStatus""])

    print(""Reading test file and making predictions"")
    data = cu.get_dataframe(test_file)
    test_features = features.extract_features(feature_names, data)
    probs = rf.predict_proba(test_features)

    print(""Calculating priors and updating posteriors"")
    new_priors = cu.get_priors(full_train_file)",test_benchmark.py,mattalcock/stack-kaggle,1
"                            each matrix features[i] of class i is [numOfSamples x numOfDimensions]
        - n_estimators:     number of trees in the forest
    RETURNS:
        - svm:              the trained SVM variable

    NOTE:
        This function trains a linear-kernel SVM for a given C value. For a different kernel, other types of parameters should be provided.
    '''

    [X, Y] = listOfFeatures2Matrix(features)
    rf = sklearn.ensemble.RandomForestClassifier(n_estimators = n_estimators)
    rf.fit(X,Y)

    return rf

def trainGradientBoosting(features, n_estimators):
    '''
    Train a gradient boosting classifier
    Note:     This function is simply a wrapper to the sklearn functionality for SVM training
              See function trainSVM_feature() to use a wrapper on both the feature extraction and the SVM training (and parameter tuning) processes.",audioTrainTest.py,tyiannak/pyAudioAnalysis,1
"# Generate a binary classification dataset.
X, y = make_classification(n_samples=500, n_features=25,
                           n_clusters_per_class=1, n_informative=15,
                           random_state=RANDOM_STATE)

# NOTE: Setting the `warm_start` construction parameter to `True` disables
# support for paralellised ensembles but is necessary for tracking the OOB
# error trajectory during training.
ensemble_clfs = [
    (""RandomForestClassifier, max_features='sqrt'"",
     RandomForestClassifier(warm_start=True, oob_score=True,
                            max_features=""sqrt"",
                            random_state=RANDOM_STATE)),
    (""RandomForestClassifier, max_features='log2'"",
     RandomForestClassifier(warm_start=True, max_features='log2',
                            oob_score=True,
                            random_state=RANDOM_STATE)),
    (""RandomForestClassifier, max_features=None"",
     RandomForestClassifier(warm_start=True, max_features=None,
                            oob_score=True,",projects/scikit-learn-master/examples/ensemble/plot_ensemble_oob.py,DailyActie/Surrogate-Model,1
"
def rfDecisionPlot(XTrain, yTrain, XTest, yTest):
    plt.figure(figsize=(7,5))
    h = .02  # step size in the mesh
    Xtrain = XTrain[:, :2] # we only take the first two features.

    # Create color maps
    cmap_light = ListedColormap([""#AAAAFF"", ""#AAFFAA"", ""#FFAAAA""])
    cmap_bold  = ListedColormap([""#0000FF"", ""#00FF00"", ""#FF0000""])

    clf = RandomForestClassifier()
    clf.fit(Xtrain, yTrain)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, m_max]x[y_min, y_max].
    x_min, x_max = Xtrain[:, 0].min() - 1, Xtrain[:, 0].max() + 1
    y_min, y_max = Xtrain[:, 1].min() - 1, Xtrain[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),  np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    # Put the result into a color plot",modelling/visplots.py,M-R-Houghton/euroscipy_2015,1
"def generateModel(context, stock):
    ''' Function oversees how ot generate a RFC model for a given stock. '''
    # Get the historical data for this stock
    historical_data = history(bar_count=context.params[stock][""years""] * 250, frequency='1d', field='price')[stock]

    # Calculate reverse price changes vector from price column (input) and
    #   calculate the actual movement context.predictionDays later (output)
    (trainingX, trainingY) = generateModelData(context, stock, historical_data)

    # Train the classifier on our training set
    clf = RandomForestClassifier(n_estimators=100)
    clf.fit(trainingX, trainingY)
    return(clf)



def generatePercentChanges(prices):
    ''' Given a price vector, generate a vector of forward price changes. '''
    priceChanges = []
",part2/RandomForestPortfolio.py,jquacinella/IS643_Projects,1
"
#  
def process(train_datas, train_lables):
    data_train, data_test, target_train, target_test = train_test_split(train_datas, train_lables)
    estimators = {}
    estimators['bayes'] = GaussianNB()
    estimators['tree'] = tree.DecisionTreeClassifier() 
    estimators['logistic '] = linear_model.LogisticRegression()
    estimators['ada boost '] = AdaBoostClassifier(linear_model.LogisticRegression(), algorithm=""SAMME"", n_estimators=400)
    estimators['ada boost '] = AdaBoostClassifier(tree.DecisionTreeClassifier(), algorithm=""SAMME"", n_estimators=400)
    estimators['forest_3000'] = RandomForestClassifier(n_estimators=3000)
    estimators['forest_2000'] = RandomForestClassifier(n_estimators=2000)
    estimators['forest_1000'] = RandomForestClassifier(n_estimators=1000)
    estimators['forest_900'] = RandomForestClassifier(n_estimators=900)
    estimators['forest_700'] = RandomForestClassifier(n_estimators=700)
    estimators['forest_500'] = RandomForestClassifier(n_estimators=500)
    estimators['forest_300'] = RandomForestClassifier(n_estimators=300)
    estimators['forest_100'] = RandomForestClassifier(n_estimators=100)
    estimators['forest_10'] = RandomForestClassifier(n_estimators=10)
    estimators['svm_c_rbf'] = svm.SVC()",datamining_assignments/datamining_final/kaggle_homework.py,xuerenlv/PaperWork,1
"import pylab
import pandas as pd

import sound_classification.confusion_matrix
from sound_processing.features_extraction import preprocess


classification_prediction = {}
clfs = []
clfs_name = ['random_forest'] #, 'nearest neighbors']
clfs.append(sklearn.ensemble.RandomForestClassifier(n_estimators=500))

#clfs.append(sklearn.neighbors.KNeighborsClassifier(n_neighbors=5))
accuracy = {}
Cross_validation_split = namedtuple('cross_validation_split', ['X_training', 'X_testing', 'Y_training', 'Y_testing'])

def compute_cross_validation_fold(df, fold):
    """"""
    Compute testing/training data as in the paper
",evaluate_learning.py,laurent-george/protolab_sound_recognition,1
"      i,
      split_count,
    )
    )
    
    # Test/train split
    training_data, test_data = final_vectorized_features.randomSplit([0.8, 0.2])
    
    # Instantiate and fit random forest classifier on all the data
    from pyspark.ml.classification import RandomForestClassifier
    rfc = RandomForestClassifier(
      featuresCol=""Features_vec"",
      labelCol=""ArrDelayBucket"",
      predictionCol=""Prediction"",
      maxBins=4896,
    )
    model = rfc.fit(training_data)
    
    # Save the new model over the old one
    model_output_path = ""{}/models/spark_random_forest_classifier.flight_delays.baseline.bin"".format(",ch09/spark_model_with_airplanes.py,rjurney/Agile_Data_Code_2,1
"        self.features = None
        self.classes = None


    def train(self, features, data, labels, n_trees=64):
        self.run.info('RF Train', ""%d observations with %d features grouped into %d classes."" % (len(data), len(features), len(set(labels))))
        filesnpaths.is_output_file_writable(self.classifier_object_path)

        self.progress.new('Training')
        self.progress.update('...')
        rf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_trees)
        rf.fit(np.array(data), labels)
        self.progress.end()

        cPickle.dump({'features': features, 'classes': rf.classes_, 'classifier': rf}, open(self.classifier_object_path, 'w'))
        self.run.info('Classifier output', self.classifier_object_path)

    def predict_from_TAB_delimited_file(self, file_path):
        cols = utils.get_columns_of_TAB_delim_file(file_path)
        return self.predict(utils.get_TAB_delimited_file_as_dictionary(file_path, column_mapping=[str] + [float] * len(cols)))",anvio/learning.py,meren/anvio,1
"
        results = cross_val_score(pipeline, self.X, self.Y, cv=kfold)
        print('Accuracy: %.2f%% (+-%.2f%%) [Feature normalized]' % (results.mean() * 100, results.std() * 100))

    def baseline_svm(self):
        clf = svm.SVC(kernel='rbf', C=1).fit(self.train_x, self.train_y)
        accuracy = clf.score(self.test_x, self.test_y)
        print('SVM accuracy %.2f%%' % (accuracy * 100))

    def baseline_randomforest(self):
        clf = RandomForestClassifier(n_estimators=100).fit(self.train_x, self.train_y)
        accuracy = clf.score(self.test_x, self.test_y)
        print('RF accuracy %.2f%%' % (accuracy * 100))",network/evaluate.py,NTHU-CVLab/ActivityProps,1
"import unittest
from boruta import BorutaPy
from sklearn.ensemble import RandomForestClassifier
import numpy as np


class BorutaTestCases(unittest.TestCase):

    def test_get_tree_num(self):
        rfc = RandomForestClassifier(max_depth=10)
        bt = BorutaPy(rfc)
        self.assertEqual(bt._get_tree_num(10), 44, ""Tree Est. Math Fail"")
        self.assertEqual(bt._get_tree_num(100), 141, ""Tree Est. Math Fail"")

    def test_if_boruta_extracts_relevant_features(self):
        np.random.seed(42)
        y = np.random.binomial(1, 0.5, 1000)
        X = np.zeros((1000, 10))
",boruta/test/unit_tests.py,danielhomola/boruta_py,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/no_test_instances )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X,Y, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 100, n_jobs = 12, random_state = 4)
rf.fit(X,Y)
result_rf = rf.predict(Z)
#print(Z[70])
#print('X', len(X),len(Y),len(X1[train_size:dataset_size]))
#print('RF prediction : ---> ',result_rf )
#print('actual ans: -->',test_class)
CM = confusion_matrix(test_class,result_rf) 
print(""Confusion Matrix : "")
print(CM)",sandbox/petsc/solvers/scripts/ScikitClassifiersRS2.py,LighthouseHPC/lighthouse,1
"
class Classifier(object):

    def __init__(self,algorithm='XD',n_comp = 20):
        if algorithm == 'XD':
            self.algorithm='XD'
            self.lQSO_model = XDGMM(n_components=n_comp,verbose=True)
            self.dud_model = XDGMM(n_components=n_comp,verbose=True)
        elif algorithm == 'RandomForest':
            self.algorithm = 'RandomForest'
            self.trialRF = RandomForestClassifier()
            self.RF_params = {'n_estimators':(10,50,200),""max_features"": [""auto"",2,4],
                          'criterion':[""gini"",""entropy""],""min_samples_leaf"": [1,2]}
        return
    
    def train(self,train,truth,covmat=1):
        if self.algorithm == 'XD':
            self.XDtrain(train,truth,covmat)
        elif self.algorithm == 'RandomForest':
            self.RFtrain(train,truth)",PROJECTS/SDSS/PS1QLS.py,drphilmarshall/PS1QLS,1
"        from sklearn.ensemble import RandomForestClassifier
        from sklearn.linear_model import SGDClassifier
        from sklearn.svm import SVC
        from sklearn.preprocessing import RobustScaler
        from sklearn.model_selection import GridSearchCV, KFold

        logger.info('perform classification using ""%s"" method', method)
        models = {
            'randomforest': {
                # NOTE: RF could be parallelized.
                'cls': RandomForestClassifier(n_jobs=1),
                # No scaling required for decision trees.
                'scaler': None,
                'search_space': {
                    # Number of trees.
                    'n_estimators': [3, 6, 12, 24],
                    # Number of leafs in the tree.
                    'max_depth': [3, 6, 12, None],
                    'min_samples_split': [2, 4, 8],
                    # TODO: this should rather be a user defined parameter",tmlib/tools/base.py,TissueMAPS/TmLibrary,1
"	#df_test = pd.read_csv(test_file, skiprows = 0)
	#print df_test	
	
	#print df_test
	

	
	
	
	global forest
	forest = RandomForestClassifier(n_estimators = 100) 
	forest = forest.fit(X_train, y_train)


def compute(url):
	'''
	train()
	'''
	text = HTMLParser(url)
	print type(str(text))",ml/svm2.py,srujant/MLNews,1
"X,y = settings.flattened_train_paths(settings.classes)
X_test = settings.image_fnames['test']

X = np.array(X)
y = np.array(y)

detector = lambda image: neukrill_net.image_features.get_ORB_keypoints(image, n=max_num_kp, patchSize=9)

describer = neukrill_net.image_features.get_ORB_descriptions

kprf_base = sklearn.ensemble.RandomForestClassifier(n_estimators=400, max_depth=10, min_samples_leaf=20, n_jobs=12, random_state=42)

hlf = neukrill_net.highlevelfeatures.KeypointEnsembleClassifier(detector, describer, kprf_base,
                                                                     return_num_kp=True, n_jobs=16, verbosity=1, summary_method='vote')

# Partition the data

print ""Partitioning the training data""

# Remove the data which is going to be held out",generate_local_cache_ORB.py,Neuroglycerin/neukrill-net-work,1
"                                            names_to_delete)
target = 'variable'
predictors = list(df)
predictors.remove(target)
dtrain = df

kfold = StratifiedKFold(dtrain[target], n_folds=4, shuffle=True, random_state=1)


def objective(space):
    clf = RandomForestClassifier(n_estimators=space['n_estimators'],
                                 max_depth=space['max_depth'],
                                 max_features=space['max_features'],
                                 min_samples_split=space['mss'],
                                 min_samples_leaf=space['msl'],
                                 class_weight='balanced_subsample',
                                 verbose=1, random_state=1, n_jobs=4)
    estimators = list()
    estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                          axis=0, verbose=2)))",ml4vs/rf.py,ipashchenko/ml4vs,1
"    if test_data[i,7] == '':
        test_data[i,7] = np.median(test_data[(test_data[0::,7] != '') &\
                                             (test_data[0::,0] == test_data[i,0])\
            ,7].astype(np.float))

test_data = np.delete(test_data,[1,6,8],1) #remove the name data, cabin and ticket

#The data is now ready to go. So lets train then test!

print 'Training '
forest = RandomForestClassifier(n_estimators=100)

forest = forest.fit(train_data[0::,1::],\
                    train_data[0::,0])

print 'Predicting'
output = forest.predict(test_data)

open_file_object = csv.writer(open(""randomforest.csv"", ""wb""))
open_file_object.writerow([""PassengerId"",""Survived""])",py/randomforest.py,zigahertz/2013-Sep-HR-ML-sprint,1
"
    #model = Pipeline([('reducer', TruncatedSVD(n_components=400)), ('scaler', StandardScaler()), ('model', model)])
    #model = Pipeline([('scaler', StandardScaler()), ('model', SVC(C=10,gamma='auto') )])
    #model = SVC(C=10,gamma=0.001)
    #model = Pipeline([('scaler', StandardScaler()), ('model', LinearSVC(C=0.01))])
    #model = Pipeline([('reducer', MiniBatchKMeans(n_clusters=400,batch_size=400,n_init=3)), ('scaler', StandardScaler()), ('SVC', model)])
    #model = Pipeline([('feature_selection', LinearSVC(penalty=""l1"", dual=False, tol=1e-3)),('classification', LinearSVC())])
    #model = LinearSVC(C=0.01)


    #model =  RandomForestClassifier(n_estimators=250,max_depth=None,min_samples_leaf=1,n_jobs=1,criterion='gini', max_features=100)#0.627
    #model =  RandomForestClassifier(n_estimators=250,max_depth=None,min_samples_leaf=1,n_jobs=2,criterion='gini', max_features='auto')
    #model =  ExtraTreesClassifier(n_estimators=500,max_depth=None,min_samples_leaf=3,n_jobs=2,criterion='gini', max_features=150) #0.649
    #model = XgboostClassifier(n_estimators=500,learning_rate=0.2,max_depth=6,subsample=.5,n_jobs=1,objective='multi:softmax',eval_metric='error',booster='gbtree',silent=1)
    #model.fit(Xtrain,ytrain)
    #rfFeatureImportanccomputeSynonymae(model,Xtrain,Xtest,1)

    #model = OneVsRestClassifier(model,n_jobs=8)
    #model = OneVsOneClassifier(model,n_jobs=8)
    #model = SGDClassifier(alpha=.001, n_iter=200,penalty='l2',shuffle=True,loss='log')",competition_scripts/crowd.py,chrissly31415/amimanera,1
"def hyperopt_rf_opt(params):
    print ""Training with params : ""
    print params

    # create features and del parameter
    cv = TfidfVectorizer(max_features=params['features'])
    cv_train_data = cv.fit_transform(train_data['NAME'])
    del params['features']

    # learn classifier
    rf = RandomForestClassifier(**params)
    scores = cross_val_score(rf, cv_train_data, train_data['GROUP_ID'], cv=3, scoring='accuracy')
    print ""Scores : ""
    print scores.mean()
    return scores.mean()

def f(params):
    acc = hyperopt_rf_opt(params)
    return {'loss': 1-acc, 'status': STATUS_OK}
",experiment/tfidf/tfidf_random_forest.py,shiron/evotor-ml-champ,1
"					print(metrics.confusion_matrix(y_test, pred))

			print()
			clf_descr = str(clf).split('(')[0]
			return clf_descr, score, train_time, test_time, metrics.confusion_matrix(y_test, pred)


	results = []
	for clf, name in (
					(RidgeClassifier(tol=1e-2, solver=""sag""), ""Ridge Classifier""),
					(RandomForestClassifier(n_estimators=100), ""Random forest"")):
			print('=' * 80)
			print(name)
			results.append(benchmark(clf))

	for penalty in [""l2""]:
			print('=' * 80)
			print(""%s penalty"" % penalty.upper())
			# Train Liblinear model
			results.append(benchmark(LinearSVC(loss='squared_hinge', penalty=penalty, dual=False, tol=1e-3)))",genre_classification.py,sergiooramas/music-genre-classification,1
"clf  =  XGBClassifier(
        eval_metric = 'accuracy',
        learning_rate = 0.1,
        n_estimators = 100,
        max_depth = 3,
        subsample = 0.9,
        colsample_bytree = 0.9,
        silent = False
        )

#clf = RandomForestClassifier(n_estimators=100,max_features=17)  

#clf = svm.SVC()        
        
clf.fit(X_train,Y_train)

print('good')

importance = clf.feature_importances_
",Code/Spotify-XGBClassifier.py,JerryGuangXu/Spotify-Music-Data-Analysis,1
"	model = svm.SVC( kernel = 'linear')
	model.fit( X_train, y_train)
	sv_score = model.score( X_test, y_test)
	print( ""SVC:"", sv_score)

	model = kkeras.MLPC( [X_train.shape[1], 30, 10, nb_classes])
	model.fit( X_train, y_train, X_test, y_test, nb_classes)
	mlp_score = model.score( X_test, y_test)
	print( ""DNN:"", mlp_score)

	model = ensemble.RandomForestClassifier( n_estimators=10)
	model.fit( X_train, y_train)
	rf_score = model.score( X_test, y_test)
	print( ""RF:"", rf_score)

	return dt_score, sv_score, mlp_score, rf_score


def GET_clsf2_by_clst( nb_classes):
	def clsf2_by_clst( Xpart_cf, Xpart_ct):",kcell_r1.py,jskDr/jamespy_py3,1
"    
### Create prediction file that has label file too.     

    
def setup_methods(args):
    cc,k,nj,it = args.classifier,args.k,args.num_jobs,args.interp_type
    if it == ""SGF"" or it == ""WG"":
        sys.stderr.write(""DTW or related curve similarity metric must be used to compare curves with different numbers of features. Continuing with DTW calculation.\n"")
        return ""DTW-NN""
    if cc == ""RF"":
        return sklearn.ensemble.RandomForestClassifier(n_estimators=1001,n_jobs=nj)
    elif cc == ""LR"":
        return sklearn.linear_model.LogisticRegressionCV(max_iter=1001,n_jobs=nj)
    elif cc == ""GBCT"":
        return sklearn.ensemble.GradientBoostingClassifier(n_estimators=1001,)
    elif cc == ""NN"":
        return sklearn.neighbors.KNeighborsClassifier(n_neighbors=k,n_jobs=nj)
    elif cc == ""NB"":
        return sklearn.naive_bayes.GaussianNB()
    else: ## DTW-NN",expression_classification.py,cschlosberg/me-class,1
"    data_flat = dataio.flat(data)
    segmentations = []

    # class from config
    c = parse.Bunch(**config)

    n_slices = data.shape[c.slice_axis]
    if c.start_slice == ""center"":
        c.start_slice = int(n_slices/2)

    forest = RandomForestClassifier(**parse.config_for_function(RandomForestClassifier.__init__, config))
    training_data = []
    training_labels = []

    visited_slices = []
    current_slice = c.start_slice

    # loop over desired number of cycles
    for i in range(c.max_slices):
",MultiVolumeActiveLearning/HypothesisRandomForestReproducibility.py,jenspetersen/Experiments,1
"    xlen = len(x)
    i = range(xlen)
    np.random.shuffle(i)
    trainpct = 0.7
    trainlen = int(trainpct * xlen)
    testlen = xlen - trainlen
    xtrain = x.ix[:trainlen,:]
    ytrain = y.ix[:trainlen]
    xtest = x.ix[trainlen:,:]
    ytest = y.ix[trainlen:]
    rf = RandomForestClassifier()
    rf.fit(xtrain, ytrain)
    ypred = rf.predict(xtest)
    return ytest, ypred

def crossval(x, y, k=5):
    for i in range(k):
        i = range(len(X))
        np.random.shuffle(i)
        xlen = len(x)",Scripts/pscvread.py,coreyabshire/color-names,1
"
        if rf_test is False:
            [train_X, train_Y] = pd.read_pickle(util.features_prefix + name + '_XY.pkl')
            [X_train, X_validate, X_test, y_train, y_validate, y_test] = pd.read_pickle(
                util.features_prefix + name + '_XXXYYY.pkl')
            x = np.concatenate([X_train, X_validate], axis=0)
            y = np.concatenate([y_train, y_validate], axis=0)
            print 'rf'
            print time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime())
            # for max in range(12, 23, 5):
            clf = RandomForestClassifier(n_jobs=4, n_estimators=400, max_depth=22)
            clf.fit(x, y)
            print time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime())
            pd.to_pickle(clf, util.models_prefix + name + '_rf.pkl')
            y_p = clf.predict(X_test)
            print name + ' score:' + util.score_lists(y_test, y_p)

        if xgb_test is False:
            [train_X, train_Y] = pd.read_pickle(util.features_prefix + name + '_XY.pkl')
            [X_train, X_validate, X_test, y_train, y_validate, y_test] = pd.read_pickle(",Step8_basal_classifier_save.py,lyoshiwo/resume_job_matching,1
"    Uses grid search and cross-validation to discover the best meta-parameters for a Random Forest
    Classifier. Prints out the best parameters and best score (using the specified scoring method).
    Returns the best parameters.
    '''
    # Scale variables:
    #scaler = RobustScaler()
    scaler = StandardScaler()
    scaled_X = scaler.fit_transform(X)

    # SVC:
    rf = RandomForestClassifier()

    # Cross-validation and grid search:
    cv = StratifiedShuffleSplit(n_splits = 100, test_size = 0.3, random_state = seed * 2)
    gscv = GridSearchCV(rf, param_grid = param_grid, scoring = scoring, cv = cv, verbose = 1, 
        n_jobs = -1)

    # Fit model:
    gscv.fit(scaled_X, y)
",project/ml_models.py,LucFrachon/enron_fraud_detection,1
"        result = random_forest_model(_train_set, _test_set)
        time_cost.append(result[0])
        correct_cost.append(result[1])
    print(""random_fores time avg :"" + str(sum(time_cost) / len(time_cost)))
    print(""random_fores correct avg :"" + str(sum(correct_cost) / len(correct_cost)))
    pass


def random_forest_model(_train_set, _test_set):
    _start_time = datetime.now().timestamp()
    clf = RandomForestClassifier(n_estimators=10)
    clf = clf.fit(_train_set[0], _train_set[1])

    num = 0
    correct = 0
    for i in clf.predict(_test_set[0]):
        if i == _test_set[1][num]:
            correct += 1
        num += 1
    _end_time = datetime.now().timestamp()",src/hw1_Classification/PerformanceReoprt.py,MyXOF/Data-Mining-Algorithm,1
"



####################### MAIN!!! ########################
if __name__ == '__main__':
    train, target, test, passanger_id = load_data()
    train, test = preprocessing(train, test)

    clf = tree.DecisionTreeClassifier(min_samples_leaf=10)      # DECISION TREE
    # clf = ensemble.RandomForestClassifier(n_estimators=100)     # RANDOM FOREST
    # clf = svm.SVC(kernel='poly', degree=4)                        # SVM
    # clf = clf = ensemble.AdaBoostClassifier(n_estimators=10, base_estimator=ensemble.RandomForestClassifier())       # ADA BOOST
    clf.fit(train, target)
    performance(clf, train, target)
    my_prediction = clf.predict(test)
    submit(my_prediction, passanger_id, submit_csv='submission.csv')


",Kagle_Titanic_Python/Decision_tree.py,Skobnikoff/Datasets,1
"def EnsembleClassifier(bags, *Classifiers):
    return VoteClassifier(*[BaggingClassifier(bags, C) for C in Classifiers])


'''
classifier_fns = (
    lambda: SMOTEClassifier(KNNClassifier(1)),
    lambda: SMOTEClassifier(KNNClassifier(2)),
    lambda: SMOTEClassifier(SVMClassifier(6, kernel='rbf')),
    lambda: SMOTEClassifier(SVMClassifier(8, kernel='poly', degree=3)),
    lambda: SMOTEClassifier(RandomForestClassifier(k=128))
    )

bagged_classifiers = [BaggingClassifier(10, C) for C in classifier_fns]
ensemble = VoteClassifier(*bagged_classifiers)
test_classifier(VoteClassifier(*bagged_classifiers))
'''
#test_classifier(SMOTEClassifier(SVMClassifier(6, kernel='rbf')))",classifier/EnsembleClassifier.py,kosigz/DiabetesPatientReadmissionClassifier,1
"from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np


from skml.problem_transformation import ClassifierChain
from skml.datasets import load_dataset

X, y = load_dataset('yeast')
cc = ClassifierChain(RandomForestClassifier(),
                     threshold=.5)
cc.fit(X, np.array(y))
y_pred = cc.predict(X)
y_pred_proba = cc.predict_proba(X)

print(""real: "", y.shape)
print(""y_pred: "", y_pred.shape)

print(""hamming loss: "")",examples/example_cc.py,ChristianSch/skml,1
"    print(""-- Decision Tree Classifier"")
    t = time.time()
    model = DecisionTreeClassifier()
    model.train(train_data)
    labels = model.predict(ambiguous_data)
    print(labels)
    print(""Done,"", time.time() - t, ""s\n\n"")

    # print(""-- Random Forest Classifier (default parameters)"")
    # t = time.time()
    # model = RandomForestClassifier()
    # model.train(train_data)
    # labels = model.predict(ambiguous_data)
    # print(labels)
    # print(""Done,"", time.time() - t, ""s\n\n"")",tests/test.py,Ambiruptor/Ambiruptor,1
"  # set_trace()
  return _Abcd(before=actual, after=preds, show=False)[-1]


def rforest(train, test, tunings=None, smoteit=True, duplicate=True):
  ""RF ""
  # Apply random forest Classifier to predict the number of bugs.
  if smoteit:
    train = SMOTE(train, atleast=50, atmost=101, resample=duplicate)
  if not tunings:
    clf = RandomForestClassifier(n_estimators=100, random_state=1)
  else:
    clf = RandomForestClassifier(n_estimators=int(tunings[0]),
                                 max_features=tunings[1] / 100,
                                 min_samples_leaf=int(tunings[2]),
                                 min_samples_split=int(tunings[3])
                                 )
  train_DF = formatData(train)
  test_DF = formatData(test)
  features = train_DF.columns[:-2]",Prediction.py,ai-se/Transfer-Learning,1
"multi_Y_train = pd.read_csv(""../input/y_train.csv"", skiprows=0).as_matrix()[:, 1:]
X_test = pd.read_csv(""../input/X_test.csv"", skiprows=0).as_matrix()[:, 1:]
multi_Y_test = []
multi_Y_test_true = pd.read_csv(""../input/y_scoring.csv"", skiprows=0).as_matrix()[:, 1:3]

error_list = []
for current_column in range(multi_Y_train.shape[1]):
    Y_train = multi_Y_train[:, current_column]
    Y_train, encoder = preprocess_labels(Y_train)

    model = RandomForestClassifier(n_estimators=100)
    model.fit(X_train, Y_train)
    classes = model.predict(X_test)
    prediction = encoder.inverse_transform(classes)

    Y_test_true = multi_Y_test_true[:, current_column]
    current_MAE = mean_absolute_error(Y_test_true, prediction)
    error_list.append(current_MAE)
    print(""For column {:d}, MAE is {:.2f}"".format(current_column, current_MAE))
",Extra/solution_RandomForest.py,nixingyang/Kaggle-Head-Pose-Estimation,1
"
    def predict(self, X):
        return np.vstack(self.clf.predict(X))

    def __str__(self):
        return str(self.clf)


class MultiLabelRF(object):
    def __init__(self, verbosity):
        self.clf = RandomForestClassifier(n_estimators=100,
                                          random_state=seed,
                                          n_jobs=-1)
        # self = MultiOutputClassifier(forest, )

    def fit(self, X, y):
        self.clf.fit(X, y)

    def predict(self, X):
        return self.clf.predict(X)",structured/models.py,tritas/structured-predictions,1
"
# comma delimited is the default
df = pd.read_csv(input_file, header=0)
nd_data = df.as_matrix()
X = nd_data[:, 1:]
Y = nd_data[:, 0]
split_pos = int(X.shape[0] * 0.8)
print split_pos
# print X
# print Y
clf = RandomForestClassifier(n_estimators=100)
clf = clf.fit(X[:split_pos], Y[:split_pos])
print clf.score(X[:split_pos], Y[:split_pos])
print clf.score(X[split_pos:], Y[split_pos:]    )",ML/uic_predict.py,jeffzhengye/pylearn,1
"    featureCount = sum([sum(v) for v in trainData])
    # print(""Feature found: "", featureCount, ""times."")


    classifiers = [DecisionTreeClassifier(),
                    SVC(kernel=""linear""), 
                    SVC(), 
                    LinearSVC(), 
                    MultinomialNB(),
                    GaussianNB(),
                    RandomForestClassifier(),
                    LogisticRegression(),]

    # Cross validation
    if testSetFilename == None:
        for c in classifiers:
            applyCrossValidation(c, trainData, trainTargets)
            
            # Show star distribution for each classifier
            # applyCrossValidation(c, trainData, trainTargets, stars=stars)",src/machineLearning.py,kbuschme/irony-detection,1
"import matplotlib.pyplot as plt
plt.style.use('ggplot')
import numpy as np
import sklearn as skl

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier( \
    n_estimators=2000, max_features=128, n_jobs=-1, class_weight='auto')
features = np.load('train_features.npy')
labels = np.load('train_labels.npy')
if __name__ == '__main__': clf.fit(features, labels)
print(clf)

test_feats = np.load('test_features.npy')
predicts = clf.predict(test_feats)
test_labels = np.load('test_labels.npy')",classify_rforests.py,masaki-y/Feature-Extraction-with-Caffe,1
"test_classifier(
    BaggingClassifier(5, lambda: SMOTEClassifier(SVMClassifier(C=6, kernel=""rbf""))),
    folds=5)
test_classifier(BaggingClassifier(5, lambda: SMOTEClassifier(RandomForestClassifier(128))), folds=5)

''''''
test_classifier(
    VoteClassifier(
        BaggingClassifier(5, lambda: SMOTEClassifier(SVMClassifier(C=9, kernel=""poly"", degree=3))),
        BaggingClassifier(5, lambda: SMOTEClassifier(SVMClassifier(C=6, kernel=""rbf""))),
        BaggingClassifier(5, lambda: SMOTEClassifier(RandomForestClassifier(128))),
        BaggingClassifier(10, lambda: SMOTEClassifier(RandomForestClassifier(128)))
    ),
    folds=5)
    '''",classifier/BaggingClassifier.py,kosigz/DiabetesPatientReadmissionClassifier,1
"

if __name__ == '__main__':


    X, y, X_submission = load_data.load()


    skf = list(StratifiedKFold(y, n_folds))

    clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]

    print ""Creating train and test sets for blending.""

    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))
    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))",models/blender.py,lukovnikov/hatespeech,1
"from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np


from skml.ensemble import EnsembleClassifierChain
from skml.datasets import load_dataset

X, y = load_dataset('yeast')
ensemble = EnsembleClassifierChain(RandomForestClassifier(),
                                   threshold=.5,
                                   max_features=1.0)
ensemble.fit(X, y)
y_pred = ensemble.predict(X)
print y.shape
print y_pred.shape

print ""hamming loss: ""
print hamming_loss(y, y_pred)",doc/auto_examples/example_ecc.py,ChristianSch/skml,1
"x_full_path = current_data_dir + slash + file_name
y_full_path = current_data_dir + slash + y_file_name

if __name__ == '__main__':
	
	X = np.loadtxt(x_full_path)
	X = np.asarray(X, dtype=np.float32, order='F')
	Y = np.loadtxt(y_full_path)
	Y = np.asarray(Y, dtype=np.short, order='F')

	clf = RandomForestClassifier(n_estimators=RF_size, oob_score=True, n_jobs= -1, compute_importances = True, min_samples_split=1)
	clf = clf.fit(X.T, Y)

	print x_full_path
	print clf.oob_score_

	fh = open(results_dir_full_path + subject, 'w')
	pickler = pickle.Pickler(fh)
	pickler.dump(clf)",BCI_Framework/RF_singleDataset.py,lol/BCI-BO-old,1
"                        # 
                        if inClassifier == 'RF':
                            param_grid_rf = dict(n_estimators=3**sp.arange(1,5),max_features=sp.arange(1,4))
                            y.shape=(y.size,)    
                            if model_selection : 
                                cv = StratifiedKFold(n_splits=3).split(x,y)
                                #cv = cv.get_n_splits(y)
                            else:
                                cv = StratifiedKFold(y, n_folds=3)
                                
                            grid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid_rf, cv=cv,n_jobs=n_jobs)
                            grid.fit(x, y)
                            model = grid.best_estimator_
                            model.fit(x,y)        
                        elif inClassifier == 'SVM':
                            param_grid_svm = dict(gamma=2.0**sp.arange(-4,4), C=10.0**sp.arange(-2,5))
                            y.shape=(y.size,)    
                            if model_selection : 
                                cv = StratifiedKFold(n_splits=5).split(x,y)
                            else:",scripts/mainfunction.py,lennepkade/dzetsaka,1
"                                           svm.SVC(probability=True, kernel=""sigmoid"", C=0.1, coef0=0.01, gamma=0.01),
                                           True, True, False)
    available_clfs[""svm_gs2""] = Classifier(""svm"", ""Co-best SVM according to Skll Grid Search"",
                                           svm.SVC(probability=True, kernel=""sigmoid"", C=0.01, coef0=0.01, gamma=0.0),
                                           True, True, False)
    available_clfs[""mnb""] = Classifier(""mnb"", ""Multinomial Naive Bayes"", naive_bayes.MultinomialNB(), False, True,
                                       False)  # MNB can't do default scaling: ValueError: Input X must be non-negative
    available_clfs[""knn""] = Classifier(""knn"", ""k Nearest Neighbour"", neighbors.KNeighborsClassifier(), True, True,
                                       False)  # knn can do feature scaling
    available_clfs[""raf""] = Classifier(""raf"", ""Random Forest"",
                                       ensemble.RandomForestClassifier(n_estimators=15, max_depth=5, oob_score=True),
                                       True, True, False)
    return available_clfs


def __load_arg_parser():
    arg_parser = argparse.ArgumentParser(description=""Perform data manipulation to extract features and finally run ""
                                                     ""classification."")
    group_get_model = arg_parser.add_mutually_exclusive_group(required=True)
    group_get_model.add_argument(""--training"", help=""The base path to the training set"")",glad-main.py,rug-compling/glad,1
"        clf = svm.SVC(kernel = 'linear', probability = True)
    elif name == 'rbfSVM':
        clf = svm.SVC(kernel = 'rbf', probability = True)
    elif name == 'sigmoidSVM':
        clf = svm.SVC(kernel = 'sigmoid')
    elif name == 'polySVM':
        clf = svm.SVC(kernel = 'poly')
    elif name == 'decisiontree':
        clf = DecisionTreeClassifier()
    elif name == 'randomforest':
        clf = RandomForestClassifier()
    elif name == 'extratree':
        clf = ExtraTreesClassifier()
    elif name == 'adaboost':
        clf = AdaBoostClassifier()
    elif name == 'gradientboost':
        clf = GradientBoostingClassifier()
    return clf

",InstrumentRecognition/InstrumentRecognition - Copy.py,tian-zhou/Surgical-Instrument-Dataset,1
"                             min_df = 5,
                             max_df = 0.25)


train_tfidf = vectorizer.fit_transform(training_text_collection)

save_vect = open(""rf_tfidf_vect.pkl"", ""wb"")
joblib.dump(vectorizer, save_vect)
save_vect.close()

clf = RandomForestClassifier(n_estimators = 250).fit(train_tfidf, training_targets)

save_clf = open(""rf_clf.pkl"", ""wb"")
joblib.dump(clf, save_clf)
save_clf.close()",rf_train.py,npentella/CuriousCorpus,1
"
classifiers = {
        'SVCP': svm.SVC(gamma=0.001, C=10),
        'SVCR': svm.SVC(gamma=0.0001, C=50),
        'NB ': GaussianNB(),
        'BNB': BernoulliNB(),
        'NBU': neighbors.KNeighborsClassifier(5, weights='uniform'),
        'NBD': neighbors.KNeighborsClassifier(5, weights='distance'),
        'TRE': tree.DecisionTreeClassifier(),
        'GBC': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0),
        'RFC': RandomForestClassifier()
    }

scores = [(n, clf.fit(training_data, training_target).score(test_data,
    test_target)) for n, clf in classifiers.iteritems()]

for name, score in sorted(scores, key=lambda t: t[1], reverse=True):
    print name, score",score_classifiers.py,ssaamm/sign-language-translator,1
"import pandas as pd
import math

df = pd.read_csv(""parsed_heart.csv"")
y1 = df[""num""].values

cols = list(df)
mlp = MLPClassifier(hidden_layer_sizes=(100,100,100))
clf1 = BaggingClassifier(n_estimators=10)
clf2 = BaggingClassifier(n_estimators=100)
clf3 = RandomForestClassifier(n_estimators=10,criterion='gini', min_samples_split=2,max_features=None)
clf4 = AdaBoostClassifier(n_estimators=100)
clf5 = VotingClassifier(estimators=[(""rf"",clf3),('bg',clf2),('ml',mlp),('ada',clf4)],voting='soft')


dropped = set(['num','id'])
columns2 = [z for z in cols if z not in dropped]
X2 = df[columns2].values
X_train, X_test, y_train, y_test = train_test_split(X2,y1,test_size=0.90)
scaler = StandardScaler()",data/gary/heart.py,isabellewei/deephealth,1
"    SVC(kernel=""linear"", C=0.025),                                              #  3
    SVC(kernel=""linear"", C=1),                                                  #  4
    SVC(kernel=""linear"", C=100),                                                #  5
    SVC(gamma=0.5, C=0.1),                                                      #  6
    SVC(gamma=2, C=1),                                                          #  7
    SVC(gamma=50, C=100),                                                       #  8
    DecisionTreeClassifier(max_depth=5),                                        #  9
    DecisionTreeClassifier(max_depth=10),                                       # 10 - !
    SVC(gamma=2, C=1000),                                                       # 11
    SVC(gamma=2, C=100),                                                        # 12
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),       # 13
    RandomForestClassifier(max_depth=10, n_estimators=10, max_features=1),      # 14 - !
    RandomForestClassifier(max_depth=15, n_estimators=50, max_features=5),      # 15 - !
    AdaBoostClassifier(),                                                       # 16
    GaussianNB(),                                                               # 17
    MultinomialNB(),                                                            # 18
    BernoulliNB(),                                                              # 19
    LDA(),                                                                      # 20
    QDA()                                                                       # 21
    ]",KForest/KForest/KForest.py,Andrew414/KForest,1
"# with open('data.txt','w') as f:
# 	f.write(str(data))

# with open('captchas.txt','w') as f:
# 	f.write(str(captchas))

import json
with open('res_json.json','w') as fp:
	json.dump(res, fp)

# model= RandomForestClassifier(n_estimators=1000)
# #nsamples, nx, ny = data.shape
# #train_dataset = data.reshape((nsamples,nx*ny))

# import zlib
# com = zlib.compress(str(data))

# com_val = zlib.compress(str(captchas))

# import ast",captcha.py,shubh24/Real-Data-16,1
"counter = 0
for review in clean_test_views:
    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)
    counter + 1

counter = 0
for review in clean_test_views:
    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)
    counter + 1

forest = RandomForestClassifier(n_estimators=100, verbose=True)

forest.fit(train_centroids, train[""sentiment""])
result = forest.predict(test_centroids)

output = pd.DataFrame(data={""id"": test[""id""], ""sentiment"": result})
output.to_csv(""data/BagOfCentroids.csv"", index=False, quoting=3)",BagOfWordsMeetsBagsOfPopcorn/centroidmap.py,IgowWang/MyKaggle,1
"from __future__ import print_function
from sklearn.ensemble import RandomForestClassifier
from wekapyscript import ArffToArgs, uses

@uses([""num_trees""])
def train(args):
    X_train = args[""X_train""]
    y_train = args[""y_train""].flatten()
    num_trees = args[""num_trees""] if ""num_trees"" in args else 10
    rf = RandomForestClassifier(n_estimators=num_trees, random_state=0)
    rf = rf.fit(X_train, y_train)
    return rf

def describe(args, model):
    return ""RandomForestClassifier with %i trees"" % model.n_estimators

def test(args, model):
    X_test = args[""X_test""]
    return model.predict_proba(X_test).tolist()",scripts/scikit-rf.py,christopher-beckham/weka-pyscript,1
"
    #s = StandardScaler()
    #s.fit(xs)
    #xs = s.transform(xs)
    #xs_sub = s.transform(xs_sub)
    xs, xs_sub = whiten(xs, xs_sub)

    from sklearn.linear_model import LogisticRegression
    m = LogisticRegression()
    #from sklearn.ensemble import RandomForestClassifier
    #m = RandomForestClassifier(
    #    400, criterion='entropy')
    m.fit(xs, ys)
    pred_sub = m.predict_proba(xs_sub)

    name = 'queen_%s' % args.name
    dump((None, pred_sub[:, [0]], None), name)

    I = np.eye(len(xs[0]))
    pp = m.predict_proba(I)",devel/workbench.py,nishio/kagura,1
"from sklearn.ensemble import RandomForestClassifier
from techson_server.settings import BASE_DIR

path = BASE_DIR + '/db/dataset/data.csv'

train_data = pd.read_csv(path)

y_train = train_data['label']
x_train = train_data.drop('label', axis=1)

RFC = RandomForestClassifier(n_estimators=150, n_jobs=-1)

RFC.fit(x_train, y_train)

path = BASE_DIR + '/classifiers/neural_network_classifier.pkl'

with open(path, 'wb') as f:
    pickle.dump(RFC, f)",mlscripts/neural_network_classifier.py,KirovVerst/techson_server,1
"              (len(X), sum(y)/float(len(X))*100.0))
        #X = StandardScaler().fit_transform(X)

        print(""Training...."")
        t = time.time()
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

        #clf = KNeighborsClassifier(5) # EXPENSIVE at prediction, NOT suitable
        #self.clf = sklearn.linear_model.LogisticRegression() # NOT good.
        self.clf = sklearn.tree.DecisionTreeClassifier(max_depth=5)
        #self.clf = sklearn.ensemble.RandomForestClassifier(min_samples_split=len(X)/20, n_estimators=6)
        #self.clf = sklearn.svm.SVC(max_iter=1000) # can't make it work too well..
        self.clf.fit(X_train, y_train)
        print(""Training finished. T: %-3.2f"" % (time.time()-t))

        print(""Calculating scores...."")

        t = time.time()
        y_pred = self.clf.predict(X_test)
        recall = sklearn.metrics.recall_score(y_test, y_pred)",scripts/learn/predict.py,SmallLars/cryptominisat,1
"
# remove the feature indices which are in the form 'feature_no:feature_value'
for i in xrange(0, len(train_inputs)):
	for j in xrange(0, len(train_inputs[i])):
		train_inputs[i][j] = float(train_inputs[i][j].split(':')[1])

for i in xrange(0, len(test_inputs)):
	for j in xrange(0, len(test_inputs[i])):
		test_inputs[i][j] = float(test_inputs[i][j].split(':')[1])

rf = RandomForestClassifier(n_estimators = 20, max_features = 'auto')
# print '... training random forest'
rf.fit(train_inputs, train_outputs)

# print '... testing'
for i in xrange(0, len(test_inputs)):
	prediction = rf.predict(test_inputs[i])
	prediction = int(prediction[0]) # converting prediction from numpy array of single value to python int
	
	if prediction == 1:",rf.py,therainmak3r/quora-AnswerClassifier,1
"
def classification():
    # Generate a random binary classification problem.
    X, y = make_classification(n_samples=500, n_features=10, n_informative=10,
                               random_state=1111, n_classes=2,
                               class_sep=2.5, n_redundant=0)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15,
                                                        random_state=1111)

    model = RandomForestClassifier(n_estimators=10, max_depth=4)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)[:, 1]
    # print(predictions)
    print('classification, roc auc score: %s'
          % roc_auc_score(y_test, predictions))


def regression():
    # Generate a random regression problem",examples/random_forest.py,rushter/MLAlgorithms,1
"def apply_algorithm(paras, data):
    X = data[""X""]
    y = data[""y""]

    if paras['clf'] == 'svm':
        clf = svm.SVC(kernel=paras['svm'][1], C=paras['svm'][0], probability=True)
    elif paras['clf'] == 'knn':
        clf = neighbors.KNeighborsClassifier(paras['knn'][0],\
                                             weights=paras['knn'][1])
    elif paras['clf'] == 'rf':
        clf = RandomForestClassifier(max_depth=paras['rf'][0], \
                                     n_estimators=paras['rf'][1],\
                                     max_features=paras['rf'][2])
    elif paras['clf'] == 'lr':
        clf = linear_model.LogisticRegression(C=0.5)
    else:
        print str(""unknown classifier"") 
        sys.exit(2)
    
",python/per_year_predict/methods.py,Healthcast/RSV,1
"    # is_alone:
    data['is_alone'] = data.Family_Size.map(lambda x:0 if x == 1 else 1)
    train_data_y = data['Survived']
    data = data.drop(['Survived', 'Name', 'PassengerId', 'Ticket', 'Cabin', 'SibSp', 'Parch'], axis=1)
    global label
    label = data.keys()
    return data.values, train_data_y.values

def process_feature_importance(train_data_x, train_data_y) :
    global label
    forest = RandomForestClassifier(n_estimators=1000)
    forest = forest.fit(train_data_x, train_data_y)
    feature_importance = forest.feature_importances_
    print feature_importance
    print label
    feature_importance = 100. * feature_importance / feature_importance.max()
    index = np.argsort(feature_importance)[::-1]
    print len(label), len(feature_importance)
    for i in range(len(feature_importance)):
        print 'feature',label[index[i]], index[i], feature_importance[index[i]]",feature_importance.py,kdqzzxxcc/TitanicPredict,1
"from cudatree import load_data, RandomForestClassifier, timer
from cudatree import util

x,y = load_data(""digits"")

n_estimators = 13 
bootstrap = True

def test_digits_memorize():
  with timer(""Cuda treelearn""):
    forest = RandomForestClassifier(n_estimators = n_estimators/2, bootstrap = False)
    forest.fit(x, y)
  with timer(""Predict""):
    diff, total = util.test_diff(forest.predict(x), y)  
    print ""%s (Wrong) / %s (Total). The error rate is %f."" % (diff, total, diff/float(total))
  assert diff == 0, ""Didn't memorize, got %d wrong"" % diff 

from helpers import compare_accuracy 
def test_digits_vs_sklearn():
  compare_accuracy(x,y)",test/test_digits.py,EasonLiao/CudaTree,1
"from sklearn.feature_selection import SelectKBest, SelectFromModel
from sklearn.ensemble import RandomForestClassifier
import numpy as np

rng = np.random.RandomState(1)
X = rng.randint(0, 2, (200, 20))
y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

fs_univariate = SelectKBest(k=10)
fs_modelbased = SelectFromModel(RandomForestClassifier(n_estimators=100), threshold='median')

fs_univariate.fit(X, y)
print('Features selected by univariate selection:')
print(fs_univariate.get_support())
print('')

fs_modelbased.fit(X, y)
print('Features selected by model-based selection:')
print(fs_modelbased.get_support())",scipy-2017-sklearn-master/notebooks/solutions/20_univariate_vs_mb_selection.py,RPGOne/Skynet,1
"digits = load_digits()
X = digits.data
y = digits.target

#Splitting datasets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4
)

#Initializing classifier
clf = RandomForestClassifier(n_estimators=20)
clf = clf.fit(X_train, y_train)

#Predicting data
predict = clf.predict(X_test)
score = accuracy_score(y_test, predict)

print(""Score: {}"".format(score))",Classifiers/randomForestClassifier.py,pawanrai9999/SimpleMachines,1
"
data = train[:,1:]
target = train[:,0]

# Take the 400 best features
trimmer = SelectKBest(chi2, k=400).fit(data, target)

trimmed_data = trimmer.transform(data)
trimmed_test = trimmer.transform(test)

clf = ensemble.RandomForestClassifier(n_estimators=100)
pred = clf.fit(trimmed_data, target).predict(trimmed_test)

f = open('predictions.csv', 'w')
f.write(""ID,Category\n"")

for i, res in enumerate(pred):
    f.write(""%d,%d\n"" % (i+1,res))

f.close()",Black-Box/code.py,bcspragu/Machine-Learning-Projects,1
"    best = 0.0
    best_Output = []
    for i in [x**2 for x in range(1,2)]:

        X = train_data[0::,1::]
        y = train_data[0::,0]
        
        Xt = test_data[0::,1::]
        yt = test_data[0::,0]

        Forest = RandomForestClassifier(n_estimators = n_est,random_state=93758) # = 100
        Forest = Forest.fit(X,y)
        Output = Forest.predict(Xt)
    
        result = Forest.score(Xt,yt)

        if result>best:
            best = result
            best_Output = Output
",titanic/forest.py,cianmj/kaggle,1
"                                                                                   '%Y-%m-%d %H:%M:%S').hour, axis=1)
    training = df_train.loc[0:, ""datetime"":""windspeed""]
    target = df_train.loc[0:, 'count']
    test = df_test_temp.loc[0:, ""datetime"":""windspeed""]

    print(""DF Dim = "" + str(df_train.shape))
    print(""training Dim = "" + str(training.shape))
    print(""target Dim = "" + str(target.shape))
    print(""Test Dim = "" + str(test.shape))

    # multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)
    # regress = DecisionTreeRegressor(max_depth=8)
    regress = RandomForestRegressor(n_estimators=90, max_depth=9)
    # regress = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=7, random_state=0, loss='ls')

    # Simple K-Fold cross validation. 5 folds.
    cv = cross_validation.KFold(len(training), 6)

    # iterate through the training and test cross validation segments and
    # run the classifier on each one, aggregating the results into a list",BikeSharingDemand/crossValidate.py,kraktos/Kaggling,1
"        params = log_params
        model = SGDClassifier()
    elif choice=='huber':
        params = huber_params
        model = SGDClassifier()
    elif choice=='svm':
        params = SVM_params
        model = svm.SVC(C=1)
    elif choice=='rf':
        params = RF_params
        model = RandomForestClassifier(n_estimators=1000, bootstrap=False)
        #clf = RandomForestClassifier(n_estimators=1000, bootstrap=False)
    
    # Set up Grid Search
    print ""Grid search...""
    clf = GridSearchCV(model, params, n_jobs=2, scoring='f1')
    clf.fit(X, y)
    clf = clf.best_estimator_
    print clf
    ",modelMake.py,momiah/cvariants_opencv,1
"
    def fit(self, df_X, df_y):
        if not df_y.shape[0] == df_X.shape[0]:
            raise ValueError(""number of regions is not equal"")
        if df_y.shape[1] != 1:
            raise ValueError(""y needs to have 1 label column"")

        le = LabelEncoder()
        y = le.fit_transform(df_y.iloc[:,0].values)

        clf = RandomForestClassifier(n_estimators=100)
        
        # Multiclass
        if len(le.classes_) > 2:
            orc = OneVsRestClassifier(clf)
            orc.fit(df_X.values, y)

            importances = np.array([c.feature_importances_ for c in orc.estimators_]).T
        else: # Only two classes
            clf.fit(df_X.values, y)",gimmemotifs/moap.py,simonvh/gimmemotifs,1
"        
# Delcaration des pre-processing / classifier
ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3), min_df=0)
transformer = TfidfTransformer()

# Chaque paramtres reprsente la meilleur performance de l'ensemble
classifiers = [
    SGDClassifier(alpha = 1e-4),
    DecisionTreeClassifier(max_depth=None),
    SVC(gamma=2, C=1),
    RandomForestClassifier(n_estimators=60),
    AdaBoostClassifier()]
    

Result = numpy.empty((0,3), float)

# Boucle sur les classifiers
for clf in classifiers:

    Scores = numpy.array([])",BIM_Classifier.py,HugoMartin78/bim-classifier,1
"#cv_int = ShuffleSplit(n_splits=3, test_size=TESTING_SET_DECIMAL, random_state=0)


'''
GridSearchCV implementation-> 38.6-39%
'''

'''

parameters = {'n_estimators':[n for n in range(25,46,5)], 'min_samples_split':[m for m in range(3,8) if m != 5]}
rf_grid_model = RandomForestClassifier()
rf_best_grid_model = GridSearchCV(rf_grid_model, parameters)
rf_best_grid_model.fit(train_movies_X, train_movies_Y)
print('Prediction accuracy for GridSearchCV optimum model: %.3f' %rf_best_grid_model.score(test_movies_X, test_movies_Y))

'''

best_model = None	# Maintain a model and score for the best fold
best_score = 0.0
'''",random_forests.py,neilpat1995/IMDb-Movie-Rating-Predictor,1
"        help='Training labels (npz)')
        
    return parser

if __name__ == ""__main__"":
    args = opts().parse_args()
    Y_train = args.labels['labels']
    
    for i, y in enumerate(Y_train.T):
        logging.info(i)
        clf = RandomForestClassifier(
            n_estimators=48, max_features=0.2,
            min_samples_split=1, max_depth=None, max_leaf_nodes=None,
            criterion='entropy', min_samples_leaf=2,
            n_jobs=-1, random_state=42, verbose=2)
        try:
            scores = cross_validate(clf, args.train, Y_train)
        except Exception, e:",scripts/predictors/random_forest.cv.py,timpalpant/KaggleTSTextClassification,1
"from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

from skml.problem_transformation import BinaryRelevance
from skml.datasets import load_dataset

X, y = load_dataset('yeast')
clf = BinaryRelevance(RandomForestClassifier())
clf.fit(X, np.array(y))
y_pred = clf.predict(X)

print ""hamming loss: ""
print hamming_loss(y, y_pred)

print ""accuracy:""
print accuracy_score(y, y_pred)
",doc/auto_examples/example_br.py,ChristianSch/skml,1
"class RfClassifier():
    def __init__(self,**kwargs):
        self.params = kwargs
        self.clf  = None




    def train(self, X, Y):
        if self.clf is None:
            self.clf = RandomForestClassifier(**self.params)

        self.clf.fit(X,Y)


    def save(self, fname):
        assert  self.clf is not None
        with open(fname, 'wb') as fid:
            cPickle.dump(self.clf, fid)    
",classifier.py,DerThorsten/pc,1
"from datamanagers.CaltechManager import CaltechManager
import numpy as np
import pylab as pl

from runGridSearch import GridSearch

if __name__ == ""__main__"":
    category = ""airplanes""
    total = time.time()

    clf = RandomForestClassifier(n_estimators = 2000)

    # clf = AdaBoostClassifier(n_estimators = 2000)
    # clf.base_estimator.max_depth = 4

    # clf = LinearSVC(C=100)
    # clf = SVC(C=10)

    dm = CaltechManager()
    vcd = VisualConceptDetection(classifier=clf, datamanager=dm)",runClassification.py,peret/visualize-bovw,1
"    # Initialize the algorithm with alpha=1/K, eta=1/K, tau_0=1024, kappa=0.7
    lda = onlineldavb.OnlineLDA(vocab, K, D, 1./K, 1./K, 1024., 0.7)

    print(""Allocating the topics"")
    allocate_topics(lda, data, K, batchsize, D)

    print(""Extracting features"")
    fea = extract_features(feature_names, data)

    print(""Training the model"")
    rf = RandomForestClassifier(n_estimators=50, verbose=2,
                                compute_importances=True, n_jobs=4)
    rf.fit(fea, data[""OpenStatus""])

    print(""Reading test file and making predictions"")
    data = cu.get_dataframe(test_file)
    allocate_topics(lda, data, K, batchsize, D)
    test_features = extract_features(feature_names, data)
    probs = rf.predict_proba(test_features)
",submission/model.py,coreyabshire/stacko,1
"### your code here!  name your classifier object clf if you want the 
### visualization code (prettyPicture) to show you the decision boundary
def kNeighbors(features_train, labels_train, features_test, labels_test):
    clf = KNeighborsClassifier(n_neighbors=3)
    clf.fit(features_train, labels_train)
    print ""K-Neighbors accuracy: "", clf.score(features_test, labels_test)
    makePicture(clf, ""kneighbors"")


def randomForests(features_train, labels_train, features_test, labels_test):
    clf = RandomForestClassifier()
    clf.fit(features_train, labels_train)
    print ""Random Forests accuracy: "", clf.score(features_test, labels_test)
    makePicture(clf, ""rforests"")


def adaBoost(features_train, labels_train, features_test, labels_test):
    clf = AdaBoostClassifier()
    clf.fit(features_train, labels_train)
    print ""Ada Boost accuracy: "", clf.score(features_test, labels_test)",learning/ud120-projects/choose_your_own/your_algorithm.py,dmytroKarataiev/MachineLearning,1
"    param_class_weight = kwargs.pop('class_weight', None)
    n_log_space = kwargs.pop('n_log_space', 10)
    gs_n_jobs = kwargs.pop('gs_n_jobs', multiprocessing.cpu_count())

    # If the number of estimators is not specified, it will be find
    # by cross-validation
    if 'n_estimators' in kwargs:
        param_n_estimators = kwargs.pop('n_estimators', 10)

        # Construct the Random Forest classifier
        crf = RandomForestClassifier(n_estimators=param_n_estimators,
                                     criterion=param_criterion,
                                     max_depth=param_max_depth,
                                     min_samples_split=param_min_samples_split,
                                     min_samples_leaf=param_min_samples_leaf,
                                     max_features=param_max_features,
                                     bootstrap=param_bootstrap,
                                     oob_score=param_oob_score,
                                     n_jobs=param_n_jobs,
                                     random_state=param_random_state,",protoclass/classification/classification.py,glemaitre/protoclass,1
"numTrees = 1000

# Build Machine Learning pipeline
inx1 = StringIndexer(inputCol=""hour"", outputCol=""hour-inx"")
inx2 = StringIndexer(inputCol=""month"", outputCol=""month-inx"")
inx3 = StringIndexer(inputCol=""dayofweek"", outputCol=""dow-inx"")
inx4 = StringIndexer(inputCol=""sentiment"", outputCol=""label"")
hashingTF = HashingTF(numFeatures=numFeatures, inputCol=""words"", outputCol=""hash-tf"")
idf = IDF(minDocFreq=minDocFreq, inputCol=""hash-tf"", outputCol=""hash-tfidf"")
va = VectorAssembler(inputCols =[""hour-inx"", ""month-inx"", ""dow-inx"", ""hash-tfidf"", ""pscore"", ""nscore""], outputCol=""features"")
rf = RandomForestClassifier(numTrees=numTrees, maxDepth=4, maxBins=32, labelCol=""label"", seed=42)
p = Pipeline(stages=[inx1, inx2, inx3, inx4, hashingTF, idf, va, rf])

# Split feature matrix into train/test sets
(trainSet, testSet) = hc.table(""fm"").randomSplit([0.7, 0.3])
trainData = trainSet.cache()
testData = testSet.cache()
model = p.fit(trainData)                 # Train the model on training data

# Helper function to evaluate precision/recall/accuracy",ch08/script.py,ofermend/data-science-with-hadoop-book,1
"


#
# TODO: Create an RForest classifier 'model' and set n_estimators=30,
# the max_depth to 10, and oob_score=True, and random_state=0
#
# .. your code here ..

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=30, max_depth = 10, oob_score=True,  random_state=0)

# 
# TODO: Split your data into test / train sets
# Your test size can be 30% with random_state 7
# Use variable names: X_train, X_test, y_train, y_test
#
# .. your code here ..

from sklearn.cross_validation import train_test_split",Module 6/assignment6.py,LamaHamadeh/Microsoft-DAT210x,1
"names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA""]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",projects/scikit-learn-master/examples/classification/plot_classifier_comparison.py,DailyActie/Surrogate-Model,1
"    # Ridge(),
    # LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_pca1_dt_only.py,diogo149/CauseEffectPairsPaper,1
"    
    
    #### Support vector machines and leafy classifiers
    
    
    classifier_list = [
        (""libsvm"", lambda: svm.SVC(kernel='linear', class_weight='auto', C=0.001)),
        (""logreg"", lambda: svm.LinearSVC(C=0.01, penalty=""l1"", dual=False, class_weight='auto')),
        (""rbf_svm"", lambda: svm.SVC(kernel='rbf')),
        (""rbf_nu_svm"", lambda: svm.NuSVC(kernel='rbf')),
        (""random_forest"", lambda: ensemble.RandomForestClassifier(criterion=""entropy"", n_estimators=50, min_samples_split=3, max_depth=6)),
        (""decision_tree"", lambda: tree.DecisionTreeClassifier(criterion=""entropy"", min_samples_split=3, max_depth=6)),
        (""naive_bayes"", lambda: naive_bayes.GaussianNB())
    ]

    for name, clf_gen in classifier_list:
        try:
            clf = clf_gen()
            clf.fit(data['X_train'], data['y_train'])
        except:",scripts/train.py,sebschu/cds-detector,1
"            interaction_group.create_dataset(""indices"", data=current_indices)

            # add new indices to training data
            for index in current_indices:
                training_data.append(current_data[index])
                training_labels.append(current_segmentation[index])

            # create classifier and predict
            rf_config = parse.config_for_function(
                RandomForestClassifier.__init__, config)
            rf = RandomForestClassifier(**rf_config)
            logger.info(""Interaction {}/{} - fit model..."".format(
                interaction_count, config[""interaction_threshold""]))
            rf.fit(np.asarray(training_data, dtype=np.float32),
                   np.asarray(training_labels, dtype=np.int))
            logger.info(""Interaction {}/{} - predict..."".format(
                interaction_count, config[""interaction_threshold""]))
            probabilities = dataio.inflate(rf.predict_proba(current_data_flat),
                                           current_segmentation.shape)
            interaction_group.create_dataset(""prediction"",",MultiVolumeActiveLearning/HierarchicalRandomForest.py,jenspetersen/Experiments,1
"        # because the values don't get set until the f1_test property is used
        # this has to be called here, even though it isn't used
        # otherwise the plot breaks
        classifier.f1_test
        all_tests[size] = classifier
    return max(scores), all_tests

ClassifierData = namedtuple('ClassifierData', 'score size name container'.split())

classifiers = [LogisticRegression(),
               RandomForestClassifier(),
               KNeighborsClassifier()]
best_scores = []
line_width = 80
containers = {}
for classifier in classifiers:    
    print('')
    print('.. csv-table:: {0}'.format(classifier.__class__.__name__))
    print(""   :header: Size,Time (train),Time (predict),Train F1,Test F1"")
    print('')",student_intervention/training_and_evaluating.py,necromuralist/student_intervention,1
"    f.write(""ID,Category\n"")

    for i, res in enumerate(pred):
        f.write(""%d,%d\n"" % (i+1,res))

    f.close()

clfs = []

# Through cv testing, I found the optimal number of estimators to be 15
clfs.append(ensemble.RandomForestClassifier(n_estimators=150))
clfs.append(ensemble.GradientBoostingClassifier(n_estimators=200))
clfs.append(ensemble.AdaBoostClassifier(n_estimators=135))
#clfs.append(neighbors.KNeighborsClassifier(n_neighbors=10))
#clfs.append(svm.SVC())

predictificate(data, target, test, clfs)

# I use the following code to find good hyperparameter values
#scores = cross_validation.cross_val_score(",Beats/code.py,bcspragu/Machine-Learning-Projects,1
"
    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    sklearn_model = RandomForestClassifier()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9",deepchem/models/tests/test_overfit.py,ktaneishi/deepchem,1
"from settings import *

names = [""Nearest Neighbors"", ""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	KNeighborsClassifier(n_neighbors=25, weights='distance'),
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]

def main():

	#set the timer
	start = time.time()
",kpcaWithUVFS/mnistBackImage/classifiers.py,akhilpm/Masters-Project,1
"    X_train, X_test, y_train, y_test = train_test_split(
        features, responses, train_size=train_split_propn)

    rf_time = np.array([])
    metrics_tmp = {}
    feature_importances = {}
    for i in range(n_trials):
        t0 = time.time()

        # run random forest and time
        rf = RandomForestClassifier(n_estimators=n_estimators)
        rf.fit(X=X_train, y=y_train)
        rf_time = np.append(rf_time, time.time() - t0)

        # get metrics
        metrics_tmp[i] = irf_utils.get_validation_metrics(rf, y_test, X_test)

        # get feature importances
        feature_importances[i] = rf.feature_importances_
",benchmarks/py_irf_benchmarks_utils.py,Yu-Group/scikit-learn-sandbox,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/sum_aggregate_only.py,diogo149/CauseEffectPairsPaper,1
"from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.externals import joblib
import datetime
import os

estimators = {}
estimators['bayes'] = GaussianNB()
estimators['tree'] = tree.DecisionTreeClassifier()
estimators['forest_100'] = RandomForestClassifier(n_estimators = 100)
estimators['forest_10'] = RandomForestClassifier(n_estimators = 10)
estimators['svm_c_rbf'] = svm.SVC()
estimators['svm_c_linear'] = svm.SVC(kernel='linear')
estimators['svm_linear'] = svm.LinearSVC()
estimators['svm_nusvc'] = svm.NuSVC()

import csv
scoreFile = open('csv/scorefile.txt', 'w', newline='')
#csvWriter = csv.writer(scoreFile, delimiter =',', quotechar ='""', quoting=csv.QUOTE_MINIMAL)",scikit-learn/classifications.py,yuantw/MachineLearning,1
"			for j in range(len(dataTest)):
				if self.macs[i] == dataTest[j][""mac""]:
					value = dataTest[j][""rssi""] 
					break
				else:
					value = 0
			item.append(value)
		return item '''

	def randomFC(self):
		clf = RandomForestClassifier(n_estimators=500, n_jobs = -1)
		clf.fit(self.trainX, self.trainY)
		print(self.locations)
		print(clf.score(self.testX, self.testY))		
		
	
randomF = RF()
data = randomF.get_data(""data/hackduke.rf.data"")
randomF.splitDataset(data, 0.6)",RF.py,schollz/find,1
"
# end

##names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
##         ""Random Forest"", ""AdaBoost"", ""Naive Bayes""]
##classifiers = [
##    KNeighborsClassifier(3),
##    SVC(kernel=""linear"", C=0.025),
##    SVC(gamma=2, C=1),
##    DecisionTreeClassifier(max_depth = 5),
##    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
##    AdaBoostClassifier(),
##    NaiveBayesClassifier()]


def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)
    word_features = wordlist.keys()
    return word_features
",ml/mergeallof5.py,john136/exercises,1
"    calculates:
    the score, the report, the kappa and the confusion matrix
    it returns the report and the confusion matrix only
    """"""
    # load data using gen_sample_all
    X,y=gs.gen_data(t)
    # spilt the dataset into training data and test data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
    # random forest classifier
    # adapt the parameters to investigate in the behavior 
    clf1=RandomForestClassifier(n_estimators=50,
                                min_samples_split=2,
                                random_state=2,
                                max_features=0.4,
                                oob_score=True,
                                max_depth=4) 
    clf1.fit(X_train,y_train)
    y_pred1=clf1.predict(X_test)
    # compare from score, classification report,cohen kappa, confusion matrix   
    score=accuracy_score(y_test, y_pred1, normalize=False)",Random_Forest.py,Ralf3/Remote-Sensing,1
"#too slow
'''
print 'svm'
S = SVC(C = 10.0)
S.fit(Xtr,ytr)
print S.score(Xte,yte)
print S.score(Xtr,ytr)
'''

print 'forest'
R = RandomForestClassifier()
R.fit(Xtr,ytr)
print R.score(Xte,yte)
print R.score(Xtr,ytr)
print getf1(R, Xte, yte)

#too slow
'''
print 'knn'
K = KNeighborsClassifier(99)",baseline_t.py,ricsoncheng/sarcasm_machine,1
"
spam_X, spam_y = make_classification(5000)

# split the datainto training and test set
spam_X_train, spam_X_test, spam_y_train, spam_y_test = xval.train_test_split(
                                                       spam_X, spam_y,
                                                       test_size=0.2)

# create RandomForestClassifier
n_trees = 500
spam_RFC = RandomForestClassifier(max_features=5, n_estimators=n_trees,
                                  random_state=42)
spam_RFC.fit(spam_X_train, spam_y_train)
spam_y_hat = spam_RFC.predict_proba(spam_X_test)

# calculate inbag and unbiased variance
spam_inbag = fci.calc_inbag(spam_X_train.shape[0], spam_RFC)
spam_V_IJ_unbiased = fci.random_forest_error(spam_RFC, spam_inbag,
                                             spam_X_train, spam_X_test)
",forest-confidence-interval/examples/plot_spam.py,RPGOne/Skynet,1
"  # n_estimators = 10
  # cls = OneVsRestClassifier(
  #   BaggingClassifier(SVC(kernel='linear', probability=True, class_weight='balanced'), max_samples=1.0 / n_estimators,
  #                     n_estimators=n_estimators))


  # GRADIENT BOOSTING
  # cls = ensemble.GradientBoostingClassifier(learning_rate=0.1, max_depth=5, verbose=0)

  # RANDOM FOREST
  cls = ensemble.RandomForestClassifier(n_estimators=100, criterion=""gini"", max_features=None, verbose=0, n_jobs=-1)

  # Run the actual training and prediction phases
  cls.fit(X_train, y_train)
  y_pred = cls.predict(X_test)

  precision = skl.metrics.precision_score(y_test, y_pred)
  recall = skl.metrics.recall_score(y_test, y_pred)
  f1 = skl.metrics.f1_score(y_test, y_pred)
  accuracy = metrics.accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)",src/binary_class/different_classifiers.py,cassinius/right-to-forget-data,1
"    df = pd.read_csv('test.csv')
    X = df.values
    X_test, ids = X[:, 1:], X[:, 0]
    return X_test.astype(float), ids.astype(str)


def train_rf():
    X_train, X_valid, y_train, y_valid = load_train_data()
    # Number of trees, increase this to beat the benchmark ;)
    n_estimators = 100
    clf = RandomForestClassifier(n_estimators=n_estimators)
    print("" -- Start training Random Forest Classifier."")
    clf.fit(X_train, y_train)
    y_prob = clf.predict_proba(X_valid)
    print("" -- Finished training."")

    encoder = LabelEncoder()
    y_true = encoder.fit_transform(y_valid)
    assert (encoder.classes_ == clf.classes_).all()
",kaggle-otto/kaggle_benchmark.py,ryanswanstrom/Sense.io-Projects,1
"

if __name__ == ""__main__"":
    np.random.seed(712)

    sz_t = 14
    sz_height_span = [1, ]
    sz_image_size = 24
    sz_downsample_size = 3

    rf_learner = ensemble.RandomForestClassifier(n_estimators=100, n_jobs=4)
    xgc_learner = xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=50, min_child_weight=1, subsample=1, colsample_bytree=1)
    isof_learner = ensemble.IsolationForest(contamination=0.001, max_samples=0.5)

    cross_validataion_classification_one_timeslot(sz_t, sz_height_span, sz_image_size, sz_downsample_size, xgc_learner, detect_outlier=False)",rainfall_classification.py,wangleye/rainfall_prediction_cikm17,1
"subset = ""full""
pdbbind_tasks, pdbbind_datasets, transformers = load_pdbbind_pockets(
    split=split, subset=subset)
train_dataset, valid_dataset, test_dataset = pdbbind_datasets

metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

current_dir = os.path.dirname(os.path.realpath(__file__))
model_dir = os.path.join(current_dir, ""pocket_%s_%s_RF"" % (split, subset))

sklearn_model = RandomForestClassifier(n_estimators=500)
model = dc.models.SklearnModel(sklearn_model, model_dir=model_dir)

# Fit trained model
print(""Fitting model on train dataset"")
model.fit(train_dataset)
model.save()

print(""Evaluating model"")
train_scores = model.evaluate(train_dataset, [metric], transformers)",examples/binding_pockets/binding_pocket_rf.py,joegomes/deepchem,1
"    #clf = linear_model.LogisticRegression(penalty='l2', C=1.2)
    _ = linear_model.LogisticRegression()
    _ = svm.LinearSVC()
    _ = naive_bayes.BernoulliNB()  # useful for binary inputs (MultinomialNB is useful for counts)
    _ = naive_bayes.GaussianNB()
    _ = naive_bayes.MultinomialNB()
    _ = ensemble.AdaBoostClassifier(n_estimators=100, base_estimator=tree.DecisionTreeClassifier(max_depth=2, criterion='entropy'))
    #clf = ensemble.AdaBoostClassifier(n_estimators=100, base_estimator=tree.DecisionTreeClassifier(max_depth=2))
    _ = tree.DecisionTreeClassifier(max_depth=50, min_samples_leaf=5)
    #clf = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf=5, criterion='entropy')
    #clf = ensemble.RandomForestClassifier(max_depth=20, min_samples_leaf=5, n_estimators=10, oob_score=False, n_jobs=-1, criterion='entropy')
    _ = ensemble.RandomForestClassifier(max_depth=10, min_samples_leaf=5, n_estimators=50, n_jobs=-1, criterion='entropy')
    #clf = ensemble.RandomForestClassifier(max_depth=30, min_samples_leaf=5, n_estimators=100, oob_score=True, n_jobs=-1)
    clf = neighbors.KNeighborsClassifier(n_neighbors=11)

    print(clf)

    kf = cross_validation.KFold(n=len(target), n_folds=5, shuffle=True)

    f = plt.figure(1)",learn1_experiments.py,ianozsvald/social_media_brand_disambiguator,1
"with open('../trainLabels.csv','r') as f:
    for i in range(num_examples):
        line = f.readline()
        words = line.split(',')
        #print i, int(words[1].strip())
        y[i] = l2i[words[1].strip()]
print ""Train labels loaded! \n""

print ""Training Model""
# Train the classifier
clf = RandomForestClassifier(n_jobs=5,n_estimators=100)
clf.fit(train_data_X[1:,:], y)    # Change this!
# del train_data_X
# del y

# save the model
with open('model.pkl','w') as f:
    pickle.dump(clf,f)
print ""Model trained and saved\n""
",rf_train.py,Valay/cifar-10,1
"    
    from sklearn.model_selection import cross_val_score
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import ExtraTreesClassifier
    from sklearn import svm
    from sklearn.ensemble import GradientBoostingClassifier
    import xgboost as xgb
    from sklearn.linear_model import LogisticRegression
    #clf = LogisticRegression()
    #clf = ExtraTreesClassifier(n_estimators=100)
    clf = RandomForestClassifier(n_estimators=150)
    #clf = GradientBoostingClassifier(n_estimators=40,max_depth=5)
    #clf = svm.SVC(kernel='linear', C=1, probability=True)
    #clf = xgb.XGBClassifier(objective=""binary:logistic"",nthread=-1,scale_pos_weight=1,n_estimators=50, max_depth=6)

    scores = cross_val_score(clf, X, y, cv=10, scoring=""neg_log_loss"")
    print scores
    print sum(scores) / float(len(scores))
    scores = cross_val_score(clf, X, y, cv=5, scoring=""recall"")
    print scores",dsb_create_voxel_model_predictions.py,tondonia/data-science-bowl-2017,1
"# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division

import sklearn.ensemble

import submissions
from data import *

rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, n_jobs=2)
rf.fit(train, target)
pred = rf.predict(test)

submissions.save_csv(pred, ""random_forest.csv"")",whats_cooking/random_forest.py,wjfwzzc/Kaggle_Script,1
"    assert set(get_default_target_names(clf)) == {'y'}

    clf = ElasticNet()
    X, y = make_regression(n_targets=2, n_features=3)
    clf.fit(X, y)
    assert set(get_default_target_names(clf)) == {'y0', 'y1'}


@pytest.mark.parametrize(['clf'], [
    [LogisticRegression()],
    [RandomForestClassifier()],
    [GaussianNB()],
    [DecisionTreeClassifier()],
    [OneVsRestClassifier(DecisionTreeClassifier())],
    [OneVsRestClassifier(LogisticRegression())],
    [BernoulliNB()],
])
def test_get_num_features(clf):
    X_bin, y_bin = make_classification(n_features=20, n_classes=2)
    X, y = make_classification(n_features=20, n_informative=4, n_classes=3)",tests/test_sklearn_utils.py,TeamHG-Memex/eli5,1
"        
        # # Initialize hyper-parameter space
        # param_grid = [
        #     {'criterion': ['gini'], 'max_depth': [None, 5, 6, 7, 8, 9, 10], 'n_estimators': [10, 20, 30, 40, 50, 75, 100, 150, 200],
        #      'max_features': [None, int, float, 'auto', 'sqrt', 'log2']},
        #     {'criterion': ['entropy'], 'max_depth': [None, 5, 6, 7, 8, 9, 10], 'n_estimators': [10, 20, 30, 40, 50, 75, 100, 150, 200],
        #      'max_features': [None, int, float, 'auto', 'sqrt', 'log2']}
        # ]

        # # Optimize classifier over hyper-parameter space
        # clf = grid_search.GridSearchCV(estimator=ensemble.RandomForestClassifier(), param_grid=param_grid, scoring='accuracy')
        # self.classifiers[2] = clf

    def predict(self, examples):
        # Extract results from the three chained random forest
        results = self.classifiers[0].predict_proba(examples)
        results = self.classifiers[1].predict_proba(results)",project/HierarchicalRandomForest_keraudren.py,grantathon/computer_vision_machine_learning,1
"    plt.ylim(-1, len(importance))
    plt.xlim([0.0, 0.7])
    plt.show()

# acc,/
def accAndCov(param_name,range_,X_train,y_train,X_test,y_test):
    import sklearn.metrics as metrics
    x=[]
    accs=[]
    covs=[]
    rf = RandomForestClassifier(n_jobs = workers, random_state = choosen_random_state, n_estimators = 196)
    if(len(range_) == 2):start,stop = range_
    else: 
        start,stop,unit = range_
        stop += 1
    
    def record():
        rf.fit(X_train, y_train)
        predict = rf.predict(X_test)
        score = rf.score(X_test,y_test)    # == metrics.accuracy_score(y_test, predict)",dataMining/main.py,hdmy/LagouSpider,1
"#Machine Learning Methods
#-------------------------

#linear regression
#-----------------
model_Reg = linear_model.LinearRegression()


#Random Forest
#-------------
model_RF = RandomForestClassifier(n_estimators=30, max_depth = 10, oob_score=True,  random_state=0)


#Comparison between the linear regression and random forest methods
#-----------------------------------------------------------------


'''
The results of this study support the feasibility of using machine learning 
tools to estimate building parameters as a convenient and acuurate approach",Energy_Efficiency.py,LamaHamadeh/Energy-Efficiency-UCI,1
"    Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_pca1_ridge_only.py,diogo149/CauseEffectPairsPaper,1
"    """"""

    def __init__(self, sentence_embedding, **kwargs):
        IClassificationAlgorithm.__init__(self)
        self.sentence_embedding = sentence_embedding

        # suppress this param, as random forest doesn't use such parameter
        if 'probability' in kwargs:
            del kwargs['probability']

        self.clf = RandomForestClassifier(**kwargs)

    def fit(self, features, labels):
        self.clf.fit(features, labels)

    def predict(self, sentence):
        return int(self.clf.predict([self.sentence_embedding[sentence]])[0])

    def predict_proba(self, sentence):
        return self.clf.predict_proba([self.sentence_embedding[sentence]])[0]",src/models/algorithms/random_forest_algorithm.py,mikolajsacha/tweetsclassification,1
"                            each matrix features[i] of class i is [numOfSamples x numOfDimensions]
        - n_estimators:     number of trees in the forest
    RETURNS:
        - svm:              the trained SVM variable

    NOTE:
        This function trains a linear-kernel SVM for a given C value. For a different kernel, other types of parameters should be provided.
    '''

    [X, Y] = listOfFeatures2Matrix(features)
    rf = sklearn.ensemble.RandomForestClassifier(n_estimators = n_estimators)
    rf.fit(X,Y)

    return rf

def trainGradientBoosting(features, n_estimators):
    '''
    Train a gradient boosting classifier
    Note:     This function is simply a wrapper to the sklearn functionality for SVM training
              See function trainSVM_feature() to use a wrapper on both the feature extraction and the SVM training (and parameter tuning) processes.",audioTrainTest.py,muthu1993/InteliEQ,1
"    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_pca1_gbm_only.py,diogo149/CauseEffectPairsPaper,1
"    def train(self, features_indep_df, feature_target, model_labals, **kwargs):
        """"""
        kwargs:
        n_estimators=20, criterion='gini', max_depth=None, min_samples_split=100,
        min_samples_leaf=50, min_weight_fraction_leaf=0.0, max_features='auto',
        max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=-1, random_state=None,
        verbose=0, warm_start=False, class_weight=""balanced_subsample""
        """"""
        self._logger.debug(__name__)

        model_train = ensemble.RandomForestClassifier(**kwargs)
        model_train.fit(features_indep_df.values, feature_target)
        return model_train

    def train_summaries(self, model_train):
        self._logger.debug(__name__)
        summaries = dict()
        summaries['estimators_'] = model_train.estimators_
        summaries['classes_'] = model_train.classes_
        summaries['n_classes_'] = model_train.n_classes_",Stats/_RandomForestClassifier.py,mesgarpour/T-CARER,1
"    (train_features, train_labels,test_features, test_labels) = ex.extract(input_filename, output_filename)
    classifiers = {
        ""NB Multinomial"" : MultinomialNB(),
        ""NB Gaussian"": GaussianNB(),
        ""Logistic Regression"" : LogisticRegression(C=1e5, tol=0.001, fit_intercept=True),
        ""Decision Tree"" : DecisionTreeClassifier(min_samples_split=1, random_state=0),
        ""KNN"" : KNeighborsClassifier(n_neighbors=3),
        ""SVM"" : SVC(gamma=2, C=1),
        ""LDA"" : LDA(),
        ""QDA"" : QDA(reg_param=0.5),
        ""Random Forest"" : RandomForestClassifier(n_estimators=200),
        ""AdaBoost"" : AdaBoostClassifier(n_estimators=200),
    }
    
    print ""-""*80, ""\n"", ""Raw Dataset"", ""\n"", ""-""*80
    for name, classifier in classifiers.iteritems():
        clf = classifier.fit(train_features,train_labels)
        print name, clf.score(test_features,test_labels)
    
    print ""-""*80, ""\n"", ""Scaled Feature Dataset"", ""\n"", ""-""*80",execute.py,krishnasumanthm/Quora_Answer_Classifier,1
"        # Split to grow cascade and validate
        mask = np.random.random(y.shape[0]) < self.validation_fraction
        X_tr, X_vl = X[mask], X[~mask]
        y_tr, y_vl = y[mask], y[~mask]

        self.classes_ = unique_labels(y)
        self.layers_, inp_tr, inp_vl = [], X_tr, X_vl
        self.scores_ = []

        # First layer
        forests = [RandomForestClassifier(max_features=1, n_estimators=self.n_estimators, min_samples_split=10, criterion='gini', n_jobs=-1),  # Complete random
                    RandomForestClassifier(max_features=1, n_estimators=self.n_estimators, min_samples_split=10, criterion='gini', n_jobs=-1),  # Complete random
                    RandomForestClassifier(n_estimators=self.n_estimators, n_jobs=-1),
                    RandomForestClassifier(n_estimators=self.n_estimators, n_jobs=-1)]
        _ = [f.fit(inp_tr, y_tr) for f in forests]
        p_vl = [f.predict_proba(inp_vl) for f in forests]
        labels = [self.classes_[i] for i in np.argmax(np.array(p_vl).mean(axis=0), axis=1)]
        score = self.scoring(y_vl, labels)
        self.layers_.append(forests)
        self.scores_.append(score)",bleedml/classifiers.py,sig-ml/bleedml,1
"from sklearn.ensemble import ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA, KernelPCA
from sklearn.datasets import make_circles

#Settinf seed
np.random.seed(44)


def fitRF(X,Y):
  clf = RandomForestClassifier(n_estimators=10)
  clf = clf.fit(X, Y)
  return clf


def loadTransformed(filepath):
  return np.load(filepath)

def splitData(data):
  y, x, n = np.hsplit(data,np.array([1, data.shape[1]]))",seizure_prediction/feature_selection.py,tofguerrier/kaggle,1
"from sklearn.ensemble import RandomForestClassifier

#Read training data and split into train and test data
data=pd.read_csv('train.csv')
data1=data.values
X=data1[:,1:]
y=np.ravel(y)
Xtrain,Xtest,ytrain,ytest=cross_validation.train_test_split(X,y,test_size=0.25)

#Run RandomForestClassifier
rf=RandomForestClassifier()
rf.fit(Xtrain,ytrain)
ypred=rf.predict(Xtest) 
metrics.precision_score(ytest,ypred)
",randomforest.py,lingcheng99/kagge-digit-recognition,1
"def random_forest(dataset, DV, max_dep):
	start = time.time()
	# Load Data to Pandas
	data = pd.read_csv(dataset, index_col=0)
	data.columns = [camel_to_snake(col) for col in data.columns]

	#DV
	y = data[str(DV)]
	X = data[data.columns - [str(DV)]]

	clf = RandomForestClassifier(n_jobs=2, max_depth=max_dep)
	model = clf.fit(X, y)

	end = time.time()
	print ""Classifier: Random Forest,"", ""Depth = "", max_dep
	print ""Runtime, base model: %.3f"" % (end-start), ""seconds.""
	return model 

# Unhash to test:
#random_forest('data/cs-training#3B.csv', 'serious_dlqin2yrs', 5)",pipeline/__5_Classifier.py,BridgitD/school-dropout-predictions,1
"# def execute_score(perfMap):
# 	(X_pos_test, X_neg_test) = loadTestData()
# 	(X_train, y_train) = extractTrainingData(perfMap)
# 	le = preprocessing.LabelEncoder()
# 	le.fit(['A', 'C', 'G', 'T'])
# 	X_test = encode(le, X_test)
# 	# y_test = encode(le, y_test)
# 	X_train = encode(le, X_train)
# 	# y_train = encode(le, y_train)

# 	clf = RandomForestClassifier(n_estimators=25)
# 	clf.fit(X_train, y_train)
# 	y_pos = clf.predict_proba(X_pos_test)
# 	y_neg = clf.predict_proba(X_neg_test)
# 	# y_predict = clf.predict(X_test)
# 	# score = log_loss(y_test, clf_probs)
# 	# print(score)
# 	return (y_predict, y_test, score)

def execute(perfMap):",src_python/bio_ga/ga_rf.py,tapomay/libgenetic,1
"        X, y = [], []
        for question in train_questions:
            score_vector = self.get_answer(question)
            answer = np.argmax(score_vector)
            label = int([""A"", ""B"", ""C"", ""D""][answer] == question.correct_answer)
            question_features = solver_utils.get_feature_vector(question)
            feature_vector = score_vector + question_features
            X.append(feature_vector)
            y.append(label)

        clf = RandomForestClassifier(n_estimators=300, n_jobs=-1)
        calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=10)
        calibrated_clf.fit(X, y)
        self.classifier = calibrated_clf

    def load_answer_cache(self, questions, prefix):
        cache_file = 'caches/' + prefix + '_cache.txt'

        # load cached answers from file
        if isfile(cache_file):",Solver.py,sjvasquez/AIChallenge,1
"        params_used = svm_params
    elif clf_used == 'ada_boost':
        params_used = rf_params
    elif clf_used == 'lr':
        params_used = lr_params
    else:
        params_used = rf_params
    if clf_used == 'svm':
        clf = SVC(**params_used)
    elif clf_used == 'ada_boost':
        rf = RandomForestClassifier(**rf_params)
        clf = AdaBoostClassifier(base_estimator=rf, **params_used)
    elif clf_used == 'lr':
        clf = LogisticRegressionCV(**params_used)
    else:
        clf = RandomForestClassifier(**params_used)
    return clf


def load_rfcs(rf_axoness_p, rf_spiness_p):",syconn/processing/learning_rfc.py,StructuralNeurobiologyLab/SyConn,1
"	# pixValid = find_valid_pixels[mData]
	# yTrain = create_labels_pixel(np.ravel(np.reshape(sDataTrain, (sDataTrain.size, 1))), pixValid, pad)

	yTrain = np.ravel(np.reshape(sDataTrain, (sDataTrain.size, 1)))
	yTrain[yTrain != 0] = 1
	yTest = np.ravel(np.reshape(sDataTest, (sDataTest.size, 1)))
	yTest[yTest != 0] = 1


	# train classifier
	clf = RandomForestClassifier(n_estimators = 10)
	clf.fit(xTrain, yTrain)
	yResult = clf.predict(xTest)
	clf_probs = clf.predict_proba(xTest)[:, 1]

	# Compute Percision-Recall and plot curve
	precision, recall, thresholds = precision_recall_curve(yTest, clf_probs)
	average_precision = average_precision_score(yTest, clf_probs)

    # Plot Precision-Recall curve",driver.py,Connectomics-Classes/team-awesome,1
"

class TestReportGeneration(TestCase):
    def setUp(self):
        iris = load_iris()
        X_train, X_test, y_train, y_test = train_test_split(iris.data,
                                                            iris.target,
                                                            test_size=0.30,
                                                            random_state=0)

        model = RandomForestClassifier()
        model.fit(X_train, y_train)

        y_pred = model.predict(X_test)
        y_score = model.predict_proba(X_test)
        target_names = ['setosa', 'versicolor', 'virginica']
        feature_names = range(4)
        model_name = 'a model'

        self.results = ClassifierEvaluator(estimator=model, y_true=y_test,",sklearn_evaluation/tests/test_report.py,edublancas/sklearn-evaluation,1
"        len(test_Y),
        str(sklnr).replace('\n','')[:140])
    print 'output: {}'.format(actual_vs_predict[-10:])

# choose different learners
learner = [
        # naive_bayes.GaussianNB(),
        # linear_model.SGDClassifier(),
        # svm.SVC(),
        # tree.DecisionTreeClassifier(),
        # ensemble.RandomForestClassifier(),
        ensemble.AdaBoostRegressor(),
        ensemble.BaggingRegressor(),
        ensemble.ExtraTreesRegressor(),
        ensemble.GradientBoostingRegressor(),
        ensemble.RandomForestRegressor(),
        gaussian_process.GaussianProcessRegressor(),
        linear_model.HuberRegressor(),
        linear_model.PassiveAggressiveRegressor(),
        linear_model.RANSACRegressor(),",ml/stock_prediction.py,james-jz-zheng/jjzz,1
"from sklearn.ensemble import RandomForestClassifier

from ..Classifier import Classifier
from ...language.C import C


class RandomForestClassifierCTest(C, Classifier, TestCase):

    def setUp(self):
        super(RandomForestClassifierCTest, self).setUp()
        self.mdl = RandomForestClassifier(n_estimators=100, random_state=0)

    def tearDown(self):
        super(RandomForestClassifierCTest, self).tearDown()",tests/classifier/RandomForestClassifier/RandomForestClassifierCTest.py,nok/sklearn-porter,1
"from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, train_test_split

#########################################################
'''

program_table = dict(addreadcsv=['Read csv', source_box_class,
    'data = pd.read_csv(""data.csv"")'],
        addrfclassifier=['RandomForest Classifier', source_box_class,
            'est = RandomForestClassifier(n_jobs=-1)'],
        addscore=['Add Score', de_box_class,
        '''X, Y = data.drop(""target"", axis=1), data.target
p = est.predict(X)
score = roc_auc_score(Y, p)
#score = accuracy_score(Y, p)
#score = f1_score(Y, p)
'''],
        addtrainclassifier=['Train Classifier', de_box_class,
        '''X, Y = data.drop(""target"", axis=1), data.target",dataflow/config.py,theSage21/dataflow,1
"features = list(train.columns)
features.remove('WnvPresent')
features.remove('day')
features.remove('year')
# Random Forest Classifier CV
for year in set(train.year):
    X = train[train.year != year][features]
    V = train[train.year == year][features]
    labelsX = train[train.year != year]['WnvPresent']
    labelsV = train[train.year == year]['WnvPresent']
    rf = ensemble.RandomForestClassifier(n_estimators=5000, min_samples_split=1)
    rf.fit(X, labelsX)
    predV = rf.predict_proba(V)[:, 1]
    fpr_valid, tpr_valid, thresholds = metrics.roc_curve(labelsV, predV, pos_label=1)
    auc_valid = metrics.auc(fpr_valid, tpr_valid)
    print 'rf', year, auc_valid

# create predictions and submission file
All = train[features]
labelsAll = train['WnvPresent']",random_forest.py,gaborfodor/WNVP,1
"from scipy.stats import chi2
from makeMatrix import MatrixFactory
from myDb import ExtendedDB

class Main(object):
    signifikanzNiveau = 0.7

    def __init__(self):
        self.db = ExtendedDB(commitOnClose=False)
        self.X,self.y,self.X_eval,self.ids_eval,self.columns = MatrixFactory().loadFromFile()
        #clf=ensemble.RandomForestClassifier(n_estimators=66)
        #clf=svm.SVC(kernel=""linear"",gamma=0.01, C=250, class_weight='balanced')
        clf=naive_bayes.MultinomialNB(alpha=0.01)
        self.clf = clf
        
    def classify(self):
        print(""classifying"")
        self.clf.fit(self.X,self.y)
        y_pred = self.clf.predict(self.X_eval)
        self.db.query(""DELETE FROM predictions"")",motherless-ai/classify.py,juix/scripts,1
"
log_mod = LogisticRegression()
log_mod.fit(dh.x, dh.y)

knn_mod = KNeighborsClassifier(40)
knn_mod.fit(dh.x, dh.y)

gbc_mod = GradientBoostingClassifier()
gbc_mod.fit(dh.x, dh.y)

rf_mod = RandomForestClassifier(1000)
rf_mod.fit(dh.x, dh.y)

#|---Predict using Test data---
dh.partition(ratio=1.0)

log_prob = log_mod.predict_proba(dh.x)

knn_prob = knn_mod.predict_proba(dh.x)
",depr/0.2.5/examples/telstra_demos.py,rosspalmer/DataTools,1
"        tt.append(i % 15)
    Tfeature['is_train'] = tt
    rightAll = 0
    for i in range(15):
        print i
        train, test = Tfeature[Tfeature['is_train'] != i], Tfeature[Tfeature['is_train'] == i]
        tmp1 = np.array([t != i for t in Tfeature['is_train']])
        tmp2 = np.array([t == i for t in Tfeature['is_train']])
        trainTar, testTar = target[tmp1], target[tmp2]
        testId = idList[tmp2]
        clf = RandomForestClassifier(n_estimators=200, min_samples_split=13)  # ,max_depth=35,max_features=0.4)
        features = Tfeature.columns[:-1]
        clf.fit(train[features], trainTar)
        preds = clf.predict(test[features])
        right = 0
        for i in range(len(preds)):
            if preds[i] == testTar[i]:
                right += 1.0
                rightAll += 1.0
            outFiles.write(",position_predict/merge_data/Mposi_merge.py,yinzhao0312/Position-predict,1
"        with open(json_path) as data_file:
            best_estimator = json.load(data_file)
        return best_estimator

    def build_best_estimator(self,json_path):
        if self.model_type == 'Decision_Tree':
            reg = tree.DecisionTreeClassifier(random_state=1234)
        elif self.model_type == 'Random_Forest':
            best_estimator = self.get_best_estimator(json_path)
            params = best_estimator
            reg = ensemble.RandomForestClassifier(**params)
        else:
            # TODO raise exception
            return 0
        return reg.fit(self.x_train,self.y_train)

    def eval(self):
        reg = self.build_best_estimator(output_dir + ""/"" + self.model_type + ""_best_parameters.json"")
        print (output_dir + ""/"" + self.model_type + ""_best_parameters.json"")
        y_true, y_pred = self.y_test, reg.predict(self.x_test)",love_matcher/refactored/evaluation/evaluation.py,xebia-france/luigi-airflow,1
"from toxcast_dataset import load_toxcast
import deepchem as dc

toxcast_tasks, toxcast_datasets, transformers = load_toxcast(
    base_data_dir, reload=reload)
(train_dataset, valid_dataset, test_dataset) = toxcast_datasets

classification_metric = Metric(metrics.roc_auc_score, np.mean)

def model_builder(model_dir):
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=500, n_jobs=-1)
  return dc.models.SklearnModel(sklearn_model, model_dir)

model = SingletaskToMultitask(toxcast_tasks, model_builder)

# Fit trained model
model.fit(train_dataset)
model.save()
",examples/toxcast/toxcast_rf.py,lilleswing/deepchem,1
"
# drop NaNs
traindf = traindf.dropna()
validationdf = validationdf.dropna()
X_train = traindf[traindf.columns[0:-1]].values
Y_train = traindf[traindf.columns[-1]].values
X_test = validationdf[validationdf.columns[0:-1]].values
Y_test = validationdf[validationdf.columns[-1]].values

# train a random forest
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_train, Y_train)
scores = cross_val_score(clf, X_train, Y_train, cv=5)
Y_test_RFC = clf.predict(X_test)

print(""Results from cross-validation on training set:"")
print(scores, scores.mean(), scores.std())

testscore = accuracy_score(Y_test, Y_test_RFC)
",Code/learnposition.py,sbussmann/sensor-fusion,1
"                  X, y_class, sample_weight=np.asarray([-1]))


def test_base_estimator():
    # Test different base estimators.
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)",scikit-learn-0.18.1/sklearn/ensemble/tests/test_weight_boosting.py,RPGOne/Skynet,1
"
    counter = 0
    for review in clean_test_reviews:
        test_centroids[counter] = create_bag_of_centroids( review, \
            word_centroid_map )
        counter += 1


    # ****** Fit a random forest and extract predictions
    #
    forest = RandomForestClassifier(n_estimators = 100)

    # Fitting the forest may take a few minutes
    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit(train_centroids,train[""sentiment""])
    result = forest.predict(test_centroids)

    # Write the test results
    output = pd.DataFrame(data={""id"":test[""id""], ""sentiment"":result})
    output.to_csv(""BagOfCentroids.csv"", index=False, quoting=3)",code/tba/DeepLearningMovies/Word2Vec_BagOfCentroids.py,computational-class/cjc,1
"                n_estimators = 500

            if max_features < 1:
                max_features = '\'auto\''
            elif max_features == 1:
                max_features = 'None'
            else:
                max_features = 'min({MAX_FEATURES}, len({INPUT_DF}.columns) - 1)'.format(MAX_FEATURES=max_features, INPUT_DF=operator[2])

            operator_text += '\n# Perform classification with a random forest classifier'
            operator_text += ('\nrfc{OPERATOR_NUM} = RandomForestClassifier('
                              'n_estimators={N_ESTIMATORS}, max_features={MAX_FEATURES})\n').format(OPERATOR_NUM=operator_num,
                                                                                                    N_ESTIMATORS=n_estimators,
                                                                                                    MAX_FEATURES=max_features)
            operator_text += ('''rfc{OPERATOR_NUM}.fit({INPUT_DF}.loc[training_indices].drop('class', axis=1).values, '''
                              '''{INPUT_DF}.loc[training_indices, 'class'].values)\n''').format(OPERATOR_NUM=operator_num,
                                                                                                INPUT_DF=operator[2])
            if result_name != operator[2]:
                operator_text += '{OUTPUT_DF} = {INPUT_DF}.copy()\n'.format(OUTPUT_DF=result_name, INPUT_DF=operator[2])
            operator_text += ('''{OUTPUT_DF}['rfc{OPERATOR_NUM}-classification'] = '''",tpot/export_utils.py,pronojitsaha/tpot,1
"loadedData = c.loadFile('datasets/adult.data')[:1000]

#all but the last column are features
features = [l[0:-1] for l in loadedData]

#last column is a label, turn into a float
labels = [1.0*(l[-1]==' <=50K') for l in loadedData]

#run the experiment, results are stored in uscensus.log
#features, label, sklearn model, name
e = Experiment(features, labels, RandomForestClassifier(), ""uscensus"")
e.runAllAccuracy()

",exampleExperiment.py,sjyk/activedetect,1
"

# *********************************
# Choose the model
# *********************************

# Linear Classifier
#clf = SGDClassifier(loss=""modified_huber"")

# Random forest
clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)


# *********************************
#  Evaluate Model
# *********************************

X_train, X_test, y_train, y_test = train_test_split(train_features[feature_list], train_labels, test_size=0.1, random_state=0)

clf.fit(X_train, np.ravel(y_train))",examples/remigius-thoemel/titanic-iteration-0.py,remigius42/code_camp_2017_machine_learning,1
"    predictions = model.predict(testSet)
    ids = test[0][:, 0]
    ids = ids.astype(int)
    submission = pd.DataFrame({""id"": ids, ""prediction"": predictions})
    return submission


def trainRF(train, test, KfoldDataSet, features=None):
    if features is None:
        features = range(KfoldDataSet[0][0])
    model = RandomForestClassifier(n_estimators=300, n_jobs=-1, min_samples_split=10, random_state=1,
                                   class_weight='auto')
    RFcrossValidationTest = None
    if toTestModel:
        RFcrossValidationTest = KappaOnCrossValidation(model, KfoldDataSet, features)
    rf_final_predictions = ActivateModelAndformatOutput(model, train, test, features)
    return RFcrossValidationTest, rf_final_predictions


def trainSVC(train, test, KfoldDataSet, features=None):",modelCreation.py,Ilya-Simkin/NLP-crowdflower-assignment,1
"        print(""%0.6f %-6s %s"" % (weight, label, attr))    

print(""\nTop positive:"")
print_state_features(Counter(info.state_features).most_common(20))

print(""\nTop negative:"")
print_state_features(Counter(info.state_features).most_common()[-20:])

print ""\nRandomForest\n""
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 100)
rf = rf.fit(x_train, y_train)

y_pred = [rf.predict(xseq) for xseq in x_test]
print(my_classification_report(y_test, y_pred))

print ""\nDecisionTree\n""
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1,random_state=0)
clf = clf.fit(x_train, y_train)",scripts/crf_model.py,neuroelectro/neuroelectro_org,1
"from sklearn.base import BaseEstimator
from xgboost import XGBClassifier


class stacking(BaseEstimator):
    def __init__(self, models: list=None, n_folds: int=3):
        if models is None:
            self.models = [
                #GradientBoostingClassifier(),
                XGBClassifier(),
                RandomForestClassifier(),
                ExtraTreeClassifier(),
                LogisticRegression(),
                #FMClassification(),
                KNeighborsClassifier(),
                LinearSVC(),
                RidgeClassifier(),
                MLPClassifier()]
        else:
            self.models = models",bards/ensembling.py,dbftdiyoeywga/bards,1
"    return X, Y




def train_rf_for_outliers(train_data, Y):

    mean = Y.mean()
    std = Y.std()

    rf = RandomForestClassifier(n_estimators=1000, bootstrap=False)

    res = np.zeros((train_data.shape[0],1))
    for i in range(train_data.shape[0]):
        v = Y[i]
        if np.sqrt((v - mean) ** 2) > (std * 1.):
            res[i] = 1.

    rf.fit(train_data, res)
",train.py,baxton/RRP,1
"                        (images.shape[0],
                         images.shape[1]*images.shape[2]*images.shape[3]))
    labels = np.reshape(labels,
                        (labels.shape[0],
                         labels.shape[1]*labels.shape[2]*labels.shape[3]))

    X_train, X_test, y_train, y_test = train_test_split(
        images, labels, test_size=0.20, random_state=42)


    clf = RandomForestClassifier(verbose=3,
                                 n_jobs=-1)
    print(""Fitting..."")
    clf.fit(X_train, y_train)

    IPython.embed()


    # IT seems like the labels are wrong",miscpy/simpleClassifier.py,MarcoDalFarra/semseg,1
"
from sklearn import cross_validation
from sklearn.ensemble import RandomForestClassifier

predictors = [""Pclass"", ""Sex"", ""Age"", ""SibSp"", ""Parch"", ""Fare"", ""Embarked""]

# Initialize our algorithm with the default paramters
# n_estimators is the number of trees we want to make
# min_samples_split is the minimum number of rows we need to make a split
# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)
alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)
# Compute the accuracy score for all the cross validation folds.  (much simpler than what we did before!)
kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)
scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[""Survived""], cv=kf)

# Take the mean of the scores (because we have one for each fold)
print(scores.mean())

## 4. Parameter tuning ##
",Kaggle Competitions/Improving your submission-74.py,vipmunot/Data-Analysis-using-Python,1
"from __future__ import print_function, absolute_import, nested_scopes, generators, division, with_statement, unicode_literals
from hytra.core.probabilitygenerator import RandomForestClassifier

def test_rf():
    rf = RandomForestClassifier('/CountClassification', 'tests/mergerResolvingTestDataset/tracking.ilp')
    assert(len(rf._randomForests) == 1)
    assert(len(rf.selectedFeatures) == 4)",tests/core/test_random_forest.py,chaubold/hytra,1
"    print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
    #clf.fit(df[features], df[target])
    #print(clf.feature_importances_)
    #print(features)
    
def test_cross_validations(df):
    features = df.columns[2:]
    classifiers = [
        {'label': 'Decision Tree', 'algorithm': tree.DecisionTreeClassifier()},
        {'label': 'Gradient Boost', 'algorithm': GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)},
        {'label': 'Random Forest', 'algorithm': RandomForestClassifier(n_jobs=-1)},
        {'label': 'Gaussian Naive Bayes', 'algorithm': GaussianNB()},
        {'label': 'AdaBoost', 'algorithm': AdaBoostClassifier(n_estimators=100)},
        {'label': 'Linear SVC', 'algorithm': LinearSVC()}
    ]
    
    for classifier in classifiers: 
        print('\n' + classifier['label'])
        cross_validate(classifier['algorithm'], df, features, 'Survived')        
        print('\n' + classifier['label'] + ' Pipelined')",predict.py,orangepips/kaggle-titantic,1
"
            if len(relevant_cases) == 0:
                continue
            elif len(relevant_cases) < n_min_cases_in_state:
                dt_train_cluster = train_prefixes[train_prefixes[case_id_col].isin(relevant_cases)]
                train_y = [1 if label==pos_label else 0 for label in dt_train_cluster.groupby(case_id_col).first()[label_col]]
                hardcoded_predictions[cl] = np.mean(train_y)
            elif len(train_prefixes[train_prefixes[case_id_col].isin(relevant_cases)][label_col].unique()) < 2:
                hardcoded_predictions[cl] = 1 if str(train_prefixes[train_prefixes[case_id_col].isin(relevant_cases)][[label_col]].iloc[0]) == pos_label else 0
            else:
                cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=rf_max_features, random_state=random_state)
                feature_combiner = FeatureUnion([(method, init_encoder(method)) for method in methods])
                pipelines[cl] = Pipeline([('encoder', feature_combiner), ('cls', cls)])

                # fit pipeline
                dt_train_cluster = train_prefixes[train_prefixes[case_id_col].isin(relevant_cases)].sort_values(timestamp_col, ascending=True)
                train_y = dt_train_cluster.groupby(case_id_col).first()[label_col]

                start = time()
                pipelines[cl].fit(dt_train_cluster, train_y)",experiments_final/run_all_state.py,irhete/predictive-monitoring-benchmark,1
"

# In[41]:

data.fillna(0, inplace=True)
count_nonzero(pandas.isnull(data.ix[train_idx,feats1]))


# In[42]:

rf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1)
rf1.fit(data.ix[train_idx,feats1], data['tipped'].ix[train_idx])


# In[43]:

preds1 = rf1.predict_proba(data.ix[test_idx,feats1])


# In[44]:",examples/realWorldMachineLearning/Chapter+6+-+NYC+Taxi+Full+Example.py,remigius42/code_camp_2017_machine_learning,1
"                ]

def main():
    print(""Reading the data"")
    data = cu.get_dataframe(train_file)

    print(""Extracting features"")
    fea = features.extract_features(feature_names, data)

    print(""Training the model"")
    rf = RandomForestClassifier(n_estimators=50, verbose=2, compute_importances=True, n_jobs=-1)
    rf.fit(fea, data[""OpenStatus""])

    print(""Reading test file and making predictions"")
    data = cu.get_dataframe(test_file)
    test_features = features.extract_features(feature_names, data)
    probs = rf.predict_proba(test_features)

    print(""Calculating priors and updating posteriors"")
    new_priors = cu.get_priors(full_train_file)",basic_benchmark.py,mattalcock/stack-kaggle,1
"X = iris.data
y = iris.target
#X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)

num_estimators = 30

clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)
scores = cross_val_score(clf, X, y)
print(""DecisionTreeClassifier ="", scores.mean())

clf = RandomForestClassifier(n_estimators=num_estimators, max_depth=None, min_samples_split=2, random_state=0)
scores = cross_val_score(clf, X, y)
print(""RandomForestClassifier ="", scores.mean())

clf = ExtraTreesClassifier(n_estimators=num_estimators, max_depth=None, min_samples_split=2, random_state=0)
scores = cross_val_score(clf, X, y)
print(""ExtraTreesClassifier ="", scores.mean())

clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=num_estimators)
scores = cross_val_score(clf, X, y)",sw_dev/python/rnd/test/machine_learning/scikit_learn/sklearn_ensemble_learning.py,sangwook236/SWDT,1
"from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

from skml.problem_transformation import LabelPowerset
from skml.datasets import load_dataset

X, y = load_dataset('yeast')
clf = LabelPowerset(RandomForestClassifier())
clf.fit(X, np.array(y))
y_pred = clf.predict(X)

print ""hamming loss: ""
print hamming_loss(y, y_pred)

print ""accuracy:""
print accuracy_score(y, y_pred)
",doc/auto_examples/example_lp.py,ChristianSch/skml,1
"def random_forest(dataset, DV, max_dep):
	start = time.time()
	# Load Data to Pandas
	data = pd.read_csv(dataset, index_col=0)
	data.columns = [camel_to_snake(col) for col in data.columns]

	#DV
	y = data[str(DV)]
	X = data[data.columns - [str(DV)]]

	clf = RandomForestClassifier(n_jobs=2, max_depth=max_dep)
	model = clf.fit(X, y)

	end = time.time()
	print ""Classifier: Random Forest,"", ""Depth = "", max_dep
	print ""Runtime, base model: %.3f"" % (end-start), ""seconds.""
	return model 

# Unhash to test:
#random_forest('data/cs-training#3B.csv', 'serious_dlqin2yrs', 5)",pipeline/__5_Classifier.py,jmausolf/HIV_Status,1
"			Y += [2]*20
		else:
			Y += [3]*20

		# Create training data
		for row in coords:
			X.append(create_features(p, row[0], row[1], box_size))

	# ========== Train model =========
	print(""Training Random Forest"")
	clf = RandomForestClassifier(n_estimators=500)
	# clf = SVC()
	clf = clf.fit(X, Y)

	# ========== Test model =========
	print(""Predicting"")

	# Initilize dict for prediction results
	preds = {'white':np.empty((1,2)),
		   	   'brown':np.empty((1,2)),",slum_kibera/ml_playground.py,Bubblbu/ssip_2016_team_d,1
"    Transformer to select best features
    using RF
  """"""
  def __init__(self, clf = None, k = 10, n_estimators=100, n_jobs=1):
    self.clf = clf
    self.k = k
    self.n_estimators = n_estimators
    self.n_jobs = n_jobs
  
  def fit(self, X, y):
    clf = RandomForestClassifier(n_estimators=self.n_estimators, max_depth=self.k,
                n_jobs=self.n_jobs, random_state=random_state, verbose=0)
    #clf = DecisionTreeClassifier(criterion='entropy', max_depth=self.k, max_features=1.0, min_density=None, min_samples_leaf=1, min_samples_split=2, random_state=random_state, splitter='best')
    self.clf = clf.fit(X,np.ravel(y))
    feature_importances = clf.feature_importances_
    fi = sorted(zip(feature_importances,range(len(feature_importances))),reverse=True)
    self.features_selected = [e[1] for e in fi[:self.k]]
    #print ""sorted features:"",fi[:10],""selected:"",self.features_selected
    return self
",kgml/feasel.py,orazaro/kgml,1
"            The features that the model will be trained on
        language : :class:`~revscoring.languages.language.Language`
            The language context applied when extracting features.
        version : str
            A version string representing the version of the model
        `**kwargs`
            Passed to :class:`sklearn.ensemble.RandomForestClassifier`
    """"""
    def __init__(self, features, *, language=None, version=None, rf=None, **kwargs):

        if rf is None: rf = RandomForestClassifier(**kwargs)

        super().__init__(features, classifier_model=rf, language=language,
                         version=version)
RFModel = RF
""Alias for backwards compatibility""",revscoring/scorer_models/rf.py,aetilley/revscoring,1
"
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.cross_validation import StratifiedShuffleSplit

from revenue import RevenueCompetition, RevenueTransform
from xgboost import XGBClassifier

if __name__ == '__main__':
        
    train_size = 0.75
    cls = RandomForestClassifier()
    reg = RandomForestRegressor(n_estimators=20, max_features=5, max_depth=None,
                                 min_samples_split=2, min_samples_leaf=1,
                                 max_leaf_nodes=None, bootstrap=True,
                                 oob_score=False, n_jobs=-1)
    train_df_orig = RevenueCompetition.load_data()
    y = train_df_orig['revenue'].values
    del train_df_orig['revenue']

    test_df_orig = RevenueCompetition.load_data(train=False)",revenue/rf.py,ldamewood/kaggle,1
"	Examples
	--------
	>>> from sklearn.datasets import load_iris
	>>> from sklearn.ensemble import RandomForestClassifier
	>>> from nonconformist.icp import IcpClassifier
	>>> from nonconformist.nc import ClassifierNc, MarginErrFunc
	>>> from nonconformist.evaluation import ClassIcpCvHelper
	>>> from nonconformist.evaluation import class_mean_errors
	>>> from nonconformist.evaluation import cross_val_score
	>>> data = load_iris()
	>>> nc = ProbEstClassifierNc(RandomForestClassifier(), MarginErrFunc())
	>>> icp = IcpClassifier(nc)
	>>> icp_cv = ClassIcpCvHelper(icp)
	>>> cross_val_score(icp_cv,
	...                 data.data,
	...                 data.target,
	...                 iterations=2,
	...                 folds=2,
	...                 scoring_funcs=[class_mean_errors],
	...                 significance_levels=[0.1])",nonconformist/evaluation.py,donlnz/nonconformist,1
"fixed_text = text[pd.notnull(text)]
fixed_target = target[pd.notnull(text)]

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
count_vect.fit(fixed_text)

counts = count_vect.transform(fixed_text)

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

from sklearn.model_selection import cross_val_score

scores = cross_val_score(clf, counts, fixed_target, cv=10)
print(scores)
print(scores.mean())",test-algorithm-cross-validation-rf.py,lukas/ml-class,1
"target[np.isnan(target)] = 0
target = target[w]

# =============================================
# Run Random Forest
# =============================================

# Create and train Random Forest
# For Multi-core CPUs, use n_jobs argument
njobs = max(1, psutil.cpu_count()-2)
rf = RandomForestClassifier(n_estimators=500, n_jobs=njobs)
print (""Run Random Forest on "" + str(njobs) + "" CPU"")
rf.fit(train, target)

# Variable importance
importances = rf.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]
",deforestprob/rfmodel.py,ghislainv/deforestprob,1
"    training_label = [arr for idx_arr, arr in enumerate(label)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = label_binarize(np.hstack(training_label).astype(int),
                                    [0, 255])
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    pred_prob = crf.fit(training_data, np.ravel(training_label)).predict_proba(
        testing_data)

    result_cv.append([pred_prob, crf.classes_])

# Save the information
path_store = '/data/prostate/results/mp-mri-prostate/exp-1/mrsi-spectra'
if not os.path.exists(path_store):
    os.makedirs(path_store)",pipeline/feature-classification/exp-1/mrsi/pipeline_classifier_mrsi_spectra.py,I2Cvb/mp-mri-prostate,1
"
  print('- Unique titles: %s' % df.Title.unique())
  for title in df.Title.unique():
    counts = len(df[df.Title == title])
    print('title %d: %d times, rate: %f' % (title, counts, len(df[(df.Title == title) & (df.Survived == 1)]) / counts))

  print('... done printing statistics!')
  print(SEPARATOR)

def run_prediction(train, test):
  forest = RandomForestClassifier(n_estimators=100)
  forest = forest.fit(train[0::,1::], train[0::,0] )

  return forest.predict(test).astype(int)

def write_predictions(ids, predictions):
  with open('prediction.csv', 'wt') as predictions_file:
    open_file_object = csv.writer(predictions_file)
    open_file_object.writerow(['PassengerId','Survived'])
    open_file_object.writerows(zip(ids, predictions))",titanic/submissions/5/randomforest.py,furgerf/kaggle-projects,1
"                    if downFactor > 1.0:
                        tmpData = downSample(tmpData,downFactor)
                featureSet = convertToFeatureSeries(tmpData,featureFunctions,isTest=True,testFile=phil)
                entries.append(featureSet)
    testSample = pd.concat(entries,ignore_index=True)
    return testSample

def trainRandomForest(trainDF):
    #trains a random forest on the training sample and returns the trained forest
    trainArray = trainDF.values
    forest = RandomForestClassifier(n_estimators=1000)
    return forest.fit(trainArray[:,0:-3],trainArray[:,-2:])

def validateRandomForest(forest,validDF,latencyBinWidth=-1):
    #prints efficiency and false positive metrics and plots efficiency vs. latency for a given forest using the validation sample
    output = forest.predict(validDF.values[:,0:-3])
    validDF['PiS'] = output[:,0].astype(int)
    validDF['PiE'] = output[:,1].astype(int)
    for key,group in validDF.groupby('isSeizure'):
        if key:",utils.py,asood314/SeizureDetection,1
"					best_algo = 'Linear' 
			else:
				X = np.array(stocksDf.drop(['Decision'],1))
				X = preprocessing.scale(X)
				y = np.array(stocksDf['Decision']) # y is the 1% forcast 
				# y = y[:predict_index-2] # to keep consistent

				# to convert into numbers
				# y = le_decision.fit_transform(y)
				X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) # 20% training data, 80% testing 
				# clf = RandomForestClassifier(min_samples_leaf=2, n_estimators=100)

				# best is {'max_depth': 8, 'min_child_weight': 2}

				# clf = xgboost.XGBClassifier(
				# 							learning_rate =0.1,
				# 							n_estimators=1000,
				# 							max_depth=8,
				# 							min_child_weight=2,
				# 							gamma=0,",predictStocks.py,carriercomm/wallstreetbot,1
"        test_features = data[""test_features""]
        test_groundtruths = data[""test_groundtruths""]
    else:
        train = utils.abs_path_file(train)
        test = utils.abs_path_file(test)
        train_features, train_groundtruths = read_file(train)
        test_features, test_groundtruths = read_file(test)
    if not utils.create_dir(res_dir):
        res_dir = utils.abs_path_dir(res_dir)
    classifiers = {
        ""RandomForest"": RandomForestClassifier(n_jobs=-1)
        # ""RandomForest"": RandomForestClassifier(n_estimators=5),
        # ""KNeighbors"":KNeighborsClassifier(3),
        # ""GaussianProcess"":GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
        # ""DecisionTree"":DecisionTreeClassifier(max_depth=5),
        # ""MLP"":MLPClassifier(),
        # ""AdaBoost"":AdaBoostClassifier(),
        # ""GaussianNB"":GaussianNB(),
        # ""QDA"":QuadraticDiscriminantAnalysis(),
        # ""SVM"":SVC(kernel=""linear"", C=0.025),",src/identify_singing_voice_gender.py,ybayle/ISMIR2017,1
"    test_counts_tf = lsvm.transform(test_counts_tf)
    #ch2 = SelectKBest(chi2, k=1100)
    #train_counts_tf = ch2.fit_transform(train_counts_tf, target_vals)
    #test_counts_tf = ch2.transform(test_counts_tf)
    print train_counts_tf.shape
    # DONE PICKING FEATURES
    ##############################

    clfs = [(DecisionTreeClassifier(), \
            ""Decision Tree Classifier""),\
            (RandomForestClassifier(n_estimators=10),\
            ""Random Forest Classifier""),\
            (ExtraTreesClassifier(n_estimators=10, max_depth=None,\
                    min_samples_split=1, random_state=0),\
            ""Extra Random Trees Classifier""),\
            (Perceptron(n_iter=50),\
            ""Perceptron""),\
            (SGDClassifier(alpha=.0001, n_iter=50, penalty=""elasticnet""),\
            ""Stochastic Gradient Descent""),\
            (GradientBoostingClassifier(n_estimators=100,\",okstereotype/pick_best_model_v1.py,lqdc/okstereotype,1
"  # set_trace()
  return _Abcd(before=actual, after=preds, show=False)[-1]


def rforest(train, test, tunings=None, smoteit=True):
  ""    RF""
  # Apply random forest Classifier to predict the number of bugs.
  if smoteit:
    train = SMOTE(train)
  if not tunings:
    clf = RandomForestClassifier(random_state=1)
  else:
    clf = RandomForestClassifier(n_estimators=int(tunings[0]),
                                 max_features=tunings[1] / 100,
                                 min_samples_leaf=int(tunings[2]),
                                 min_samples_split=int(tunings[3]),
                                 max_leaf_nodes=int(tunings[4]),random_state=1)
  train_DF = formatData(train)
  test_DF = formatData(test)
  features = train_DF.columns[:-2]",old/SOURCE/Prediction.py,ai-se/Transfer-Learning,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.ml.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = spark.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> model.featureImportances
    SparseVector(1, {0: 1.0})
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0",python/pyspark/ml/classification.py,apache/spark,1
"from builder import CPUBuilder, GPUBuilder

#kill the child process if any
def cleanup(proc):
  if proc.is_alive():
    proc.terminate()

class RandomForestClassifier(object):
  """"""
  This RandomForestClassifier uses both CudaTree and cpu 
  implementation of RandomForestClassifier(default is sklearn) 
  to construct random forest. The reason is that CudaTree only 
  use one CPU core, the main computation is done at GPU side, 
  so in order to get maximum utilization of the system, we can 
  trai one CudaTree random forest with GPU and one core of CPU,
  and simultaneously we construct some trees on other cores by 
  other multicore implementaion of random forest.
  """"""
  def __init__(self, 
              n_estimators = 10, ",hybridforest/hybridforest.py,EasonLiao/CudaTree,1
"#X = df[['Pclass', 'Age', 'Gender']]
X = df[['Pclass', 'Age','Fare', 'Gender', 'EmbarkPort', 'FamilySize']]
y = df['Survived']

X2 = test[['Pclass', 'Age','Fare', 'Gender', 'EmbarkPort', 'FamilySize']]

features = X.values
target = y.values

# Initialize the model
model = RandomForestClassifier(150, random_state=0)


#  Split the features and the target into a Train and a Test subsets.  
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                            test_size = 0.2, random_state=0)

# Train the model
model.fit(X_train, y_train)
",code/kaggle-titanic.py,heyengel/kaggle-titanic,1
"        xnums = np.array(range(top_10.shape[0]))
        plt.bar(xnums, importances[top_10])
        plt.xticks(xnums + 0.5, map(lambda x: '\n'.join(x.split('_')), train_dummies.columns[1:][top_10]), rotation='vertical')
        plt.tight_layout()
        plt.show()

        C = confusion_matrix(y_val, dt.predict(X_val))
        show_confusion_matrix(C, ['Denied', 'Approved!'])

        # random forest
        rf = RandomForestClassifier(random_state=42)
        params = {
                    'n_estimators': [50, 100, 200, 300],
                    'max_features': [3, 5, 10, 'sqrt'],
                    'max_depth': [3, 6, 9],
                    'min_samples_split': [2, 4, 6, 8],
                    'class_weight': [None, 'balanced'],
                    'random_state': [42]
        }
        # this takes a while...",P4-crediworthiness/clean_data_run_models.py,nateGeorge/udacity_pred_analytics_4_biz,1
"        return True
    except:
        return False

@unittest.skipUnless(can_import_scikit(), 'no scikit learn, skipping test')
class SciKitLearnTest(unittest.TestCase):
    def test_model_training(self):
        from sklearn.ensemble import RandomForestClassifier
        X = [[0,0],[1,1],[0,0],[1,1],[0,0],[1,1],[0,0],[1,1],[0,0],[1,1]]
        y = [0,1,0,1,0,1,0,1,0,1]
        model = RandomForestClassifier()
        list(zip(X,y) | TrainScikitModel(model, batch_size=2))",chevrons/tests/tests.py,lmc2179/chevrons,1
"                          ""AdditiveChi2Sampler"",
                          ""Nystroem"",
                          ""RBFSampler"",
                          ""SkewedChi2Sampler""]
                models += [neighbors.KNeighborsClassifier(),
                           svm.LinearSVC(max_iter=10),
                           ensemble.AdaBoostClassifier(),
                           ensemble.BaggingClassifier(),
                           ensemble.ExtraTreesClassifier(),
                           ensemble.GradientBoostingClassifier(),
                           ensemble.RandomForestClassifier(),
                           linear_model.SGDClassifier(),
                           kernel_approximation.AdditiveChi2Sampler(),
                           kernel_approximation.Nystroem(),
                           kernel_approximation.RBFSampler(),
                           kernel_approximation.SkewedChi2Sampler()]
        elif is_number_categories_known:
            # clustering
            names += [""KMeans"",
                      ""MiniBatchKMeans"",",models.py,llautert/psychoPYTHON,1
"    training_X = scaler.fit_transform(training_X)
    testing_X = scaler.transform(testing_X)
    clf = svm.SVC(kernel='linear', degree=3, cache_size=1000)
  #Naive Bayes
  if name == ""MultinomialNB"":
    clf = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
  #Ensemble Methods
  if name == ""adaboost"" :
    clf = AdaBoostClassifier(n_estimators=100)
  if name == ""random_forest"" :
    clf = RandomForestClassifier(n_estimators=100)
  if name == ""decision_tree"" :
    clf = tree.DecisionTreeClassifier()

  #use cross validation and grid search
  if use_CV :
    print 'Using Cross Validation'
    pprint.pprint( np.mean(cross_validation.cross_val_score(clf, training_X, training_Y, cv=cv_folds)) )

    if not name == ""svm"" and not name ==""nearest_neighbor"" and not name == ""MultinomialNB"":",classification.py,dnr2/fml-twitter,1
"                        '%(message)s'), level=logging.INFO)
    log.info(""Starting FACT example"")

    data_df = pd.read_hdf(test_filename1)
    mc_df = pd.read_hdf(test_filename2)

    log.info(""Reducing Features"")
    data_df = data_df.loc[:10000, training_variables]
    mc_df = mc_df.loc[:, training_variables]

    clf = RandomForestClassifier(n_jobs=4, n_estimators=20)

    log.info(""Data preparation"")
    X, y, sample_weight, X_names = disteval.prepare_data(mc_df[:10000],
                                                         data_df,
                                                         test_weight=None,
                                                         ref_weight=None,
                                                         test_ref_ratio=1.,
                                                         )
    del data_df",examples/fact_example.py,tudo-astroparticlephysics/pydisteval,1
"from sklearn.ensemble import RandomForestClassifier

def random_forest_model(ffs = None, featurefile=""forest_features"", outputfile = ""forest_predictions.csv"", train_dir = ""train"", test_dir = ""test""):
    # do a quick load of feature data 
    X_train, global_feat_dict, t_train, train_ids = save_and_load(featurefile, train_dir, ffs)

    pdFrame = toPandasDataFrame(X_train, global_feat_dict, t_train)
    
    # generate random forest model
    model = RandomForestClassifier(n_estimators = 5, compute_importances = True)

    # build a forest of trees from training set (X,y) where X = feature set, Y = target values
    y = pdFrame['class']
    pdFrame.drop('class', axis=1, inplace=True)
    model.fit(pdFrame, y)

    # extract features from test data
    print ""extracting test features...""
    X_test,_, t_ignore, test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)",prac2/aidi_random_forests.py,kandluis/machine-learning,1
"    training_label = [arr for idx_arr, arr in enumerate(label)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = label_binarize(np.hstack(training_label).astype(int),
                                    [0, 255])
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, max_features=None,
                                 n_jobs=-1)
    pred_prob = crf.fit(training_data, np.ravel(training_label)).predict_proba(
        testing_data)

    result_cv.append([pred_prob, crf.classes_])

# Save the information
path_store = '/data/prostate/results/mp-mri-prostate/exp-1/mrsi-citrate-choline-no-fit'
if not os.path.exists(path_store):",pipeline/feature-classification/exp-1/mrsi/pipeline_classifier_mrsi_citrate_choline_no_fit.py,I2Cvb/mp-mri-prostate,1
"        return [[0, 0], [0, 0]]

    print l_num, m_num
    df = pd.DataFrame(matrix, columns=head)
    df = df.reindex(np.random.permutation(df.index))
    X = df.as_matrix(head[:19])
    y = np.array(df[""class""].tolist())

    # import sklearn.ensemble
    # X = df.as_matrix(head[1:20])
    # clf = sklearn.ensemble.RandomForestClassifier(n_estimators=20)
    # scores = sklearn.cross_validation.cross_val_score(clf, X, y, cv=10, n_jobs=-1)
    # print scores
    # rdf = pd.DataFrame(scores, columns=[""Precision""])
    # rdf.plot()
    # X = df.as_matrix(head[1:20])

    import sklearn.ensemble
    clf = sklearn.ensemble.RandomForestClassifier(n_estimators=20)
    from sklearn.cross_validation import train_test_split",src/classify_domain.py,whodewho/FluxEnder,1
"
# standardize features: removing the mean and scaling to unit variance
sc = StandardScaler()
sc.fit(X_train) # compute the mean and std to be used for later scaling
X_train_std = sc.transform(X_train) # fit to data, then transform it
X_test_std = sc.transform(X_test)

# define forest
# n_estimators = decision trees
# n_jobs = parallel jobs 
forest = RandomForestClassifier(criterion='entropy',
                                n_estimators=10,
                                random_state=1,
                                n_jobs=4)
forest.fit(X_train, y_train)

# combine training and test data
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
",test/test_scikit/iris_random_forest.py,viniciusguigo/the_magic_kingdom_of_python,1
"    [  0.00000000e+00,   7.51318453e-03,   0.00000000e+00,   5.17122730e-03,
       0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   1.00643423e-01,
       0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   2.70698692e-04,
       1.45406955e-03,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
       0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
       1.65213916e-01,   2.05304918e-02,   3.18705597e-01,   8.43881639e-02,
       7.43857708e-04,   0.00000000e+00,   6.64732679e-04,   2.93743220e-01,
       0.00000000e+00,   9.57419390e-04]


rf = RandomForestClassifier(n_estimators=specs['n_estimators'], random_state = 409)
rf.fit(X=X_train, y=y_train, feature_weight=feature_importances)
# rf.fit(X=X_train, y=y_train) # try with unweighted rf -- this should work fine

for idx, dtree in enumerate(rf.estimators_):
    print(idx)

    #dtree_out = irf_utils._get_tree_data(X_train=X_train,
    #                           X_test=X_test,
    #                           y_test=y_test,",test_bug/iRF_bug_wRF.py,Yu-Group/scikit-learn-sandbox,1
"# Choose the model
# *********************************

# Linear Classifier
#clf = SGDClassifier(loss=""modified_huber"")


for estimator_param in n_estimations_list:
    estimator_param = int(estimator_param)
    # Random forest
    clf = RandomForestClassifier(n_estimators=estimator_param, n_jobs=-1, random_state=0)
    # *********************************
    #  Evaluate Model
    # *********************************

    X_train, X_test, y_train, y_test = train_test_split(train_features[feature_list], train_labels, test_size=0.1, random_state=0)

    clf.fit(X_train, np.ravel(y_train))

    # predict with propabilities, run on test data",examples/remigius-thoemel/titanic-iteration-3.py,remigius42/code_camp_2017_machine_learning,1
"X[""class""] = X[""class""].astype(""category"").cat.codes
X[""user""] = X[""user""].astype(""category"").cat.codes
X = X.dropna()

y = X[""class""]
y = pd.DataFrame(y)
X = X.drop(""user"",1)
X = X.drop(""class"",1)


model = RandomForestClassifier(n_estimators=30,max_depth=10,oob_score=True,random_state=0)

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=7)
y_test = y_test[""class""]
y_train = y_train[""class""]


print ""Fitting...""
s = time.time()
model.fit(X_train,y_train)",MachineLearning/Random-Forests.py,adrianjg/Tools,1
"    BusTrainFeatureIndices = list(xrange(19,22))
    logging.debug(""generic features = %s"" % genericFeatureIndices)
    logging.debug(""advanced features = %s"" % AdvancedFeatureIndices)
    logging.debug(""location features = %s"" % LocationFeatureIndices)
    logging.debug(""time features = %s"" % TimeFeatureIndices)
    logging.debug(""bus train features = %s"" % BusTrainFeatureIndices)
    return genericFeatureIndices + BusTrainFeatureIndices

  def buildModelStep(self):
    from sklearn import ensemble
    forestClf = ensemble.RandomForestClassifier()
    model = forestClf.fit(self.selFeatureMatrix, self.cleanedResultVector)
    return model

  def generateFeatureMatrixAndIDsStep(self, sectionQuery):
    toPredictSections = self.Sections.find(sectionQuery)
    logging.debug(""Predicting values for %d sections"" % toPredictSections.count())
    featureMatrix = np.zeros([toPredictSections.count(), len(self.featureLabels)])
    sectionIds = []
    sectionUserIds = []",CFC_DataCollector/modeinfer/pipeline.py,sdsingh/e-mission-server,1
"def cal_f1(prediction_set, reference_set):
    intersection = prediction_set & reference_set
    precision = len(intersection) / len(prediction_set)
    recall = len(intersection) / len(reference_set)
    return 2 * precision * recall / (precision + recall)


def execute():
    train = pd.read_csv(""data/train.csv"")
    test = pd.read_csv(""data/test.csv"")
    clf = RandomForestClassifier(n_estimators=10)
    clf.fit(train.iloc[:,0:-1], train.iloc[:,-1])
    print clf.classes_
    p = clf.predict(test)

    with open('result.csv', 'w') as target:
        target.writelines('user_id,item_id\n')
        for i in range(len(test)):
            if p[i] > 0:
                target.writelines(str(test.iloc[i, 0]) + ',' + str(test.iloc[i, 1]) + '\n')",tianchi_start/predict.py,wenjunyang/toast,1
"        # {'max_depth' : range(1,20,1), 'n_estimators' : range(5, 30+1,5)},

        if not kwargs:
            kwargs = {""n_jobs"": 8,
                      ""max_depth"": 6,
                      # ""min_samples_leaf"": 100,
                      ""n_estimators"": 100,
                      # ""max_features"": None
                      }

        self.clf = RandomForestClassifier(**kwargs)
        self.clf.fit(allrows[columns].values, allrows[""tag""].values)

        # add audit trail into the model output:
        self.clf.columns = columns


    def classify(self, instances, columns, *args, **kwargs):
        """""" Classify a set of instances after training
",src/python/scoringModelTraining/somatic/lib/strelka_rf.py,Illumina/strelka,1
"for x,y in cv: 
    clf = SVC(kernel='rbf',C=10.0,gamma = 0.005,)
    clf.fit(reg_imgs[x],labels[x])
    svc_rslt.append(clf.score(reg_imgs[y], labels[y]))
svc_rslt = np.array(svc_rslt)

print('cross validated SVC score is ' , svc_rslt.mean())


# tried ensenble with various algorithm
ens1 = RandomForestClassifier(n_estimators =  250 , max_depth= None,verbose=1)

ens2 = AdaBoostClassifier(SVC(kernel='rbf',gamma=0.005,C = 10.0),
                          algorithm=""SAMME"",
                          n_estimators=100,
                          learning_rate=0.01)

ens3  = AdaBoostClassifier(DecisionTreeClassifier(max_depth=None),
                         algorithm=""SAMME"",
                         n_estimators=100,",modeling.py,Jesse-Back/mach_image_proc,1
"    # svm = SVC(kernel=""linear"", C=0.06)
    # svm.fit(training_matrix, target)
    #
    # scores_svm = cross_validation.cross_val_score(svm, training_matrix, target, cv=5)
    # print(""(svm) Accuracy: %0.5f (+/- %0.2f)"" % (scores_svm.mean(), scores_svm.std() * 2))
    #
    # return svm
    ##### Works well ######

    # Random Forest
    rf = RandomForestClassifier(n_estimators=1500, max_depth=2, max_features=4)
    scores_rf = cross_validation.cross_val_score(rf, training_matrix, target, cv=5)
    print(""(Random Forest) Accuracy: %0.5f (+/- %0.2f)"" % (scores_rf.mean(), scores_rf.std() * 2))
    rf.fit(training_matrix, target)
    return rf

    # Create and fit an AdaBoosted decision tree
    # bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=10),
    #                          algorithm=""SAMME.R"",
    #                          n_estimators=600)",AV_Loan_Prediction/Main.py,kraktos/Data_Science_Analytics,1
"        
    train_y = le.transform(train_y)
    test_y = le.transform(rawTest_y)    
    
    return train_x, train_y, test_x, test_y


def selectFeatureSet_RF(data_x, data_y, nFeatures):
    """"""Use Random Forest to find the best numFeatures of features, based on the given data_x.""""""

    rf_filter = RandomForestClassifier(max_features = 'auto')
    rf_filter.fit(data_x, data_y);
    rankings = rf_filter.feature_importances_;
    selectedBool = np.argsort(rankings)[-nFeatures:]
#    selectedBool = sorted(range(len(rankings)), key = lambda x: rankings[x])[-nFeatures:];
    return data_x.columns[selectedBool]    



def evalFeatureSet(train_x, train_y, test_x, test_y, selectedFeatures, classifier):",bruteForceSearches.py,cocoaaa/ml_gesture,1
"
X_train = X[:train_samples]
X_test = X[train_samples:]
y_train = y[:train_samples]
y_test = y[train_samples:]

# Create classifiers
lr = LogisticRegression()
gnb = GaussianNB()
svc = LinearSVC(C=1.0)
rfc = RandomForestClassifier(n_estimators=100)

###############################################################################
# Plot calibration plots

plt.figure(figsize=(10, 10))
ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
ax2 = plt.subplot2grid((3, 1), (2, 0))

ax1.plot([0, 1], [0, 1], ""k:"", label=""Perfectly calibrated"")",projects/scikit-learn-master/examples/calibration/plot_compare_calibration.py,DailyActie/Surrogate-Model,1
"                           converters={0:lambda s: ord(s.split(""\"""")[1])})
    trainDataResponse = trainData[:,1]
    trainDataFeatures = trainData[:,0]

    # Train H2O GBM Model:
    #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")
    rf_h2o = h2o.random_forest(x=alphabet[['X']], y=alphabet[""y""], ntrees=1, max_depth=1, nbins=100)

    # Train scikit GBM Model:
    # Log.info(""scikit GBM with same parameters:"")
    rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
    rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

    # h2o
    rf_perf = rf_h2o.model_performance(alphabet)
    auc_h2o = rf_perf.auc()

    # scikit
    auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_smallcatRF.py,pchmieli/h2o-3,1
"    Y = data[""target""]

    print('End data loading...')

    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import LinearSVC
    from sklearn.ensemble import ExtraTreesClassifier
    
    clf = GaussianNB()
    clf1 = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)
    clf2 = LogisticRegression()
    clf3 = LinearSVC(random_state=0)
    clf4 = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0) 
    
    # copy css and javascript on project folder
    createproject(options.outputfolder)
    expirements(options.title, options.outputfolder + ""/index.html"", [clf, clf1, clf2, clf3, clf4], X, Y)

if __name__ == '__main__':",baseclassifier.py,theofilis/base-line-classifier,1
"counter = 0
for review in clean_test_reviews:
    test_centroids[counter] = create_bag_of_centroids( review, \
        word_centroid_map )
    counter += 1


# ****** Fit a random forest and extract predictions
#
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 100)

# Fitting the forest may take a few minutes
print ""Fitting a random forest to labeled training data...""
forest = forest.fit(train_centroids,train[""sentiment""])
result = forest.predict(test_centroids)

# Write the test results 
output = pd.DataFrame(data={""id"":test[""id""], ""sentiment"":result})
output.to_csv(""BagOfCentroids.csv"", index=False, quoting=3)",TutorialCode_Final/Word2Vec_BagOfCentroids.py,angelachapman/Kaggle-DeepLearning-Tutorial,1
"        DriverData = np.vstack([DriverData, MakeFeat(history)])
        DriverTarget = np.r_[DriverTarget, [1]]

    DriverData[np.isnan(DriverData)] = 0
            
    Data = np.append(RefData, DriverData, axis=0)
    Target = np.append(RefTarget, DriverTarget)

    Data[np.isnan(Data)] = 0
     
    clf = ensemble.RandomForestClassifier(n_estimators=500, criterion='entropy', min_samples_leaf=5, oob_score=True)
    #scores = cross_validation.cross_val_score(clf, Data, Target, cv=5, scoring='roc_auc')
    #print scores.mean()
    clf.fit(Data,Target)
 
    Driver = np.array(['%s_%s' % (driver, j) for j in range(1, 201)]).reshape((200, 1))
    Prob = np.append(np.mat(Driver), np.mat(clf.predict_proba(DriverData)[:,1]).reshape((200,1)), axis=1)

    List = np.append(List, Prob, axis=0)
    #print 'No.: %s, Driver: %s, # True: %s, Score: %s' % (count, driver, clf.predict(DriverData)[clf.predict(DriverData) == 1].size, clf.score(DriverData, DriverTarget))",Driver-telematics-analysis/Driver_classify.py,brainsqueeze/Kaggle-competitions,1
"    # Transform data into a vector of TF-IDF values
    count_vect = CountVectorizer(ngram_range=(1, 2))
    X_train_counts = count_vect.fit_transform(X_train)
    tfidf_transformer = TfidfTransformer(use_idf=True)
    X_train_dtm = tfidf_transformer.fit_transform(X_train_counts)
    # Transform test data
    X_test_counts = count_vect.transform(X_test)
    X_test_dtm = tfidf_transformer.fit_transform(X_test_counts)

    # Not optimized
    clf = RandomForestClassifier()
    clf.fit(X_train_dtm, y_train)
    y_pred_class = clf.predict(X_test_dtm)

    # utilities.print_misclassified_samples(X_test, y_pred_class, y_test)
    utilities.print_stats(y_pred_class, y_test)


if __name__ == '__main__':
    # probably not the best way to measure time, but, we only want a ballpark figure",random_forest.py,anmolshkl/oppia-ml,1
"        filename = pathbtr+dirListbtr[i]
        breadtrain[i] = ins.getFDs(filename)
        filename = pathbte+dirListbte[i]
        breadtest[i] = ins.getFDs(filename)

        filename = pathnbtr+dirListnbtr[i]
        nonbreadtrain[i] = ins.getFDs(filename)
        filename = pathnbte+dirListnbte[i]
        nonbreadtest[i] = ins.getFDs(filename)

    cfr = RandomForestClassifier(n_estimators=100)
    data = np.vstack((breadtrain,breadtest,nonbreadtrain,nonbreadtest))
    labels = np.zeros((len(data),1)) # FIX ME
    for i in range(len(data)):
        labels[i] = i
    labels = map(lambda i: np.floor(i/(2*(cant)))+1, labels)
    labels = np.array(labels)
    labels = np.transpose(labels)[0]   # FIX ME
    print ""Testing...""
    scores = cross_validation.cross_val_score(cfr, data, labels, cv=4)",tests/test_classifier.py,rbaravalle/imfractal,1
"
    #tune para meters
    # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
    #n_estimators_values = [5, 10, 100, 1000, 3000]
    n_estimators_values = [1000]
    max_features_values = [0.1, 'auto', 'sqrt', 'log2', None] # (float)0.1=>10%
    criterion_values = ['gini', 'entropy']
    
    param_grid = dict(n_estimators=n_estimators_values, max_features=max_features_values, criterion=criterion_values)
    
    model = RandomForestClassifier()
    
    kfold = cross_validation.KFold(n=len(X_train), n_folds=NUM_FOLDS, random_state=RAND_SEED)
    grid = GridSearchCV(n_jobs=N_JOBS, verbose=10, estimator=model, param_grid=param_grid, scoring=SCORING, cv=kfold)
    
    grid_result = grid.fit(rescaledX, Y_train)
    print(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))    
        
    best_idx = grid_result.best_index_
",lib/eda4.py,FabricioMatos/ifes-dropout-machine-learning,1
"               initialize_raw_model=True):
    super(SklearnModel, self).__init__(
        model_type, task_types, model_params, initialize_raw_model)
    self.task_types = task_types
    self.model_params = model_params
    if initialize_raw_model:
      if self.model_type == ""rf_regressor"":
        raw_model = RandomForestRegressor(
            n_estimators=500, n_jobs=-1, warm_start=True, max_features=""sqrt"")
      elif self.model_type == ""rf_classifier"":
        raw_model = RandomForestClassifier(
            n_estimators=500, n_jobs=-1, warm_start=True, max_features=""sqrt"")
      elif self.model_type == ""logistic"":
        raw_model = LogisticRegression(class_weight=""auto"")
      elif self.model_type == ""linear"":
        raw_model = LinearRegression(normalize=True)
      elif self.model_type == ""ridge"":
        raw_model = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0], normalize=True)
      elif self.model_type == ""lasso"":
        raw_model = LassoCV(max_iter=2000, n_jobs=-1)",deepchem/models/standard.py,evanfeinberg/deep-learning,1
"svm_linear_rates = rates(data, svm_linear_grid, lambda C: SVC(C=C, kernel='linear'))
print_result(svm_linear_rates)
svm_linear_pca_rates = rates(data_pca, svm_linear_grid, lambda C: SVC(C=C, kernel='linear'))
print_result(svm_linear_pca_rates)

svm_rbf_rates = rates(data, svm_rbf_grid, lambda C: SVC(C=C[0], gamma=C[1], kernel='rbf'))
print_result(svm_rbf_rates)
svm_rbf_pca_rates = rates(data_pca, svm_rbf_grid, lambda C: SVC(C=C[0], gamma=C[1], kernel='rbf'))
print_result(svm_rbf_pca_rates)

randforest_rates = rates(data, randforest_grid, lambda C: RandomForestClassifier(max_depth=C))
print_result(kneighbors_rates)
randforest_pca_rates = rates(data_pca, randforest_grid, lambda C: RandomForestClassifier(max_depth=C))",proj03/proj03.py,rerthal/mc886,1
"# -*- coding: utf-8 -*-
""""""
Created on Sun Jul  5 06:37:25 2015

@author: tanay
""""""
from sklearn.ensemble import RandomForestClassifier
from sklearn import pipeline, metrics, grid_search

clf = pipeline.Pipeline([('rf', RandomForestClassifier())])



param_grid = {'rf__n_estimators': [140,160,180,200],'rf__max_depth': [12,14,16]}

model = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, 
                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=5)
                                 
# Fit Grid Search Model",Search_result/srr10.py,tanayz/Kaggle,1
"def get_clf(cl, n_jobs=1, random_state=0, class_weight='balanced'):
    """""" Select clasifier by name
    """"""
    lm1 = {'C': [0.0001, 0.001, 0.01, 0.1, 0.3, 1, 3, 10]}
    C_range = 10.0 ** np.arange(-5, 3)
    C_range = np.hstack([C_range, [0.3, 3]])
    # lm2 = dict(C=C_range)
    rf1 = {'max_depth': [2, 4, 8, 16, 24, 32]}

    if cl == 'rf2':
        clf = RandomForestClassifier(
                n_estimators=100, min_samples_leaf=1,
                max_features='auto', class_weight=class_weight,
                n_jobs=n_jobs, random_state=random_state, verbose=0)
    elif cl == 'rf':
        clf1 = RandomForestClassifier(
                n_estimators=100, max_depth=2,
                max_features='auto', class_weight=class_weight,
                n_jobs=n_jobs, random_state=random_state, verbose=0)
        clf = grid_search.GridSearchCV(clf1, rf1, cv=4, n_jobs=n_jobs,",kgml/classifier.py,orazaro/kgml,1
"names_to_delete = ['meaningless_1', 'meaningless_2', 'star_ID',
                   'Npts', 'CSSD', 'clipped_sigma', 'lag1', 'L', 'Lclp', 'Jclp',
                   'MAD', 'Ltim']
X, y, df, feature_names, delta = load_data([file_0, file_1], names,
                                           names_to_delete)
target = 'variable'
predictors = list(df)
predictors.remove(target)

# Create model for RF
clf = RandomForestClassifier(n_estimators=1400,
                             max_depth=16,
                             max_features=5,
                             min_samples_split=16,
                             min_samples_leaf=2,
                             class_weight={0: 1, 1: 28},
                             verbose=1, random_state=1, n_jobs=4)
estimators = list()
estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                      axis=0, verbose=2)))",ml4vs/rf_final.py,ipashchenko/ml4vs,1
"test_data = test_df.dropna().values


# And FINALLY, we train our model and make predictions
# A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the
# dataset and use averaging to improve the predictive accuracy and control over-fitting
# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
from sklearn.ensemble import RandomForestClassifier

# Create an RFC with 100 decision trees
forest = RandomForestClassifier(n_estimators=100)

# Build a forest of trees using:
#  X = the training data features (except PassengerId and Survived)
#  y = the training data outputs (just Survived)
forest = forest.fit(train_data[0::, 2::], train_data[0::, 1])

# Compute the predicted output (survival outcomes) for each test row's features (except PassengerId)
# In this process each tree votes for an outcome weighted by its probability estimates. The predicted class is the one
# with the highest mean probability estimate across all trees in the forest",scripts/PandasML.py,SgfPythonDevs/cboschert-intro2ml,1
"    assert_raises(Exception, float_range, '2.0')


def test_float_range_3():
    """"""Assert that the TPOT CLI interface's float range throws an exception when input is not a float.""""""
    assert_raises(Exception, float_range, 'foobar')


def test_StackingEstimator_1():
    """"""Assert that the StackingEstimator returns transformed X with synthetic features in classification.""""""
    clf = RandomForestClassifier(random_state=42)
    stack_clf = StackingEstimator(estimator=RandomForestClassifier(random_state=42))
    # fit
    clf.fit(training_features, training_target)
    stack_clf.fit(training_features, training_target)
    # get transformd X
    X_clf_transformed = stack_clf.transform(training_features)

    assert np.allclose(clf.predict(training_features), X_clf_transformed[:,0])
    assert np.allclose(clf.predict_proba(training_features), X_clf_transformed[:,1:1+len(np.unique(training_target))])",tests.py,weixuanfu2016/tpot,1
"    because the training data is not as larger or diverse as one might like.
    This is good for making sure that each classifier is not overfit while at 
    the same time utilizing all of the data for training.

    size_train -- a vector of training set sizes.  The first entry is the number
    of positive examples that will be drawn from X_pos, and remainder are for 
    the negs and should be in the same order as X_neg_set.

    size_test -- same as size_train but for the test set. 

    clf = RandomForestClassifier(n_estimators=10)
    sklearn.ensemble.RandomForestClassifier(n_estimators=10, 
    criterion='gini', max_depth=None, min_samples_split=1, min_samples_leaf=1, 
    min_density=0.1, max_features='auto', bootstrap=True, 
    compute_importances=False, 
    oob_score=False, n_jobs=1, random_state=None, verbose=0)
    '''

    # compute the sizes of the input datasets
    Lp = len(X_pos)",grit/random_forest/fit_forests_human_polyA.py,nboley/grit,1
"                    learning_momentum=0.9,
                    batch_size=100,
                    valid_size=0.01,
                    n_stable=20,
                    n_iter=200,
                    verbose=True)

# Define classifiers
if sample:
    classifiers = [
        RandomForestClassifier(max_depth=16,n_estimators=1024),
        GradientBoostingClassifier(n_estimators=10, learning_rate=1.0,max_depth=5, random_state=0),
        KNeighborsClassifier(n_neighbors=100, weights='uniform', algorithm='auto', leaf_size=100, p=10, metric='minkowski'),
        AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=8), algorithm=""SAMME.R"", n_estimators=128),
        XGBClassifier(max_depth=8,n_estimators=128),
        MyNNClassifier
    ]
else:
    classifiers = [# Other methods are underperformed yet take very long training time for this data set
        RandomForestClassifier(max_depth=16,n_estimators=1024)",source/sf-crime-classification.py,ccervantes/E03MACHINE,1
"
    return cv_iterator


class InstanceHardnessThreshold(BaseCleaningSampler):
    """"""Class to perform under-sampling based on the instance hardness
    threshold.

    Parameters
    ----------
    estimator : object, optional (default=RandomForestClassifier())
        Classifier to be used to estimate instance hardness of the samples.  By
        default a :class:`sklearn.ensemble.RandomForestClassifer` will be used.
        If ``str``, the choices using a string are the following: ``'knn'``,
        ``'decision-tree'``, ``'random-forest'``, ``'adaboost'``,
        ``'gradient-boosting'`` and ``'linear-svm'``.  If object, an estimator
        inherited from :class:`sklearn.base.ClassifierMixin` and having an
        attribute :func:`predict_proba`.

        .. deprecated:: 0.2",imblearn/under_sampling/prototype_selection/instance_hardness_threshold.py,scikit-learn-contrib/imbalanced-learn,1
"                          ""Nystroem"",
                          ""RBFSampler"",
                          ""SkewedChi2Sampler"",
                          ""LogisticRegression""]
                models += [neighbors.KNeighborsClassifier(),
                           svm.LinearSVC(max_iter=10),
                           ensemble.AdaBoostClassifier(),
                           ensemble.BaggingClassifier(),
                           ensemble.ExtraTreesClassifier(),
                           ensemble.GradientBoostingClassifier(),
                           ensemble.RandomForestClassifier(),
                           linear_model.SGDClassifier(),
                           kernel_approximation.AdditiveChi2Sampler(),
                           kernel_approximation.Nystroem(),
                           kernel_approximation.RBFSampler(),
                           kernel_approximation.SkewedChi2Sampler(),
                           linear_model.LogisticRegression()]
        elif is_number_categories_known:
            # clustering
            names += [""KMeans"",",models.py,Rafaelahelbing/psychoPYTHON,1
"   def transform(self,xTest):
      nRow = len(xTest)
      sc = self.scale_(xTest)
      xf = self.xform_.transform(self.scale_(xTest))  
      # return transformed plus original scaled columns
      return [ numpy.array(sc[i]).tolist() + numpy.array(xf[i]).tolist() for i in range(nRow) ]


# map of modelfactories (each takes a random state on instantiation)
model_dict = {""SVM"" : (lambda rs: sklearn.svm.SVC(kernel='rbf',gamma=0.001,C=10.0,probability=True,random_state=rs)),\
              ""Random Forest"" : (lambda rs: sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1,random_state=rs)),\
              ""Gradient Boosting"" : (lambda rs: sklearn.ensemble.GradientBoostingClassifier(random_state=rs)), \
              ""Decision Tree"" : (lambda rs: sklearn.tree.DecisionTreeClassifier(random_state=rs)), \
              ""Naive Bayes"" : (lambda rs: sklearn.naive_bayes.GaussianNB()), \
              ""Logistic Regression"" : (lambda rs: sklearn.linear_model.LogisticRegression(random_state=rs)), \
              ""KNN5"" : (lambda rs: sklearn.neighbors.KNeighborsClassifier(n_neighbors=5))}



tarFileName = sys.argv[1] # data.tar.gz",ScoreModels/processArffs.py,WinVector/ExploreModels,1
"        self.transform_note = preprocessing.OneHotEncoder(categorical_features=[0,1,2,5,6,7,10,11,12,15,16,17,20,21,22])
        self.transform_note.fit(note_data[:, :-1])

        self.transform_duration = preprocessing.OneHotEncoder(categorical_features=[0,1,2,5,6,7,10,11,12,15,16,17,20,21,22])
        self.transform_duration.fit(duration_data[:, :-1])

        self.transform_time_delta = preprocessing.OneHotEncoder(categorical_features=[0,1,2,5,6,7,10,11,12,15,16,17,20,21,22])
        self.transform_time_delta.fit(time_delta_data[:, :-1])

        print ""training instruments""
        self.instrument_clf = RandomForestClassifier(n_estimators=10)
        self.instrument_clf = self.instrument_clf.fit(
            self.transform_instrument.transform(instrument_data[:, :-1]), instrument_data[:, -1:])
        print ""training notes""
        self.note_clf = RandomForestRegressor(n_estimators=10)
        print note_data[:, :-1]
        self.note_clf = self.note_clf.fit(self.transform_note.transform(note_data[:, :-1]), note_data[:, -1:])
        print ""training velocity""
        self.velocity_clf = RandomForestClassifier(n_estimators=10)
        self.velocity_clf = self.velocity_clf.fit(",meka-technique/generator.py,chrisranderson/music-generation,1
"
		self.vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
		                                 stop_words='english')

		X2 = self.vectorizer.fit_transform(self.data_text)
		self.clf2 = SGDClassifier(loss='log', penalty='l2',alpha=1e-5, n_iter=25, random_state=42,shuffle=True)
		self.clf2.fit(X2, labels)
		vocab = my_dict2 = {y:x for x,y in self.vectorizer.vocabulary_.iteritems()}


		#clf = RandomForestClassifier(n_estimators=750)
		self.allTextInOrders = self.data_text + self.data_text + unhelpful_exp_text + helpful_exp_text


		trainingDistributions = []

		# for reviewT in data_text:
		# 	trainingDistributions.append(lda[corpus.dictionary.doc2bow(corpus.proc(reviewT))])

",src/dialectic_pipeline/dialectic_pipeline/model_trainer.py,cudbg/Dialectic,1
"Xall = np.vstack((Xtrain, Xtest))

# Encode data
encoder = OneHotEncoder()
encoder.fit(Xall)
Xtrainsparse = encoder.transform(Xtrain)
Xtestsparse = encoder.transform(Xtest)

# Fit data
print ""fitting data...""
RF = RandomForestClassifier(10, ""entropy"", None)
RF.fit(Xtrainsparse, ytrain)
preds = range(len(testXc))
scores = np.zeros(len(testXc))
for i in range(len(testXc)):
	X, y = removePadding(testXc[i], testyc[i], yp)
	Xsparse = encoder.transform(X)
	preds[i] = RF.predict(Xsparse)
	scores[i] = RF.score(Xsparse, y)
	print ""Chorale #%d: %.2f%%"" % (i, scores[i] * 100.0)",bach_code/decoder.py,glasperfan/thesis,1
"
pca = PCA(n_components = 30)
X = np.array([np.reshape(img, (101*101)) for img in images])
X = X[nlab > 2]
X_transform = pca.fit_transform(X)
print len(X_transform)

scores=[]
for n in range(1,50):
    print n
    scores += [np.mean(cross_validation.cross_val_score(RandomForestClassifier(n_estimators = n), X_transform, labels, cv = 5))]

plt.plot(scores)
plt.show()",src/classify.py,prcastro/BYCells-MitoticIndex,1
"        print(confusion_matrix(y_test, y_predicted))

if __name__ == '__main__':
    languages = 'JavaScript Python Java PHP C C++ C-sharp R Go Ruby Scala'.split()
    samples_amount = 1000

    classifiers = [
        MultinomialNB(),
        KNeighborsClassifier(len(languages)),
        DecisionTreeClassifier(max_depth=5),
        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
        AdaBoostClassifier(),
    ]

    benchmark(languages, samples_amount, classifiers)",train.py,javierhonduco/slang,1
"    airquality = pd.read_csv(os.path.join(here, 'data', 'airquality.csv'))
    airquality.columns = ['Unnamed', 'Ozone', 'SolarR', 'Wind',
                          'Temp', 'Month', 'Day']
    airquality = airquality.dropna()

    return airquality


def make_example_classifier(filename):
    # Create a dummy RF model for train/classify testing
    rf = RandomForestClassifier()
    p, n_class = 42, 2
    n = n_class * 5
    X = np.random.rand(n, p)
    y = np.repeat(range(n_class), n / n_class)
    rf.fit(X, y)
    jl.dump(rf, filename)


# EXAMPLE DATASETS",tests/conftest.py,ceholden/yatsm,1
"
from sklearn.base import BaseEstimator


class feat_sel(BaseEstimator):
    def __init__(self, thresh: float=0.8):
        self.thresh = thresh
        self.pf = PolynomialFeatures(interaction_only=True)
        self.vt = VarianceThreshold(self.thresh * (1 - self.thresh))
        self.lr = SelectFromModel(LogisticRegression(penalty='l1'))
        self.rf = SelectFromModel(RandomForestClassifier(n_jobs=-1))
        self.pca = PCA()
 
    def fit_transform(self, X, Y):
        X = self.pf.fit_transform(X)
        X = self.vt.fit_transform(X)
        X = self.lr.fit_transform(X, Y)
        X = self.rf.fit_transform(X, Y)
        self.pca.fit(X)
        for i, r in enumerate(self.pca.explained_variance_ratio_.cumsum()):",bards/feature_selection.py,dbftdiyoeywga/bards,1
"        # feature and label arrays
        self.colNames = list(self.training.columns.values)
        self.colNames.remove(""asite"")
        dummyTraining = pd.get_dummies(self.training[self.colNames])
        self.dummyTrainingColNames = dummyTraining.columns.values
        self.X = sparse.csr_matrix(np.array(dummyTraining))
        # self.X = sparse.csr_matrix(np.array(pd.get_dummies(self.training[self.colNames])))
        self.y = np.array(self.training[""asite""])
        # feature selection
        cpus = 10 if multiprocessing.cpu_count() > 10 else max(1, multiprocessing.cpu_count() - 1)
        self.clf = RandomForestClassifier(n_jobs=cpus, max_depth=10, min_samples_split=100)
        self.clf = self.clf.fit(self.X, self.y)
        self.importance = self.clf.feature_importances_
        self.selector = RFECV(self.clf, step=1, cv=5)
        self.selector = self.selector.fit(self.X, self.y)
        sltX = self.selector.transform(self.X)
        print(""[result]\tOptimal number of features by recursive selection: %d"" % self.selector.n_features_, flush=True)
        # define a new classifier for reduced features
        self.reducedClf = RandomForestClassifier(n_jobs=cpus, max_depth=10, min_samples_split=100)
        self.reducedClf = self.reducedClf.fit(sltX, self.y)",scikit_ribo/asite_predict.py,hanfang/scikit-ribo,1
"    dt_selectedFeatures = ""all""
    nb_selectedFeatures = ""all""
        
    rf_features, rf_labels = sc.Data_Preparation(inputfile, rf_selectedFeatures)
    svm_features, svm_labels = sc.Data_Preparation(inputfile, svm_selectedFeatures)
    dt_features, dt_labels = sc.Data_Preparation(inputfile, dt_selectedFeatures)
    nb_features, nb_labels = sc.Data_Preparation(inputfile, nb_selectedFeatures)

    # Model declaration

    RandomForest_Classification = ensemble.RandomForestClassifier(n_estimators=510, criterion='gini', max_depth=7,
                                                                 min_samples_split=2, min_samples_leaf=1, max_features='sqrt',
                                                                 bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, verbose=0,
                                                                 min_density=None, compute_importances=None)

    SVM_Classification = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)

    DecisionTree_Classification = tree.DecisionTreeClassifier(max_depth=7, max_features='sqrt')
    AdaBoostDecisionTree_Classification = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=7, max_features='sqrt'),
                                                      n_estimators=600, learning_rate=1)",Blending_test.py,DistributedSystemsGroup/YELP-DS,1
"  trainDataFeatures = trainData[:,0]

  # Train H2O GBM Model:
  #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")

  rf_h2o = H2ORandomForestEstimator(ntrees=1, max_depth=1, nbins=100)
  rf_h2o.train(x='X', y=""y"", training_frame=alphabet)

  # Train scikit GBM Model:
  # Log.info(""scikit GBM with same parameters:"")
  rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
  rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

  # h2o
  rf_perf = rf_h2o.model_performance(alphabet)
  auc_h2o = rf_perf.auc()

  # scikit
  auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_smallcatRF.py,hsaputra/h2o-3,1
"	if os.path.getsize(filepath) == 0:
		filename = os.path.basename(filepath)
		df_full = df_full[df_full.file != filename]
		if filename in test_files:
			print(""Found empty file in submission: "", filename)



#https://www.youtube.com/watch?v=0GrciaGYzV0
print('--- Training random forest')
clf = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=0)
train_data = df_full[df_full.sponsored.notnull()].fillna(0)
test = df_full[df_full.sponsored.isnull() & df_full.file.isin(test_files)].fillna(0)
clf.fit(train_data.drop(['file', 'sponsored'], 1), train_data.sponsored)

#normalized value between 0 and 1
feature_importances = pd.Series(clf.feature_importances_, index=train_data.drop(['file', 'sponsored'], 1).columns)
feature_importances.sort()
with pd.option_context('display.max_rows', len(feature_importances), 'display.max_columns', 10):
	print(feature_importances)",scikit_generate_prediction2.py,carlsonp/kaggle-TrulyNative,1
"        t = None
        f = None
    return probGenerator, t, f


def loadTransitionClassifier(transitionClassifierFilename, transitionClassifierPath):
    """"""
    Load a transition classifier random forest from a HDF5 file
    """"""
    import hytra.core.probabilitygenerator as traxelstore
    rf = traxelstore.RandomForestClassifier(transitionClassifierPath, transitionClassifierFilename)
    return rf


def getDetectionFeatures(traxel, max_state):
    return hypothesesgraph.getTraxelFeatureVector(traxel, ""detProb"", max_state)


def getDivisionFeatures(traxel):
    prob = hypothesesgraph.getTraxelFeatureVector(traxel, ""divProb"", 1)[0]",scripts/hypotheses_graph_to_json.py,chaubold/hytra,1
"
    #LIST OF MODEL THAT WE WANT TO USE
    #We have all the model that we want to use listed in the 'classifiers' variable
    #For now we are just looking that their mean cross-validation accuracy. We can easily look into more
    #informative metrics like F1-score, confusing metrics etc. But, cv-accuracy should be enough, I think!
    #Here, I have commented some to save some time. You can try adding more and un-commmenting these codes.
    #NOTE: You might need to import the models.
    classifiers = [(""Multinomial Naive Bayes Classifier"", MultinomialNB(alpha=.01)),\
                   #(""Bagging Classifier (Decision Tree)"", BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=seed)),\
                   (""SVM"", SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)),\
                   (""Random Forest Classifier"", RandomForestClassifier(n_estimators=100, max_features=10)),\
                   (""Ridge Classifier"", RidgeClassifier(tol=1e-2, solver=""sag"")),\
                   (""Logistic Regression"", LogisticRegression(solver=""sag"", multi_class=""multinomial"")),\
                   (""Linear SVC"", LinearSVC(random_state=seed)),\
                   (""Perceptron"", Perceptron(random_state=seed))]


    #MAJORITY VOTING CLASSIFIER
    #This model is our ""ultimate"" model ;) This looks into predictions from all the classifiers
    #and outputs the majority class.",analysis/ghost_model_picking.py,mab1290/GHOST,1
"            
            # Initialize SVD
            svd = TruncatedSVD(n_components=175,random_state=None)
            X=svd.fit_transform(X)+trssv.sensimvar()
            X_test=svd.fit_transform(X_test)+tsssv.sensimvar()
            # Initialize the standard scaler 
            scl = StandardScaler()
            
            # We will use SVM here..
            svm_model = SVC()
#            rf=RandomForestClassifier(n_estimators=400,max_depth=15,n_jobs=4,verbose=True)
            
            # Create the pipeline 
            clf = pipeline.Pipeline([
            						 ('scl', scl),
#                            	     ('rf', rf)])
                                     ('svm', svm_model)])
            
            # Create a parameter grid to search for best parameters for everything in the pipeline
            param_grid = {'svm__C': [9]}",Search_result/srr6.py,tanayz/Kaggle,1
"
def knn_classifier(data, labels, columns):
    print 'Applying k-nearest neighbor classification'
    # create param grid
    n_numeric = len([c.TYPE for c in columns if c.TYPE is Types.NUMERICAL and c.CATEGORIES is None])
    n_neighbors = list(range(1, 51, 1))
    parameters = dict(knn__n_neighbors=n_neighbors)

    # create model pipeline
    ns = NumericScaler(n_numeric)
    rf = RandomForestClassifier() #random_state=8)
    knn = KNeighborsClassifier()
    rfe = feature_selection.RFE(rf)
    pipe = Pipeline(steps=[('ns', ns),
                           ('rfe', rfe),
                           ('knn', knn)])

    # run grid search with 10-fold cross validation
    clf = GridSearchCV(pipe, parameters, cv=10, verbose=1)
    clf.fit(data, labels)",src/model.py,nikhilnrng/german-credit-risk,1
"

def cross_validation(X,Y,cv_param = 20, max_depth = None): 
    """"""
        Cross_validation takes as input a dataset X and the labels Y and performs the cv_param-fold cross validation.
        It uses a random forest classifier in order to do so and plots the cross-validation score, the f1-score
        as well as the confusion matrix.
	@return Y_predicted : the predicted outcome from our model.
    """"""
    # 1. Creates the classifier that we will use 
    forest = RandomForestClassifier(max_depth = max_depth)
    # 2. Predicts the output of the classification after cv_param-fold cross-validation
    Y_predicted = cross_val_predict(forest, X, Y, cv=cv_param)
    
    # 3. Print the results : scores, 
    print('Cross Validation result :', cross_val_score(forest,X,Y,cv = cv_param).mean(),
        '\nF1 score result :',sklearn.metrics.f1_score(Y, Y_predicted,average='micro'))
    
    print_confusion_matrix(sklearn.metrics.confusion_matrix(Y,Y_predicted))
    ",03-ML/ML_helpers.py,thom056/ada-parliament-ML,1
"    train_sample_size = train_prct

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, train_size=train_sample_size,
        random_state=1234)
    
    #--------------------------------------------------
    # Implement the Random Forest Classification Method
    #--------------------------------------------------
    # Initialize our model with 500 trees
    rf_param = RandomForestClassifier(
                n_estimators=500, oob_score=True)

    print('Using Random Forest Parameters to train the model...')
    t0 = time()
    # Fit our model to training Data
    rf_model = rf_param.fit(X_train, y_train)
    print('Done in {s:0.3f}s.'.format(
        s=time()-t0))
",src/python/utils/naive_classification.py,jejjohnson/manifold_learning,1
"     ll = ll * -1.0/len(act) 
     return ll 

def main():
    #read in  data, parse into training and target sets
    dataset = np.genfromtxt(open('Data/train.csv','r'), delimiter=',', dtype='f8')[1:]    
    target = np.array([x[0] for x in dataset])
    train = np.array([x[1:] for x in dataset])

    #In this case we'll use a random forest, but this could be any classifier
    cfr = RandomForestClassifier(n_estimators=100)

    #Simple K-Fold cross validation. 5 folds.
    #(Note: in older scikit-learn versions the ""n_folds"" argument is named ""k"".)
    cv = cross_validation.KFold(len(train), n_folds=5, indices=False)

    #iterate through the training and test cross validation segments and
    #run the classifier on each one, aggregating the results into a list
    results = []
    for traincv, testcv in cv:",src/example_to_run.py,cyrta/UrbanSounds,1
"from sklearn.ensemble import RandomForestClassifier

def train_and_predict(trainer, tuning_params):
    for param in tuning_params:
        print ""Training for param value: {}"".format(param)
        clf = RandomForestClassifier(
            n_jobs=2, 
            oob_score=True, 
            random_state=50, 
            max_features = ""auto"", 
            min_samples_leaf=param, 
            n_estimators=500
        )
        clf.fit(trainer.X_train, trainer.Y.ravel())
        trainer.timer.interval('trained model')",movie_rec/lib/random_forest.py,yujinjcho/movie_recommendations,1
"        neg_data = read_csv(sys.argv[2], sys.argv[3])[:num_data]
        data = np.concatenate((pos_data, neg_data))
        labels = np.array([1]*num_data + [0]*num_data)
        
        # split data
        data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size = 0.3, random_state=0)
        
        print(""Random Forests Classifier"")
        estimator = 20
        features = 5
        clf = RandomForestClassifier(n_estimators=estimator, max_features=features, oob_score=True)
        clf = clf.fit(data_train, labels_train)
        pred = clf.predict(data_test)
        pscore = metrics.accuracy_score(labels_test, pred)
        print(pscore)
        
        feature_importance = clf.feature_importances_
        most_important = sorted(range(len(feature_importance)), key=lambda i: feature_importance[i], reverse=True)[:5]
        percentages = sorted(feature_importance, reverse=True)[:5]
        for (feature, percentage) in zip(most_important, percentages):",scripts/random_forests.py,manderelee/csc2521_final,1
"cat_feats = ['purpose']
final_data = pd.get_dummies(loans, columns=cat_feats, drop_first=True)

# train data
X = final_data.drop('not.fully.paid', axis=1)
y = final_data['not.fully.paid']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Make rfc object, fit and predict
# n_estimators is the amount of trees we use.
RFC = RandomForestClassifier(n_estimators=300)
RFC.fit(X_train, y_train)
rfc_pred = RFC.predict(X_test)

# Report
print(confusion_matrix(y_test, rfc_pred))
print(classification_report(y_test, rfc_pred))",Tree/exp_RandomForest.py,dreadjesus/MachineLearning,1
"vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
                                 stop_words='english')


X2 = vectorizer.fit_transform(data_text)
clf2 = SGDClassifier(loss='log', penalty='l2',alpha=1e-5, n_iter=25, random_state=42,shuffle=True)
clf2.fit(X2, labels)
vocab = my_dict2 = {y:x for x,y in vectorizer.vocabulary_.iteritems()}


#clf = RandomForestClassifier(n_estimators=750)

allTextInOrders = data_text + data_text + unhelpful_exp_text + helpful_exp_text
trainingDistributions = []

# for reviewT in data_text:
# 	trainingDistributions.append(lda[corpus.dictionary.doc2bow(corpus.proc(reviewT))])


",src/old_pipeline/trees.py,cudbg/Dialectic,1
"        return ""Random Forest (RFC) Classifier""  # Model name string.

    def model_postfix(self):  # Must be unique for each model.
        return ""rfc""

    def model_description(self):
        return (""Implements the Random Forest (RFC) Classifier defined in the SKLearn modules.\n""
                "" For more information, Google SKLearn and read the documentation.\n"")

    def model_define(self):
        return RandomForestClassifier(n_estimators=10, criterion='gini')


######################################################################################################
#
# Naive Bayes. Implemented as a classifier.
#
######################################################################################################

",OSMSKLearnClassify.py,kellerberrin/OSM-QSAR,1
"__maintainer__ = ""Ricardo Sousa""
__email__ = ""rsousa@rsousa.org""
__status__ = ""Dev""


def main(args):

    if args.analyse != None:
        train_data_x, test_data_x,train_data_y, test_data_y  = process_data(args.analyse)

        RT = RandomForestClassifier(n_estimators=100)
        RT.fit(train_data_x, train_data_y)
        print RT.score(test_data_x, test_data_y)

    return


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description=""Twitter Sentiment Analysis"")
    parser.add_argument('--analyse', dest='analyse', default=None, type=str)",src/main.py,rjgsousa/sentiment_analysis,1
"    return testData, result

def kFoldCrossValidation(kFold):
    trainingData = getTrainingData()
    label = trainingData['hand']
    features = trainingData.drop(['id'], axis=1)
    crossValidationResult = dict()

    print(""Start Cross Validation ...\n"")

    randomForest = RandomForestClassifier(n_estimators=100)
    kNearestNeighbour = KNeighborsClassifier(n_neighbors=100)
    crossValidationResult['RF'] = cross_val_score(randomForest, trainingData, label, cv=kFold).mean()
    crossValidationResult['KNN'] = cross_val_score(kNearestNeighbour, trainingData, label, cv=kFold).mean()

    print(""KNN: %s\n"" % str(crossValidationResult['KNN']))
    print(""RF: %s\n"" % str(crossValidationResult['RF']))
    print(""\n"")

    return crossValidationResult['KNN'], crossValidationResult['RF']",Easy/PokerRuleInduction/Main.py,AhmedHani/Kaggle-Machine-Learning-Competitions,1
"from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn import cross_validation


def main():
    """"""Run experiment with multiple classifiers.""""""
    # Get classifiers
    classifiers = [
        ('Decision Tree', DecisionTreeClassifier(max_depth=5)),
        ('Random Forest', RandomForestClassifier(n_estimators=50,
                                                 n_jobs=10,
                                                 max_features=50)),
        ('AdaBoost', AdaBoostClassifier()),
        ('Naive Bayes', GaussianNB()),
        ('LDA', LinearDiscriminantAnalysis()),
        ('QDA', QuadraticDiscriminantAnalysis()),
        # ('Random Forest 2', RandomForestClassifier(max_depth=5,
        #                                            n_estimators=10,
        #                                            max_features=1,",ML/hasy/classifier_comp.py,MartinThoma/algorithms,1
"        desc = TextProcessing.process(data[10])
        friend = data[11]
        location = TextProcessing.process(data[12])
        geo_enabled = FeatureMapping.mapping_other('geo_enabled', data[13])
        
        y.append(label)
        x.append([text, source, re_tweet, geo, place, hash_tag, media, verified, follower, statues, desc, friend, location, geo_enabled])
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import f1_score, accuracy_score
    clf = RandomForestClassifier()
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    fsc = f1_score(y_test, y_pred)
    acc = accuracy_score(y_test, y_pred)
    print fsc, acc
    print y_pred
    print y_test
                    
if __name__ == '__main__':",harvest.py,chaluemwut/smcdemo,1
"    X_test = preprocesser.transform(X_test)

  if dimensions != None:
    pca = PCA(n_components=dimensions)
    pca.fit(X_train)
    X_train = pca.transform(X_train)
    X_test = pca.transform(X_test)


  # Create and fit a random forest
  tree = RandomForestClassifier(n_estimators=n, max_depth=max_depth)
  tree.fit(X_train, y_train)
  y_test_pred = tree.predict(X_test)
  score = np.mean(y_test == y_test_pred)
  confusion = confusion_matrix(y_test, y_test_pred) 
  if verbose:
    print score
    print confusion 

  return score",kaggle/adult-census-income/adult-census-income-randomforest.py,jajoe/machine_learning,1
"
#%% Load processed numpy file
test = False
if test:
    os.chdir('..\\Data\\test')
else:
    os.chdir('..\\Data\\images_training_rev1')
images = np.loadtxt(""processed_images.txt"")

#%% Train random forest
rf = RandomForestClassifier(n_estimators=100, n_jobs=4)
rf = rf.fit(train_data[0::,1::],train_data[0::,0])

# Take the same decision trees and run on the test data
pred = rf.predict(test_data)

#%% Revert working directory
os.chdir('C:\Projects\Kaggle\Galaxy_Zoo\Code')",Galaxy_Zoo/Code/RandomForest.py,charlesjlee/Kaggle,1
"        beta1=.9,
        beta2=.999)

  elif model_name == 'rf':
    # Loading hyper parameters
    n_estimators = hyper_parameters['n_estimators']
    nb_epoch = None

    # Building scikit random forest model
    def model_builder(model_dir_rf):
      sklearn_model = RandomForestClassifier(
          class_weight=""balanced"", n_estimators=n_estimators, n_jobs=-1)
      return deepchem.models.sklearn_models.SklearnModel(sklearn_model,
                                                         model_dir_rf)

    model = deepchem.models.multitask.SingletaskToMultitask(tasks,
                                                            model_builder)

  if nb_epoch is None:
    model.fit(train_dataset)",deepchem/molnet/run_benchmark_models.py,joegomes/deepchem,1
"        
    def classifer_setup(self,Type='SVM',**kwargs):
        if Type.lower() == 'svm':
            self.classifer = svm.SVC(kernal='RBF')
        elif Type.lower() == 'rfc':
            self.classifer = ens.RandomForestClassifier(**kwargs)
        else:
            raise TypeError('Classifier Type undefined')
    
    def prototype_analysis(self,**kwargs):
        clf = RandomForestClassifier(n_estimators=200, n_jobs=4, **kwargs)
        
        scores = cross_val_score(estimator = clf,
                                 X=self.features.value,
                                 y=self.annotations.value,
                                 cv=8, n_jobs=6, pre_dispatch=20)
        
        filename = ""Prototype_Analysis_Report.txt""
        out_string = ""mean score is: {:.03f} +/- {:.03f}"".format(scores.mean(),scores.std())
        ",FlowAnal/Feature_Analysis.py,davidpng/FCS_Database,1
"                           converters={0:lambda s: ord(s.split(""\"""")[1])})
    trainDataResponse = trainData[:,1]
    trainDataFeatures = trainData[:,0]

    # Train H2O GBM Model:
    #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")
    rf_h2o = h2o.random_forest(x=alphabet[['X']], y=alphabet[""y""], ntrees=1, max_depth=1, nbins=100)

    # Train scikit GBM Model:
    # Log.info(""scikit GBM with same parameters:"")
    rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
    rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

    # h2o
    rf_perf = rf_h2o.model_performance(alphabet)
    auc_h2o = rf_perf.auc()

    # scikit
    auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_smallcatRF.py,kyoren/https-github.com-h2oai-h2o-3,1
"

# ## Random Forest: Training

# Create the random forest object:

# In[38]:

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=100)


# Fit the training data and create the decision trees:

# Training data features, skip the first column 'Survived'
train_features = train_data[:, 1:]

# 'Survived' column values
train_target = train_data[:, 0]",cdsw_usecases/04_titanic_passenger_survival_analysis/01_titanic_rf.py,ravi9/aaisg-cdsw,1
"              (best_rbf_c, best_rbf_gamma),
               RbfSVMClassifier(C=best_rbf_c,
                                gamma=best_rbf_gamma)),
              (""Gaussian Naive Bayes"",
               NaiveBayesClassifier()),
              (""Decision Tree"",
               DecisionTreeClassifier()),
              (""KNeighbors"",
               KNeighborsClassifier()),
              # (""Random Forests with %d estimators"" % best_n_estimators,
              #  RandomForestClassifier())
              ]

    benchmark(models, X, y)",tests/models_benchmark.py,Ambiruptor/Ambiruptor,1
"from sklearn.ensemble import RandomForestClassifier

from ..Classifier import Classifier
from ...language.JavaScript import JavaScript as JS


class RandomForestClassifierJSTest(JS, Classifier, TestCase):

    def setUp(self):
        super(RandomForestClassifierJSTest, self).setUp()
        self.mdl = RandomForestClassifier(n_estimators=100, random_state=0)

    def tearDown(self):
        super(RandomForestClassifierJSTest, self).tearDown()",tests/classifier/RandomForestClassifier/RandomForestClassifierJSTest.py,nok/sklearn-porter,1
"
def generate_value(name, known_list):
    if name not in known_list:
        return 0
    else:
        return known_list.index(name) + 1


if __name__ == ""__main__"":
    # clf = tree.DecisionTreeClassifier()
    # clf = RandomForestClassifier(n_estimators=10)
    clf = svm.SVC()

    train_set = read_from_file(""../../data/hw1/adult.data.txt"")
    clf = clf.fit(train_set[0], train_set[1])
    test_set = read_from_file(""../../data/hw1/adult.test.txt"")

    num = 0
    correct = 0
    for i in clf.predict(test_set[0]):",src/hw1_Classification/Classification.py,MyXOF/Data-Mining-Algorithm,1
"	train_target = np.array(load_datasets('Data/trainLabels.csv'),dtype=np.int);

	test_feature = np.array(load_datasets('Data/test.csv'),dtype=np.float);
	""""""
	svm = svm.SVC(C=1e-10,kernel='linear');
	svm.fit(train_feature,train_target);

	print svm.score(train_feature,train_target);
	""""""
	#knn = neighbors.KNeighborsClassifier(weights='uniform');
	rfc = ensemble.RandomForestClassifier(n_estimators=10);
	#for i in xrange(1,21):
	#knn.fit(train_feature,train_target);
		#knn.n_neighbors = i;
		#rfc.n_estimators = i;
		#result = cross_validation.cross_val_score(rfc,train_feature,train_target,n_jobs=1,cv=10);
		#print ""(%d)mean:%f sd:%f "" % (i,np.mean(result),np.std(result));
	#print knn.score(train_feature,train_target);
	#result = knn.predict(train_feature);
	#print len(map(lambda x,y: x==y,result,train_target));",DataScience/classifier.py,dz1984/Kaggle,1
"        models.append(('GaussianNB', GaussianNB()))
        models.append(('KNeighborsClassifier', KNeighborsClassifier()))
        models.append(('DecisionTreeClassifier', DecisionTreeClassifier()))
        models.append(('LogisticRegression', LogisticRegression()))

        #Bagging and Boosting
        #models.append(('ExtraTreesClassifier', ExtraTreesClassifier(n_estimators=150)))
        models.append(('ExtraTreesClassifier', ExtraTreesClassifier()))
        models.append(('AdaBoostClassifier', AdaBoostClassifier(DecisionTreeClassifier())))
        #models.append(('AdaBoostClassifier', AdaBoostClassifier(DecisionTreeClassifier())))
        models.append(('RandomForestClassifier', RandomForestClassifier()))
        models.append(('GradientBoostingClassifier', GradientBoostingClassifier(n_estimators=150)))

        #Voting
        estimators = []
        estimators.append((""Voting_GradientBoostingClassifier"", GradientBoostingClassifier(n_estimators=150)))
        estimators.append((""Voting_ExtraTreesClassifier"", ExtraTreesClassifier()))
        voting = VotingClassifier(estimators)
        models.append(('Voting(GBC-ET)', voting))
",pymach/evaluate.py,gusseppe/pymach,1
"    fs.fit(X, y)
    New_X = fs.transform(X)
    return New_X

def combine_rfs(rf_a, rf_b):
    rf_a.estimators_ += rf_b.estimators_
    rf_a.n_estimators = len(rf_a.estimators_)
    return rf_a

def RF(n_trees,  seed, train_x, train_y, test_x, test_y):
    clf = RandomForestClassifier(n_estimators=n_trees,
                                  random_state = seed, oob_score=True,n_jobs=-1)
    clf = clf.fit(train_x,train_y)
    oob_error = 1 - clf.oob_score_
    test_error = clf.score(test_x,test_y)
    test_auc = clf.predict_proba(test_x)
    #filename = './tmp1/RF_%d_.pkl'%seed
    #_ = joblib.dump(clf, filename, compress=9)
    return test_error, test_auc
",final20/WeightedMVdis20.py,hongliuuuu/Results_Dis,1
"
X = normalize(X)

k = 4
kf = KFold(n_splits=k)

average_accuracy = 0

for train, test in kf.split(X):
    X_train, X_test, Y_train, Y_test = X[train], X[test], Y.loc[train], Y.loc[test]
    rf = RandomForestClassifier(n_estimators=150, max_features=7, max_depth=1)
    rf.fit(X_train, Y_train)
    Y_Result = rf.predict(X_test)
    prf = precision_recall_fscore_support(Y_test, Y_Result, average='binary')
    average_accuracy += prf[2]

average_accuracy /= k
print ""Average Accuracy using Random Forest Classifier = "", str(average_accuracy)

average_accuracy = 0",src/train_test/cross_validation.py,mohitreddy1996/Gender-Detection-from-Signature,1
"        if labels[i] < 0:        
            labels[i] = 0
    return labels


def cluster_pca_and_plot(featureData,labels):
    X = featureData
    y = labels
    
    
    clf = RandomForestClassifier(n_estimators=25,random_state=0)
    scores = cross_val_score(clf, X, y)    

    print( scores.mean())
    return
    pca=PCA(n_components=4)
    X_new = pca.fit_transform(X)    
    
    #iso = Isomap(n_neighbors=30, n_components=2)
    #iso.fit(X)",energy_models/mike/run_eval.py,nglrt/virtual_energy_sensor,1
"    n_features = X_train.shape[1]   
    print ""XX:"", n_features

    g =   1.0/float((3*n_features))
    print g
 
    clf_lsvm = svm.SVC(kernel='linear', C=10)
    clf_ksvm = svm.SVC(kernel='rbf', C=10, gamma=g )
    clf_lasso = LogisticRegression(C=1000,penalty='l1',random_state=0)
    clf_ridge = LogisticRegression(C=0.01,penalty='l2',random_state=0) 
    clf_rf = RandomForestClassifier(n_estimators=850, max_depth=None, max_features=int(math.sqrt(n_features)), min_samples_split=100, random_state=144, n_jobs=4);
    clf_etree = ExtraTreesClassifier(n_estimators=1000, max_depth=None, max_features=int(math.sqrt(n_features)), min_samples_split=100, random_state=144, n_jobs=4);
    clf_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=""SAMME"", n_estimators=500, random_state=74494, learning_rate=0.8) 
    clf_gboost = GradientBoostingClassifier(n_estimators=200, random_state=74494, learning_rate=0.2) 

    print ""Training.""
    clf_lsvm.fit(X_train, y_train)
    clf_ksvm.fit(X_train, y_train)
    clf_lasso.fit(X_train, y_train)
    clf_ridge.fit(X_train, y_train)",python_scripts_from_net/ensemble.py,sankar-mukherjee/DecMeg2014,1
"	test = map(lambda z: 1 if z>=np.mean(test) else 0, test)

	print(""Estimated survivors ~159"", sum(test))
	print(""Min, max, mean"", min(scores), max(scores), np.mean(scores))

	
	
	
	
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()",CrossValidate.py,timestocome/titanic-windows,1
"#Xn = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
#scores = cross_validation.cross_val_score(knn1, Xn, y, cv=10)
#print 'Normalized K Nearest Neighbors Score = %f +/- %f' % (scores.mean(), scores.std())

#knn2 = KNeighborsClassifier(n_neighbors=3)
#scores = cross_validation.cross_val_score(knn, X, y, cv=10)
#print 'Non-normalized K Nearest Neighbors Score = %f +/- %f' % (scores.mean(), scores.std())

# How good could we do, with say Random Forest
#from sklearn.ensemble import RandomForestClassifier
#rf = RandomForestClassifier(n_estimators=100)
#rf.fit(X_train, y_train)
#scores = cross_validation.cross_val_score(rf, X, y, cv=10)
#print 'Random Forest Score = %f +/- %f' % (scores.mean(), scores.std())

# Now we'll learn a metric on the same dataset and see if we can improve
import sys
sys.path.append('..')
from metric_learn.itml.ItmlAlgorithm import ItmlAlgorithm
from metric_learn.ParameterizedKNeighborsClassifier import ParameterizedKNeighborsClassifier",examples/breast_cancer_comparison.py,johncollins/metric-learn,1
"    #To change the order!! (and in the calling functions)
    return features, accessions, feature_names

'TODO: Set params of model, by user selected/pretuned values (obtained in pipetakss via CV)'
def getClassifier(classifierType):
    if (classifierType == 'SGD'):
        model = SGDClassifier(penalty='elasticnet',class_weight='auto',n_jobs=-1,n_iter=150,l1_ratio =0.2)
    if (classifierType == 'LSVC'):
       model = LinearSVC(class_weight='auto')
    if (classifierType == 'forest'):
        model = RandomForestClassifier(n_jobs=-1, bootstrap=True, n_estimators=350,
                                        min_samples_leaf=1, min_samples_split =2,
                                        oob_score=False,max_features='auto',
                                        criterion='gini')
    if (classifierType == 'SVCrbf'):
       model = SVC(kernel=""rbf"", class_weight=""auto"", cache_size=1400, shrinking=True)
    if (classifierType == 'SVCpoly'):
        model = SVC(kernel=""poly"", class_weight=""auto"", cache_size=1400, shrinking=True)
    return model
",ProFET/feat_extract/Model_trainer.py,ddofer/ProFET,1
"# Generate data
X, y = make_blobs(n_samples=1000, n_features=2, random_state=42,
                  cluster_std=5.0)
X_train, y_train = X[:600], y[:600]
X_valid, y_valid = X[600:800], y[600:800]
X_train_valid, y_train_valid = X[:800], y[:800]
X_test, y_test = X[800:], y[800:]

# Train uncalibrated random forest classifier on whole train and validation
# data and evaluate on test data
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train_valid, y_train_valid)
clf_probs = clf.predict_proba(X_test)
score = log_loss(y_test, clf_probs)

# Train random forest classifier, calibrate on validation data and evaluate
# on test data
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train, y_train)
clf_probs = clf.predict_proba(X_test)",projects/scikit-learn-master/examples/calibration/plot_calibration_multiclass.py,DailyActie/Surrogate-Model,1
"
combined = numpy.sum(predictions, axis=0) / 10
rounded = numpy.round(combined)

print(roc_auc_score(test[""high_income""], rounded))

## 8. Putting it all together ##

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=5, random_state=1, min_samples_leaf=2)
clf.fit(train[columns],train['high_income'])
preds = clf.predict(test[columns])
print(roc_auc_score(test['high_income'],preds))

## 9. Parameter tweaking ##

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=2)",Decision Trees/Introduction to random forests-144.py,vipmunot/Data-Analysis-using-Python,1
"    assert_array_equal(X_transformed_sparse.toarray(), X_transformed.toarray())


def test_parallel_train():
    rng = check_random_state(12321)
    n_samples, n_features = 80, 30
    X_train = rng.randn(n_samples, n_features)
    y_train = rng.randint(0, 2, n_samples)

    clfs = [
        RandomForestClassifier(n_estimators=20, n_jobs=n_jobs,
                               random_state=12345).fit(X_train, y_train)
        for n_jobs in [1, 2, 3, 8, 16, 32]
    ]

    X_test = rng.randn(n_samples, n_features)
    probas = [clf.predict_proba(X_test) for clf in clfs]
    for proba1, proba2 in zip(probas, probas[1:]):
        assert_array_almost_equal(proba1, proba2)
",summary/sumy/sklearn/ensemble/tests/test_forest.py,WangWenjun559/Weiss,1
"    tfidf = IDF(inputCol=hashingTF.getOutputCol(),
                outputCol=""features"")

    if model_type == 'log_reg':
        model = LogisticRegression()
    elif model_type == 'gbt':
        model = GBTClassifier()
    elif model_type == 'naive_bayes':
        model = NaiveBayes()
    elif model_type == 'rf':
        model = RandomForestClassifier()

    return Pipeline(stages=[remover, hashingTF, tfidf,
                                model])


def load_target(key):
    """"""
    Loads precomputed doc2vec predictions for each category.
    """"""",code/feature_blending.py,Nathx/parental_advisory_ml,1
"         ""Decision Tree"", ""Random Forest"", ""MLPClassifier"", ""AdaBoost"",
         ""Naive Bayes""]

classifiers = [
    KNeighborsClassifier(59),
    LinearSVC(),
    SVC(gamma=2, C=1),
    SGDClassifier(loss=""log"", n_iter=10),
    #GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=15),
    RandomForestClassifier(n_estimators=100, max_features='sqrt'),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(learning_rate=0.1),
    GaussianNB()]

# closs_validation accuracy experiments
from sklearn.model_selection import cross_val_score
results = {}
for name, clf in zip(names, classifiers):
    scores = cross_val_score(clf, X_train, y_train, cv=5)",src/py/classifiers_experiment.py,samleoqh/machine-ln,1
"
# load data
sample_names_tr, var_names_tr, x_tr = load_data(TR_TOPFEATS)
y_tr = np.loadtxt(LABELSFILE, dtype=np.int, delimiter='\t')
sample_names_ts, var_names_ts, x_ts = load_data(TS_TOPFEATS)
# load the TS labels if available
if TSLABELSFILE is not None:
    y_ts = np.loadtxt(TSLABELSFILE, dtype=np.int, delimiter='\t')

# prediction
forest = RandomForestClassifier(n_estimators=500, criterion='gini', random_state=0)
forest.fit(x_tr, y_tr)

p_tr = forest.predict(x_tr)
p_ts = forest.predict(x_ts)

prob_tr = forest.predict_proba(x_tr)
prob_ts = forest.predict_proba(x_ts)

print ""MCC on train: %.3f"" % (perf.KCCC_discrete(y_tr, p_tr))",scripts/sklearn_rf_validation_writeperf.py,AleZandona/INF,1
"    assert_equal(linear_clf.score(X_reduced, y), 1.)


def test_parallel_train():
    rng = check_random_state(12321)
    n_samples, n_features = 80, 30
    X_train = rng.randn(n_samples, n_features)
    y_train = rng.randint(0, 2, n_samples)

    clfs = [
        RandomForestClassifier(n_estimators=20, n_jobs=n_jobs,
                               random_state=12345).fit(X_train, y_train)
        for n_jobs in [1, 2, 3, 8, 16, 32]
    ]

    X_test = rng.randn(n_samples, n_features)
    probas = [clf.predict_proba(X_test) for clf in clfs]
    for proba1, proba2 in zip(probas, probas[1:]):
        assert_array_almost_equal(proba1, proba2)
",scikit-learn-c604ac39ad0e5b066d964df3e8f31ba7ebda1e0e/sklearn/ensemble/tests/test_forest.py,RPGOne/Skynet,1
"#eclf1 = VotingClassifier(estimators=[('lr', clf1), ('gb', clf2), ('gnb', clf3)], voting='soft')
#eclf1 = eclf1.fit(train_data, train_label)

#result = eclf1.predict_proba(test_data)

#eclf2 = VotingClassifier(estimators=[('lr', clf1), ('gb', clf2), ('gnb', clf3)], voting='soft')

#clf = ExtraTreesClassifier(n_estimators=10000, verbose=1)
#clf.fit(train_data, train_label)
#result = clf.predict_proba(test_data)
#clf  = RandomForestClassifier(n_estimators=6000, max_depth = 4, verbose=1).fit(train_data, train_label)
#knn = neighbors.KNeighborsClassifier()
#logistic = linear_model.LogisticRegression()
#clf = svm.SVC(probability = True)
#clf = tree.DecisionTreeClassifier()


#print('KNN score: %f' % knn.fit(train_data, train_label).score(valid_data, valid_label))
#result = knn.fit(train_data, train_label).predict_proba(test_data)
#train_data = train_data[0:5000,:]",liao/model.py,NCLAB2016/DF_STEALL_ELECTRIC,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

knn = neighbors.KNeighborsClassifier(weights='distance')
logistic = linear_model.LogisticRegression()
dt = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
ab = AdaBoostClassifier(dt, n_estimators=300)

rf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)
et = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)

models = [knn,logistic,dt,ab,rf,et]
for traincv, testcv in cv:
    for clf in models:
        clf.fit(train_df.ix[traincv,columns],train_df.ix[traincv,'Survived'])
    
        #predict the test set
        y_preds = clf.predict(train_df.ix[testcv,columns])",Titanic: Machine learning from Disaster/titanic.py,Radzell/KagglePractice,1
"            property_list_list.append(property_list)
        dataDescrs_array = np.asarray(property_list_list)
        dataActs_array   = np.array(TL_list)

        for randomseedcounter in range(1,11):
                if self.verbous: 
                    print(""################################"")
                    print(""try to calculate seed %d"" % randomseedcounter)
                X_train,X_test,y_train,y_test = cross_validation.train_test_split(dataDescrs_array,dataActs_array,test_size=.4,random_state=randomseedcounter)
#            try:
                clf_RF     = RandomForestClassifier(n_estimators=100,random_state=randomseedcounter)
                clf_RF     = clf_RF.fit(X_train,y_train)

                cv_counter = 5

                scores = cross_validation.cross_val_score( clf_RF, X_test,y_test, cv=cv_counter,scoring='accuracy')

                accuracy_CV = round(scores.mean(),3)
                accuracy_std_CV = round(scores.std(),3)
   ",Contrib/pzc/p_con.py,soerendip42/rdkit,1
"from text.sentence import Sentence

pp = pprint.PrettyPrinter(indent=4)
text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(7,20), min_df=0.2, max_df=0.5)),
                             #('vect', CountVectorizer(analyzer='word', ngram_range=(1,5), stop_words=""english"", min_df=0.1)),
                             #     ('tfidf', TfidfTransformer(use_idf=True, norm=""l2"")),
                                  #('tfidf', TfidfVectorizer(analyzer='char_wb', ngram_range=(6,20))),
                                  #('clf', SGDClassifier(loss='hinge', penalty='l1', alpha=0.01, n_iter=5, random_state=42)),
                                  #('clf', SGDClassifier())
                                  #('clf', svm.SVC(kernel='rbf', C=10, verbose=True, tol=1e-5))
                                  #('clf', RandomForestClassifier(n_estimators=10))
                                    #('feature_selection', feature_selection.SelectFromModel(LinearSVC(penalty=""l1""))),
                                  ('clf', MultinomialNB(alpha=0.1, fit_prior=False))
                                  #('clf', DummyClassifier(strategy=""constant"", constant=True))
                                 ])
class SeeDevCorpus(Corpus):
    """"""
    Corpus for the BioNLP SeeDev task
    self.path is the base directory of the files of this corpus.
    """"""",src/reader/seedev_corpus.py,AndreLamurias/IBRel,1
"
# Generate the features vector
num_features = df_features.shape[1]
features = df.columns[1:num_features]

# Split the data into training and test sets
df['is_train'] = np.random.uniform(0, 1, len(df)) <= .5 
train, test = df[df['is_train']==True], df[df['is_train']==False]

# Set up the Random Forest classifier
clf = RandomForestClassifier(n_jobs=2, n_estimators = 1000)
# Turn the lossimpacting column into a factor (categorical variable)
y, _ = pd.factorize(train['lossimpacting'])
# Train the model
clf.fit(train[features], y)

# Check the accuracy using the test data
preds = clf.predict(test[features])
y_tst = test['lossimpacting']
print pd.crosstab(y_tst, preds, rownames=['actual'], colnames=['preds'])",model/build_model.py,impactlab/jps-handoff,1
"

names = [""Nearest Neighbors"", ""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	KNeighborsClassifier(25),
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]


def main():

	#set the timer
	start = time.time()",kpcaWithTreeFS/mnistBackRandom/classifiers.py,akhilpm/Masters-Project,1
"    def _classify(self, test_X):
        return self._forest.predict(test_X)



from . import test_classifier, BaggingClassifier, SMOTEClassifier, SVMClassifier, KNNClassifier
##
#for bags in (3, 5, 10):
#    for n in (5, 10, 25):
#        test_classifier(
#            BaggingClassifier(bags, lambda: RandomForestClassifier(n, n_jobs=4)),
#            unfold=false)
#test_classifier(
#    BaggingClassifier(10,
#        lambda: RandomForestClassifier(128, class_weight=""balanced""),
#        BalancedClassifier=(lambda x: x)),
#    folds=10,
#    num_samples=5000,
#    unfold=False)
",classifier/RandomForestClassifier.py,kosigz/DiabetesPatientReadmissionClassifier,1
"

class TestRandomForestClassifierParity(TestCase, JPMMLClassificationTest):

    @classmethod
    def setUpClass(cls):
        if JPMMLTest.can_run():
            JPMMLTest.init_jpmml()

    def setUp(self):
        self.model = RandomForestClassifier(
            n_estimators=3,
            max_depth=3
        )
        self.init_data()
        self.converter = RandomForestClassifierConverter(
            estimator=self.model,
            context=self.ctx
        )
",sklearn_pmml/convert/test/test_randomForestConverter.py,YuHuaCheng/sklearn-pmml,1
"    training_label = [arr for idx_arr, arr in enumerate(label_bal)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = np.ravel(label_binarize(
        np.hstack(training_label).astype(int), [0, 255]))
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    crf_cv.append(crf.fit(training_data, training_label))

percentiles = [1., 2., 5., 10., 15., 20., 30.]

results_p = []
feat_imp_p = []
for p in percentiles:

    print 'Computing for percentile: {}'.format(p)",pipeline/feature-classification/exp-3/selection-extraction/rf/pipeline_classifier_adc.py,I2Cvb/mp-mri-prostate,1
"	if os.path.getsize(filepath) == 0:
		filename = os.path.basename(filepath)
		df_full = df_full[df_full.file != filename]
		if filename in test_files:
			print(""Found empty file in submission: "", filename)



#https://www.youtube.com/watch?v=0GrciaGYzV0
print('--- Training random forest')
clf = RandomForestClassifier(n_estimators=150, n_jobs=-1, random_state=0)
train_data = df_full[df_full.sponsored.notnull()].fillna(0)
test = df_full[df_full.sponsored.isnull() & df_full.file.isin(test_files)].fillna(0)
clf.fit(train_data.drop(['file', 'sponsored'], 1), train_data.sponsored)

#normalized value between 0 and 1
feature_importances = pd.Series(clf.feature_importances_, index=train_data.drop(['file', 'sponsored'], 1).columns)
feature_importances.sort()
print(feature_importances)
",scikit_generate_prediction.py,carlsonp/kaggle-TrulyNative,1
"    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.NumpyDataset(X, y, w, ids)
    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

    sklearn_model = RandomForestClassifier()
    model_dir = tempfile.mkdtemp()
    model = dc.models.SklearnModel(sklearn_model, model_dir)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Load trained model
    reloaded_model = dc.models.SklearnModel(None, model_dir)",deepchem/models/tests/test_reload.py,peastman/deepchem,1
"
    if do_score:
        clf = ExtraTreesClassifier(random_state=RANDOM_STATE, n_estimators=n_estimators)
        score = get_score(clf, X, y.values.ravel(), cv=cv, n_runs=n_runs, verbose=True,
            # scoring='f1'
            scoring='accuracy'
            )
        print('score=%f' % score)

    clf = ExtraTreesClassifier(random_state=RANDOM_STATE, n_estimators=n_estimators)
    # clf = RandomForestClassifier()
    clf.fit(X, y.values.ravel())
    y_self = clf.predict(X)
    y_self = DataFrame(y_self, columns=['hat'], index=X.index)
    n = len(y)
    m = sum(y['hat'])
    s = sum(y_self['hat'])
    assert n == len(y_self)

    print('****', n, m, s, m / n, s / n)",explore.py,peterwilliams97/prince,1
"    pairwise_feats, labels = random_case_set()
    X = pairwise_feats
    y = labels
    X_train, y_train = X[:600], y[:600]
    X_valid, y_valid = X[600:800], y[600:800]
    X_train_valid, y_train_valid = X[:800], y[:800]
    X_test, y_test = X[800:], y[800:]

    # Train uncalibrated random forest classifier on whole train and validation
    # data and evaluate on test data
    clf = RandomForestClassifier(n_estimators=25)
    clf.fit(X_train_valid, y_train_valid)
    clf_probs = clf.predict_proba(X_test)
    score = log_loss(y_test, clf_probs)
    print('score = %r' % (score,))

    # Train random forest classifier, calibrate on validation data and evaluate
    # on test data
    clf = RandomForestClassifier(n_estimators=25)
    clf.fit(X_train, y_train)",ibeis/algo/hots/testem.py,SU-ECE-17-7/ibeis,1
"		yt = sd.LD.data['Y_train'][:split]
		xv = sd.LD.data['X_train'][split:]
		yv_raw = sd.yt_raw[split:] 
		
		if model != 'raw_data':
			model.fit(xt, yt)
			xt = model.transform(xt)
			xv = model.transform(xv)
		del model
		
		test_model = ensemble.RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)
		
		if psutil.phymem_usage()[2] > 50:
			time.sleep(10)
		if psutil.phymem_usage()[2] > 85:
			destroy = this_worker
			
		test_model.fit(xt, yt)
		
		preds = test_model.predict_proba(xv)",lib/engine_preprocess.py,djajetic/AutoML3,1
"model = neighbors.KNeighborsClassifier()
classifier = OneVsRestClassifier(model)
y_score = classifier.fit(X_train, y_train).decision_function(X_test)
# Compute Precision-Recall and plot curve
precision[3], recall[3], _ = precision_recall_curve(y_test, y_score)
average_precision[3] = average_precision_score(y_test, y_score)



# now do random forrest
model = ensemble.RandomForestClassifier()
classifier = OneVsRestClassifier(model)
y_score = classifier.fit(X_train, y_train).decision_function(X_test)
# Compute Precision-Recall and plot curve
precision[4], recall[4], _ = precision_recall_curve(y_test, y_score)
average_precision[4] = average_precision_score(y_test, y_score)



# now do decision trees",precision_recall.py,dolphyin/cs194-16-data_manatees,1
"plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.title('Decision Tree Classification')
plt.show()
export_graphviz(tree, out_file = 'Tree-Illustration.dot', feature_names = ['petal length', 'petal width'])

# Random Forest classification illustration - combines multiple decision tree to build a strong learner
# from a group of weak learners.

forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2)
forest.fit(X_train, y_train)
plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105,150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.title('Random Forest Classification')
plt.legend(loc='upper left')
plt.show()

# Finally K-Nearest Neighbour classification",ClassifiersTour.py,petritn/MachineLearning,1
"    :return: Three elements tuple respectively method used (String), best accuracy score on parameters grid in 5-folds 
    CV (float), accuracy score on test set
    """"""
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import GridSearchCV
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score
    method = ""random_forest""
    scaler = StandardScaler()
    scaled_feats_train = scaler.fit_transform(training_set_features)
    svr = RandomForestClassifier(random_state=0)
    parameters = {'max_depth': range(1, 51), 'min_samples_split': range(2, 11)}
    clf = GridSearchCV(svr, parameters, cv=5, scoring='accuracy')
    clf.fit(scaled_feats_train, training_set_labels)
    scaled_feats_test = scaler.transform(testing_set_features)
    predicted_lab_test = clf.predict(scaled_feats_test)
    best_score = clf.best_score_
    test_score = accuracy_score(testing_set_labels, predicted_lab_test, normalize=True)
    return method, best_score, test_score
",problem3.py,swoldetsadick/MachineLearning,1
"        else:
            model_select_ini = MLPClassifier()
            model_select_best = MLPClassifier()
            
    # RandomForest -----------------------------------------------------------------------------------
    if model[""algorithm""] == ""RandomForest"":    
        if model[""type""] == ""Regressor"":
            model_select_ini = ensemble.RandomForestRegressor()
            model_select_best = ensemble.RandomForestRegressor()
        else:
            model_select_ini = ensemble.RandomForestClassifier()
            model_select_best = ensemble.RandomForestClassifier()
            
    # SupportVectorMachines --------------------------------------------------------------------------
    if model[""algorithm""] == ""SupportVectorMachines"":    
        if model[""type""] == ""Regressor"":
            model_select_ini = SVR()
            model_select_best = SVR()
        else:
            model_select_ini = SVC()",framework/workflow/model_loader.py,kvistrup/mlfp,1
"# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division

import sklearn.ensemble

import submissions
from data import *

rf = sklearn.ensemble.RandomForestClassifier(n_estimators=300, oob_score=True, n_jobs=2)
rf.fit(train, target)
pred = rf.predict_proba(test)

submissions.save_csv(pred, ""random_forest.csv"")",airbnb_recruiting_new_user_bookings/random_forest.py,wjfwzzc/Kaggle_Script,1
"    scaler = StandardScaler()
    train_x = scaler.fit_transform(train_x)
    es = n_estimators
    fs = max_feature
    if n_estimators is None:
        es = [500, 100]
    if max_feature is None:
        fs = [10, 20, 30, 40, 50, 60, 70, 80, 90, 99]
    if importance_level is None:
        importance_level = 0.05
    rf = RandomForestClassifier()
    clf = GridSearchCV(
        estimator=rf, param_grid=dict(n_estimators=es, max_features=fs))
    clf.fit(train_x, train_y)
    clf = clf.best_estimator_
    imp = clf.feature_importances_
    important_feature = []
    names = test_x.columns
    for i in range(len(imp)):
        sorted_imp = sorted(zip(imp, names))",mousestyles/classification/classification.py,berkeley-stat222/mousestyles,1
"    
        # ASSESSING GINI INDEX FOR EACH INVIVIDUAL IN THE INITIAL POOL 
                
        X_train=data[var_model]
        Y_train=data[output_var]
    
        ######
        # CHANGE_HERE - START: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.
        #####             
        '''        
        rf = RandomForestClassifier(n_estimators=100, random_state=50)                
        rf1 = rf.fit(X_train, Y_train)
        Y_predict = rf1.predict_proba(X_train)
        Y_predict  = Y_predict[:,1]
        '''
        Fin_model = linear_model.LinearRegression()
        Fin_model.fit(X_train, Y_train)
        Y_predict = Fin_model.predict(X_train)
        ######
        # CHANGE_HERE - END: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.",manoelgadi12/__init__.py,ersh24/manoelgadi12,1
"    ss = StandardScaler()
    data['train_X'] = ss.fit_transform(data['train_X'])
    data['test_X'] = ss.transform(data['test_X'])


#    from sklearn.neighbors import KNeighborsClassifier
#    clf = KNeighborsClassifier(weights='uniform', n_neighbors=5)


    from sklearn.ensemble import RandomForestClassifier
    clf = RandomForestClassifier(random_state=1, n_estimators=10, n_jobs=1)
    rfc = RandomForestClassifier(random_state=1, n_jobs=3)

    #from sklearn.ensemble import GradientBoostingClassifier
    #clf = GradientBoostingClassifier(n_estimators=10)
    #from sklearn.ensemble import AdaBoostClassifier
    #clf = AdaBoostClassifier(rfc, n_estimators=30, random_state=1)
    #from sklearn.ensemble import ExtraTreesClassifier
    #clf = ExtraTreesClassifier(n_jobs=3, n_estimators=50, random_state=1)
",src/toycode.py,WojciechMigda/KAGGLE-prudential-life-insurance-assessment,1
"		""AdaBoost"",
		#""NaiveBayes"",
		#""LDA"",
		#""QDA""]
	]	
classifiers = [
		#KNeighborsClassifier(5),
		#SVC(kernel=""linear"", C=0.025),
		#SVC(gamma=2, C=1),
		DecisionTreeClassifier(max_features=""sqrt""),
		RandomForestClassifier(n_estimators=20,max_features=""sqrt""),
		AdaBoostClassifier(),
		#GaussianNB(),
		#LDA(),
		#QDA()]
		]

data = np.loadtxt(sys.argv[1],dtype=np.int32)
# load the data
#print data.shape",ErrorCorrectionClassifier.py,raunaq-m/MultiRes,1
"from splearn.ensemble import SparkRandomForestClassifier
from splearn.utils.testing import SplearnTestCase
from splearn.utils.validation import check_rdd_dtype


class TestSparkRandomForest(SplearnTestCase):

    def test_same_predictions(self):
        X, y, Z = self.make_classification(2, 10000)

        local = RandomForestClassifier()
        dist = SparkRandomForestClassifier()

        y_local = local.fit(X, y).predict(X)
        y_dist = dist.fit(Z, classes=np.unique(y)).predict(Z[:, 'X'])
        y_conv = dist.to_scikit().predict(X)

        assert_true(check_rdd_dtype(y_dist, (np.ndarray,)))
        assert(sum(y_local != y_dist.toarray()) < len(y_local) * 5./100.)
        assert(sum(y_local != y_conv) < len(y_local) * 5./100.)",splearn/ensemble/tests/__init__.py,taynaud/sparkit-learn,1
"    train = np.loadtxt(traindata)
    test = np.loadtxt(testdata)
    X = train[0:4628,0:27]
    y = train[0:4628,27]
    test_x = test[0:1437,0:27]
    test_y = test[0:1437,27]

    model1 = LinearSVC()
    model2 = LogisticRegression()
    model3 = GaussianNB()
    model4 = RandomForestClassifier()
    model5 = KNeighborsClassifier()
    model1.fit(X,y)
    model2.fit(X,y)
    model3.fit(X,y)
    model4.fit(X,y)
    model5.fit(X,y)
    predicted1 = model1.predict(test_x)
    predicted2 = model2.predict(test_x)
    predicted3 = model3.predict(test_x)",/svm.py,vimilimiv/weibo-popularity_judge-and-content_optimization,1
"
# We use a random classification data set generated by sklearn
# As commonly done, we use a train-test split to avoid overfitting.
X,Y = sklearn.datasets.make_classification(1000, 20)
X_train, X_test, Y_train, Y_test = \
	sklearn.cross_validation.train_test_split(X,Y, test_size=0.33, random_state=1)


# training a random forest
def random_forest (n_trees=None, criterion=None, max_features=None, max_depth=None):
	predictor = sklearn.ensemble.RandomForestClassifier(n_trees, criterion, max_features, max_depth)
	predictor.fit(X_train, Y_train)
	return (-predictor.score(X_test, Y_test))

# and defining some of its parameters
parameters_trees = dict(\
	max_depth =  (""integer"", [1,10],  4),
	max_features=(""integer"", [1,20], 10),
	n_trees=(""integer"", [1,100],10 ,'log'),          
	criterion =(""categorical"", ['gini', 'entropy'], 'entropy'),",examples/merge_pcs.py,automl/pysmac,1
"    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))

    dataset = dc.data.NumpyDataset(X, y, w, ids)
    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

    sklearn_model = RandomForestClassifier()
    model_dir = tempfile.mkdtemp()
    model = dc.models.SklearnModel(sklearn_model, model_dir)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Load trained model
    reloaded_model = dc.models.SklearnModel(None, model_dir)",deepchem/models/tests/test_reload.py,ktaneishi/deepchem,1
"    tfidf = feature_extraction.text.TfidfTransformer()
    train = tfidf.fit_transform(train).toarray()
    test = tfidf.transform(test).toarray()

    # encode labels 
    lbl_enc = preprocessing.LabelEncoder()
    labels = lbl_enc.fit_transform(labels)

    print 'running random forest'
    # train a random forest classifier
    clf = ensemble.RandomForestClassifier(n_jobs=-1, n_estimators=800)
    clf.fit(train, labels)

    print 'predicting test data'
    # predict on test set
    preds = clf.predict_proba(test)

    print 'writing submission'
    # create submission file
    preds = pd.DataFrame(preds, index=sample.id.values, columns=sample.columns[1:])",kaggle-otto/randomforest.py,ryanswanstrom/Sense.io-Projects,1
"        else:
            ytest.loc[idx, 'prediction'] = 0.051167 # pure hack
    print ytest.shape
    ytest.to_csv('submission.csv', index=False)
    return

if __name__ == '__main__':
    xtrain, ytrain, xtest, ytest, yid = load_data(do_plots=False)

#    model = LogisticRegression()
#    model = RandomForestClassifier()
    model = GradientBoostingClassifier()
    
    test_model(model, xtrain, ytrain)

    model.fit(xtrain, ytrain)
    
    prepare_submission(model, xtest, ytest, yid)",my_model.py,ddboline/kaggle_facebook_recruiting_human_or_bot,1
"from sklearn.ensemble import RandomForestClassifier
from techson_server.settings import BASE_DIR

path = BASE_DIR + '/db/dataset/data.csv'

train_data = pd.read_csv(path)

y_train = train_data['label']
x_train = train_data.drop('label', axis=1)

RFC = RandomForestClassifier(n_estimators=200, n_jobs=-1)

RFC.fit(x_train, y_train)

path = BASE_DIR + '/classifiers/random_forest_classifier.pkl'

with open(path, 'wb') as f:
    pickle.dump(RFC, f)",mlscripts/random_forest_classifier.py,KirovVerst/techson_server,1
"def random_forest(dataset, out):
    print('random_forest')
    X = dataset[['x', 'y']]
    y = dataset.label
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)
    best_score = 0
    best_clf = None
    for max_depth in range(1,51):
        for min_sample in range(2,11):
            print('md: ', max_depth, ' ms: ', min_sample)
            clf = RandomForestClassifier(max_depth=max_depth, min_samples_split=min_sample)
            clf.fit(X_train, y_train)
            current_test_score = cross_val_score(clf, X_train, y_train, cv=5).mean()
            if current_test_score > best_score:
                best_score = current_test_score
                best_clf = clf

    print('best score: ' + str(best_score))
    test_score = best_clf.score(X_test, y_test)
    print('test score: ' +  str(test_score))",src/classification_sklearn/problem3_3.py,lucafon/ArtificialIntelligence,1
"   
    def switchModel(self, model, modelStats = modelStats, kNearestK = kNearestK, randomForestEstimators = randomForestEstimators, numberOfLastGames = numberOfLastGames):
        """"""
        #Use: oc.switchModel(m)
        #Pre: m is a valid model
        #Post: The current model has been set to m.
        """"""
        self.estimator = None
        self.trained = False 
        if model == ""randomForest"":
            self.estimator = RandomForestClassifier(n_estimators=self.randomForestEstimators)
        elif model == ""kNeighbors"" :
            self.estimator = KNeighborsClassifier(self.kNearestK)
        elif model == ""gradientBoosting"" :
            self.estimator = GradientBoostingClassifier()
        else:
            self.estimator = None
            self.trained = True
        self.model = model
        self.modelStats = modelStats",Oracle.py,Tritlo/OracleOfHundredAcreWood,1
"        if X_train.shape[0] > X_train.shape[1]:
            dual = False
        else:
            dual = True

        model = OneVsRestClassifier(LogisticRegression(C=svm_hardness, random_state=0, dual=dual,
                                                       fit_intercept=fit_intercept),
                                    n_jobs=number_of_threads)
        model.fit(X_train, y_train)
    elif classifier_type == ""RandomForest"":
        model = OneVsRestClassifier(RandomForestClassifier(n_estimators=1000, criterion=""gini"",
                                                           n_jobs=number_of_threads, random_state=0))
        if issparse(X_train):
            model.fit(X_train.tocsc(), y_train.toarray())
        else:
            model.fit(X_train, y_train.toarray())
    else:
        print(""Invalid classifier type."")
        raise RuntimeError
",reveal_graph_embedding/learning/classification.py,MKLab-ITI/reveal-graph-embedding,1
"X = train[cols].values
y = train['Survived'].values

# To use an SVC, data needs to be scaled between [-1, 1] with a 0 mean.
# Scaling won't negatively affect any of the other classifiers.
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Create the classifiers
clf1 = ExtraTreesClassifier(n_estimators=200, max_depth=None, min_samples_split=1, random_state=0)
clf2 = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=1, random_state=0)
clf3 = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
clf4 = AdaBoostClassifier(n_estimators=500)
clf5 = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, max_depth=1, random_state=0)
clf6 = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.2, kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False)

clfs = [clf1, clf2, clf3, clf4, clf5, clf6]

# Fit each classifier based on the training data
for clf in clfs:",train_and_predict.py,ageitgey/titanic_machine_learning_example,1
"        print(training_params)
        # Overwrite our stock params with what the user passes in (i.e., if the user wants 10,000 trees, we will let them do it)
        model_params.update(training_params)
        print('After overwriting our defaults with your values, here are the final params that will be used to initialize the model:')
        print(model_params)


    model_map = {
        # Classifiers
        'LogisticRegression': LogisticRegression(),
        'RandomForestClassifier': RandomForestClassifier(),
        'RidgeClassifier': RidgeClassifier(),
        'GradientBoostingClassifier': GradientBoostingClassifier(),
        'ExtraTreesClassifier': ExtraTreesClassifier(),
        'AdaBoostClassifier': AdaBoostClassifier(),


        'SGDClassifier': SGDClassifier(),
        'Perceptron': Perceptron(),
        'PassiveAggressiveClassifier': PassiveAggressiveClassifier(),",auto_ml/utils_models.py,ClimbsRocks/auto_ml,1
"                # Numpy arrays are easy to work with, so convert the result to an
                # array
                # train_data_features = train_data_features.toarray()
                # print(train_data_features.shape)

                # ******* Train a random forest using the bag of words
                #
                # print ""Training the random forest (this may take a while)...""

                # Initialize a Random Forest classifier with 100 trees
                forest = RandomForestClassifier(n_estimators=e)

                # Fit the forest to the training set, using the bag of words as
                # features and the sentiment labels as the response variable
                #
                # This may take a few minutes to run
                # forest = forest.fit( train_data_features, train[""dependencia_id""] )

                pipe = pipeline.Pipeline([('vectorizer', vectorizer), ('forest', forest)])
                train_data_features = pipe.named_steps['vectorizer'].fit_transform(clean_train_descripcions).toarray()",BagOfWords.py,mxabierto/sara,1
"

if __name__ == ""__main__"":
    
    iris = datasets.load_iris()
    X = iris.data[:, [2,3]]
    y = iris.target

    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)
          
    forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2)
    forest.fit(X_train, y_train)
    
    X_combined = np.vstack((X_train, X_test))
    y_combined = np.hstack((y_train, y_test))
    
    plot_decision_regions(X_combined, y_combined, classifier = forest)

    plt.xlabel('petal length [cm]')
    plt.ylabel('petal width [cm]')",misc/random_forest.py,vsmolyakov/ml,1
"    pred_LR = resultLR.predict(dfaux) 
    gini_LR = 2*roc_auc_score(df[objetivo], pred_LR)-1
    gini.append(gini_LR)
    
    # Random Forest
    metodo.append('Random Forest')
                 
    #Hora de inicio.
    start_time = time.time()
    
    model= RandomForestClassifier(n_estimators=1000, max_depth=60, n_jobs=2 )
    resultRF = model.fit(dfaux, df[objetivo])
    
    # Tiempo transcurrido en la ejecucin
    tiempo = time.time() - start_time  
    tiempos.append(tiempo)
    
    pred_RF = resultRF.predict(dfaux)
    gini_RF = 2*roc_auc_score(df[objetivo], pred_RF)-1
    gini.append(gini_RF)",prusontchm/modelSelection.py,ciffcesarhernandez/AF5_PRACTICA3,1
"from sklearn.qda import QDA


names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"",""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]
    
    
 #read training features,training label and testing set   
i_filename = raw_input(""Your testing features file name? "")
threshold = input(""set your threshold for classification (<1) : "")",scikit_code/multi_classifier_plot.py,chakpongchung/RIPS_2014_BGI_source_code,1
"from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier

def loadData():
    iris = load_digits()
    X = iris.data
    y = iris.target
    return X, y

def createClassifier():
    clf = RandomForestClassifier(n_estimators=20)
    return clf

def report(grid_scores, n_top=3):
    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]
    for i, score in enumerate(top_scores):
        print(""Model with rank: {0}"".format(i + 1))
        print(""Mean validation score: {0: .3f} (std: {1: .3f})"".format(\
              score.mean_validation_score, \
              np.std(score.cv_validation_scores)))",examples/scikit-learn/examples/general/comparing_randomized_search_and_grid_search_for_hyperparameter_estimation.py,KellyChan/Python,1
"                    encode_time_test = time() - start
                    
                    fout.write(""%s;%s;%s;%s;%s;%s;%s;%s\n""%(part, dataset_name, method_name, 0, hmm_n_states, nr_events, ""encode_time_train"", encode_time_train))
                    fout.write(""%s;%s;%s;%s;%s;%s;%s;%s\n""%(part, dataset_name, method_name, 0, hmm_n_states, nr_events, ""encode_time_test"", encode_time_test))
                    
                    for rf_max_features in rf_max_featuress:
                        print(""Training RF with max features = %s...""%rf_max_features)
                        sys.stdout.flush()

                        #### FIT CLASSIFIER ####
                        cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=rf_max_features, random_state=random_state)
                        start = time()
                        cls.fit(dt_train, train_y)
                        cls_fit_time = time() - start


                        #### PREDICT ####
                        start = time()
                        if len(cls.classes_) == 1:
                            hardcoded_prediction = 1 if cls.classes_[0] == pos_label else 0",experiments_param_optim_cv/run_index_lossy_optim_cv.py,irhete/predictive-monitoring-benchmark,1
"            # ""Naive Bayes"", 
            # ""LDA"", 
            # ""QDA""
        ]

        self.classifiers = [
            KNeighborsClassifier(2),
            # SVC(kernel=""linear"", C=0.025),
            # SVC(gamma=2, C=1),
            DecisionTreeClassifier(),#max_depth=5
            RandomForestClassifier(),#max_depth=5, n_estimators=10, max_features=1
            # AdaBoostClassifier(),
            # GaussianNB(),
            # LDA(),
            # QDA()
        ]

        for name, clf in zip(names, self.classifiers):

            clf = clf.fit(train_x, train_y)",Classifier.py,fahadsultan/CausalRelations,1
"                            
                            t0 = time()
                            
                            for i in range(0, n_trials):
                                
                                X1_train, X1_rest, y1_train, y1_rest = train_test_split(X, y1, train_size = train_size)
                                X2_train, X2_rest, y2_train, y2_rest = train_test_split(X, y2, train_size = train_size)
                                X1_waste, X1_test, y1_waste, y1_test = train_test_split(X1_rest, y1_rest, test_size = test_size/(1.001-train_size))
                                X2_waste, X2_test, y2_waste, y2_test = train_test_split(X2_rest, y2_rest, test_size = test_size/(1.001-train_size))
                                
                                clf1 = RandomForestClassifier(n_estimators = n_est,
                                                              criterion = criterion,
                                                              max_features = m_f,
                                                              max_depth = m_d,
                                                              min_samples_split = m_s_s,
                                                              min_samples_leaf = m_s_l
                                                              )
                                clf2 = RandomForestClassifier(n_estimators = n_est,
                                                              criterion = criterion,
                                                              max_features = m_f,",data/kyle/random_forest.py,isabellewei/deephealth,1
"        
    train_y = le.transform(train_y)
    test_y = le.transform(rawTest_y)    
    
    return train_x, train_y, test_x, test_y


def selectFeatureSet_RF(data_x, data_y, nFeatures):
    """"""Use Random Forest to find the best numFeatures of features, based on the given data_x.""""""

    rf_filter = RandomForestClassifier(max_features = 'auto')
    rf_filter.fit(data_x, data_y);
    rankings = rf_filter.feature_importances_;
    selectedBool = np.argsort(rankings)[-nFeatures:]
#    selectedBool = sorted(range(len(rankings)), key = lambda x: rankings[x])[-nFeatures:];
    return data_x.columns[selectedBool]    



def evalFeatureSet(train_x, train_y, test_x, test_y, selectedFeatures, classifier):",feature_selection.py,cocoaaa/ml_gesture,1
"import pandas as pd
import sklearn.ensemble as en
import sklearn.tree as sk

@hlp.timeit
def fitRandomForest(data):
    '''
        Build a random forest classifier
    '''
    # create the classifier object
    forest = en.RandomForestClassifier(n_jobs=-1, 
        min_samples_split=100, n_estimators=10, 
        class_weight=""auto"")

    # fit the data
    return forest.fit(data[0],data[1])

# the file name of the dataset
r_filename = '../../Data/Chapter03/bank_contacts.csv'
",Codes/Chapter03/classification_randomForest.py,drabastomek/practicalDataAnalysisCookbook,1
"#feature reduction (on HOG part)
gain, j = mutual_info_classif(data[:, 8:-1], data[:, -1], discrete_features='auto', n_neighbors=3, copy=True, random_state=None), 0
for i in np.arange(len(gain)):
	if gain[i] <= 0.001:
		data = np.delete(data, 8+i-j, 1)
		j += 1

X_train, X_test, y_train, y_test = train_test_split(data[:, 0:-1], data[:, -1], test_size = 0.4, random_state = 0)

start = timer()
clf = RandomForestClassifier(n_estimators=30).fit(X_train, y_train)
y_pred = clf.predict(X_test)
end = timer()

print(""Confusion Matrix: \n"")
print(confusion_matrix(y_test, y_pred))

target_names = ['Helmet', 'No Helmet']
print(""\n\nClassification Report: \n"")
print(""Accuracy: %s"" % round(accuracy_score(y_test, y_pred), 4))",Holdout/Random_Forests.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"X_test = preprocesser.transform(X_test)

if plot:
  pca = PCA(n_components=2)
  pca.fit(X_train)
  X_train = pca.transform(X_train)
  X_test = pca.transform(X_test)


# Create and fit a decision tree
tree = RandomForestClassifier(n_estimators=10, max_depth=10)
tree.fit(X_train, y_train)
y_test_pred = tree.predict(X_test)
score = np.mean(y_test == y_test_pred)
print score
print confusion_matrix(y_test, y_test_pred) 

if plot:
  plotDecisionBoundary(tree, X_test, y_test)
",kaggle/default-of-credit-card/default-of-credit-card-randomforest.py,jajoe/machine_learning,1
"    Icard = Icard + 52 * np.arange(N).reshape(N, 1)
    Xcard = np.zeros((N, 52), dtype='int')
    Xcard.flat[Icard] +=1
    return Xcard

Xcard = imbed52(Xperms)
Xcard_test = imbed52(Xtest)


# RAndom Forest classifier
rfc = RandomForestClassifier()
# print(""Random Forest CV score: {0}"".format(cross_val_score(rfc, Xperms, yperms)))
rfc52 = RandomForestClassifier()
N = Xtrain.shape[0]
rfc52.fit(Xcard[:N], yperms[:N])
print(""Random Forest CV score: {0}"".format(cross_val_score(rfc52, Xcard, yperms)))

ytest = rfc52.predict(Xcard_test)

",poker/random_forest.py,wgm2111/wgm-kaggle,1
"                                    timesteps)

            # ------------------------------------------------------------
            # compute new object features
            objectFeatures = self._computeObjectFeatures(timesteps)

            # ------------------------------------------------------------
            # load transition classifier if any
            if transition_classifier_filename is not None:
                getLogger().info(""\tLoading transition classifier"")
                transitionClassifier = probabilitygenerator.RandomForestClassifier(
                    transition_classifier_path, transition_classifier_filename)
            else:
                getLogger().info(""\tUsing distance based transition energies"")
                transitionClassifier = None

            # ------------------------------------------------------------
            # run min-cost max-flow to find merger assignments
            getLogger().info(""Running min-cost max-flow to find resolved merger assignments"")
",hytra/core/mergerresolver.py,chaubold/hytra,1
"	# # y = dataset[:,60]
	# x_ =x[0:10000,:]
	# # y = dataset[0:10000,60]
	# y_ = y[0:10000]
	# x_test = x[11001:12001,:]
	# # y_test = dataset[1001:1201, -1].astype(int)
	# y_test = y[11001:12001]
	# print y_test
	# create a base classifier used to evaluate a subset of attributes
	print ""starting""
	pca = PCA()#n_components = 2)#DecisionTreeRegressor() #RandomForestClassifier() #ExtraTreesClassifier()
	X_reduced = pca.fit_transform(scale(X_train))
	model = RandomForestRegressor() #ExtraTreesClassifier()
	model.fit(scale(X_reduced), y_train)
	print (model.score(scale(X_test),y_test))
	y_predict = model.predict(scale(X_test))
	df = pd.DataFrame(y_predict)
	path = 'data/results_RF_PCA.csv'
	# print (model.explained_variance_ratio_)
	print ""done""",src/main/python/RFwithPCA.py,nelango/ViralityAnalysis,1
"plt.plot(k_feat,sbs.scores_,marker='o')
plt.ylim([0.7,1.1])
plt.ylabel('Accuracy')
plt.xlabel('Number of features')
plt.grid()
plt.show()

#3 assessing feature importance with random forests
from sklearn.ensemble import RandomForestClassifier
feat_labels = df_wine.columns[1:]
forest = RandomForestClassifier(n_estimators = 10000,random_state = 0,n_jobs = -1)
forest.fit(X_train,y_train)
importances = forest.feature_importances_
indices = np.argsort(importances)[::-1]
for f in range(X_train.shape[1]):
    print(""%2d) %-*s %f"" % (f + 1, 30,feat_labels[f],importances[indices[f]]))

plt.title('Feature Importances')
plt.bar(range(X_train.shape[1]),
importances[indices],",0-Data Preprocessing/data_preprocessing/data_preprocessing.py,PhenixI/machine-learning,1
"from sklearn.ensemble import RandomForestClassifier


def classify(trainData,trainTarget):
  model = RandomForestClassifier(n_estimators = 30, random_state = 84)
  model.fit(trainData, trainTarget)
  return model
",code/rf_classify.py,SudharshanaSL/crime-data-analysis,1
"

@scope.define
def sklearn_KNeighborsClassifier(*args, **kwargs):
    star_star_kwargs = kwargs.pop('starstar_kwargs')
    kwargs.update(star_star_kwargs)
    return sklearn.neighbors.KNeighborsClassifier(*args, **kwargs)


@scope.define
def sklearn_RandomForestClassifier(*args, **kwargs):
    return sklearn.ensemble.RandomForestClassifier(*args, **kwargs)


@scope.define
def sklearn_ExtraTreesClassifier(*args, **kwargs):
    return sklearn.ensemble.ExtraTreesClassifier(*args, **kwargs)

@scope.define
def sklearn_RandomForestRegressor(*args, **kwargs):",autokit/hpsklearn/components.py,tadejs/autokit,1
"# Define Data
df = pd.read_csv('data/gss2014.csv', index_col=0)
outcome  = 'partyid_str_rep'
features = ['age', 'sex', 'race', 'educ', 'rincome']

# Set, X, y, Train-Test Split
X, y = df[features], df[outcome]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Build Model and Fit
clf = RandomForestClassifier(n_estimators=10, max_depth=None,
    min_samples_split=2, random_state=0)
y_pred = clf.fit(X_train, y_train).predict_proba(X_test)[:,1]

# Calculate Result
result = metrics.roc_auc_score(y_test, y_pred)
print(result)",Machine_Learning/basic_example.py,jmausolf/Python_Tutorials,1
"    fs.fit(X, y)
    New_X = fs.transform(X)
    return New_X

def combine_rfs(rf_a, rf_b):
    rf_a.estimators_ += rf_b.estimators_
    rf_a.n_estimators = len(rf_a.estimators_)
    return rf_a

def RF(n_trees,  seed, train_x, train_y, test_x, test_y):
    clf = RandomForestClassifier(n_estimators=n_trees,
                                  random_state = seed, oob_score=True,n_jobs=1)
    clf = clf.fit(train_x,train_y)
    oob_error = 1 - clf.oob_score_
    test_error = clf.score(test_x,test_y)
    test_auc = clf.predict_proba(test_x)
    #filename = './tmp1/RF_%d_.pkl'%seed
    #_ = joblib.dump(clf, filename, compress=9)
    return test_error, test_auc
",final7/WeightedMVdis7.py,hongliuuuu/Results_Dis,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/no_test_instances )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X,Y, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 100, n_jobs = 12, random_state = 4)
rf.fit(X,Y)
result_rf = rf.predict(Z)
#print(Z[70])
#print('X', len(X),len(Y),len(X1[train_size:dataset_size]))
#print('RF prediction : ---> ',result_rf )
#print('actual ans: -->',test_class)
CM = confusion_matrix(test_class,result_rf) 
print(""Confusion Matrix : "")
print(CM)",sandbox/petsc/solvers/scripts/ScikitClassifiersRS2CVSolverList.py,LighthouseHPC/lighthouse,1
"
    X, y, X_submission = load()

    if shuffle:
        idx = np.random.permutation(y.size)
        X = X[idx]
        y = y[idx]

    skf = list(StratifiedKFold(y, n_folds))

    clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]

    print ""Creating train and test sets for blending.""

    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))
    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))",8.BoostingTree_AND_EnsemblyMethod/blend.py,mahatmaWM/statistic_learning_method_by_lihang,1
"    print ""sigmoid SVM score: "", clf.score(Xtrain, ytrain)

    clf = svm.SVC(kernel = 'poly', C=1e6)
    clf.fit(Xtrain, ytrain)
    print ""poly SVM score: "", clf.score(Xtrain, ytrain)

    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    clf.fit(Xtrain, ytrain)
    print ""DecisionTreeClassifier score: "", clf.score(Xtrain, ytrain)

    clf = RandomForestClassifier(n_estimators=10)
    clf.fit(Xtrain, ytrain)
    print ""RandomForestClassifier score: "", clf.score(Xtrain, ytrain)

    clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)
    clf.fit(Xtrain, ytrain)
    print ""ExtraTreesClassifier score: "", clf.score(Xtrain, ytrain)

    clf = AdaBoostClassifier(n_estimators=100)
    clf.fit(Xtrain, ytrain)",code only split/InstrumentRecognition/InstrumentRecognition.py,tian-zhou/Surgical-Instrument-Dataset,1
"np.random.seed(123)

# Load Tox21 dataset
tox21_tasks, tox21_datasets, transformers = load_tox21()
(train_dataset, valid_dataset, test_dataset) = tox21_datasets

# Fit models
metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean)

def model_builder(model_dir):
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=500)
  return dc.models.SklearnModel(sklearn_model, model_dir)
model = dc.models.SingletaskToMultitask(tox21_tasks, model_builder)

# Fit trained model
print(""About to fit model"")
model.fit(train_dataset)
model.save()
",examples/tox21/tox21_sklearn_models.py,deepchem/deepchem,1
"            ytest_rf = ytest_rf[0, :]
        else:
            Xtest_rf, ytest_rf = dh.part_features_for_rf(all_image_infos, all_segmentaion_infos, cub_parts, IDtest, Parts.HEAD_PART_NAMES)

            rf_safe.save_large_instance(Xtest_rf_ip, Xtest_rf, instance_split)
            rf_safe.save_instance(ytest_rf_ip, ytest_rf)
    toc = time()
    print 'loaded or calculated in', toc - tic

    tic = time()
    model_rf = sklearn.ensemble.RandomForestClassifier(n_estimators=10, bootstrap=False, max_depth=10, n_jobs=3, random_state=None, verbose=0)
    model_rf.fit(Xtrain_rf, ytrain_rf)
    toc = time()
    print 'fitted rf model in', toc - tic

    dense_points = gen_dense_points(227, 227)

    # load whole and bbox and head part data
    # load data
    tic = time()",src/scripts/rf_experiment.py,yassersouri/omgh,1
"		y_train = Y_train_full[train_mask] >= n
		
		test_mask = test_deceased | (Y_test_full >= n)
		x_test = X_test_full[test_mask]
		y_test = Y_test_full[test_mask] >= n
		
		lr = sklearn.linear_model.LogisticRegression()
		lr.fit(x_train, y_train)
		pred = lr.predict(x_test)

		rf = sklearn.ensemble.RandomForestClassifier(n_estimators = 150)
		rf.fit(x_train, y_train)
		rf_pred = rf.predict(x_test)
		rf_prob = rf.predict_proba(x_test)

		svm1 = sklearn.svm.SVC(probability = True, kernel = 'linear', C = 1)
		svm1.fit(x_train, y_train)
		svm_pred = svm1.predict(x_test)

		svm2 = sklearn.svm.SVC(probability = True, kernel = 'linear', C = 10)",models.py,iskandr/brainmets,1
"    train_indices, valid_indices = next(iter(kf))

    train_data = clean_tweets_features[train_indices]
    train_labels = clean_tweets_sentiments[train_indices]
    valid_data = clean_tweets_features[valid_indices]
    valid_labels = clean_tweets_sentiments[valid_indices]

    # only retrain the model when requested, to save time
    if retrain:
        print('training random forest...')
        forest = RandomForestClassifier(n_estimators=100)
        t0 = time()
        forest = forest.fit(train_data, train_labels)
        print('training completed in %.2f' % (time() - t0))
        print('saving model to %s' % (modelfile))
        with open(modelfile, 'wb') as mfile, open(vectorfile, 'wb') as vfile:
            pickle.dump(forest, mfile)
            pickle.dump(vectorizer, vfile)
    else:
        print('loading random forest...')",learn_tweets.py,hjweide/rpi-datathon-2015,1
"
(train_data, val_data) = train_test_split(train)

X_tr = train_data.values[:, 3:]
y_tr = list(train_data.values[:, 2])
X_val = val_data.values[:, 3:]
y_val = list(val_data.values[:, 2])

import code; code.interact(local=dict(globals(), **locals()))

recognizer = RandomForestClassifier(NUM_TREES, max_depth=TREE_DEPTH, verbose=1, n_jobs=NUM_THREADS)
recognizer.fit(X_tr, y_tr)

prediction = recognizer.predict_proba(X_val)

if (os.name == 'nt'):
    ll = log_loss(y_val.tolist(), prediction)
else:
    ll = log_loss(y_val, prediction)
",classify-random-forrest.py,natesholland/talking-data-kaggle,1
"predicted = model.predict(X_test)

#Checking accuracy
print(""Decision Tree Classifier Accuracy: {0}"".format(accuracy_score(y_test, predicted)))


#Using Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

clf = RandomForestClassifier(min_samples_split=50, min_samples_leaf=200)

#80%
X_train = train_df[['screen_name_binary', 'description_binary', 'location_binary', 'verified_binary']] #train_data
y_train = train_df['bot'] #train_target

#20%
X_test = test_df[['screen_name_binary', 'description_binary', 'location_binary', 'verified_binary']] #test_Data
y_test = test_df['bot'] #test_target
",Project/BotDetection.py,jubins/ML-TwitterBotDetection,1
"    return parameters_decision_tree


# # Random Forest

# In[31]:

#Trains the Random Forest model. Performing a grid search to select the optimal parameter values
def train_random_forest(train_X, train_Y, parameters_random_forest):
    
    random_forest_model = RandomForestClassifier()
    model_gs = grid_search.GridSearchCV(random_forest_model, parameters_random_forest, scoring = metric_grid_search)
    model_gs.fit(train_X,train_Y)
    return model_gs


# In[32]:

#Predicts the output on a set of data, the built model is passed as a parameter, which is used to predict
def predict_random_forest(data_X, data_Y, random_forest):",ensembles/conjunto.py,unnati-xyz/ensemble-package,1
"        # concated
        feature = bundle_f['features'].value

    train_filter = (np.bitwise_and(~is_valid, ~is_test))
    valid_filter = (np.bitwise_and(is_valid, ~is_test))
    test_filter = is_test

    ##############
    # validation #
    ##############
    clf = RandomForestClassifier()
    clf.fit(feature[train_filter], label[train_filter])
    print('validation score: (Accuracy)',
          clf.score(feature[valid_filter], label[valid_filter]))

    ##############
    # prediction #
    ##############
    clf.fit(feature[~test_filter], label[~test_filter])
    prediction = clf.predict(feature[test_filter])",examples/titanic/model.py,ianlini/feagen,1
"    trainData = np.loadtxt(h2o.locate(""smalldata/gbm_test/alphabet_cattest.csv""), delimiter=',', skiprows=1, converters={0:lambda s: ord(s.split(""\"""")[1])})
    trainDataResponse = trainData[:,1]
    trainDataFeatures = trainData[:,0]

    # Train H2O GBM Model:
    #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")
    rf_h2o = h2o.random_forest(x=alphabet[['X']], y=alphabet[""y""], ntrees=1, max_depth=1, nbins=100)

    # Train scikit GBM Model:
    # Log.info(""scikit GBM with same parameters:"")
    rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
    rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

    # h2o
    rf_perf = rf_h2o.model_performance(alphabet)
    auc_h2o = rf_perf.auc()

    # scikit
    auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_NOPASS_smallcatRF.py,bikash/h2o-dev,1
"train = train.drop(['Delay'], axis=1)
test = test.drop(['Delay'], axis=1)

from sklearn import preprocessing
for col in train.columns:
    scaler = preprocessing.StandardScaler()
    train[col] = scaler.fit_transform(train[col])
    test[col] = scaler.transform(test[col])

from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 500, random_state = 2543, max_features = None, n_jobs = -1)
forest = forest.fit(train, trainDelay)
pred = forest.predict(test)
acc = forest.score(test, testDelay)

df = pd.DataFrame()
df[""Orginal""] = testDelay
df[""Predicted""] = pred
df.to_csv('classifyDelay.csv', index = False)
",classify.py,DEK11/Predicting-EOB-delay,1
"  Trains an sklearn RandomForestClassifier on the provided data.
  
  Args:
    data - a Pandas data frame with the filename, label, and features
    n_trees - number of trees to use in model
    
  Returns:
    RandomForestClassifier trained on provided data
  '''
  start = datetime.now()
  rf = RandomForestClassifier(n_estimators=n_trees,
                              n_jobs=-1,
                              oob_score=True)
  rf.fit(data.drop(['file', 'sponsored'], 1), data.sponsored)
  oob_auc = roc_auc_score(data.sponsored, rf.oob_decision_function_[:, 1])
  print 'auc score: %.5f' % oob_auc
  finish = datetime.now()
  print 'elapsed time: %d sec.' % (finish - start).seconds
  return rf
  ",tree_models.py,davidthaler/Kaggle_Truly-Native,1
"    

if __name__ == ""__main__"":
    print ""Start training""
    # Get the probability predictions for computing the log-loss function
    kf = KFold(y, n_folds=5)
    # prediction probabilities number of samples, by number of classes
    y_pred = np.zeros((len(y), len(set(y))))
    for train, test in kf:
        X_train, X_test, y_train, y_test = X[train, :], X[test, :], y[train], y[test]
        clf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, n_jobs=1)
        clf.fit(X_train, y_train)
        y_pred[test] = clf.predict_proba(X_test)
    print ""With CV"", multiclass_log_loss(y, y_pred)
    clf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, n_jobs=1)
    clf.fit(X, y)
    y_pred = clf.predict_proba(X)
    print ""With Cheat"", multiclass_log_loss(y, y_pred)",NDSB/benchmark.py,takacsg84/kaggle,1
"from sklearn.externals import joblib

import os


class classifier:
  def __init__(self):
    self.features = []
    self.classes = []
    #self.models = [GaussianNB(), DecisionTreeClassifier(), DecisionTreeClassifier(class_weight = 'balanced'), RandomForestClassifier(), RandomForestClassifier(class_weight = 'balanced'), LogisticRegression(), LogisticRegression(class_weight = 'balanced')]#, AdaBoostClassifier(), AdaBoostClassifier(DecisionTreeClassifier(class_weight = 'balanced'))]
    #self.modelnames = ['GaussianNB', 'DecisionTreeClassifier', 'DecisionTreeClassifier(balanced)', 'RandomForestClassifier', 'RandomForestClassifier(balanced)', 'LogisticRegression', 'LogisticRegression(balanced)']#, 'AdaBoostClassifier', 'AdaBoostClassifier(balanced)']
    
    
    self.models = [GaussianNB(), DecisionTreeClassifier(class_weight = 'balanced'), RandomForestClassifier(class_weight = 'balanced'), LogisticRegression(class_weight = 'balanced')]#, AdaBoostClassifier(), AdaBoostClassifier(DecisionTreeClassifier(class_weight = 'balanced'))]
    self.modelnames = ['GaussianNB', 'DecisionTreeClassifier', 'RandomForestClassifier', 'LogisticRegression(balanced)']#, 'AdaBoostClassifier', 'AdaBoostClassifier(balanced)']
    
    
    self.best_model = GaussianNB()
  
  def add(self, num1, num2):",src/python/blackbox.py,algomaus/PAEC,1
"
X = np.array( X, dtype='float32').copy()
Y = np.array(Y, dtype='int32').copy()

n_positive = Y.sum()
n_points = Y.shape[0]
print ""RATIO = "", n_positive, n_points, float(n_positive) / float(n_points) * 100
print ""learning...""

from sklearn.ensemble import RandomForestClassifier
neigh = RandomForestClassifier( n_estimators=args.ntrees,
                                criterion='gini',
                                n_jobs=args.cpu )
neigh.fit(X, Y)
neigh.set_params(n_jobs=1)

def mask_image( file_img, file_mask, ga, r, neigh, output_dir ):
    img = irtk.imread( file_img, dtype='float32' )

    input_mask = irtk.imread( file_mask )",wrapping/cython/scripts/fetalMask_segmentation.py,BioMedIA/irtk-legacy,1
"def logistic_regression_classifier(train_x, train_y):
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(penalty='l2')
    model.fit(train_x, train_y)
    return model


# Random Forest Classifier
def random_forest_classifier(train_x, train_y):
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(n_estimators=8)
    model.fit(train_x, train_y)
    return model


# Decision Tree Classifier
def decision_tree_classifier(train_x, train_y):
    from sklearn import tree
    model = tree.DecisionTreeClassifier()
    model.fit(train_x, train_y)",knn.py,nanshihui/ipProxyDec,1
"
from sklearn.ensemble import RandomForestClassifier
from sklearn.externals import joblib
from feature.sift import Sift
import os

CURPATH = os.path.split(os.path.realpath(__file__))[0]

class RandomForest:
    def __init__(self):
        self.rf = RandomForestClassifier()
        self.model_path =os.path.join(os.path.join(CURPATH,'model'),'random_forest.pkl')

    def fit(self,X,y):
        self.rf.fit(X,y)

    def save(self):
        joblib.dump(self.rf,self.model_path)

    def load(self):",src/classifier/clf_model.py,15cm/clothing-classifier,1
"# # import the machine learning library that holds the randomforest
# import sklearn.ensemble as ske
# Create an acceptable formula for our machine learning algorithms
formula_ml = 'Survived ~ C(Pclass) + C(Sex)'

# # Create the random forest model and fit the model to our training data
y, x = dmatrices(formula_ml, data=df, return_type='dataframe')
# # RandomForestClassifier expects a 1 demensional NumPy array, so we convert
y = np.asarray(y).ravel()
# #instantiate and fit our model
# results_rf = ske.RandomForestClassifier(n_estimators=100).fit(x, y)

# # Score the results
# score = results_rf.score(x, y)
# print ""Mean accuracy of Random Forest Predictions on the data was: {0}"".format(score)
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=30)
clf.fit(x, y) 
print ""Sucess""
",titanic.py,supernoobabhi/titanic-kaggle,1
"# Generate data
X, y = make_blobs(n_samples=1000, n_features=2, random_state=42,
                  cluster_std=5.0)
X_train, y_train = X[:600], y[:600]
X_valid, y_valid = X[600:800], y[600:800]
X_train_valid, y_train_valid = X[:800], y[:800]
X_test, y_test = X[800:], y[800:]

# Train uncalibrated random forest classifier on whole train and validation
# data and evaluate on test data
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train_valid, y_train_valid)
clf_probs = clf.predict_proba(X_test)
score = log_loss(y_test, clf_probs)

clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train, y_train)
clf_probs = clf.predict_proba(X_test)
sig_clf = CalibratedClassifierCV(clf, method=""sigmoid"", cv=""prefit"")
sig_clf.fit(X_valid, y_valid)",src_python/bio_ga/randomforest/rf.py,tapomay/libgenetic,1
"                        scale_pos_weight=6,
                        max_delta_step=6)
estimators = list()
estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                      axis=0, verbose=2)))
estimators.append(('clf', clf))
pipeline_xgb = Pipeline(estimators)


# Create model for RF
clf = RandomForestClassifier(n_estimators=1200,
                             max_depth=17,
                             max_features=3,
                             min_samples_split=2,
                             min_samples_leaf=3,
                             class_weight='balanced_subsample',
                             verbose=1, random_state=1, n_jobs=4)
estimators = list()
estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                      axis=0, verbose=2)))",ml4vs/ensembling.py,ipashchenko/ml4vs,1
"    training_label = [arr for idx_arr, arr in enumerate(label)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = label_binarize(np.hstack(training_label).astype(int),
                                    [0, 255])
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, max_features=None,
                                 n_jobs=-1)
    pred_prob = crf.fit(training_data, np.ravel(training_label)).predict_proba(
        testing_data)

    result_cv.append([pred_prob, crf.classes_])

# Save the information
path_store = '/data/prostate/results/mp-mri-prostate/exp-1/mrsi-citrate-choline-fit'
if not os.path.exists(path_store):",pipeline/feature-classification/exp-1/mrsi/pipeline_classifier_mrsi_citrate_choline_fit.py,I2Cvb/mp-mri-prostate,1
"from sklearn.ensemble import RandomForestClassifier;

def dump_file(result,fname='result.pkl'):
	cPickle.dump(result,open(fname,'wb'));

def load_file(result,fname='result.pkl'):
	return cPickle.load(open(fname,'rb'));

if __name__ == '__main__':
	digits = myData.load_digits();
	rfc = RandomForestClassifier();
	rfc.fit(digits.feature,digits.target);
	print rfc.score(digits.feature,digits.target);
	
	result = rfc.predict(digits.test);
	out_f = file('result.csv','w');
	writer = csv.writer(out_f);
	writer.writerow(['Id','Solution']);
	for i,r in enumerate(result,start=1):
		writer.writerow([i,r]);",DigitRecognizer/recongizer3.py,dz1984/Kaggle,1
"        beta1=.9,
        beta2=.999)

  elif model_name == 'rf':
    # Loading hyper parameters
    n_estimators = hyper_parameters['n_estimators']
    nb_epoch = None

    # Building scikit random forest model
    def model_builder(model_dir_rf):
      sklearn_model = RandomForestClassifier(
          class_weight=""balanced"", n_estimators=n_estimators, n_jobs=-1)
      return deepchem.models.sklearn_models.SklearnModel(sklearn_model,
                                                         model_dir_rf)

    model = deepchem.models.multitask.SingletaskToMultitask(tasks,
                                                            model_builder)

  elif model_name == 'xgb':
    # Loading hyper parameters",deepchem/molnet/run_benchmark_models.py,peastman/deepchem,1
"# split training and test data
def split_data(features, labels):
  train_features, test_features, train_labels, test_labels = sklearn.model_selection.train_test_split(features, labels, test_size = 0.2)
  return train_features, test_features, train_labels, test_labels

def get_accuracy(predictions, labels):
  return float(np.sum(predictions == labels))/len(labels)

# build RF model
def predict_rf(train_features, test_features, train_labels, test_labels):
  model = RandomForestClassifier(n_estimators=1000)
  model.fit(train_features, train_labels)
  predictions = model.predict(train_features)
  # print get_accuracy(predictions, train_labels)
  predictions = model.predict(test_features)
  # print get_accuracy(predictions, test_labels)
  return predictions

# build LR model
def predict_lr(train_features, test_features, train_labels, test_labels):",app/analysis/views.py,BIDS-collaborative/EDAM,1
"from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
import featherweight_api


class Classifier(object):
    def __init__(self):
        self.iris = datasets.load_iris()
        X = self.iris.data
        y = self.iris.target
        self.clf = RandomForestClassifier()
        self.clf.fit(X, y)

    def score(self,
              sepal_length,
              sepal_width,
              petal_length,
              petal_width):
        guessed_class_arr = self.clf.predict([sepal_length, sepal_width, petal_length, petal_width])
        guessed_class = guessed_class_arr[0]  # extract the only item in the array result",example_iris.py,ianozsvald/featherweight_web_api,1
"valid_x -= mean
train_x /= std
valid_x /= std

tf = ZCA()
tf.fit(train_x)

train_x = tf.transform(train_x)
valid_x = tf.transform(valid_x)

clf = RandomForestClassifier(n_estimators=128)
clf.fit(train_x, train_y)
train_y_hat = clf.predict(train_x)
valid_y_hat = clf.predict(valid_x)
print(accuracy_score(train_y, train_y_hat))
print(accuracy_score(valid_y, valid_y_hat))",simple_classifier.py,kastnerkyle/ift6266h15,1
"idx = np.random.choice(len(y0), size=280000, replace=False)
X, y = X0[idx], y0[idx]
param_dist = {'n_estimators': [20, 100, 200, 500],
              'max_depth': [3, 5, 20, None],
              'max_features': ['auto', 5, 10, 20],
              'bootstrap': [True, False],
              'criterion': ['gini', 'entropy']}
from sklearn import grid_search as gs
from time import time
from sklearn import ensemble
rf = ensemble.RandomForestClassifier()
random_search = gs.GridSearchCV(rf, param_grid=param_dist, refit=False,
                                verbose=2, n_jobs=12)
start=time(); random_search.fit(X, y); stop=time()
print('took %s seconds' % (stop - start))",gala-training-crossval-sub.py,jni/gala-scripts,1
"    # Fit the model for DNN
    m.fit(input_fn=lambda: input_fn(LYA_TRAINING), steps=train_steps)

    # Evaluate the accuracy of the model for DNN
    results = m.evaluate(input_fn=lambda: input_fn(LYA_TEST), steps=1)
    for key in sorted(results):
        print(""%s: %s"" % (key, results[key]))
    print ""\n====== Done with DNN Classifier ======\n""

    # ======================== RANDOM FOREST ===================================
    clf = RandomForestClassifier(n_jobs=2)
    y, _ = pd.factorize(LYA_TRAINING[LABEL_COLUMN])
    clf.fit(LYA_TRAINING[CONTINUOUS_COLUMNS],y)

    predictions = clf.predict(LYA_TEST[CONTINUOUS_COLUMNS])
    pd.crosstab(LYA_TEST[LABEL_COLUMN], predictions, rownames=['actual'], colnames=['predictions'])
    print ""Random Forest Classification Accuracy: {}"".format(np.sum(LYA_TEST[LABEL_COLUMN]==predictions) / float(len(LYA_TEST)))

tf.app.run(main=main)",Lya_Emitters/learnLya.py,nigelmathes/Data_Science,1
"    samples_in_bin = map(lambda x: x < (i+1), rain_data)
    rain_percent.append(np.array(samples_in_bin).astype(np.int).sum())
rain_percent = np.array(rain_percent) / (1.0 * rain_samples)

# Keeping 'DistanceToRadar', 'RRR', 'MassWeighteMean', 'MassWeightedSD'
train_df = train_df.drop(['TimeToEnd', 'Id', 'Composite', 'HybridScan', 'HydrometeorType', 'Kdp', 'RR1','RR2', 'RR3', 'Reflectivity', 'ReflectivityQC', 'RhoHV', 'Velocity', 'Zdr', 'LogWaterVolume', 'Expected', 'RadarQualityIndex'], axis=1) 

train_data = train_df.values

print 'Training...'
forest = RandomForestClassifier(n_estimators=10, n_jobs=7)
#forest = RandomForestClassifier(n_estimators=1000, n_jobs=-1)
forest = forest.fit( train_data, it_rained)


# TEST DATA
print 'Loading test data...'
test_df = pd.read_csv(split_test_data, header=0)

",rain/code/classifier.py,ryanbahneman/kaggle,1
"        XX = create_features(XX, tmin, tmax, sfreq)

        X_test.append(XX)
        ids_test.append(ids)

    X_test = np.vstack(X_test)
    ids_test = np.concatenate(ids_test)
    print ""Testset:"", X_test.shape

    print
    clf = RandomForestClassifier(n_estimators=1000, max_depth=None, max_features=375, n_jobs=-1)
    print ""Classifier:""
    print clf
    print ""Training.""
    clf.fit(X_train, y_train)
    print ""Predicting.""
    y_pred = clf.predict(X_test)

    print
    filename_submission = ""../output/submissionForestRandomTrees.csv""",src/forests_random_trees.py,LooseTerrifyingSpaceMonkey/DecMeg2014,1
"    def inner_staged_predict_proba(self, X):
        X = self.get_train_variables(X)
        return self.trained_estimator.predict_proba(X)


def test_reweighting():
    trainX, trainY = commonutils.generate_sample(4000, 5, 2.0)
    testX, testY = commonutils.generate_sample(4000, 5, 2.0)

    reweighting = ReweightClassifier(uniform_variables=trainX.columns[:1],
                                     base_estimator=RandomForestClassifier(n_estimators=10),
                                     iterations=10, learning_rate=100)
    reweighting.fit(trainX, trainY)
    reweighting.predict(testX)
    reweighting.predict_proba(testX)
    reweighting.staged_predict_proba(testX)
    print(""reweighting is ok"")

",hep_ml/experiments/reweighting.py,anaderi/lhcb_trigger_ml,1
"    wc_components = build_word2vec_wc(feature_vec_size=120, avg=True, idf=wc_idf_map, verbose=(verbose > 0))
    y_train = wc_components['train']['df']['cuisine_code'].as_matrix()
    X_train = wc_components['train']['features_matrix']
    # standardize features aka mean ~ 0, std ~ 1
    if std:
        scaler = StandardScaler()
        scaler.fit(X_train)
        X_train = scaler.transform(X_train)
    # random forest supervised classifier
    time_0 = time.time()
    clf = RandomForestClassifier(n_estimators=100, max_depth=None,
        n_jobs=n_jobs, random_state=random_state, verbose=verbose)
    # perform cross validation
    cv_n_fold = 8
    print 'cross validating %s ways...' % cv_n_fold
    scores_cv = cross_val_score(clf, X_train, y_train, cv=cv_n_fold, n_jobs=-1)
    print 'accuracy: %0.5f (+/- %0.5f)' % (scores_cv.mean(), scores_cv.std() * 2)
    time_1 = time.time()
    elapsed_time = time_1 - time_0
    print 'cross validation took %.3f seconds' % elapsed_time",train_word2vec_rf.py,eifuentes/kaggle_whats_cooking,1
"from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.utils.validation import check_array

ESTIMATORS = {
    ""dummy"": DummyClassifier(),
    ""random_forest"": RandomForestClassifier(n_estimators=100,
                                            max_features=""sqrt"",
                                            min_samples_split=10),
    ""extra_trees"": ExtraTreesClassifier(n_estimators=100,
                                        max_features=""sqrt"",
                                        min_samples_split=10),
    ""logistic_regression"": LogisticRegression(),
    ""naive_bayes"": MultinomialNB(),
    ""adaboost"": AdaBoostClassifier(n_estimators=10),
}",projects/scikit-learn-master/benchmarks/bench_20newsgroups.py,DailyActie/Surrogate-Model,1
"my_dataset = data_dict

# ---------------------------------------------------------------------
# classifiers and hyper-parameters

clf_names = ['GaussianNB', 'LinearSVC', 'SVC',
             'DecisionTree', 'RandomForest',
             'KNeighbors', 'AdaBoost', 'LogisticRegression']

clf_list = [GaussianNB(), LinearSVC(), SVC(),
            DecisionTreeClassifier(), RandomForestClassifier(),
            KNeighborsClassifier(), AdaBoostClassifier(),
            LogisticRegression(max_iter=1000)]

classify_params_list = [my_params.gaussian_nb_params,
                        my_params.linear_svc_params,
                        my_params.svc_params,
                        my_params.decision_tree_params,
                        my_params.random_forest_params,
                        my_params.k_neighbors_params,",IdentifyFraudFromEnronEmails/poi_id.py,zhujun98/dataAnalysis,1
"# split data
xtrain,xtest,ytrain,ytest = cross_validation.train_test_split(xall,yall,test_size=0.4)

if cv:
	# setup grid search parameters
	nsplits = [2,4,8,16,32]
	nfeats = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]
	grid_pars = {'min_samples_split':nsplits, 'max_features':nfeats}

	# setup classifiers and train and evaluate on test
	basemod = ensemble.RandomForestClassifier(n_estimators=Ntrees)
	clf = grid_search.GridSearchCV(basemod,grid_pars,verbose=2,cv=10,n_jobs=4)
	clf.fit(xtrain,ytrain)
	print '\nCross-validation best params: ',clf.best_params_
	print 'Train score = ',clf.best_score_
	print 'Test score = ',clf.score(xtest,ytest),'\n'

	ofp = open('RF_gridsearch_scores.txt','w')
	ofp.write('max_features\tmin_samples_split\tmean_validation_score\n')
	for gs in clf.grid_scores_:",runTree.py,PBGraff/SwiftGRBpipe,1
"                ]

def main():
    print(""Reading the data"")
    data = cu.get_dataframe(train_file)

    print(""Extracting features"")
    fea = features.extract_features(feature_names, data)

    print(""Training the model"")
    rf = RandomForestClassifier(n_estimators=50, verbose=2, compute_importances=True, n_jobs=1)
    rf.fit(fea, data[""OpenStatus""])

    print(""Reading test file and making predictions"")
    data = cu.get_dataframe(test_file)
    test_features = features.extract_features(feature_names, data)
    probs = rf.predict_proba(test_features)

    print(""Calculating priors and updating posteriors"")
    new_priors = cu.get_priors(full_train_file)",src/basic_benchmark.py,coreyabshire/stacko,1
"f1 = [x for x in cols if x not in set(['admission_type_id', 'discharge_disposition_id'])]

features1 = df[list(f1)].values
features2 = df[list(f2)].values
f1_train, f1_test, f2_train, f2_test, l1_train, l1_test, l2_train, l2_test = \
    train_test_split(features1, features2, labels1, labels2, test_size=0.25)

# clf = KNeighborsClassifier(10, weights='distance')
# clf1 = AdaBoostClassifier(n_estimators=50)
# clf2 = AdaBoostClassifier(n_estimators=50)
clf1 = RandomForestClassifier(n_jobs=-1, n_estimators=50, min_samples_split=12, max_features='auto')
clf2 = RandomForestClassifier(n_jobs=-1, n_estimators=50, min_samples_split=12, max_features='auto')
# clf1 = GaussianNB()
# clf2 = GaussianNB()
clf1.fit(f1_train, l1_train)
clf2.fit(f2_train, l2_train)

pred1 = clf1.predict(f1_test)
acc1 = accuracy_score(pred1, l1_test)
f1_test = np.insert(f1_test,6, pred1,1)",data/jerry/classify.py,isabellewei/deephealth,1
"
def test_with_iris():
    iris = datasets.load_iris()
    X, y = iris.data, iris.target
    n, d = X.shape
    assert n == len(y), ""Missing label(s)!""
    
    ids = shuffle_ids(n)
    tr_ids, te_ids = split_ids(ids, percent=0.8)
    
    rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)
    rf.fit(X[tr_ids], y[tr_ids])
    print ""Score of Random Forests on test data:"", rf.score(X[te_ids], y[te_ids])
    pred_probs = rf.predict_proba(X[te_ids])
    print pred_probs # pred_probs is a list of [p1, p2, p3] item, where pi is prob of class i
    
    '''
    gtb = GradientBoostingClassifier(n_estimators=1000, learning_rate=1.0, 
                                    max_depth=1, random_state=0).fit(X[tr_ids], y[tr_ids])
                                    ",Ensemble-Methods/test_random_forests.py,ntduong/ML,1
"    X = np.c_[X1, X2, X3]
    
    metrics = { 
        'num_cases': len(X),   
        'curves': [],
        'aucs': [],
        'pr': []
    }
    cross_validation_steps = 10
    kf = KFold(n_splits=cross_validation_steps, shuffle=True)
    forest = RandomForestClassifier(n_estimators=100)

    for train, test in kf.split(X):
        X_train, X_test = X[train], X[test]
        y_train, y_test = y[train], y[test]

        forest.fit(X_train, y_train)
        probabilities = forest.predict_proba(X_test)[:,1]
        precision, recall, thresholds = precision_recall_curve(y_test, probabilities, pos_label=1)
",paper_recommendation_engine.py,fbeutler/arXiv_recommendation_engine,1
"    def cv_kfold(self, feat_table, k=5, method=""decisiontree"", **kwargs):
        if not isinstance(feat_table, CandidateSet):
            raise TypeError('Input object should be of type CandidateSet')
        table = feat_table.candset_table
        data, target = self._get_data_target(table)
        if method == ""decisiontree"":
            clf = DecisionTreeClassifier()
        elif method == ""svm"":
            clf = SVC()
        elif method == ""randomforest"":
            clf = RandomForestClassifier()
        elif method == ""boosting"":
            clf = AdaBoostClassifier()
        elif method == ""bagging"":
            clf = BaggingClassifier()
        elif method == ""naivebayes"":
            clf = GaussianNB()
        elif method == ""sgd"":
            clf = SGDClassifier()
        else:",magellan/matcher/ml_matcher.py,kvpradap/magellan-v0.0,1
"from sklearn.ensemble import RandomForestClassifier
X = [[0, 0], [1, 1]]
Y = [0, 1]
 clf = RandomForestClassifier(n_estimators=10)",Python/scikit_learn_random_forest.py,gmg444/ForestCanopySimulator,1
"    ----------
    classes_ : array-like, shape = [n_predictions]

    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.naive_bayes import GaussianNB
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> y = np.array([1, 1, 1, 2, 2, 2])
    >>> eclf1 = VotingClassifier(estimators=[
    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
    >>> eclf1 = eclf1.fit(X, y)
    >>> print(eclf1.predict(X))
    [1 1 1 2 2 2]
    >>> eclf2 = VotingClassifier(estimators=[",summary/sumy/sklearn/ensemble/voting_classifier.py,WangWenjun559/Weiss,1
"trainFile = '../data/train_validation.csv'
X, y, means, stds = loadData(trainFile)

print ""#Passengers"", len(y)
print ""Survived"", sum(y)
print ""Deceased"", len(y) - sum(y)

testFile = '../data/test_validation.csv'
XTest, yTest = loadData(testFile, means, stds)

rf_clf = RandomForestClassifier(n_estimators=100)
rf_clf.fit(X, y)

yTestPredicted = rf_clf.predict(XTest)
yTrainPredicted = rf_clf.predict(X)

print """"
print ""Random forest (10)""
print ""Correctly predicted Train:"", sum(y == yTrainPredicted)
print ""Incorrectly predicted Train:"", sum(y != yTrainPredicted)",titanic/bin/random_forest.py,MatthewThe/kaggle,1
"
    results_logistic_df = pd.concat(results_logistic)
    f = mpath + r'\results_logistic.csv'
    results_logistic_df.to_csv(f, index=False, encoding='utf-8')

    best_result_df = pd.DataFrame(best_result, columns=best_cols)
    f = mpath + r'\results_logistic_best.csv'
    best_result_df.to_csv(f, index=False, encoding='utf-8')

    # set up random forest classifier
    clf = RandomForestClassifier(n_jobs=-1, random_state=0)

    # set up grid to search
    params = {""criterion"" : ['gini', 'entropy'],
              ""bootstrap"": [True, False],
              ""n_estimators"" : [5, 10, 50],
              ""max_depth"" : [10, 20],
              ""max_features"" : [1, 2, 6],
              ""min_samples_split"" : [10, 20, 40],
              ""min_samples_leaf"": [1, 10, 20],",src/models/ml_train.py,mworles/capstone_one,1
"        vertical_symmetry,
        horizontal_symmetry,
]

SVM_EXTRACTORS = [scale_image_down(positions)]
def svm_engine():
    return svm.SVC(kernel='poly', degree=2)

FOREST_EXTRACTORS = ALL_EXTRACTORS
def forest_engine():
    return ensemble.RandomForestClassifier(n_estimators=50, n_jobs=2)

class CaptchaDecoder(object):
    def __init__(self, *args, **kwargs):
        self.engine = svm_engine()
        self.feature_extractor = compose_extractors(SVM_EXTRACTORS)

    def fit(self, x, y):
        digits = []
        labels = []",models.py,aflag/captcha-study,1
"# All classifiers except convnet, RNN, LSTM.

def test_linear_model():
    model = LogisticRegression(lr=0.01, max_iters=500, penalty='l1', C=0.01)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    assert roc_auc_score(y_test, predictions) >= 0.95


def test_random_forest():
    model = RandomForestClassifier(n_estimators=10, max_depth=4)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)[:, 1]
    assert roc_auc_score(y_test, predictions) >= 0.95


def test_svm_classification():
    y_signed_train = (y_train * 2) - 1
    y_signed_test = (y_test * 2) - 1
",mla/tests/test_classification_accuracy.py,rushter/MLAlgorithms,1
"         uniform(2, 5),  # Possible leg lengths for chairs
         gauss(2, 0.25)]  # Possible top surface areas for chairs
        for i in range(num_chairs)] + \
       [[randint(0, 5),  # Possible colors for tables
         uniform(4, 10),  # Possible leg lengths for tables
         gauss(5, 1)]  # Possible top surface areas for tables
        for i in range(num_tables)]

labels = asarray(['chair']*num_chairs + ['table']*num_tables)

rfc = RandomForestClassifier(n_estimators=100)
# rfc.fit(data, labels)

scores = cross_val_score(rfc, data, labels, cv=10)
print('Accuracy: %0.2f +/- %0.2f' % (scores.mean(), scores.std()*2))",src/random_forest_chairs.py,koverholt/random-forest-example,1
"
from sklearn.preprocessing import scale
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier

average_click_accuracy = 0
average_book_accuracy = 0

for k in xrange(10):
    def create_classifier():
        return RandomForestClassifier(n_estimators=40, max_features=15)

    train_data = pd.read_csv('shredded/%02d.csv' % (k))
    evaluation_data = pd.read_csv('shredded/%02d.csv' % (k + 1))

    print 'Train shape:', train_data.shape

    columns_to_remove = ['gross_bookings_usd',
                         'srch_id',
                         'prop_id',",Assignment-2/Code/feature_cross_validation.py,Ignotus/DataMining,1
"    Examples
    --------
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.models import CostSensitiveDecisionTreeClassifier
    >>> from costcla.metrics import savings_score
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> y_pred_test_rf = RandomForestClassifier(random_state=0).fit(X_train, y_train).predict(X_test)
    >>> f = CostSensitiveDecisionTreeClassifier()
    >>> y_pred_test_csdt = f.fit(X_train, y_train, cost_mat_train).predict(X_test)
    >>> # Savings using only RandomForest
    >>> print savings_score(y_test, y_pred_test_rf, cost_mat_test)
    0.12454256594
    >>> # Savings using CSDecisionTree
    >>> print savings_score(y_test, y_pred_test_csdt, cost_mat_test)
    0.481916135529
    """"""",costcla/models/cost_tree.py,madjelan/CostSensitiveClassification,1
"        """"""
        r_pars = [
                {
                    ""max_depth""          : [3, None],
                    ""bootstrap""          : [True, False],
                    ""criterion""          : [""gini"", ""entropy""]
                    }
                ]
        data_score = 0.0
        len_xy = Y.shape[0]
        est = RandomForestClassifier(
                n_estimators=2000,
                max_features='sqrt') 

        if predFlag:
            estimator = GridSearchCV(est, r_pars)
        else:
            estimator = RandomForestClassifier(
                    n_estimators=200,
                    max_features='sqrt')",programs/machine_learning/learning_sample.py,CORDEA/analysis_of_1000genomes-data,1
"# print len(heart.columns)

dim = data.shape[1]
target_index = dim-1

X = v[:,0:target_index]
y = v[:,target_index]

random = 99 # pick reproducible pseudo-random sequence

clf = RandomForestClassifier(n_estimators=50, oob_score=True,
                             max_features=""sqrt"", bootstrap=True,
                             min_samples_leaf=20, criterion=""entropy"",
                             random_state=random)
clf = clf.fit(X, y)
oob_error = 1 - clf.oob_score_
tree.export_graphviz(clf.estimators_[0], out_file=""/tmp/t0.dot"", feature_names=colnames)

kfold = KFold(n_splits=5, shuffle=True, random_state=random)
",python/voting.py,parrt/AniML,1
"    # Convert the list to array
    t2w_training_data = np.concatenate(t2w_training_data, axis=0)
    t2w_validation_data = np.concatenate(t2w_validation_data, axis=0)
    t2w_testing_data = np.array(t2w_testing_data)
    # Select subset
    t2w_training_data = t2w_training_data[:, t2w_feat_sel_idx]
    t2w_validation_data = t2w_validation_data[:, t2w_feat_sel_idx]
    t2w_testing_data = t2w_testing_data[:, t2w_feat_sel_idx]

    # Train an rf
    crf_t2w = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    crf_t2w.fit(t2w_training_data, training_label)

    # ADC
    # Load data
    adc_training_data = [arr for idx_arr, arr in enumerate(data[1])
                         if idx_arr in idx_patient_training]
    adc_validation_data = [arr for idx_arr, arr in enumerate(data[1])
                         if idx_arr in idx_patient_validation]
    adc_testing_data = data[1][idx_lopo_cv]",pipeline/feature-classification/exp-4/pipeline_classifier_stacking.py,I2Cvb/mp-mri-prostate,1
"#        X=d.iloc[:,:10]
    #    X=d.iloc[200:300,:10]
#        X=d[df['site'] == site].iloc[:,:10]
        X = d[df['site'] == site]
        X = X[pd.notnull(X).any(1)]
        y = df['control'].loc[X.index]
      
        #%% RF
        mtry = np.sqrt(X.shape[1]).round()
    #    mtry=np.sqrt(n_components).round()
        rf = RandomForestClassifier(n_estimators=5000)
        gbm = GradientBoostingClassifier(n_estimators=10000, learning_rate=0.001)
        # Parameter Grids
        param_grid_rf = dict(max_features=np.arange(int(mtry-round(mtry/2)), int(mtry+round(mtry/2)), 2 ) )
        param_grid_gbm = dict(max_depth= range(1,10))
    #    param_grid=dict(max_features=range(5,100,5))
        param_dist = {""max_features"": sp_randint(5,100)}
        random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=40)
        grid_search_rf = GridSearchCV(estimator = rf, param_grid = param_grid_rf, cv = 10) 
        grid_search_gbm = GridSearchCV(estimator = gbm, param_grid = param_grid_gbm, cv = 10) ",different_pipelines.py,himalayajung/neuropy,1
"
    def __init__method(self, method_name, model_labels=None, model_train=None,
                       model_predict=dict(), model_cross_validate=None):
        if method_name == ""lr"":
            self.__method = _LogisticRegression()
        elif method_name == ""lr_cv"":
            self.__method = _LogisticRegressionCV()
        elif method_name == ""mlm"":
            self.__method = _MixedLinearModel()
        elif method_name == ""rfc"":
            self.__method = _RandomForestClassifier()
        elif method_name == ""gbc"":
            self.__method = _GradientBoostingClassifier()
        elif method_name == ""dtc"":
            self.__method = _DecisionTreeClassifier()
        elif method_name == ""knc"":
            self.__method = _KNeighborsClassifier()
        elif method_name == ""nb"":
            self.__method = _NaiveBayes()
        elif method_name == ""nn"":",Stats/TrainingMethod.py,mesgarpour/T-CARER,1
"        
        clf=SVC()
        clf.fit(self.X_train,self.y_train)
        
        return clf


    def getRandomForest (self) :
        ""Get the model Fit to the training data""
        
        clf=RandomForestClassifier()
        clf.fit(self.X_train,self.y_train)
        
        return clf





",TrainModels.py,SoimulPatriei/TMCleaner,1
"    ### Classifiers

    # clf = SVC()
    # params = {
    #     'C': sp.stats.expon(scale=100),
    #     'gamma': sp.stats.expon(scale=.1),
    #     'kernel': ['rbf'],
    #     'class_weight': ['balanced', None],
    # }

    # clf = RandomForestClassifier(n_estimators=20)
    # params = {
    #     'max_depth': [3, None],
    #     'max_features'     : sp.stats.randint(1, 11),
    #     'min_samples_split': sp.stats.randint(1, 11),
    #     'min_samples_leaf' : sp.stats.randint(1, 11),
    #     'bootstrap': [True, False],
    #     'criterion': ['gini', 'entropy'],
    # }
",scripts/make_loop_classifier.py,ebt-hpc/cca,1
"from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

#####################################################################
# Global Variables
#####################################################################
estimators = [LogisticRegression(),GaussianNB(),KNeighborsClassifier(),\
              DecisionTreeClassifier(),RandomForestClassifier()]

#####################################################################
# Classification
#####################################################################
def openFile(fname):
    """"""
    Opens data file.
    """"""
    with open(fname, 'rb') as csvfile:",selection/classi.py,rebeccabilbro/orlo,1
"    for strict in [True, False]:
        strictStr = 'strict' if strict else ''

        X = cpe.buildConceptPairList(d, inputDs[0], strict)
        y = len(X) * ['anto']

        negSample = cpe.shuffledConceptPairList(X)
        X += negSample
        y += len(negSample) * ['notAnto']

        for clfModel in [RandomForestClassifier(n_estimators=11, max_depth=7), KNeighborsClassifier()]:
            for feature in [cppf.subCarth, cppf.subPolar, cppf.subAngular,
                            cppf.concatCarth, cppf.concatPolar, cppf.concatAngular,
                            cppf.pEuclDist, cppf.pManaDist, cppf.pCosSim,
                            cppf.pdEuclDist, cppf.pdManaDist, cppf.pdCosSim]:
                for post in [pf.noPost, pf.postNormalize, pf.postAbs]:
                    expStr = '_'.join([inputDs[1], strictStr, str(clfModel.__class__.__name__), str(feature.__name__), str(post.__name__)])
                    savePath = '../data/learnedModel/anto/' + expStr + '.dill'
                    print '&&&&&&&&&&'
                    print expStr",toolbox/experiment/trainAll_antoClf.py,pelodelfuego/word2vec-toolbox,1
"                ]

def main():
    print(""Reading the data"")
    data = cu.get_dataframe(train_file)

    print(""Extracting features"")
    fea = features.extract_features(feature_names, data)

    print(""Training the model"")
    rf = RandomForestClassifier(n_estimators=60, verbose=2, compute_importances=True, n_jobs=-1)
    rf.fit(fea, data[""OpenStatus""])

    print(""Reading test file and making predictions"")
    data = cu.get_dataframe(test_file)
    test_features = features.extract_features(feature_names, data)
    probs = rf.predict_proba(test_features)

    print(""Calculating priors and updating posteriors"")
    new_priors = cu.get_priors(full_train_file)",basic_classify.py,mattalcock/stack-kaggle,1
"    # split the data into a training set and a validation set
    from sklearn.model_selection import LeaveOneGroupOut
    logo = LeaveOneGroupOut()
    
    print(logo.get_n_splits(features, labels, groups))
    
    return logo

def compileModel():
    
    rf = RandomForestClassifier() 
    
    return rf

def recompileModel():
    
    import cPickle as pickle
    
    algoloadname = str(""RandomForestClassifier"" + '.pickle')
    ",crosslingual/MLModelCreatorWord.py,Ninad998/deepstylometry-python,1
"	Y=np.zeros(n_samples)
	i=-1
	for line in f:
		if i>=0:
			line=line.split(',')
			Y[i]=float(line[1])
		i+=1
	return Y

def train(path_to_features_matrix,path_to_labels):
	clf=RandomForestClassifier(n_estimators=n_est, max_features=n_feat,max_depth=9, n_jobs=-1)
	X=load_features(path_to_features_matrix)
	Y=load_labels(path_to_labels)
	clf.fit(X,Y)
	joblib.dump(clf, '../models/trained_random_forest.pkl')
	return clf



",code/trainClassifier.py,danielmiorandi/riskPredictorBDC2015,1
"    scaler = preprocessing.StandardScaler()
    train[col] = scaler.fit_transform(train[col])
    test[col] = scaler.transform(test[col])

train['target'] = target
test['ID'] = testid
train.to_csv('cleantrain.csv', index = False)
test.to_csv('cleantest.csv', index = False)

#from sklearn.ensemble import RandomForestClassifier
#forest = RandomForestClassifier(n_estimators = 1000, random_state = 2543, max_features = None)
#forest = forest.fit(train, target)
#probs = forest.predict_proba(test)
#output = [""%f"" % x[1] for x in probs]


#df = pd.DataFrame()
#df[""ID""] = testid
#df[""target""] = output
#df.to_csv('RandomTree.csv', index = False)",Final.py,DEK11/Spring-Leaf-Competition,1
"inx2 = StringIndexer(inputCol=""service"", outputCol=""service-cat"")
inx3 = StringIndexer(inputCol=""flag"", outputCol=""flag-cat"")
inx4 = StringIndexer(inputCol=""is_anomaly"", outputCol=""label"")
ohe2 = OneHotEncoder(inputCol=""service-cat"", outputCol=""service-ohe"")
feat_cols = [c for c in kdd.columns + 
	      ['protocol-cat', 'service-ohe', 'flag-cat', 'label'] 
              if c not in ['protocol', 'service', 'flag', 'is_anomaly']]
vecAssembler = VectorAssembler(inputCols = feat_cols, 
				 outputCol = ""features"")

rf = RandomForestClassifier(numTrees=500, maxDepth=6, maxBins=80, seed=42)
pipeline = Pipeline(stages=[inx1, inx2, inx3, inx4, ohe2, vecAssembler, rf])
model = pipeline.fit(trainData)

# Evaluate model performance
def eval_metrics(lap):
    labels = lap.select(""label"").distinct().toPandas()['label'].tolist()
    tpos = [lap.filter(lap.label == x).filter(lap.prediction == x).count() for x in labels]
    fpos = [lap.filter(lap.label == x).filter(lap.prediction != x).count() for x in labels]
    fneg = [lap.filter(lap.label != x).filter(lap.prediction == x).count() for x in labels]",ch10/anomaly.py,ofermend/data-science-with-hadoop-book,1
"    """"""

    print('Performing ' + method + ' Classification...')
    print('Size of train set: ', X_train.shape)
    print('Size of test set: ', X_test.shape)
    print('Size of train set: ', y_train.shape)
    print('Size of test set: ', y_test.shape)
    

    classifiers = [
        RandomForestClassifier(n_estimators=100, n_jobs=-1),
        neighbors.KNeighborsClassifier(),
        SVC(degree=100, C=10000, epsilon=.01),
        AdaBoostRegressor(),
        AdaBoostClassifier(**parameters)(),
        GradientBoostingClassifier(n_estimators=100),
        QDA(),
    ]

    scores = []",scripts/Algorithms/regression_helpers.py,scorpionhiccup/StockPricePrediction,1
"    log_clf = LogisticRegression()
    log_clf.fit(X_train,y_train)
    y_pred = log_clf.predict(X_test)
    print ""scoring logisistic classifier:""
    print ""score is: "", log_clf.score(X_test, y_test)
    print(classification_report(y_test, y_pred, target_names=ArticleDataset.Label_Names[5]))

    print ""training forest classifier""
    Y_labels = data.get_feature('label')
    X_train, X_test, y_train, y_test = train_test_split(X, Y_labels)
    forest_clf = RandomForestClassifier(n_estimators=250, n_jobs=-1, max_depth=None)
    forest_clf.fit(X_train,y_train)
    y_pred = forest_clf.predict(X_test)
    print ""scoring forest classifier:""
    print ""score is: "", forest_clf.score(X_test, y_test)
    print(classification_report(y_test, y_pred, target_names=ArticleDataset.Label_Names[5]))

    print ""training extra-trees forest classifier""
    Y_labels = data.get_feature('label')
    X_train, X_test, y_train, y_test = train_test_split(X, Y_labels)",Machine Learning/ensemble.py,samouri/kinja-api,1
"    Examples
    --------
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.models import CostSensitiveDecisionTreeClassifier
    >>> from costcla.metrics import savings_score
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> y_pred_test_rf = RandomForestClassifier(random_state=0).fit(X_train, y_train).predict(X_test)
    >>> f = CostSensitiveDecisionTreeClassifier()
    >>> y_pred_test_csdt = f.fit(X_train, y_train, cost_mat_train).predict(X_test)
    >>> # Savings using only RandomForest
    >>> print(savings_score(y_test, y_pred_test_rf, cost_mat_test))
    0.12454256594
    >>> # Savings using CSDecisionTree
    >>> print(savings_score(y_test, y_pred_test_csdt, cost_mat_test))
    0.481916135529
    """"""",costcla/models/cost_tree.py,albahnsen/CostSensitiveClassification,1
"
    features_train = iso.transform(features_train)
    features_test = iso.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))
",nytimes/step4_analysis_supervised_4(isomap).py,dikien/Machine-Learning-Newspaper,1
"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier

from ClassifierWrapper import ClassifierWrapper


def get_classifier(method, n_estimators, max_features, gbm_learning_rate=None, random_state=None, min_cases_for_training=30):

    if method == ""rf"":
        return ClassifierWrapper(
            cls=RandomForestClassifier(n_estimators=n_estimators, max_features=max_features, random_state=random_state), 
            min_cases_for_training=min_cases_for_training)
               
    elif method == ""gbm"":
        return ClassifierWrapper(
            cls=GradientBoostingClassifier(n_estimators=n_estimators, max_features=max_features, learning_rate=gbm_learning_rate, random_state=random_state), 
            min_cases_for_training=min_cases_for_training)
    elif method == ""dt"":
        return ClassifierWrapper(
            cls=DecisionTreeClassifier(random_state=random_state), ",ClassifierFactory.py,irhete/predictive-monitoring-benchmark,1
"    def _estimator(self):

        return MultinomialNB()


class RandomForestModel(BaseModel):

    @property
    def _estimator(self):

        return RandomForestClassifier()


if __name__ == '__main__':

    nb_params = {
        'features__text_processing__vect__ngram_range': [(1, 1), (1, 2), (1, 3)],
        'clf__alpha': np.logspace(-2, 0, num=10)
    }
",model/src/_model.py,jjardel/distractingdonald,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans10_nb_only.py,diogo149/CauseEffectPairsPaper,1
"    #NB: LDA representation using an ngram range of (1,1)
    #train_features: Train reviews that have been transformed into the relevant features
    #train_labels: Labels for the training reviews, transformed into a binary variable
    #Output: A fitted model object

    if svm_clf == True:
        clf = svm.LinearSVC()
        clf.fit(train_features, train_labels)
        return clf
    elif RandomForest == True:
        clf = RandomForestClassifier(max_depth = 100, max_leaf_nodes=50, criterion='entropy')
        clf.fit(train_features, train_labels)
        return clf
    elif nb == True:
        clf = GaussianNB()
        clf.fit(train_features, train_labels)
        return clf
    else:
        return None
",machine_learning/yelp_ml.py,georgetown-analytics/yelp-classification,1
"import sys
import xgboost
sys.path.append('..')
from sklearn import ensemble
from sklearn import neighbors
import embedding_forest
def get_classifier(name, vectorizer):
  if name == 'logreg':
    return linear_model.LogisticRegression(fit_intercept=True)
  if name == 'random_forest':
    return ensemble.RandomForestClassifier(n_estimators=1000, random_state=1, max_depth=5, n_jobs=10)
  if name == 'svm':
    return svm.SVC(probability=True, kernel='rbf', C=10,gamma=0.001)
  if name == 'tree':
    return tree.DecisionTreeClassifier(random_state=1)
  if name == 'neighbors':
    return neighbors.KNeighborsClassifier()
  if name == 'embforest':
    return embedding_forest.EmbeddingForest(vectorizer)
class ParzenWindowClassifier:",parzen_windows.py,marcotcr/lime-experiments,1
"
clf_svm = svm.SVC(kernel='linear')
clf_svm.fit(tfidf_train, y_train)

clf_mNB=MultinomialNB()
clf_mNB.fit(tfidf_train, y_train)

clf_knn = KNeighborsClassifier()
clf_knn.fit(tfidf_train, y_train)

clf_ada=RandomForestClassifier(n_estimators=25)
clf_ada.fit(tfidf_train, y_train)

print clf_svm.score(tfidf_test, y_test)
print clf_mNB.score(tfidf_test, y_test)
print clf_knn.score(tfidf_test, y_test)
print clf_ada.score(tfidf_test, y_test)

predicted_svm = clf_svm.predict(tfidf_test)
#print np.mean(predicted_svm == y_train)",code_python27/tfidfANDclasification/simpleClassify.py,rcln/tag.suggestion,1
"
import numpy as np 
import sklearn.ensemble
import sklearn.linear_model

class TwoPassRegressor(object):

    def fit(self,X,Y,W=None):
        category_base = 100
        categories =  np.maximum(0, (np.log10(Y) / np.log10(category_base)).astype('int')) 
        self.first_pass = sklearn.ensemble.RandomForestClassifier(n_estimators = 20) #sklearn.linear_model.LogisticRegression()
        self.first_pass.fit(X, categories)
        
        Y = np.log(Y)
        self.regressors = [None] * (np.max(categories) + 1)
        for category in np.unique(categories):
            mask = categories == category
            print ""-- Category #%d (base %d): %d samples"" % (category, category_base, mask.sum())
            regressor = sklearn.linear_model.RidgeCV()
            regressor.fit(X[mask], Y[mask])",old/two_pass_regressor.py,iskandr/mhcpred,1
"
    #accumulate
    xTrain = numpy.append(xTrain, xTrainTemp, axis=0); xTest = numpy.append(xTest, xTestTemp, axis=0)
    yTrain = numpy.append(yTrain, yTrainTemp, axis=0); yTest = numpy.append(yTest, yTestTemp, axis=0)
    
missCLassError = []
nTreeList = range(50, 2000, 50)
for iTrees in nTreeList:
    depth = None
    maxFeat  = 4 #try tweaking
    glassRFModel = ensemble.RandomForestClassifier(n_estimators=iTrees, max_depth=depth, max_features=maxFeat,
                                                 oob_score=False, random_state=531)

    glassRFModel.fit(xTrain,yTrain)

    #Accumulate auc on test set
    prediction = glassRFModel.predict(xTest)
    correct = accuracy_score(yTest, prediction)

    missCLassError.append(1.0 - correct)",machine-learning/python_codes/07/glassRF.py,nafis/Data-Science,1
"class SkLearnWrapper(base_wrapper.BaseWrapper):
    """"""docstring for SkLearnWrapper""""""
    data = {}
    def __init__(self, config):
        super(SkLearnWrapper, self).__init__()
        self.config = config

    def __create_sk_instance(self, config):
        if(config.implementation == 'scikit' or config.implementation == 'all'):
            if config.mode == 'classification':
                instance = ensemble.RandomForestClassifier(n_estimators=config.trees, 
                                                     criterion=config.splitting_criterion.lower(),
                                                     max_features=config.features,
                                                     min_samples_split=config.min_split,
                                                     n_jobs=config.threads, random_state=config.seed,
                                                     verbose=0, oob_score=True)
            elif  config.mode == 'gpu':
                instance = hybridforest.RandomForestClassifier(n_estimators=config.trees, 
                                                     max_features=config.features,
                                                     bootstrap=True, n_jobs=config.threads)",scikit_wrapper.py,vodev/vocloud-RDF,1
"    labelIndexer = StringIndexer(inputCol=""label"", outputCol=""indexedLabel"").fit(data)
    # Automatically identify categorical features, and index them.
    # Set maxCategories so features with > 4 distinct values are treated as continuous.
    featureIndexer =\
        VectorIndexer(inputCol=""features"", outputCol=""indexedFeatures"", maxCategories=4).fit(data)

    # Split the data into training and test sets (30% held out for testing)
    (trainingData, testData) = data.randomSplit([0.7, 0.3])

    # Train a RandomForest model.
    rf = RandomForestClassifier(labelCol=""indexedLabel"", featuresCol=""indexedFeatures"", numTrees=10)

    # Chain indexers and forest in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])

    # Train model.  This also runs the indexers.
    model = pipeline.fit(trainingData)

    # Make predictions.
    predictions = model.transform(testData)",src/main/python/ml/random_forest_classifier_example.py,mrchristine/spark-examples-dbc,1
"    # Using ternary operator for shortness
    n = n_layers if n_layers else len(weights)
    
    for i in range(n):
        print('Weights and biases for layer: ' + str(i+1))
#         print np.asarray(weights[i]).shape, np.asarray(biases[i]).shape
        if bias_node:
            act = np.insert(act, 1, np.ones_like(act[:,0]), 1)
        act = get_activations(act, weights[i], biases[i])
        
    rf = ensemble.RandomForestClassifier(n_estimators=1000, oob_score=True, max_depth=5)
    rfit = rf.fit(act, labels)
    print('OOB score: %.8f\n' % rfit.oob_score_)


def plot_tSNE(data, labels, random_state=7074568, plot_name='tsne-generated_{}.png'):
    # Calculate t-SNE projections
    x_projection = TSNE(random_state=random_state).fit_transform(data)
    
    # Form the output file name",Train_SDAE/tools/evaluate_model.py,glrs/StackedDAE,1
"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=7)



print (""Fitting..."")
s = time.time()

# TODO: train your model on your training set
# .. your code here ..
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators = 30, max_depth = 10, oob_score = True, random_state = 0)
model.fit(X_train, y_train)
print (""Fitting completed in: "", time.time() - s)

# INFO: Display the OOB Score of your data
score = model.oob_score_
print (""OOB Score: "", round(score*100, 3))


",Module6/assignment6.py,jeffmkw/DAT210x-Lab,1
"    le = LabelEncoder()
    y = le.fit_transform(df_cell_train.place_id.values)
    X = df_cell_train.drop(['place_id', 'grid_cell'], axis=1).values
    X_test = df_cell_test.drop(['grid_cell'], axis=1).values
    if (X_test.shape[0] > 0):
        # Training Classifier
        if (X.shape[0] == 0):
            print(""empty training set - grid_id:""+str(grid_id))

        ##clf = SGDClassifier(loss='modified_huber', n_iter=1, random_state=0, n_jobs=-1)
        #clf = RandomForestClassifier(n_estimators=200)
        #clf = KNeighborsClassifier(n_neighbors=25, weights='distance',
        #                           metric='manhattan')
        clf = tree.DecisionTreeClassifier()

        clf.fit(X, y)
        y_pred = clf.predict_proba(X_test)

        pred_labels = le.inverse_transform(np.argsort(y_pred, axis=1)[:, ::-1][:, :3])
        return pred_labels, row_ids",competitions/facebook-v-predicting-check-ins/bench2.py,gtesei/fast-furious,1
"        if probe in test.columns:
            try:
                newtest[probe] = test[probe]
            except:
                pd.concat([newtest,test[probe]], axis = 1)
    test = newtest
    labels_test = pd.read_csv('./datasets/labels_test.txt', header = None, sep = '\t')
    labels_test = labels_test.unstack().tolist()

    pipe_svc = Pipeline([('scl', StandardScaler()),
                ('clf', RandomForestClassifier(random_state=5))])

    estimators_range = [3,10,100,1000]
    feature_range = [5,10,15,20]
    depth_range = [2, 5, 10, 20, 50]
    split_range = [2,4,6,8]
    leaf_range = [1,2,4,8]

    param_grid = [{'clf__n_estimators': estimators_range,
                   'clf__max_features': feature_range, ",scleroderma-prediction/RF_grid_search.py,Karl-Marka/data-mining,1
"    def _estimator(self):

        return MultinomialNB()


class RandomForestModel(BaseModel):

    @property
    def _estimator(self):

        return RandomForestClassifier()


if __name__ == '__main__':

    nb_params = {
        'features__text_processing__vect__ngram_range': [(1, 1), (1, 2), (1, 3)],
        'clf__alpha': np.logspace(-2, 0, num=10)
    }
",model/src/_model.py,jjardel/probablyPOTUS,1
"

def main():
    # read in data,parse into training and target sets
    dataset = np.genfromtxt(open('data/train.csv', 'r'),
                            delimiter=',', dtype='f8')[1:]
    target = np.array([x[0] for x in dataset])
    train = np.array([x[1:] for x in dataset])

    # In this case we'll use a random forest,but this could be any classifier
    cfr = RandomForestClassifier(n_estimators=100)

    # Simple K-Fold cross validation. 5 folds
    cv = cross_validation.KFold(len(train), n_folds=5, indices=False)

    # iterate through the training and test cross validation segments and
    # run the classifier on each one,aggregating the results into a list
    results = []
    for traincv, testcv in cv:
        probas = cfr.fit(train[traincv],",kaggle/predicting-biological-response/cross-validation.py,Alexoner/data-solution,1
"import parakeet
#parakeet.config.backend = ""c""

@jit
def convert_result(tran_table, res):
    return np.array([tran_table[i] for i in res])

#Restore pickled forest, just pickle the trees, the kernel is NOT pickled
def restore_forest(trees, dtype_labels):
  n_estimators = len(trees)
  f = RandomForestClassifier(n_estimators)
  f._trees = trees
  f.dtype_labels = dtype_labels
  return f


class RandomForestClassifier(object):
  """"""A random forest classifier.

    A random forest is a meta estimator that fits a number of classifical",cudatree/random_forest.py,EasonLiao/CudaTree,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn import clone

from ..conway_board import DEAD,ALIVE
from ..tools import transform_board,inverse_transform
from .classifier import Classifier

class GlobalClassifier(Classifier):
    ''' Predict each cell 1 at a time but use entire board as features. '''

    def __init__(self,clf=RandomForestClassifier()):
        '''
        GlobalClassifier constructor.
        '''
        self.base_classifier = clf
        self.classifiers = dict()
    
    def train(self, examples,verbosity=0):
        time_start = time.time()
",reverse_game_of_life/classifier/global_classifier.py,valisc/reverse-game-of-life,1
"
    features_train = pca.transform(features_train)
    features_test = pca.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))
",nytimes/step4_analysis_supervised_4(RandomizedPCA).py,dikien/Machine-Learning-Newspaper,1
"
h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",python/mlAlgorithms/demo_classifier.py,veksev/cydi,1
"        return svm.SVC(**kwargs)
    elif methodName == 'linear_svr':
        return svm.LinearSVR(**kwargs)
    elif methodName == 'svr':
        return svm.SVR()
    elif methodName == 'dtree':
        return tree.DecisionTreeClassifier(**kwargs)
    elif methodName == 'dtree_regressor':
        return tree.DecisionTreeRegressor()
    elif methodName == 'random_forests':
        return RandomForestClassifier(**kwargs)
    elif methodName == 'bayesian':
        return naive_bayes.BernoulliNB(**kwargs)
    elif methodName == 'ada_boost':
        return AdaBoostClassifier(**kwargs)
    elif methodName == 'dummy':
        return DummyClassifier(**kwargs)
    elif methodName == 'linear':
        return LinearRegression(**kwargs)
    elif methodName == 'ridge':",script/ClassiferHelperAPI.py,smenon8/AnimalWildlifeEstimator,1
"print( ""AUC score"", gmean(s) )

stage2_train[""XGBoost2""] = pd.DataFrame(temp, index=df.index)

stage2_test[""XGBoost2""] = xgbc2.predict_proba(X_test)[:,1]

x = pd.DataFrame(xgbc2.feature_importances_, index=chosen_feat)
x.sort_values(by=0, inplace=True, ascending=False)
#x.plot(kind=""bar"")

rfc = RandomForestClassifier(n_estimators=10, criterion=""entropy"", max_features=None, max_depth=7,
                             min_samples_leaf=9, n_jobs=4, random_state=1)
rfc.fit(X_sel, y)
#rfc = pickle.load( open(""rfc.p"", ""rb""))
temp = rfc.predict_proba(X_sel)[:, 1]

s = cross_val_score(rfc, X_sel, y, scoring=""roc_auc"", cv=5) 
print(s)
print( ""AUC score"", gmean(s) )
",project.py,lttviet/Santander-customer-satisfaction,1
"

# *********************************
# Choose the model
# *********************************

# Linear Classifier
#clf = SGDClassifier(loss=""modified_huber"")

# Random forest
clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)


# *********************************
#  Evaluate Model
# *********************************

X_train, X_test, y_train, y_test = train_test_split(train_features[feature_list], train_labels, test_size=0.1, random_state=0)

clf.fit(X_train, np.ravel(y_train))",examples/remigius-thoemel/titanic-iteration-2.py,remigius42/code_camp_2017_machine_learning,1
"    assert len(training_labels) == len(training_features)
    assert len(gold_labels) == len(test_features)
    return training_features, training_labels, test_features, gold_labels


def classify(training_features, training_labels, test_features, gold_labels, optimize=False, niters=1, verbose=False,
             evaluate=False):

    if optimize:
        # create parameter grid
        clf = RandomForestClassifier(n_estimators=20, n_jobs=-1)
        param_grid = {
            'n_estimators': (100, 300, 500, 700),
            'max_features': range(1, 20)
        }

        # Optimize
        print ""optimizing...""
        CV_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)
        CV_clf.fit(training_features, training_labels)",discourseParsing/relation_tagger.py,cligu/compdisc,1
"if __name__ == ""__main__"":
    ids, data, labels = load_data()
    evaluate(data,labels)

OTHER_CLASSIFEIRS_TO_TRY = [
    naive_bayes.GaussianNB(),
    naive_bayes.BernoulliNB(), 
    Pipeline([(""normalize"", StandardScaler()),
              (""svm"",svm.SVC()),]),
    svm.LinearSVC(), 
    RandomForestClassifier(),
    AdaBoostClassifier(),
    # # LinearRegression(),
    LogisticRegression(),
    GIVEN_OPT,
    KNeighborsClassifier(),
    DecisionTreeClassifier(),
    Pipeline([('linear_svm',svm.LinearSVC()),
              ('bernoulli',naive_bayes.BernoulliNB())]),
    Pipeline([(""rfe_svm"",",make_benchmarks.py,afoss925/kaggle_schizophrenia_2014,1
"    xgb_prams = {""max_depth"": 30, ""seed"": 1, ""nthread"": cpu_counts, ""silent"": False, ""n_estimators"": 50,
                 ""subsample"": 0.8}
    lsvm_prams = {}
    lr_prams = {""random_state"": 1, ""n_jobs"": cpu_counts}
    gnb_prams = {}
    mnb_prams = {}
    svm_prams = {""probability"": True, ""kernel"": 'linear', ""max_iter"": 1000}
    bnb_prams = {}

    rf_grid_search_prams = {""max_depth"": range(50, 150, 20)}
    gsearch = GridSearchCV(estimator=RandomForestClassifier(n_estimators=200, oob_score=True,
                                                            random_state=1, n_jobs=cpu_counts, min_samples_leaf=15,
                                                            min_samples_split=15),
                           param_grid=rf_grid_search_prams, iid=False, cv=3)

    # boosting
    is_single_model = False
    is_grid_search = True

    # ",config/config.py,codemayq/final_txt_classification,1
"targets_tr = traindf['cuisine']

predictors_ts = tfidfts


# classifier = LinearSVC(C=0.80, penalty=""l2"", dual=False)
# parameters = {'C':[1, 10]}
# parameters = {""max_depth"": [3, 5,7]}
# clf = LinearSVC()
# clf = LogisticRegression()
# clf = RandomForestClassifier(n_estimators=100, max_features=""auto"",random_state=50)

# classifier = grid_search.GridSearchCV(clf, parameters)
# classifier = GridSearchCV(clf, parameters)
classifier = RandomForestClassifier(n_estimators=200)

classifier=classifier.fit(predictors_tr,targets_tr)

predictions=classifier.predict(predictors_ts)
testdf['cuisine'] = predictions",deep_learn/whatiscooking/ReadCookingPlus.py,zhDai/CToFun,1
"from sklearn.qda import QDA
clf = QDA(priors=None, reg_param=0.001).fit(X_cropped, np.ravel(y_cropped[:]))
y_validation_predicted = clf.predict(X_validation)
print ""Error rate for QDA (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)



# Start Random Forest Classification
print ""Performing Random Classification:""
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=500)
forest = forest.fit(X_cropped, np.ravel(y_cropped[:]))
y_validation_predicted = forest.predict(X_validation)
print ""Error rate for Random Forest (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)
# ml_aux.plot_confusion_matrix(y_validation, y_validation_predicted, ""CM Random Forest (t1)"")
# plt.show()


# Start k nearest neighbor Classification
print ""Performing kNN Classification:""",Code/Machine_Learning_Algos/training_t4.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"        i,
        split_count,
      )
    )
  
    # Test/train split
    training_data, test_data = final_vectorized_features.randomSplit([0.8, 0.2])
  
    # Instantiate and fit random forest classifier on all the data
    from pyspark.ml.classification import RandomForestClassifier
    rfc = RandomForestClassifier(
      featuresCol=""Features_vec"",
      labelCol=""ArrDelayBucket"",
      predictionCol=""Prediction"",
      maxBins=4657,
    )
    model = rfc.fit(training_data)
  
    # Save the new model over the old one
    model_output_path = ""{}/models/spark_random_forest_classifier.flight_delays.baseline.bin"".format(",ch09/improved_spark_mllib_model.py,naoyak/Agile_Data_Code_2,1
"    """"""
    random_state = 42
    if name == 'RandomForestClassifier':
        param_dist = {""max_depth"": [3, 5, 8, None],
                      ""max_features"": [1, 3, 4],
                      ""min_samples_split"": [2, 3, 4],
                      ""min_samples_leaf"": [1, 3, 10],
                      ""bootstrap"": [True, False],
                      ""criterion"": [""gini"", ""entropy""],
                      ""n_estimators"": [10, 20, 50, 100, 200]}
        clf = RandomForestClassifier(n_estimators=20, random_state = random_state)
    elif name == 'AdaBoostClassifier':
        param_dist = {""learning_rate"": [0.01, 0.1, 1],
                      ""n_estimators"": [10, 20, 50, 100]}
        clf = AdaBoostClassifier(random_state = random_state)
    elif name == 'DecisionTreeClassifier':
        param_dist = {""max_depth"": [3, 5, 8, None],
                      ""min_samples_split"": [2, 3, 4],
                      ""min_samples_leaf"": [1, 3, 10, 20, 30]
                      }",src/classifier.py,yaricom/brainhash,1
"#  	X = X.sort(['on_ip_that_has_a_bot_mean'])
#  	X = X.fillna(method='pad')
	X = X.fillna(method='backfill')
	X.sort_index(inplace=True)
	X = X.fillna(0)
	
	X = X.drop('most_common_country', 1)
	X = X.drop('bidder_id', 1)
		
	#clf0 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=4, min_samples_split=2)
	#clf0 = RandomForestClassifier(n_estimators=100, max_depth=4, min_samples_split=2, random_state=0) 
	
	#clf = SGDClassifier(loss=""modified_huber"", penalty=""elasticnet"", n_iter=20000, class_weight='auto', alpha=0.1, epsilon=0.01)
	#clf = SVC(C=1.0, class_weight='auto', probability=True, kernel='rbf', gamma=0.0, tol=1e-1)
	#clf = AdaBoostClassifier(n_estimators=3000)
	#clf = GradientBoostingClassifier(n_estimators=3000, learning_rate=0.01, max_depth=None, min_samples_leaf=1)
	#clf = ExtraTreesClassifier(n_estimators=3000, max_depth=None, min_samples_leaf=1, random_state=0)
	
	#clf = RandomForestClassifier(n_estimators=2500, max_depth=None, min_samples_leaf=1, random_state=0, criterion='entropy') 
	clf = []",facebook/facebook_auction.py,advnturecaptlst/kaggle_archive,1
"print('Training accuracy:', knn.score(X_train_std[:, k5], y_train))
print('Test accuracy:', knn.score(X_test_std[:, k5], y_test))

#############################################################################
print(50 * '=')
print('Section: Assessing Feature Importances with Random Forests')
print(50 * '-')

feat_labels = df_wine.columns[1:]

forest = RandomForestClassifier(n_estimators=10000,
                                random_state=0,
                                n_jobs=-1)

forest.fit(X_train, y_train)
importances = forest.feature_importances_

indices = np.argsort(importances)[::-1]

for f in range(X_train.shape[1]):",code/optional-py-scripts/ch04.py,1iyiwei/pyml,1
"#                out_file='tree.dot',
#                feature_names=['petal length', 'petal width'])


#############################################################################
print(50 * '=')
print('Section: Combining weak to strong learners via random forests')
print(50 * '-')


forest = RandomForestClassifier(criterion='entropy',
                                n_estimators=10,
                                random_state=1,
                                n_jobs=2)
forest.fit(X_train, y_train)

plot_decision_regions(X_combined, y_combined,
                      classifier=forest, test_idx=range(105, 150))

plt.xlabel('petal length [cm]')",code/optional-py-scripts/ch03.py,1iyiwei/pyml,1
"    return list(cv)


def get_model():
    if FLAGS.model == 'logistic':
        return linear_model.LogisticRegressionCV(class_weight='balanced',
                                                 scoring='roc_auc',
                                                 n_jobs=FLAGS.n_jobs,
                                                 max_iter=10000, verbose=1)
    elif FLAGS.model == 'random_forest':
        return ensemble.RandomForestClassifier(n_estimators=100,
                                               n_jobs=FLAGS.n_jobs,
                                               class_weight='balanced',
                                               verbose=1)
    elif FLAGS.model == 'svm':
        return grid_search.GridSearchCV(
            estimator=svm.SVC(kernel='rbf', gamma='auto',
                              class_weight='balanced'),
            param_grid={'C': np.logspace(-4, 4, 10)}, scoring='roc_auc',
            n_jobs=FLAGS.n_jobs, verbose=1)",paper/code/models.py,skearnes/color-features,1
"    print(""Data size:"", len(data))
    ranges = data_loader_instance.get_ranges()

    verification_data_n = int(verification_data_perc * len(data))
    training_data = data[:-verification_data_n]
    verification_data = data[-verification_data_n:]

    np_training_data = as_numpy(training_data)
    np_verification_data = as_numpy(verification_data)

    rf = RandomForestClassifier(n_jobs=4, max_features=""log2"", criterion=""entropy"", n_estimators=10, bootstrap=True)
    rf.fit([t[0] for t in training_data], [t[1] for t in training_data])
    score = rf.score([t[0] for t in verification_data], [t[1] for t in verification_data])
    print(""Crisp Forest score"", score)

    ff = FuzzyEnsemble(classifier_n=8)
    ranges = [list(r) for r in ranges]
    # for i in range(len(ranges)):
        # diff = ranges[i][1] - ranges[i][0]
        # ranges[i][0] = min(0, ranges[i][0])",python/main.py,tms1337/fuzzy-classification,1
"                clfObj.shStr = 'NB'

            # KNN :
            elif clf == 'knn' or clf == 5:
                clfObj = KNeighborsClassifier(n_neighbors=n_knn, **kwargs)
                clfObj.lgStr = 'k-Nearest Neighbor (neighbor=' + str(n_knn) + ')'
                clfObj.shStr = 'KNN-' + str(n_knn)

            # Random forest :
            elif clf == 'rf' or clf == 6:
                clfObj = RandomForestClassifier(n_estimators=n_tree, **kwargs)
                clfObj.lgStr = 'Random Forest (tree=' + str(n_tree) + ')'
                clfObj.shStr = 'RF-' + str(n_tree)

            # Logistic regression :
            elif clf == 'lr' or clf == 7:
                clfObj = LogisticRegression(**kwargs)
                clfObj.lgStr = 'Logistic Regression'
                clfObj.shStr = 'LogReg'
",brainpipe/clf/utils/_classif.py,EtienneCmb/brainpipe,1
"precision, recall, f1, accuracy, support, fn, roc_auc = 0, 0, 0, 0, 0, 0, 0
colors = ['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange']

k = 10
kf = KFold(n_splits = k)

start = timer()
for train, test in kf.split(data):
	X_train, X_test = data[train, 0:-1], data[test, 0:-1]
	y_train, y_test = data[train, -1], data[test, -1]
	clf = RandomForestClassifier(n_estimators=15).fit(X_train, y_train)
	y_pred = clf.predict(X_test)
	
	#ROC curve
	y_prob = clf.predict_proba(X_test)[:,1]
	fpr, tpr, thresholds = roc_curve(y_test, y_prob, pos_label=1)
	roc_auc += auc(fpr, tpr)
	plt.plot(fpr, tpr, color=colors[randint(0, len(colors)-1)])
	
	precision += precision_score(y_test, y_pred, average = 'macro')",K-Fold/Random_Forests.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"    A = df_sub.as_matrix()
    X = A[:,6:116]
    X = X.astype(np.int64, copy=False)
    y = A[:,2]
    y = y.astype(np.int64, copy=False)

    # Split the data into a training set and a test set
    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=0)

    #Training data
    forest = RandomForestClassifier(n_estimators=10, max_depth=None, 
            min_samples_split=1, random_state=None, max_features=None)
    clf = forest.fit(X, y)
    scores = cross_val_score(clf, X, y, cv=5)
    print scores
    print ""Random Forest Cross Validation of %s: %s""%(subject,scores)
    precision_rf[subject] = scores.mean()
    df_precision.loc[subject]=precision_rf[subject]
    print ""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2)
    ",pae/final_code/src/feature_eachSub_dropNaResult.py,wasit7/book_pae,1
"test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1)


# The data is now ready to go. So lets fit to the train, then predict to the test!
# Convert back to a numpy array
train_data = train_df.values
test_data = test_df.values


#print('Training...')
forest = RandomForestClassifier(n_estimators=100)
forest = forest.fit( train_data[0::,1::], train_data[0::,0] )

#print('Predicting...')
output = forest.predict(test_data).astype(int)

from sklearn.metrics import accuracy_score

accuracy_score(y_true, output) ##   
",Python/exercise/titanic2.py,HGladiator/MyCodes,1
"

dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

ab=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.1, n_estimators=10, random_state=1)

dt=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5, max_features=10, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=1, splitter='best')

gb=GradientBoostingClassifier(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=25, presort='auto', random_state=1, subsample=1.0, verbose=0, warm_start=False)

rf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=5, max_features=10, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=75, n_jobs=1, oob_score=False, random_state=1, verbose=0, warm_start=False)",scripts/histograms/non-normalised-ml.py,jmrozanec/white-bkg-classification,1
"
    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    sklearn_model = RandomForestClassifier()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9",deepchem/models/tests/test_overfit.py,deepchem/deepchem,1
"    #branches: 5 levels or no limit.
    parameter_grid = {
    'max_features': [0.5, 1.],
    'max_depth': [5., None]
    }
    '''
    A hyperparameter optimization algorithm that is simply an exhaustive search through a manually specified subset of data
    The model used is Random Forest using 100 estimators. This comes from the sci-kit learn module. This allows us to test
    the desired range of input parameters and review the performance of each set of values on a cross-validation process.
    '''
    grid_search = GridSearchCV(RandomForestClassifier(n_estimators=100), parameter_grid, cv=10)

    #We use the result to fit our training data
    grid_search = grid_search.fit(X_train, y_train)

    #If you would like to see the output of the grid search scores for each iteration, uncomment the line below.
    #print grid_search.grid_scores_

    #This sorts the results from grid search to pick out the best performing parameters
    sorted(grid_search.grid_scores_, key=lambda x: x.mean_validation_score)",code/RandomForest.py,caynan/Titanic,1
"    """"""
    Args:
        tfidf_matrix (compressed sparse row format matrix): tfidf matrix
        of the tokenized tweets
        topic_label (1d numpy array): the topic label assigned
    Returns:
        word_importance (1d numpy array): Returns the importance of each
        word as its feature importance from a random forest
    """"""
    sparse_tfidf = sparse.csr_matrix(tfidf_matrix)
    model = RandomForestClassifier(n_jobs=-1, n_estimators=100)
    model.fit(sparse_tfidf, topic_label)
    return model.feature_importances_


def get_most_important_tweets_and_words_per_topic(tfidf, H, W, tfidf_matrix,
                                                  topic_label,
                                                  word_importance, documents,
                                                  verbose=False):
    """"""",src/tweet_text_processor.py,brityboy/BotBoosted,1
"if __name__ == ""__main__"":
    with open('models/tuned_random_forest_model.pkl') as f:
        model = pickle.load(f)
    df = pd.read_csv('data/training_df.csv')
    df.drop('Unnamed: 0', axis=1, inplace=True)
    user_id_array = df.pop('id')
    y = df.pop('label')
    y = y.values
    X = df.values
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)
    model = RandomForestClassifier(n_jobs=-1)
    model.fit(X_train, y_train)
    etcb = EvalTestCVBS(model, .05, .5, .05, 10)
    etcb.evaluate_data(X_test, y_test)
    etcb.plot_performance()",src/evaltestcvbs.py,brityboy/BotBoosted,1
"print ""Creating average feature vecs for training reviews""

trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )

print ""Creating average feature vecs for test reviews""

testDataVecs = getAvgFeatureVecs( getCleanReviews(test), model, num_features )

# ****** Fit a random forest to the training set, then make predictions
# Fit a random forest to the training data, using 100 trees
forest = RandomForestClassifier( n_estimators = 100 )

print ""Fitting a random forest to labeled training data...""
forest = forest.fit( trainDataVecs, train[""sentiment""] )

# Use the random forest to make sentiment label predictions
print ""Predicting test labels...\n""
rf_p = forest.predict_proba(testDataVecs)
auc = AUC( test['sentiment'].values, rf_p[:,1] )
print ""random forest AUC:"", auc",MovieReviewSentimentAnalysis/MovieReveiw/Word2vec_model.py,anilcs13m/Projects,1
"from cudatree import RandomForestClassifier
import hybridforest

def compare_accuracy(x,y, n_estimators = 11, bootstrap = True, slop = 0.98, n_repeat = 10):
  n = x.shape[0] / 2 
  xtrain = x[:n]
  ytrain = y[:n]
  xtest = x[n:]
  ytest = y[n:]
  cudarf = RandomForestClassifier(n_estimators = n_estimators, bootstrap = bootstrap)
  import sklearn.ensemble
  skrf = sklearn.ensemble.RandomForestClassifier(n_estimators = n_estimators, bootstrap = bootstrap)
  cuda_score_total = 0 
  sk_score_total = 0
  for i in xrange(n_repeat):
    cudarf.fit(xtrain, ytrain)
    skrf.fit(xtrain, ytrain)
    sk_score = skrf.score(xtest, ytest)
    cuda_score = cudarf.score(xtest, ytest)",test/helpers.py,EasonLiao/CudaTree,1
"    neg_x = np.vstack(neg_x)

    pos_y = np.ones(pos_x.shape[0], dtype=np.int)
    neg_y = np.zeros(neg_x.shape[0], dtype=np.int)

    X = np.vstack((pos_x, neg_x))
    y = np.concatenate((pos_y, neg_y))

    print 'started learning'
    l_start = dt.now()
    model = sklearn.ensemble.RandomForestClassifier(n_estimators=10, max_depth=20, n_jobs=1, random_state=912)
    model.fit(X, y)

    preds = model.predict_proba(X)
    print sklearn.metrics.auc(y, preds[:, 0])

    joblib.dump(model, 'detectors/test_model.mdl', compress=3)
    print 'learned in', (dt.now() - l_start)
    print 'finsihed in', (dt.now() - start)
",create_rf_model.py,WangDequan/fast-bird-part-localization,1
"						  ""penalty='l2', dual=False, C=4.0 @@ "" \
						  ""penalty='l2', dual=False, C=8.0 @@ "" 
			},
			{""model"": 'naive_bayes.GaussianNB()',
			""blend_group"":   ""NB"",  
			    ""getter"":   ""Lestimators = model_last.get_params()"",
			    ""updater"":   ""tries_left = 0"",
			     ""setter"":   """",
			     ""generator"":  """" 
			},
			{""model"": 'f16.RandomForestClassifier(n_estimators=60, random_state=Lnum, n_jobs=1)',
			""blend_group"":   ""RDT1"",  
			    ""getter"":   ""Lestimators = model_last.get_params()['n_estimators']"",
			    ""updater"":   ""Lestimators += 20"",
			    ""setter"":   ""n_estimators = Lestimators, warm_start = True"",
			    ""generator"": 
						""max_depth=2 @@"" \
						""max_depth=6 @@"" \
						""min_samples_split=8 @@"" \
						""min_samples_split=4 @@"" \",lib/engine_models.py,djajetic/AutoML3,1
"    X : numpy array
        X data points with features
        
    y : numpy array
        Y values. 1 for Preictal and 0 for Interictal
    
    Returns
    -------
    
    """"""
    clf = RandomForestClassifier(n_estimators = 10, n_jobs = -1)
    clf = clf.fit(X, y)

    return clf


if __name__ == ""__main__"":
    preictal_files = glob(""../Dog_1/*preictal_segment_000*.mat"")
    preictal_records = readdata(preictal_files)
    ",pred.py,navtejsingh/epilepsypred,1
"                            # weight components in FeatureUnion - could use for fitting
                            transformer_weights={
                                             'countvec': 0.5,
                                             'features': 0.5,
                                             }
                            )
                 
     
# Make a pipeline with the classifiers
pipeline = Pipeline([ ('vectorizer', vectorizer),
                       ('clf', RandomForestClassifier(max_features='auto', n_estimators=100))
                    ])

if True:
    # Extract the data - can either use the pkl files or extract from Excell
    data, target, dataframe = get_dataset(filename='../When I am criticised.xlsx')
    #with open('data.pkl') as f: data, target, dataframe = cPickle.load(f)
    data_ref, target_ref, _ = get_dataset(filename='../WhenCriticisedRef.xlsx')
    #with open('data_ref.pkl') as f: data_ref, target_ref = cPickle.load(f)
     ",sklearn_final.py,linucks/textclass,1
"f1 = [x for x in cols if x not in set(['admission_type_id', 'discharge_disposition_id'])]

features1 = df[list(f1)].values
features2 = df[list(f2)].values
f1_train, f1_test, f2_train, f2_test, l1_train, l1_test, l2_train, l2_test = \
    train_test_split(features1, features2, labels1, labels2, test_size=0.25)

# clf = KNeighborsClassifier(10, weights='distance')
# clf1 = AdaBoostClassifier(n_estimators=50)
# clf2 = AdaBoostClassifier(n_estimators=50)
clf1 = RandomForestClassifier(n_jobs=-1, n_estimators=50, min_samples_split=70, max_features='auto')
clf2 = RandomForestClassifier(n_jobs=-1, n_estimators=50, min_samples_split=70, max_features='auto')
# clf1 = GaussianNB()
# clf2 = GaussianNB()
clf1.fit(f1_train, l1_train)
clf2.fit(f2_train, l2_train)

pred1 = clf1.predict(f1_test)
acc1 = accuracy_score(pred1, l1_test)
f1_test = np.insert(f1_test,6, pred1,1)",flask/classify.py,isabellewei/deephealth,1
"    plt.xticks(range(nfeatures), columns[index],fontsize=14)
    plt.xlim([-1,nfeatures])
#    plt.show()
    plt.clf()
    plt.cla()

    return top_features

def feature_direction(idx,dataframe,label,threshold):
    y_pred = [];
    clf_rf = RandomForestClassifier(n_estimators=100, max_depth=80,
                               min_samples_split=5)
    x_trainr,x_test,y_train,y_test = train_test_split(dataframe,label,test_size = 0.2)
    col_names = list(dataframe.columns.values)
    maximum_val = x_train[:,idx].max()
    minimum_val = x_train[:,idx].min()
    feature_name = col_names[idx]
    for i,col in enumerate(x_train):
        if i != idx:
            x_train[:,i] = np.mean(x_train[:,i])",WorldBank2015/Code/modeling/model_pipeline_script.py,eredmiles/Fraud-Corruption-Detection-Data-Science-Pipeline-DSSG2015,1
"print(""Best estimator found by grid search:"")
print(clf.best_estimator_)



    
#==============================================================================
# RandomForest (Classifier Variant) Model Fitting parameters
#==============================================================================

estimator = Pipeline([(""forest"", RandomForestClassifier(random_state=0, n_estimators=100))])
estimator.fit(predictors, outcomes)

predicted = estimator.predict(predictors) 
prediction_scores  = accuracy_score(outcomes, predicted) #This code will change, fi we cross validate

#test_predicted = estimator.predict(test_predictors) 
#prediction_scores  = accuracy_score(test_outcomes, test_predicted)

with open(os.path.join(os.getcwd(), 'data','RF.pickle'), 'wb') as pickle_file:",ModelFitting/single_SVM.py,georgetown-analytics/nba-tracking,1
"    np.set_printoptions(precision=4, suppress=True, threshold=5000)
    print np.array(zip(target, res))
    
    
    '''RForest & Boosting
    C_range = 1.5 ** np.arange(-10, 5)
    gamma_range = 1.5 ** np.arange(-14, 0, 2)
    class_weight_range = ['auto']
    param_grid = dict(C=C_range, class_weight=class_weight_range) #, gamma=gamma_range)
    cv = StratifiedKFold(y=target, n_folds=5, shuffle=True)
    #grid = GridSearchCV(sklearn.ensemble.RandomForestClassifier(min_samples_split=3, min_samples_leaf=1, bootstrap=False, n_jobs=3), param_grid=dict(n_estimators=np.array([50, 100, 200, 500, 1000])), cv=cv, n_jobs=16) 
    #grid = GridSearchCV(sklearn.ensemble.GradientBoostingClassifier(min_samples_split=3, min_samples_leaf=1), param_grid=dict(n_estimators=np.array([80, 100, 200, 300])), cv=cv, n_jobs=16) 
    #grid = GridSearchCV(sklearn.ensemble.AdaBoostClassifier(), param_grid=dict(n_estimators=np.array([25, 50, 80, 100, 200, 300])), cv=cv, n_jobs=16) 
    grid = GridSearchCV(sklearn.ensemble.AdaBoostClassifier(base_estimator=svm.SVC(probability=True, kernel='linear', C=1)), param_grid=dict(n_estimators=np.array([8,12,15,20,30]), learning_rate=[0.005, 0.01, 0.05, 0.1,0.2]), cv=cv, n_jobs=16) 
    
    # train on all labeled data and predict on the unlabeled data
    grid.fit(data, target)
    print ""The best classifier is: "", grid.best_estimator_
    
    for params, mean_score, scores in grid.grid_scores_:",sklearnClassifiers/simpleClassifier4.py,rampasek/seizure-prediction,1
"feature_vectors = []
for raster in train_rasters + test_rasters + target_rasters:
    feature_vectors.append(compute_features(raster))

X = feature_vectors[:len(train_rasters)]
Y = feature_vectors[len(train_rasters):len(train_rasters)+len(test_rasters)]
Z = feature_vectors[len(train_rasters)+len(test_rasters):]

# Create classifier object.
# n_estimators is the number of trees in the random forest.
c = RandomForestClassifier(n_estimators = 100)

print 'Train classifier'
X, train_labels = np.nan_to_num(np.array(X)), np.array(train_labels)    # make sure to replace NaN with finite numbers
c.fit(X, train_labels)
class_names = c.classes_
print class_names

print 'Test classifier'
Y, test_labels = np.nan_to_num(np.array(Y)), np.array(test_labels)    ",examples/polygon_classify_random_forest/polygon_classify_rf.py,DigitalGlobe/mltools,1
"X = training.iloc[:,1:-1]
y = training['country_destination']



x_train,x_valid,y_train,y_valid = train_test_split(X,y,test_size=0.3,random_state=None)

# Train classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsOneClassifier
clf = OneVsOneClassifier(RandomForestClassifier(n_estimators=50,n_jobs=5))
clf.fit(x_train,y_train)
print( clf.feature_importances_ );

# Run Predictions
from sklearn.metrics import confusion_matrix, accuracy_score
y_preds = clf.predict(x_valid)
print( confusion_matrix(y_valid,y_preds) );
print( ""Accuracy: %f"" % (accuracy_score(y_valid,y_preds)) );
f = open('randomForest_take2.txt', 'w')",prototype_alpha/randomForest_take2.py,valexandersaulys/airbnb_kaggle_contest,1
"# import pstats
# cProfile.run(""inject_newlines, features = analyze_corpus(sys.argv[1])"", ""stats"")
# p = pstats.Stats('stats')
# p.strip_dirs().sort_stats(""time"").print_stats()

inject_newlines, indents, whitespace, features = analyze_corpus(sys.argv[1])
# for i in range(len(indents)):
#     print whitespace[i], features[i]
vec, transformed_features = convert_categorical_data(features)

newline_predictor_RF = RandomForestClassifier(n_estimators=300)
newline_forest = newline_predictor_RF.fit(transformed_features, inject_newlines)
print_importances(newline_forest, vec.get_feature_names(), n=15)

indent_predictor_RF = RandomForestClassifier(n_estimators=300)
indent_forest = indent_predictor_RF.fit(transformed_features, indents)
print_importances(indent_forest, vec.get_feature_names(), n=15)

whitespace_predictor_RF = RandomForestClassifier(n_estimators=300)
whitespace_forest = whitespace_predictor_RF.fit(transformed_features, whitespace)",python/src/groom.py,antlr/codebuff,1
"    return lambdas_ret_normalized

def assign_proba2(sigma, sigmas_ret, features_old, features_new, new_prod):#random forest classifier
    lambdas_ret = np.zeros_like(sigmas_ret[:,0], dtype=np.float32)
    #global imp
    #parameters of the Random Forest classifier
    n_estimators = 20
    
    #training
    products_ranked = (sigma!=len(sigma)-1)
    model = RandomForestClassifier(n_estimators=n_estimators)
    X = features_old[products_ranked,:20]###
    Y = sigma[products_ranked]
    weights = 1+max(Y) - Y
    model.fit(X,Y, sample_weight=weights)
    
    p = model.predict_proba(features_new[new_prod,:20].reshape(1,-1))###
    
    lambdas_ret = p[0]#to put the good shape; keeps the original values
    #imp += model.feature_importances_ ",sample/lib/fcns_generalize.py,hugopalmer/assortment_optimization,1
"
# load data
train = np.genfromtxt(os.path.join(sys.argv[1], ""digitsdata_17_train.csv""),\
	              delimiter="","", skip_header=1)
X = train[:, 1:]
y = train[:, 0]

# train model
params = {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 1,
          'random_state': 0}
clf = ensemble.RandomForestClassifier(**params)

clf.fit(X, y)
predp = clf.predict_proba(X)

# export output
np.savetxt(os.path.join(sys.argv[1], 'predict_train_py_forest_prob.csv'), predp, fmt='%f', delimiter="","")",SAS_EM_PythonIntegration/em_digitsdata_forest.py,sassoftware/enlighten-integration,1
"    path = 'data/test/Data_' + str(filename) + '.csv'
    df = pd.read_csv(path)
    df = df[df.FeedBackEvent != 0]
    df = df.drop('FeedBackEvent', axis = 1)
    if i == 0:
        test = np.array(df)
    else:
        test = np.vstack((test, np.array(df)))


clf = ensemble.RandomForestClassifier(n_jobs = -1,
				     n_estimators=150,
			             random_state=42)

clf.fit(train, labels.Prediction.values)
preds = clf.predict_proba(test)[:,1]

submission['Prediction'] = preds
submission.to_csv('benchmark.csv', index = False)
",BCI/btb.py,tanayz/Kaggle,1
"    training_label = [arr for idx_arr, arr in enumerate(label)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = label_binarize(np.hstack(training_label).astype(int),
                                    [0, 255])
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    pred_prob = crf.fit(training_data, np.ravel(training_label)).predict_proba(
        testing_data)

    result_cv.append([pred_prob, crf.classes_])

# Save the information
path_store = '/data/prostate/results/mp-mri-prostate/exp-1/mrsi-citrate-choline-fit-ratio'
if not os.path.exists(path_store):
    os.makedirs(path_store)",pipeline/feature-classification/exp-1/mrsi/pipeline_classifier_mrsi_citrate_choline_fit_ratio.py,I2Cvb/mp-mri-prostate,1
"    test_features.write.parquet(test_feature_parquet_path)

    print test_features.take(1)
    test_features_bow = test_features.rdd.map(lambda row:assign_pooling(row,clusterCenters=clusterCenters, pooling=""max""))
    test_featuresSchema = sqlContext.createDataFrame(test_features_bow)
    test_featuresSchema.registerTempTable(""testimages"")
    test_featuresSchema =  test_featuresSchema.withColumn('features',test_featuresSchema.features.cast(VectorUDT()))
    #featuresSchemaTest = test_featuresSchema.rdd.map(lambda x:parsePoint(x['label'],x['features']))

    #svm = SVMWithSGD.train(trainingData, iterations=10)
    #rf = RandomForestClassifier(labelCol=""label"", featuresCol=""features"", numTrees=5)
    lr = LogisticRegression(maxIter=10, regParam=0.1)
    ovr = OneVsRest(classifier=lr)
    model = ovr.fit(featuresSchema)
    predictions = model.transform(test_featuresSchema)

   
    # Select example rows to display.
    print predictions.show()
    print ""predictions!!!""",traditional_ml/opencvTrial.py,unisar/CIFARClassification,1
"    if optimize:
        print 'Training the classifier w/optimization...\n'
        clf = optimize_classifier(opt_method='grid', clf_name=clf_name)
    else:
        print 'Training the classifier w/o optimization...\n'
        if clf_name == 'svm':
            clf = svm.SVC(kernel='poly', degree=2)
        elif clf_name == 'tree':
            clf = tree.DecisionTreeClassifier()
        elif clf_name == 'forest':
            clf = ensemble.RandomForestClassifier(criterion='entropy', max_depth=10, max_features='auto',
                                                  n_estimators=40)
        elif clf_name == 'adaboost':
            clf = ensemble.AdaBoostClassifier(algorithm='SAMME.R', n_estimators=100, learning_rate=0.2)
        elif clf_name == 'gradientboost':
            clf = ensemble.GradientBoostingClassifier(learning_rate=0.2, n_estimators=150)
        else:
            raise ValueError, 'ERROR: Unknown classifier name %s' % (clf_name)

    if num_samples is None:",01_assignment/classify_mnist.py,grantathon/computer_vision_machine_learning,1
"def choose_classifier(classifier, # which classifier to use
				# parameters for the tree based classifiers
                trees_n_estimators = None, trees_criterion = None, 
                trees_max_features = None, trees_max_depth = None,
                # the ones for k-nearest-neighbors
                knn_n_neighbors=None, knn_weights=None):
				#note that possibly inactive variables have to be optional
				#as pysmac does not assign a value for inactive variables
				#during the minimization phase
    if classifier == 'random_forest':
        predictor = sklearn.ensemble.RandomForestClassifier(
						trees_n_estimators, trees_criterion,
						trees_max_features, trees_max_depth)
    elif classifier == 'extra_trees':
        predictor = sklearn.ensemble.ExtraTreesClassifier(
						trees_n_estimators, trees_criterion,
						trees_max_features, trees_max_depth)
    elif classifier == 'k_nearest_neighbors':
        predictor = sklearn.neighbors.KNeighborsClassifier(
						knn_n_neighbors, knn_weights)",examples/sklearn_model_selection.py,sfalkner/pySMAC,1
"print getf1(L, Xte, yte)

print 'svm'
S = SVC(C = 0.1)
S.fit(Xtr,ytr)
print S.score(Xte,yte)
print S.score(Xtr,ytr)
print getf1(S, Xte, yte)

print 'forest'
R = RandomForestClassifier()
R.fit(Xtr,ytr)
print R.score(Xte,yte)
print R.score(Xtr,ytr)
print getf1(R, Xte, yte)

print 'knn'
K = KNeighborsClassifier(9)
K.fit(Xtr,ytr)
print K.score(Xte,yte)",baseline.py,ricsoncheng/sarcasm_machine,1
"#All missing ebmbarks just make them embark from most common place
test_data[test_data[0::,9] == '',9] = np.round(np.mean(test_data[test_data[0::,9] != '',9].astype(np.float)))
#All the missing prices assume median of their respectice class
for i in xrange(np.size(test_data[0::,0])):
    if test_data[i,7] == '':
        test_data[i,7] = np.median(test_data[(test_data[0::,7] != '') & (test_data[0::,0] == test_data[i,0]) ,7].astype(np.float))

test_data = np.delete(test_data,[1,6,8],1) #remove the name data, cabin and ticket

print 'Training'
forest = RandomForestClassifier(n_estimators=100)

forest = forest.fit(train_data[0::,1::],train_data[0::,0])

print 'Predicting'
output = forest.predict(test_data)

open_file_object = csv.writer(open(""../csv/forest_basedmodel.csv"", ""wb""))
open_file_object.writerow([""PassengerId"",""Survived""])",tutorial/forest_basedmodel.py,alexeyza/Kaggle-Titanic,1
"print data_test

####################
# Building the model
####################

labels = data[:, 10]

x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.10)

clf = RandomForestClassifier()

clf = clf.fit(x_train, y_train)

preds = clf.predict(data_test)

print preds


[ x for x in input if any(map(lambda w: sorted(x) == sorted(w) and x != w, input)) ]",dota_2/dota2.py,mickaellegal/HackerRank,1
"

    

    # train random forest
    print(""\nFinding optimal random forest"")
    rf_param_grid = {""max_features"": ['sqrt', 'log2'],
                     ""max_depth"": [None]}
    # do cross validation to determine optimal parameters to the model
    rf = run_grid_search(X=x_train, y=y_train,
                         model=RandomForestClassifier(n_estimators=100, random_state=random_seed),
                         param_grid=rf_param_grid,
                         cv=cv, n_jobs=n_jobs, verbose=verbose)
    # train a model on the full training set using the optimal parameters
    print(""Best random forest performance on test set:"", rf.score(x_test, y_test))

    

    # train k-nearest neighbors
",src/classify_health_condition/eval_max_and_avg_topic_features.py,robert-giaquinto/text-analysis,1
"        clf = clf.fit(X_train_cv, y_train_cv)
        y_pred = clf.predict(X_test_cv)
        scores[test_index] = metrics.accuracy_score(y_test_cv.astype(int), y_pred.astype(int))
    print (""Mean score: {0:.3f} (+/-{1:.3f})"").format(np.mean(scores), sem(scores))


loo_cv(X_train, y_train, clf)


from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=10, random_state=33)
clf = clf.fit(X_train, y_train)
loo_cv(X_train, y_train, clf)

",c2-DecisionTree.py,chongguang/scikitLearning,1
"from sklearn.ensemble import RandomForestClassifier

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)


result1 = tpot_data.copy()

# Perform classification with a random forest classifier
rfc1 = RandomForestClassifier(n_estimators=500, max_features=min(49, len(result1.columns) - 1))
rfc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)
result1['rfc1-classification'] = rfc1.predict(result1.drop('class', axis=1).values)",tutorials/tpot_titanic_pipeline.py,pronojitsaha/tpot,1
"    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_pca1_knn_only.py,diogo149/CauseEffectPairsPaper,1
"from cross_validation_result import CrossValidationResult
from titanic_kaggle import TitanicKaggle
from feature_finder_2 import FeatureFinder

def experiment():
  print(TitanicKaggle.SEPARATOR)
  print('EXPERIMENTING')
  print(TitanicKaggle.SEPARATOR)

  # set up experiment titanic instance
  titanic = TitanicKaggle(lambda: RandomForestClassifier(n_estimators=50, random_state=123))
  titanic.initialize()

  # analyze
  titanic.print_sample_data()
  titanic.analyze_data()
  titanic.get_deviation_per_feature()
  titanic.create_some_plots()

  # prepare data for prediction",titanic/run.py,furgerf/kaggle-projects,1
"# np.exp(np.linspace(np.log(1000), np.log(50000), num = 15)).astype('int')
total_iters = len(all_classes) * len(all_examples) * len(all_features) * len(thresholds)

i = 1 
# thresholds =  [0.001, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, .1, .2]

# use just one set of random forests, since we seem to be leaking memory
rfs = {}
for f in all_features:
  max_features = max_features = int(np.sqrt(f))
  rfs[f] = cudatree.RandomForestClassifier(n_estimators = 3, bootstrap = False, max_features = max_features)

for n_classes in reversed(all_classes):
  print ""n_classes"", n_classes
  for n_examples in reversed(all_examples):
    print ""n_examples"", n_examples
    y = np.random.randint(low = 0, high = n_classes, size = n_examples)
    for n_features in reversed(all_features):
      print ""n_features"", n_features
      max_features = int(np.sqrt(n_features))",estimate_threshold.py,EasonLiao/CudaTree,1
"    train, test = prepare_data(train), prepare_data(test)


    ### Modeling
    # DECISION TREE
    # clf = tree.DecisionTreeClassifier(min_samples_leaf=10, random_state=123, criterion=""entropy"", max_leaf_nodes=20)  # Features: Pclass, Sex, Age, Fare (initial), Embarked, Title (simplified),Deck, Family

    # ADA BOOST
    # clf = ensemble.AdaBoostClassifier(n_estimators=400, base_estimator=tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=2), learning_rate=0.05)    # Kaggle score : 0.689
    #
    clf = ensemble.RandomForestClassifier(n_estimators=70, min_samples_leaf=2, criterion=""entropy"", max_depth=10)     # RANDOM FOREST


    clf.fit(train, target)
    print clf.feature_importances_
    print ""Accuracy (training):"", my_decimal(clf.score(train, target))
    print ""Accuracy (cross-validation):"", my_decimal(cross_validation.cross_val_score(clf, X=train, y=target, cv=10).mean())


    # ### Submission",Kagle_Titanic_Python/skLearn.py,Skobnikoff/Datasets,1
"#print('Labels After Preprocessing :\n', train_labels[:10])


# *********************************
# Choose the model
# *********************************

# Linear Classifier
#clf = SGDClassifier(loss=""modified_huber"")

clf = BaggingClassifier(RandomForestClassifier(n_estimators=100, n_jobs=-1),max_samples=0.5, max_features=0.5, random_state=0)

# *********************************
#  Evaluate Model
# *********************************

X_train, X_test, y_train, y_test = train_test_split(train_features[feature_list], train_labels, test_size=0.1, random_state=0)

clf.fit(X_train, np.ravel(y_train))
",examples/remigius-thoemel/titanic-iteration-4.py,remigius42/code_camp_2017_machine_learning,1
"                  X, y_class, sample_weight=np.asarray([-1]))


def test_base_estimator():
    # Test different base estimators.
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)",projects/scikit-learn-master/sklearn/ensemble/tests/test_weight_boosting.py,DailyActie/Surrogate-Model,1
"  trainDataFeatures = trainData[:,0]

  # Train H2O GBM Model:
  #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")
  from h2o.estimators.random_forest import H2ORandomForestEstimator
  rf_h2o = H2ORandomForestEstimator(ntrees=1, max_depth=1, nbins=100)
  rf_h2o.train(x='X', y=""y"", training_frame=alphabet)

  # Train scikit GBM Model:
  # Log.info(""scikit GBM with same parameters:"")
  rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
  rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

  # h2o
  rf_perf = rf_h2o.model_performance(alphabet)
  auc_h2o = rf_perf.auc()

  # scikit
  auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_smallcatRF.py,pchmieli/h2o-3,1
"    X_test, ids = X[:, 1:], X[:, 0]
    return X_test.astype(float), ids.astype(str)

def validate_model(model, validation_x, validation_y):
    y_prob = model.predict_proba(validation_x)
    score = logloss_mc(validation_y, y_prob)
    print("" -- {} Multiclass logloss on validation set: {:.4f}."".format(type(model).__name__, score))
    return score

def train_rf(training_x, training_y, n_est=10, max_d=5, max_f='auto'):
    clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, max_depth=max_d, max_features=max_f)
    clf.fit(training_x, training_y)
    return clf
    
def train_ex(training_x, training_y, n_est=10, max_d=5, max_f='auto'):
    clf = ExtraTreesClassifier(n_jobs=-1, n_estimators=n_est, max_depth=max_d, max_features=max_f)
    clf.fit(training_x, training_y)
    return clf
    
#def train_ada(training_x, training_y, n_est=10):",otto-gb.py,ryanswanstrom/kaggle-otto,1
"#print ""RBF Kernel test score: "", rbf_test_score
#print ""Poly Kernel test score: "", poly_test_score
################################### Random Forests ####################################
print ""##### Random Forest ######""
n_estimators_list = range(1, 16, 1)
result_random_forests = []
max_score_rf = float(""-inf"")
best_param_rf = None
for n_estimators in n_estimators_list:
    print ""Testing n_estimators = "", n_estimators
    rf_clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=None, min_samples_split=1, random_state=0)
    scores = cross_validation.cross_val_score(rf_clf, X_train, y_train, scoring=""accuracy"", cv=6)
    result_random_forests.append(scores.mean())
    if scores.mean() > max_score_rf:
        max_score_rf = scores.mean()
        best_param_rf = {""n_estimators"": n_estimators}
print ""number of trees: "", n_estimators_list
print ""results: "", result_random_forests
print ""best accuracy: "", max_score_rf
print ""best parameter: "", best_param_rf",analysis.py,Sapphirine/Human-Activity-Monitoring-and-Prediction,1
"    tfidf = sklearn.feature_extraction.text.TfidfTransformer(use_idf=False)
    x_text_train_tfidf = tfidf.fit_transform(x_text_train_vect)

    mutual_info = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.mutual_info_classif, k=K_BEST)
    x_text_train_k_best = mutual_info.fit_transform(x_text_train_tfidf, y_train)

    all_train_features = scipy.sparse.hstack((x_text_train_k_best, x_liwc_train)).A

    from sklearn.ensemble import *

    clf = RandomForestClassifier(n_estimators=500).fit(all_train_features, y_train)
    predicted = clf.predict(all_train_features)
    train_error = 1 - sklearn.metrics.accuracy_score(y_train, predicted)

    x_text_test_vect = vect.transform(x_text_test)
    x_text_test_tfidf = tfidf.transform(x_text_test_vect)
    x_text_test_k_best = mutual_info.transform(x_text_test_tfidf)
    all_test_features = scipy.sparse.hstack((x_text_test_k_best, x_liwc_test)).A
    predicted = clf.predict(all_test_features)
    test_error = 1 - sklearn.metrics.accuracy_score(y_test, predicted)",step_3/scripts/train_relevance_model.py,chuajiesheng/twitter-sentiment-analysis,1
"# Import the random forest package
from sklearn.ensemble import RandomForestClassifier 
from sklearn import cross_validation
import numpy as np
dataset = np.loadtxt('training_data.csv', delimiter="","")

# Create the random forest object which will include all the parameters
# for the fit
forest = RandomForestClassifier(n_estimators = 100)

# Fit the training data to the Survived labels and create the decision trees
forest_fit = forest.fit(dataset[0::,1::],dataset[0::,0])

importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)

# Take the same decision trees and run it on the test data",random_forest.py,alexcritschristoph/MicrobialGenomicsScripts,1
"import parzen_windows
import embedding_forest
from load_datasets import *
import argparse
import collections
    
def get_classifier(name, vectorizer):
  if name == 'logreg':
    return linear_model.LogisticRegression(fit_intercept=True)
  if name == 'random_forest':
    return ensemble.RandomForestClassifier(n_estimators=1000, random_state=1, max_depth=5, n_jobs=10)
  if name == 'svm':
    return svm.SVC(probability=True, kernel='rbf', C=10,gamma=0.001)
  if name == 'tree':
    return tree.DecisionTreeClassifier(random_state=1)
  if name == 'neighbors':
    return neighbors.KNeighborsClassifier()
  if name == 'embforest':
    return embedding_forest.EmbeddingForest(vectorizer)
",data_trusting.py,marcotcr/lime-experiments,1
"def train(x_train, y_train, algo=""DT""):
    """"""
    :param x_train: training data features
    :param y_train: training data labels
    :param algo: choice of learning algorithm [DT, RF, KNN, MLP]
    :return: model object
    """"""
    if algo == ""DT"":
        cla = tree.DecisionTreeClassifier()
    elif algo == ""RF"":
        cla = ensemble.RandomForestClassifier(max_features=40,
                                              n_estimators=500,
                                              n_jobs=1,
                                              max_depth=150)
    elif algo == ""KNN"":
        cla = neighbors.KNeighborsClassifier()
    elif algo == ""MLP"":
        cla = neural_network.MLPClassifier()

    # Enable one of the optimization methods",project/CharRec/CharRec.py,DrigerG/IIITB-ML,1
"    trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )

    print ""Creating average feature vecs for test reviews""

    testDataVecs = getAvgFeatureVecs( getCleanReviews(test), model, num_features )


    # ****** Fit a random forest to the training set, then make predictions
    #
    # Fit a random forest to the training data, using 100 trees
    forest = RandomForestClassifier( n_estimators = 100 )

    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit( trainDataVecs, train[""sentiment""] )

    # Test & extract results
    result = forest.predict( testDataVecs )

    # Write the test results
    output = pd.DataFrame( data={""id"":test[""id""], ""sentiment"":result} )",analysis/Word2Vec_AverageVectors.py,weiwang/popcorn,1
"f2 = np.array([4,6,8,4,4,4,2,4,4,4,4,4,4,4,4,4,2,3,4,4,4,4,4,4,4,4,4,8,5,8]);
f3 = np.array([16,19,24,24,24,13,7,15,15,16,14,16,16,16,14,12,11,12,17,16,16,13,16,15,13,13,17,31,32,28]);
f4 = np.array([38,49,70,45,40,27,57,47,53,57,48,53,49,61,49,49,51,38,34,53,63,46,56,42,36,33,70,90,91,93]);
gid=  np.array([49,50,56,179,180,188,227,230,234,244,245,248,255,260,262,266,267,269,271,274,281,287,288,289,290,291,292,301,302,303]) ; 
gt_class = np.array([4,4,3,4,4,4,4,2,4,2,4,4,4,2,4,4,4,2,2,4,2,4,4,4,2,2,2,4,4,4]) ;
proba = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,0.935135135135135,1,1,1,0.785046728971963,0.884297520661157,1,1,1,1,1,0.866666666666667,0.982456140350877,0.717592592592593,0.934689507494647,0.931665062560154,0.918892185954501]) ; 

X = np.column_stack((f1,f2,f3,f4)); 
Y = np.array( gt_class) ; 

clf = RandomForestClassifier(n_estimator, verbose=0,criterion=""entropy"") ; 
clf = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=10, min_samples_leaf=1,   max_features=None, random_state=4 ) ; 
#clf = clf.fit(X,Y ) ; 
#  clf = []
result = []
scaler = preprocessing.StandardScaler().fit(X) ;  


kf_total = cross_validation.KFold(len(X), n_folds = 3, indices = True, shuffle = True, random_state = 4) ;
result = pd.DataFrame() ; ",script/random_forest_classification.py,Remi-C/LOD_ordering_for_patches_of_points,1
"#test_labels = pd.read_csv('labels_test.txt', header = None, sep = '\t')

train_labels = train_labels.unstack().tolist()
#test_labels = test_labels.unstack().tolist()


#print(train_labels)
#print(train)


lr = RandomForestClassifier(n_estimators = 1000, max_depth = None)
lr = lr.fit(train, train_labels)

predictions = lr.predict(train)
predictions = list(predictions)
predictions_test = lr.predict(test)
predictions_test = list(predictions_test)

#print(predictions)
",exercise-scripts/Predict_RF.py,Karl-Marka/data-mining,1
"        match_col = """"
        match_col = [col for col in X.columns.values if X_score_col == col]  # Check if X Train Col exists in X Score Col
        matching_cols.extend(match_col)

    # Set dataframes to include only matching columns
    X = X[matching_cols].sort_index()
    XScore = XScore[matching_cols].sort_index()

    # RANDOM FOREST
    from sklearn.ensemble import RandomForestClassifier
    forest = RandomForestClassifier(criterion=splitcriteria_param, n_estimators=int(numbtrees_param), n_jobs=1)
    # FIT THE RANDOM FOREST
    forest.fit(X, y)

    # CROSS VALIDATION
    # Evaluate the model using 5-fold cross-validation
    from sklearn.cross_validation import cross_val_score
    # from sklearn.metrics import roc_auc_score
    accuracyScore = cross_val_score(forest, X, y, scoring='accuracy', cv=5)
    recallScore = cross_val_score(forest, X, y, scoring='recall', cv=5)",modules/ml_algorithms.py,DoubleEE/webdataconnector_ml,1
"    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=15)), ('model', KNeighborsClassifier(n_neighbors=150))])
    #model = Pipeline([('filter', SelectPercentile(chi2, percentile=20)), ('model', MultinomialNB(alpha=0.1))])
    #model = LogisticRegression(penalty='l2', dual=True, tol=0.0001, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None)#opt kaggle params
    #model = LogisticRegressionMod(penalty='l2', dual=False, tol=0.0001, C=1, fit_intercept=True, intercept_scaling=1.0, class_weight=None)#opt kaggle params
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=100)), ('model', LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1, fit_intercept=True, intercept_scaling=10.0, class_weight=None))])
    #model = AdaBoostClassifier(base_estimator=LogisticRegressionMod(penalty='l2', dual=False, tol=0.0001, C=1, fit_intercept=True,intercept_scaling=1.0),learning_rate=0.1,n_estimators=50,algorithm=""SAMME.R"")
    #model = KNeighborsClassifier(n_neighbors=10)
    #model=SVC(C=0.3,kernel='linear',probability=True)
    #model=LinearSVC(penalty='l2', loss='l2', dual=True, tol=0.0001, C=1.0)#no proba
    #model = SVC(C=1, cache_size=200, class_weight='auto', gamma=0.0, kernel='linear', probability=True, shrinking=True,tol=0.001, verbose=False)
    #model=   RandomForestClassifier(n_estimators=200,max_depth=None,min_samples_leaf=10,n_jobs=1,criterion='entropy', max_features='auto',oob_score=False)
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=80)), ('model', AdaBoostClassifier(n_estimators=100,learning_rate=0.1))])
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=50)), ('model', BernoulliNB(alpha=0.1))])#opt sparse 0.849
    #model = Pipeline([('filter', SelectPercentile(f_classif, percentile=50)), ('model', RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=10,n_jobs=1,criterion='entropy', max_features='auto',oob_score=False))])
    #opt greedy approach
    #model = AdaBoostClassifier(n_estimators=500,learning_rate=0.1)

    #model = ExtraTreesClassifier(n_estimators=100,max_depth=None,min_samples_leaf=10,n_jobs=4,criterion='entropy', max_features=20,oob_score=False)#opt
    #model = AdaBoostClassifier(n_estimators=100,learning_rate=0.1)
    #model = GradientBoostingClassifier(loss='deviance', learning_rate=0.01, n_estimators=500, subsample=0.5, min_samples_split=6, min_samples_leaf=10, max_depth=5, init=None, random_state=123,verbose=False)#opt 0.883",competition_scripts/stumble.py,chrissly31415/amimanera,1
"        f1         : f1 score
        parameters : previous parameters in the order previously specified
    """"""
    if max_depth==-1:
        max_depth = None

    labels = np.unique(Y_train)

    ## # Run rf
    # Define classifier
    rf = RandomForestClassifier(n_estimators = n_estimators,
                                criterion    = criterion,
                                max_features = max_features,
                                max_depth    = max_depth,
                                n_jobs       = n_jobs)
    # Fit
    rf.fit(X_train, Y_train)

    # Predict
    Y_hat   = rf.predict(X_test)",lobpredictrst/create_simple_model_predict.py,doutib/lobpredict,1
"clf = svm.SVR(kernel='linear', C=1)
scores_nfl = cross_val_score(clf, stats_matrix, actual_lines, cv=KFold(5), scoring='r2')
print ""SVR(kernel='linear'):"", scores_nfl
print (""Accuracy: %0.2f (+/- %0.2f)"" % (scores_nfl.mean(), scores_nfl.std() * 2))

clf = svm.SVR(kernel='rbf', C=1)
scores_nfl = cross_val_score(clf, stats_matrix, actual_lines, cv=KFold(5))
print ""SVR(kernel='rbf'):"", scores_nfl
print (""Accuracy: %0.2f (+/- %0.2f)"" % (scores_nfl.mean(), scores_nfl.std() * 2))

# clf = RandomForestClassifier(n_estimators=10)
# clf = clf.fit(stats_matrix, actual_lines)
# print clf
# scores_nfl = cross_val_score(clf, stats_matrix, actual_lines, cv=KFold(5))
# print ""RandomForestClassifier:"", scores_nfl
# print (""Accuracy: %0.2f (+/- %0.2f)"" % (scores_nfl.mean(), scores_nfl.std() * 2))
#
# BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)",MoneyMaker/cross_validation.py,yiskylee/MoneyMaker,1
"
    return cv_iterator


class InstanceHardnessThreshold(BaseBinarySampler):
    """"""Class to perform under-sampling based on the instance hardness
    threshold.

    Parameters
    ----------
    estimator : object, optional (default=RandomForestClassifier())
        Classifier to be used to estimate instance hardness of the samples.
        By default a RandomForestClassifer will be used.
        If str, the choices using a string are the following: 'knn',
        'decision-tree', 'random-forest', 'adaboost', 'gradient-boosting'
        and 'linear-svm'.
        If object, an estimator inherited from `sklearn.base.ClassifierMixin`
        and having an attribute `predict_proba`.

        NOTE: `estimator` as a string object is deprecated from 0.2 and will be",imblearn/under_sampling/instance_hardness_threshold.py,chkoar/imbalanced-learn,1
"                            type(SVR()): 'mae',
                            type(LinearSVR()): 'mae',
                            type(KNeighborsRegressor()): 'mse',
                            type(DecisionTreeRegressor()): 'mse',
                            type(RandomForestRegressor()): 'mse',
                            #classification
                            type(SGDClassifier()): 'r2',
                            type(LogisticRegression()): 'r2',
                            type(SVC()): 'r2',
                            type(LinearSVC()): 'r2',
                            type(RandomForestClassifier()): 'r2',
                            type(DecisionTreeClassifier()): 'r2',
                            type(DistanceClassifier()): 'silhouette',
                            type(KNeighborsClassifier()): 'r2',
            }[type(self.ml)]


        # Columns to always ignore when in an operator
        self.non_feature_columns = ['label', 'group', 'guess']
",few/few.py,lacava/few,1
"import numpy as np
import pandas as pd
from sklearn import ensemble

train = pd.read_csv(""train.csv"")
torig = pd.read_csv(""test.csv"")
test = pd.read_csv(""test.csv"")

rf = ensemble.RandomForestClassifier()
# Convert Male / Female ==> 0 / 1
train = train.replace('male', 0)
train = train.replace('female', 1)

test = test.replace('male', 0)
test = test.replace('female', 1)
# Drop NaNs
train = train[np.isfinite(train['passenger_id'])]
train = train[np.isfinite(train['pclass'])]",lab6/michael_subm_1_2.py,cycomachead/info290,1
"
# Create the feature extractors 

bag_of_words = CountVectorizer(stop_words='english')
tfidf = TfidfVectorizer(stop_words='english')
hashvec = HashingVectorizer(stop_words='english')

# Create the Classifier objects

adaboost = AdaBoostClassifier()
randomforest = RandomForestClassifier()
extratrees = ExtraTreesClassifier()
bagging = BaggingClassifier()

filepath = ""train.json""
f = open(filepath,""r"")
content = f.read()
jsonData = json.loads(content)
cuisine_set = set([])
ingredient_set = set([])",KaggleCookingComparison.py,rupakc/Kaggle---What-s-Cooking,1
"# from sklearn.qda import QDA
# clf = QDA(priors=None, reg_param=0.001).fit(X_cropped, np.ravel(y_cropped[:]))
# y_validation_predicted = clf.predict(X_validation)
# print ""Error rate for QDA (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)



# Start Random Forest Classification
print ""Performing Random Classification:""
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=500)
forest = forest.fit(X_cropped, np.ravel(y_cropped[:]))
y_validation_predicted = forest.predict(X_validation)
print ""Error rate for Random Forest (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)
# ml_aux.plot_confusion_matrix(y_validation, y_validation_predicted, ""CM Random Forest (t1)"")
# plt.show()

pickle.dump(forest,open('t5_random_forest.pkl','wb'))

",Code/Machine_Learning_Algos/training_t5.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"# Import some data to play with
data = datasets.make_classification(1000, 10, 5, class_sep=0.7, n_classes=8)
X = data[0]
y = data[1]

# shuffle and split training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
                                                    random_state=0)


est = RandomForestClassifier()
est.fit(X_train, y_train)
y_pred = est.predict(X_test)
y_true = y_test

confusion_matrix(y_true, y_pred, normalize=True)
plt.show()

confusion_matrix(y_true, y_pred)
plt.show()",examples/simple.py,edublancas/sklearn-evaluation,1
"test_index  = index[N:]

X = df.ix[:, 1:9]
y = df[10]

imp = Imputer(strategy='mean', axis=0)
X = imp.fit_transform(X)

print 'Training..'
#clf = SVC()
clf = RandomForestClassifier()
clf.fit(X[train_index, :], y[train_index])

print 'Test'
print clf.predict(X[test_index, :])
print y[test_index]",glass.py,minhouse/python-lesson,1
"    rfPro, gbPro = [], []
    tmp = []
    for i in range(len(trainFeatureR)):
        tt.append(i % 5)
    i = 4
    tmp1 = np.array([t != i for t in tt])
    tmp2 = np.array([t == i for t in tt])
    trainFeature, testFeature = trainFeatureR[tmp1], trainFeatureR[tmp2]
    trainTar, testTar = targetR[tmp1], targetR[tmp2]
    trainId, testId = trainIdListR[tmp1], trainIdListR[tmp2]
    clf = RandomForestClassifier(n_estimators=200, min_samples_split=17)
    clf.fit(trainFeature[Cfeature], trainTar)
    preds = clf.predict(testFeature)
    predPro = clf.predict_proba(testFeature)
    rfPro = predPro
    right = 0
    for n in range(len(preds)):
        preName = dl.get_num_position(preds[n])
        real = dl.get_num_position(testTar[n])
        if preName == real:",position_predict/merge_data/Mposi_predict.py,yinzhao0312/Position-predict,1
"
X, y = load_data(return_X_y=True)
rf = classifier_factory(RandomForestClassifier(random_state=1))
rf.fit(X, y)
rf.plot_feature_importances(feature_names=['petal length', 'petal width',
                                           'sepal length', 'sepal width'])
plt.show()

# Using the more flexible functions API
from scikitplot import plotters as skplt
rf = RandomForestClassifier()
rf = rf.fit(X, y)
skplt.plot_feature_importances(rf, feature_names=['petal length', 'petal width',
                                                  'sepal length', 'sepal width'])
plt.show()",examples/plot_feature_importances.py,reiinakano/scikit-plot,1
"from sklearn.linear_model import SGDRegressor, SGDClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn import tree
h = SVC()
h = KNeighborsClassifier(n_neighbors=10)
h = SGDClassifier()
from sklearn.ensemble import RandomForestClassifier
#h = tree.DecisionTreeClassifier()
h = RandomForestClassifier(n_estimators=100)
#h = RandomForestClassifier(n_estimators=100)

############################
# Initial window training
############################

X,Z = do_process_segment(copy(XXX)) # everything
T,D = X.shape
Y = zeros(T) ",pyfiles/prediction/run_demo.py,aalto-trafficsense/regular-routes-server,1
"    return scores, np.array([np.NaN]*len(scores))

# select the top features with the highest MAD
feature_select = SelectKBest(fs_mad, k=n_feature_kept)


# ## Define pipeline and Cross validation model fitting

# In[45]:

clf = RandomForestClassifier(min_samples_leaf=5, random_state=2)
# joblib is used to cross-validate in parallel by setting `n_jobs=-1` in GridSearchCV
# Supress joblib warning. See https://github.com/scikit-learn/scikit-learn/issues/6370
warnings.filterwarnings('ignore', message='Changing the shape of non-C contiguous array')
clf_grid = grid_search.GridSearchCV(estimator=clf, param_grid=param_grid, n_jobs=-1, scoring='roc_auc')
pipeline = make_pipeline(
    feature_select,  # Feature selection
    StandardScaler(),  # Feature scaling
    clf_grid)
",algorithms/scripts/RandomForestClassifier-mans2singh.py,cognoma/machine-learning,1
"    print ""Reading data...""
    X, Y = utils.read_data(""../files/train.csv"")
    print ""Preprocessing...""
    X = preprocess(X)
    print ""Extracting Features...""
    X = extractFeatures(X)
    Y = [int(x) for x in Y]
    X, Y = np.array(X), np.array(Y)
    classMap = sorted(list(set(Y)))
    accs = []
    rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)
    logging.info(rf)
    print ""Selecting Features...""
    X = selectFeatures(X, Y, rf)
    folds = 5
    stf = cross_validation.StratifiedKFold(Y, folds)
    logging.info(""CV Folds: "" + str(folds))
    loss = []
    print ""Testing...""
    for i, (train, test) in enumerate(stf):",Eye Movement Tracking/src/randomForestCV.py,shaileshahuja/MineRush,1
"    mean[10:] = 0.0
    std[10:] = 1.0
    X_train = (X_train - mean) / std
    X_test = (X_test - mean) / std
    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    'GBRT': GradientBoostingClassifier(n_estimators=250),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=20),
    'RandomForest': RandomForestClassifier(n_estimators=20),
    'CART': DecisionTreeClassifier(min_samples_split=5),
    'SGD': SGDClassifier(alpha=0.001, max_iter=1000, tol=1e-3),
    'GaussianNB': GaussianNB(),
    'liblinear': LinearSVC(loss=""l2"", penalty=""l2"", C=1000, dual=False,
                           tol=1e-3),
    'SAG': LogisticRegression(solver='sag', max_iter=2, C=1000)
}

",benchmarks/bench_covertype.py,kevin-coder/scikit-learn-fork,1
"
    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for knn: ', auc)

    plot_curve(fpr, tpr, 'KNN ' + str(auc))
    return clf

def train_random_forest(x_train, y_train, x_cv, y_cv):
    clf = RandomForestClassifier(n_estimators=100)
    clf.fit(x_train, y_train)

    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for knn: ', auc)

    plot_curve(fpr, tpr, 'KNN ' + str(auc))
    return clf",examples/sara/titanic_sara_5.py,remigius42/code_camp_2017_machine_learning,1
"print(""Best estimator found by grid search:"")
print(clf.best_estimator_)



    
#==============================================================================
# RandomForest (Classifier Variant) Model Fitting parameters
#==============================================================================

estimator = Pipeline([(""forest"", RandomForestClassifier(random_state=0, n_estimators=100))])
estimator.fit(predictors, outcomes)

predicted = estimator.predict(predictors) 
prediction_scores  = accuracy_score(outcomes, predicted) #This code will change, fi we cross validate

#test_predicted = estimator.predict(test_predictors) 
#prediction_scores  = accuracy_score(test_outcomes, test_predicted)

with open(os.path.join(os.getcwd(), 'data','RF.pickle'), 'wb') as pickle_file:",ModelFitting/single_SVM.py,kizzen/Baller-Shot-Caller,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/autocause_default_with_sum_aggregation.py,diogo149/CauseEffectPairsPaper,1
"	x, y = to_classification(data, repins, gap)

	# Pre-processing
	x = preprocessing.scale(x)
	x = pca(x, PCA_VARIANCE)

	folds = 5
	
#	clf = SVC(C=SVM_C, kernel='poly', degree=3)
#	clf = SVC(C=SVM_C, kernel='rbf', gamma=0.1)
#	clf = RandomForestClassifier(n_estimators=RF_ESTIMATORS)
#	clf = AdaBoostClassifier(n_estimators=BOOST_ESTIMATORS) 
	clf = ExtraTreesClassifier(**EXTRA_TREE_PARAMS)

	scores = cv.cross_val_score(clf, x, y, cv=folds, n_jobs=3)
	acc_mean, acc_std = np.mean(scores), np.std(scores)

	return acc_mean, acc_std

",analysis/prediction.py,luamct/WebSci14,1
"        max_depth = config.getint(section, 'max_depth')
    return tree.DecisionTreeClassifier(criterion='entropy', random_state=RandomState(__seed__), max_depth=max_depth)

def load_randomForestClassifier(config, section):
    max_depth = None
    n_estimators = 10
    if config.has_option(section, 'max_depth'):
        max_depth = config.getint(section, 'max_depth')
    if config.has_option(section, 'n_estimators'):
        n_estimators = config.getint(section, 'n_estimators')
    return RandomForestClassifier(criterion='entropy', random_state=RandomState(__seed__), max_depth=max_depth,
                                  n_estimators=n_estimators)

def load_logisticRegression(config, section):
    c = 1.0
    if config.has_option(section,'C'):
        c = float(config.get(section,'C'))
    return linear_model.LogisticRegression(C=c)

def load_classifier(config, section):",model_persist.py,chop-dbhi/arrc,1
"    
    data_file = r""C:\Users\jroberts\Documents\data\Digit Recognizer\train.csv""
    nr = 28
    nc = 28
    n_estimators = 100
    cv = 3
    
    
    def __init__(self):
        self.set_seed()
        self.rf = RandomForestClassifier(n_estimators=self.n_estimators)
    
    
    def set_seed(self,seed=0):
        np.random.seed(seed)


    def load(self):
        self.data = np.genfromtxt(open(self.data_file,'r'), delimiter=',', dtype='f8')[1:]   
        self.n = len(self.data)",mnist_rf.py,jamescroberts/Python,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/no_test_instances )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X,Y, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 20, n_jobs = 8)
rf.fit(X,Y)
result_rf = rf.predict(Z)
#print('X', len(X),len(Y),len(X1[train_size:dataset_size]))
#print('RF prediction : ---> ',result_rf )
#print('actual ans: -->',test_class)
CM = confusion_matrix(test_class,result_rf) 
print(""Confusion Matrix : "")
print(CM)
for i in range(0,no_test_instances):",sandbox/petsc/solvers/scripts/scikit_learn_classifiers.py,LighthouseHPC/lighthouse,1
"    feats = np.array(feats)
    div_lbls = []
    for i in range(k):
        div_lbls.append([])
    for i in range(len(feats)):
        feat = feats[i]
        div_lbls[labels[i]].append(feat)
    return div_lbls

def get_rf(X, y):
    clf = RandomForestClassifier(n_jobs=2)
    clf.fit(X, y)
    return clf
def get_adaboost():
    from sklearn.ensemble import AdaBoostClassifier
    clf = AdaBoostClassifier(n_estimators=50)
    return clf

def graph(X):
    pass",scrape/ml/ml_util.py,gamwang/WikipediaEditRecomendation,1
"          ""colsample_bytree"": 0.9,
          ""eval_metric"": ""logloss"",
          ""n_estimators"": 100,
          ""silent"": 1,
          ""seed"": 93425
          }
          
    #Defining the classifiers
    clfs = {'LRC'  : LogisticRegression(n_jobs=-1, random_state=random_state), 
            'SVM' : SVC(probability=True, max_iter=100, random_state=random_state), 
            'RFC'  : RandomForestClassifier(n_estimators=100, n_jobs=-1, 
                                       random_state=random_state), 
            'GBM' : GradientBoostingClassifier(n_estimators=50, 
                                           random_state=random_state), 
            'ETC' : ExtraTreesClassifier(n_estimators=100, n_jobs=-1, 
                                     random_state=random_state),
            'KNN' : KNeighborsClassifier(n_neighbors=30, n_jobs=-1),
            'ABC' : AdaBoostClassifier(DecisionTreeClassifier(max_depth=30),
                                       algorithm=""SAMME"", n_estimators=350,
                                       random_state=random_state),",scripts/two_layer_training.py,HighEnergyDataScientests/bnpcompetition,1
"
import numpy as np;
from sklearn.ensemble import RandomForestClassifier;

if __name__ == '__main__':
	dataSet = np.genfromtxt('./Data/train.csv',dtype='f8',delimiter=',')[1:];
	test = np.genfromtxt('./Data/test.csv',dtype='f8',delimiter=',')[1:];
	target = dataSet[:,0];
	feature = dataSet[:,1:];
	rf = RandomForestClassifier(n_estimators=100);
	rf.fit(feature,target);

	for i in rf.predict_proba(test):
		print i[1];
	pass;",BiologicalResponse/predict.py,dz1984/Kaggle,1
"        trn.fitClassifier()
        trn.predictClass(trn.threshold)
    elif mtd == 'LogReg':
        trn.model = LogisticRegression()
        trn.fitClassifier()
        trn.predictClass(trn.threshold)
    elif mtd == 'RF':
        n_estimators = 2000
        max_depth = 3
        trn.method += '_' + '_'.join(map(str, [n_estimators, max_depth]))
        trn.model = RandomForestClassifier(max_depth=max_depth,
                                           n_estimators=n_estimators,
                                           random_state=0)
        trn.fitClassifier()
        trn.predictClass(trn.threshold)
    elif mtd == 'SVM':
        #kernel = 'rbf'
        #C = 1.25
        #class_weight = 'balanced'
        kernel = 'linear'",variants/train.py,asalomatov/variants,1
"
# Run Predictions
from sklearn.metrics import confusion_matrix, accuracy_score
y_preds = clf.predict(x_valid)
print( confusion_matrix(y_valid,y_preds) );
print( ""Accuracy: %f"" % (accuracy_score(y_valid,y_preds)) );
f = open('randomForest_take4.txt', 'w')
f.write( str(confusion_matrix(y_valid,y_preds)) );
f.write( ""\nAccuracy: %f"" % (accuracy_score(y_valid,y_preds)) );
f.write( ""\nQuadraticDiscriminantAnalysis()"" );
f.write( ""\nclf = RandomForestClassifier(n_estimators=1000)"" );

# Now on to final submission
x_final = testing.iloc[:,1:].values
x_final = trans.transform(x_final)
y_final = clf.predict(x_final).reshape([62096,]);
y_final = pd.DataFrame(y_final);
numbahs = testing['id']
df = pd.concat([numbahs,y_final],axis=1)
df.columns = ['id','country']",prototype_alpha/randomForest_take4.py,valexandersaulys/airbnb_kaggle_contest,1
"                        comparison = compareTeamStats(season, teamidW, teamidL)
                        targets.append(""W"")
                    else:
                        comparison = compareTeamStats(season, teamidL, teamidW)
                        targets.append(""L"")
                    features.append(comparison)
    return {""features"": features, ""targets"": targets}

def trainForest(ntrees, season=None):
    data = prepareTrainingData(season=season)
    randomForest = RandomForestClassifier(n_estimators=ntrees, n_jobs=2)
    randomForest.fit(data[""features""], data[""targets""])
    return { ""forest"":randomForest, ""data"": data }

def testForest(rfModel, season):
    data = prepareTrainingData(season=season)
    return rfModel.score(data[""features""], data[""targets""])

# Train multiple RandomForests and calculate matchup probabilities with them
def buildModel(nForests=20, nTrees=200, save=False, load=False):",model.py,mpsonic/MarchMadnessRandomForest,1
"    '''
    '''
    clf = BaggingClassifier(ExtraTreeClassifier(), n_estimators = 20)
    '''
    '''
    param_grid = {'max_depth': np.linspace(1, 15, num = 15, dtype = np.int64),
                  'class_weight': ['balanced', 'balanced_subsample', None],
                  'min_samples_split': np.linspace(1, 15, num = 15, dtype = np.int64),
                  'criterion': ['gini', 'entropy']
                  }
    base_clf = RandomForestClassifier(n_estimators = 20)
    clf = GridSearchCV(base_clf, param_grid = param_grid, scoring = 'roc_auc',
                       cv = 10, pre_dispatch = '2*n_jobs', n_jobs = 4)
    '''
    '''
    ## grid search - gamma and C, grid_den = 20, time needed = 13.36s
    grid_den = 1
    param_grid = {#'C': np.logspace(-5, 5, num = grid_den, base = 2.0),
                  'gamma': np.logspace(-5, 5, num = grid_den, base = 2.0)
                  }",challenge.py,lidalei/DataMining,1
"                feature matrix
            y: numpy.array
                label vector
            weights: numpy.array
                vector with sample weights
            config: ConfigSpace.Configuration
                configuration

        '''

        self.model = RandomForestClassifier(n_estimators=config[""rf:n_estimators""],
                                            max_features=config[
                                                ""rf:max_features""],
                                            criterion=config[""rf:criterion""],
                                            max_depth=config[""rf:max_depth""],
                                            min_samples_split=config[
                                                ""rf:min_samples_split""],
                                            min_samples_leaf=config[
                                                ""rf:min_samples_leaf""],
                                            bootstrap=config[""rf:bootstrap""],",autofolio/selector/classifiers/random_forest.py,mlindauer/AutoFolio,1
"    Function to perform k-fold cross validation on some standard classifiers. Note, it may take a long time for
        some of the classifiers to converge on un-scaled data. Use un-scaled data with caution.
    :return: results: Library with classifier names and scores
    :param X_train: Matrix of features from the training set
    :param y_train: Class labels from the training set.
    :param cv: # of folds to use during k-folds cross validation of each model.
    :param X_test: Matrix of features from the testing set
    :param y_test: Class labels from the testing set
    :return: results: Library with classifier names and scores
    """"""
    rf = RandomForestClassifier(n_estimators=50)
    lr = LogisticRegression()
    mlp = MLPClassifier()
    lda = LinearDiscriminantAnalysis()
    sgd = SGDClassifier()
    ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50)
    gbc = GradientBoostingClassifier()
    svc = SVC(kernel='rbf', probability=True)
    knn = KNeighborsClassifier()
    et = ExtraTreeClassifier()",pymlkit/model_select.py,xfaxca/pymlkit,1
"    def __init__(self, n_estimators=10, max_features=""auto"",
                 random_state=None, max_depth=3, max_leaf_nodes=5):
        self.params = vars()

    def fit(self, X, Y, W):
        self.imputer = Imputer()
        self.imputer.fit(X)
        X = replace_nan(X, self.imputer)
        rf_model = RandomForest(**self.params)
        rf_model.fit(X, Y.ravel())
        return RandomForestClassifier(rf_model, self.imputer)


class RandomForestClassifier(Orange.classification.SklModel):
    def __init__(self, clf, imp):
        self.clf = clf
        self.imputer = imp

    def predict(self, X):
        X = replace_nan(X, imp_model=self.imputer)",Orange/classification/random_forest.py,backyes/orange3,1
"        all_output = """"
        h = .02  # step size in the mesh
        self.names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
                      ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""Linear Discriminant Analysis"",
                      ""Quadratic Discriminant Analysis""]
        classifiers = [
            KNeighborsClassifier(3),
            SVC(kernel=""linear"", C=0.025),
            SVC(gamma=2, C=1),
            DecisionTreeClassifier(max_depth=5),
            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            AdaBoostClassifier(),
            GaussianNB(),
            LinearDiscriminantAnalysis(),
            QuadraticDiscriminantAnalysis()]

        for i in range(0, len(self.names)):
            if self.names[i] == self.name:
                clf = classifiers[i]
",history/models.py,owocki/pytrader,1
"    dp = participant.iloc[:, 2:par_col-1]
    dp_test = participant_test.iloc[:, 2:par_col-1]


    #================================= Prediction ===================================
    # splitting dp = data_participiant and classe to train data and test data
    x_train, x_test, y_train, y_test = train_test_split(dp, classe, test_size=0.30, \
      random_state=0)

    # Random Forest Classifier
    har_model = RandomForestClassifier(n_estimators=10, max_depth=None, \
      max_features=4, oob_score=False)

    har_model.fit(x_train,y_train)
    y_true, y_pred = y_test, har_model.predict(x_test)
    print har_model.score


    #================================== Result ======================================
    res = le.inverse_transform(har_model.predict(participant_test.iloc[:, 2:par_col-1]))",assignment.py,ondrej-tucek/Machine-Learning-HAR,1
"import ast
import random
import numpy as np
import scipy
import scipy.sparse
from sparsesvd import sparsesvd
from sklearn.ensemble import RandomForestClassifier


def My_random_forest(train_data_features,labels,test_data_features):
	forest = RandomForestClassifier(n_estimators=100);
	forest = forest.fit(train_data_features,labels);
	return forest.predict(test_data_features);

def My_svm(train_data_features,labels,test_data_features):

	#clf = svm.SVC(kernel='linear');
	clf = svm.SVC()
	clf.fit(train_data_features,labels);
	return clf.predict(test_data_features);",Classifiers.py,yxun/SAL,1
"    X_test = X[n_train:]
    y_test = y[n_train:]

    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    ""dummy"": DummyClassifier(),
    'CART': DecisionTreeClassifier(),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),
    'RandomForest': RandomForestClassifier(n_estimators=100),
    'Nystroem-SVM':
    make_pipeline(Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'SampledRBF-SVM':
    make_pipeline(RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'LinearRegression-SAG': LogisticRegression(solver='sag', tol=1e-1, C=1e4)
}


if __name__ == ""__main__"":",scikit-learn-0.17.1-1/benchmarks/bench_mnist.py,RPGOne/Skynet,1
"def eval_classifier_cv(classifier_name, clf, X, Y, cv=5):
  acc = np.mean(sklearn.cross_validation.cross_val_score(clf, X, Y, cv = cv))
  print classifier_name, ""cross-validation accuracy"", acc
  auc = np.mean(sklearn.cross_validation.cross_val_score(clf, X, Y, cv = cv, scoring='roc_auc'))
  print classifier_name, ""cross-validation AUC"", auc
  return acc, auc

def eval_cv(X, Y, logistic_regression = True, n_trees=50, cv = 5):
  lr = sklearn.linear_model.LogisticRegression()
  eval_classifier_cv(""Logistic Regression"", lr, X, Y, cv)
  rf = sklearn.ensemble.RandomForestClassifier(n_trees)
  eval_classifier_cv(""Random Forest"", rf, X, Y, cv)

def _eval_classifier(classifier_name, clf, X_train, Y_train, X_test, Y_test):
  clf.fit(X_train, Y_train)
  Y_pred = clf.predict(X_test)
  acc = np.mean(Y_test == Y_pred)
  print classifier_name, ""Accuracy"", acc
  if len(np.unique(Y_test)) != 2:
    print ""Skipping AUC""",eval_dataset.py,hammerlab/immuno_research,1
"                learning_rate_init=0.001,
                alpha=0.001,
                max_iter=500,
                verbose=True,
                # solver='sgd',
                # early_stopping=True,
                # tol=0.0,
                random_state=prng,
        )
elif model == 'random-forest':
        clf = RandomForestClassifier(n_estimators=30,random_state=prng)
else:
        clf = tree.DecisionTreeClassifier(random_state=prng)
clf = clf.fit(train_samps.values, train_labels.values)
# print test_samps
# print test_samps.values
anses = clf.predict(test_samps.values)

# print anses
# print test_labels.values",learning/trees.py,ucsd-progsys/ml2,1
"
    class Meta:
        model = Estimator
        sqlalchemy_session = db.Session

    id = factory.Sequence(lambda n: n)
    create_date = factory.LazyFunction(datetime.now)

    _estimator = factory.Iterator([
        RandomForestRegressor(),
        RandomForestClassifier(),
    ])
    estimator = factory.SelfAttribute('_estimator')

    _hash = factory.LazyAttribute(lambda o: compute_hash(o.estimator))
    _file_name = factory.LazyAttribute(lambda o: 'files/estimators/%s' % o._hash)
    byte_size = factory.LazyAttribute(lambda o: compute_size(o.estimator))

    @factory.post_generation
    def persist_file(self, create, extracted, **kwargs):",tests/factories.py,fridiculous/estimators,1
"				negindicators += 1
	
	print ""pos "" + str(posindicators)
	print ""neg ""+ str(negindicators)
			
	train_data = np.array(train_data)

	#print 'Training '
	#print

	forest = RandomForestClassifier(n_estimators=100)

	try:

		forest = forest.fit(train_data[0::,1::], train_data[0::,0])

		#print ""estimators""
		#print forest.estimators_

		#print ""n classes""",rforestDS-Middle-Aged.py,ucsf-ckm/CodeJam-RandomForest,1
"################## Random Forest ################
print(""Random Forest model"")
try:
    # Load prebuild model
    temp_file_name = ""./model/1/model_rf""
    fh = open(temp_file_name, ""rb"")
    rf = cPickle.load(fh)
    fh.close()
except:
    print ""Prebuild model cannot be found""
    rf = RandomForestClassifier(n_estimators=250, n_jobs=-1, criterion=""entropy"", random_state=1)
    rf.fit(train[features], train[""signal""])

    # Save model
    temp_file_name = ""./model/1/model_rf""
    fh = open(temp_file_name, ""wb"")
    cPickle.dump(rf, fh)
    fh.close()
rf_preds = rf.predict_proba(test[features])[:,1]
################ Xgbosot ###########################",weak_model/weak_ensemble.py,achm6174/kaggle-physics-tau,1
"
print('Data shape:')
print('X_train: %s, X_valid: %s, X_test: %s \n' % (X_train.shape, X_valid.shape, X_test.shape))

# First layer (individual classifiers)

# defining the classifiers
clfs = {
    'LR': LogisticRegression(random_state=random_state),
    'SVM': SVC(probability=True, random_state=random_state),
    'RF': RandomForestClassifier(n_estimators=100, n_jobs=3, random_state=random_state),
    'GBM': GradientBoostingClassifier(n_estimators=50, random_state=random_state),
    'ETC': ExtraTreesClassifier(n_estimators=100, n_jobs=3, random_state=random_state),
    'KNN': KNeighborsClassifier(n_neighbors=30),
    'XGB': XGBClassifier(n_estimators=100, seed=random_state)
}

# predictions on the validation and test sets
p_valid = []
p_test = []",python/model_architecture.py,suresh/notes,1
"    ##################################################
    #Attach our unfitted model instances to the ManyModels instance
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.naive_bayes import GaussianNB
    from sklearn.svm import SVC
    

    modeler.models = {    ""KNeighbors_default"": sklearn.neighbors.KNeighborsClassifier()
                      , ""RandomForest"": sklearn.ensemble.RandomForestClassifier()
                      , ""LogisticRegression"": sklearn.linear_model.LogisticRegression(penalty='l1', C=0.1)
                      , ""GaussianNB"": GaussianNB()
                      , ""SVC_rbf"": SVC(kernel = 'rbf', probability = True, random_state = 0)
                      , ""SVC_linear"": SVC(kernel = 'linear', probability = True,  random_state = 0)
                      , ""SVC_poly"": SVC(kernel = 'poly', degree = 3, probability = True,  random_state = 0)
                      }

    #Different method for KNeighbors allows us to compare multiple k's
    for i in range(3,13):",code/prediction/run_models.py,georgetown-analytics/housing-risk,1
"                ('metabolic',240,280),
                ('blood',280,290),
                ('neurologic',320,390),
                ('heart_hypertensive',401,406),
                ('heart_ischemic',410,415),
                ('heart_failure',428,429),
                ('pulmonary',460,520),
                ('digestive',520,580),
                ('renal_insufficiency',580,630)]
    B = {d[0]: LogisticRegression(class_weight='auto', random_state=0) for d in diseases}
    #B = {d[0]: RandomForestClassifier(n_estimators=100, max_features=None, n_jobs=-1, random_state=0) for d in diseases}

    #classifiers = {d[0]:
        #[#KNeighborsClassifier(3),
         #SVC(kernel=""linear"", C=0.025),
         #SVC(gamma=2, C=1),
         #DecisionTreeClassifier(criterion='entropy',max_depth=4,min_samples_split=10,random_state=0)] for d in diseases}#,
         #RandomForestClassifier(max_depth=10, n_estimators=100, max_features=1)] for d in diseases}#,
         #AdaBoostClassifier()] for d in diseases}#,
         #GaussianNB(),",examples/mimic2/logreg.py,twareproj/tware,1
"            if result_name != operator[2]:
                operator_text += '{OUTPUT_DF} = {INPUT_DF}.copy()\n'.format(OUTPUT_DF=result_name, INPUT_DF=operator[2])
            operator_text += ('''{OUTPUT_DF}['dtc{OPERATOR_NUM}-classification'] = dtc{OPERATOR_NUM}.predict('''
                              '''{OUTPUT_DF}.drop('class', axis=1).values)\n''').format(OUTPUT_DF=result_name,
                                                                                        OPERATOR_NUM=operator_num)

        elif operator_name == '_random_forest':
            min_weight = min(0.5, max(0., operator[3]))

            operator_text += '\n# Perform classification with a random forest classifier'
            operator_text += ('\nrfc{OPERATOR_NUM} = RandomForestClassifier('
                              'n_estimators=500, min_weight_fraction_leaf={MIN_WEIGHT})\n').format(OPERATOR_NUM=operator_num, MIN_WEIGHT=min_weight)
            operator_text += ('''rfc{OPERATOR_NUM}.fit({INPUT_DF}.loc[training_indices].drop('class', axis=1).values, '''
                              '''{INPUT_DF}.loc[training_indices, 'class'].values)\n''').format(OPERATOR_NUM=operator_num,
                                                                                                INPUT_DF=operator[2])
            if result_name != operator[2]:
                operator_text += '{OUTPUT_DF} = {INPUT_DF}.copy()\n'.format(OUTPUT_DF=result_name, INPUT_DF=operator[2])
            operator_text += ('''{OUTPUT_DF}['rfc{OPERATOR_NUM}-classification'] = '''
                              '''rfc{OPERATOR_NUM}.predict({OUTPUT_DF}.drop('class', axis=1).values)\n''').format(OUTPUT_DF=result_name,
                                                                                                                  OPERATOR_NUM=operator_num)",tpot/export_utils.py,bartleyn/tpot,1
"    
    print('Loading test data')
    testdata = pd.read_csv('C:/Users/sound/Desktop/Kaggle/Leaf Classfication/Data/test.csv')
    
    #x_test = testdata.values[:, 1:].astype(float)
    #x_test = scaler.transform(x_test)
    x_test = testdata.drop(['id'], axis=1).values
    x_test = scaler.transform(x_test)
    test_ids = testdata.pop('id')
    print('Start learning...')
    random_forest = RandomForestClassifier(n_estimators=1000)
    random_forest.fit(x_tr, y_tr)
    y_pred = random_forest.predict_proba(x_test)
    
    submission = pd.DataFrame(y_pred, index=test_ids, columns=le.classes_)
    submission.to_csv('submission.csv')


if __name__ == '__main__':",Leaf Classfication/Leaf Classfication in Random Forest/code/Leaf Classification in Random Forest.py,0Steve0/Kaggle,1
"# code to upload ( Assuming pre-processing on merged dataset done )
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import RandomForestClassifier
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)
rf=RandomForestClassifier()
rf.fit(x_train,y_train)

results=[]
def topkfeatures(rf,k):
    important_features = []
    unsortarr=[]

    sortarr=[]
    for x,i in enumerate(rf.feature_importances_):",preprocessing/feature selection.py,BhavyaLight/kaggle-predicting-Red-Hat-Business-Value,1
"from sider_datasets import load_sider
from sklearn.ensemble import RandomForestClassifier

sider_tasks, datasets, transformers = load_sider()
train_dataset, valid_dataset, test_dataset = datasets

metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean,
                           mode=""classification"")

def model_builder(model_dir):
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=100)
  return dc.models.SklearnModel(sklearn_model, model_dir)
model = dc.models.SingletaskToMultitask(sider_tasks, model_builder)

# Fit trained model
model.fit(train_dataset)
model.save()

print(""About to evaluate model"")",examples/sider/sider_rf.py,rbharath/deepchem,1
"           color=""r"", yerr=stddev[indices], align=""center"")
    pl.xticks(range(len(feature_importance)), indices)
    pl.xlim([-1, len(feature_importance)])
    pl.savefig(filename + "".png"")

def run_random_forest(data, _max_depth):
    (feature_train, feature_test, label_train, label_test) = train_test_split(data[:, 0:-1], data[:, -1].astype(int),
                                                                              test_size=0.25)

    # TODO: Vary Number of Estimators
    rfc = RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=_max_depth, max_features='auto',
                                     bootstrap=True, oob_score=True, n_jobs=4, verbose = 1)
    rfc.fit(feature_train, label_train)
    training_error = rfc.score(feature_train, label_train)
    #cross_validation_score = cross_val_score(rfc, feature_train, label_train, cv=10)
    testing_error = rfc.score(feature_test, label_test)
    out_of_bag_error = rfc.oob_score_

    print ""Random Forest Results for Max Depth:"", _max_depth
    print ""Training Accuracy:"", training_error",feature-based/runClassifier.py,ameyavilankar/social-network-recommendation,1
"import numpy as np
from cudatree import load_data, RandomForestClassifier, timer
from cudatree import util

x, y = load_data(""covtype"")
x = x[:10000]
y = y[:10000]

def test_covtype_memorize():
  with timer(""Cuda treelearn""):
    forest = RandomForestClassifier(bootstrap = False)
    forest.fit(x, y, bfs_threshold = 500000)
  with timer(""Predict""):
    diff, total = util.test_diff(forest.predict(x), y)  
    print ""%s(Wrong)/%s(Total). The error rate is %f."" % (diff, total, diff/float(total))
  assert diff == 0, ""Didn't perfectly memorize, got %d wrong"" % diff

from helpers import compare_accuracy, compare_hybrid_accuracy
def test_covtype_accuracy():
  compare_accuracy(x,y)",test/test_covtype.py,EasonLiao/CudaTree,1
"import pandas as pd
from sklearn import cross_validation
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv('./data/train.csv')

features = ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Embarked', 'Gender']

train, test = cross_validation.train_test_split(data, test_size = 0.7, random_state=0)

model = RandomForestClassifier(n_estimators=100)
model.fit(train[features], train['Survived'])


test_data = pd.read_csv('./data/test.csv')

test_data['Survived'] = model.predict(test_data[features])

print(test_data['Survived'])
",forestmodel.py,JohnCrickett/KaggleTitanic,1
"
knn.fit(X_train_std[:, k5], y_train)
print('Training Accuracy (using subset):', knn.score(X_train_std[:, k5], y_train))
print('Test Accuracy (using subset):', knn.score(X_test_std[:, k5], y_test))

""""""
# Random forests for feature importance assessment

feat_labels = df_wine.columns[1:]

forest = RandomForestClassifier(n_estimators=10000,
                                random_state=0,
                                n_jobs=-1)

forest.fit(X_train, y_train)
importances = forest.feature_importances_

indices = np.argsort(importances)[::-1]

for f in range(X_train.shape[1]):",DataProcessing.py,petritn/MachineLearning,1
"
        # 
        data = self.pre_treat_data(train_data_list, train_data_feature)

        # 
        print('Start split train and test data.')
        data_train, data_test, result_train, result_test = train_test_split(data, result_list, train_size=train_size)

        # ,n_jobs-1
        print('Start training random forest.')
        clf = RandomForestClassifier(n_jobs=-1)
        self.__clf = clf.fit(data_train, result_train)
        self.__train_data_feature = train_data_feature
        if self.__is_save:
            # 
            print('Save training result.')
            RandomForestTools.train_data_save(self.__clf)
            RandomForestTools.feature_data_save(train_data_feature)
        print(""Build train data finish and accuracy is:%.2f ."" %
              (self.__clf.score(data_test, result_test)))",application/controller/random_forest.py,ZoaChou/Python-learn,1
"		f = []
		for i in features:
			f = []
			for j in range(len(i)):
				if choose[j] == True:
					f.append(i[j])
			f_tot.append(f)
		return f_tot

	def selection_features(features, TF_features):
		clf = RandomForestClassifier(n_estimators=400)
		clf.fit(features, labels_training)
		print clf.feature_importances_
		print TF_features
		print ""\n""
		boolean = []
		new_TF = []
		bad_vector = 0
		i_feat = 0
		for i in clf.feature_importances_:",MUD.py,PAJEAN/uncertaintyDetection,1
"                             ('clf', KNeighborsClassifier(n_neighbors = 20)),
                            ])
                            
    LR_pipeline = Pipeline([('vect', CountVectorizer()), 
                            ('tfidf', TfidfTransformer(norm = 'l2', use_idf = True, smooth_idf = True, sublinear_tf = True)),
                            ('clf', LogisticRegression(warm_start = True, random_state = 1)),
                           ])
                         
    RFC_pipeline = Pipeline([('vect', CountVectorizer()), 
                            ('tfidf', TfidfTransformer(norm = 'l2', use_idf = True, smooth_idf = True, sublinear_tf = True)),
                            ('clf', RandomForestClassifier(random_state = 1)),
                           ])
                            
    VC = VotingClassifier(estimators=[('MNB', MNB_pipeline), ('SGD', SGD_pipeline), ('LR', LR_pipeline)], voting = 'soft', weights = [1, 2, 1])
    
    
    for clf, label in zip([MNB_pipeline, SGD_pipeline, LR_pipeline, VC], ['MNB', 'SGD', 'LR', 'Ensemble']):
        scores = cross_validation.cross_val_score(clf, train_reviews, train_targets, cv = 5, scoring = 'accuracy')
        print(""Accuracy: %0.2f (+/- %0.2f) [%s]"" % (scores.mean(), scores.std(), label))
    ",bia-660/final/experiment_reviews.py,hulingfei/Shaka,1
"    fusion_graph = skf.FusionGraph(relations)
    transformer = skf.DfmfTransform(max_iter=50, init_type=""random_vcol"")
    transformer.transform(gene, fusion_graph, fuser)
    return transformer


def predict_term(train_idx, test_idx, term_idx):
    fuser, fuser_graph = fuse(train_idx)
    X_train = profile(fuser, fuser)
    y_train = dicty[gene][go_term][0].data[train_idx, term_idx]
    clf = ensemble.RandomForestClassifier(n_estimators=200)
    clf.fit(X_train, y_train)
    transformer = transform(fuser, test_idx)
    X_test = profile(fuser, transformer)
    y_pred = clf.predict_proba(X_test)[:, 1]
    return y_pred


def main():
    n_folds = 10",examples/dicty_chaining.py,marinkaz/scikit-fusion,1
"
    counter = 0
    for review in clean_test_reviews:
        test_centroids[counter] = create_bag_of_centroids(review, \
            word_centroid_map)
        counter += 1


    # ****** Fit a random forest and extract predictions
    #
    forest = RandomForestClassifier(n_estimators=100)

    # Fitting the forest may take a few minutes
    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit(train_centroids, train[""sentiment""])
    result = forest.predict(test_centroids)

    # Write the test results
    output = pd.DataFrame(data={""id"":test[""id""], ""sentiment"":result})
    output.to_csv(""BagOfCentroids.csv"", index=False, quoting=3)",src/main/python/Word2Vec_BagOfCentroids.py,Chaparqanatoos/kaggle-knowledge,1
"    print ""1 = Bread""
    print ""2 = Nonbread""

    train = np.vstack((breadtrain,nonbreadtrain))
    labels = np.hstack((labelsbtr,labelsnbtr))
    test = np.vstack((breadtest,nonbreadtest))

    lin_svc = svm.LinearSVC(C=1.0).fit(train, labels)
    predictionsSVM = lin_svc.predict(test)

    cfr = RandomForestClassifier(n_estimators=120)
    cfr.fit(train,labels) # train

    gtruth = np.hstack((labelsbte,labelsnbte))
    predictionsRF = cfr.predict(test) # test

    print dirListbte
    print ""Random Forest Prediction:""
    print predictionsRF
    print ""SVM Prediction:""",tests/test_real_fake.py,rbaravalle/imfractal,1
"

# ###Random Forest

# In[5]:

## Random Forests Classifier
# Training
from sklearn.ensemble import RandomForestClassifier
k = 100
forest = RandomForestClassifier(n_estimators = 100, n_jobs=-1)
forest.fit(images, digits)


# In[6]:

# Prediction
forest_prediction = forest.predict(test_images).tolist()

",recognizer/python/digits.py,NCordon/dataScience-AI,1
"    class_weight = None
    if balanced:
        class_weight = 'balanced'

    # Make appropriate delegatation
    if 'lr' in method:
        estimator = LogisticRegression(n_jobs=1)
    elif 'svm' in method:
        estimator = SVC(probability=False)
    elif 'rf' in method:
        estimator = RandomForestClassifier(n_jobs=1)
    else:
        raise ValueError(""Not implemented for method {}"".format(method))

    estimator = estimator.set_params(**{'class_weight': class_weight, 'random_state': random_state})
    if hasattr(estimator, 'n_jobs'):
        estimator.set_params(**{'n_jobs': 1})

    if 'bagged' in method:
        for l in labels:",clf_br.py,daniaki/ppi_wrangler,1
"    for threshold in [""gobbledigook"", "".5 * gobbledigook""]:
        model = SelectFromModel(clf, threshold=threshold)
        model.fit(data, y)
        assert_raises(ValueError, model.transform, data)


def test_input_estimator_unchanged():
    """"""
    Test that SelectFromModel fits on a clone of the estimator.
    """"""
    est = RandomForestClassifier()
    transformer = SelectFromModel(estimator=est)
    transformer.fit(data, y)
    assert_true(transformer.estimator is est)


@skip_if_32bit
def test_feature_importances():
    X, y = datasets.make_classification(
        n_samples=1000, n_features=10, n_informative=3, n_redundant=0,",phy/lib/python2.7/site-packages/sklearn/feature_selection/tests/test_from_model.py,marcsans/cnn-physics-perception,1
"# Remove the Name column, Cabin, Ticket, and Sex (since I copied and filled it to Gender)
test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1) 

# The data is now ready to go. So lets fit to the train, then predict to the test!
# Convert back to a numpy array
train_data = train_df.values
test_data = test_df.values


print 'Training...'
forest = RandomForestClassifier(n_estimators=100)
forest = forest.fit( train_data[0::,1::], train_data[0::,0] )

print 'Predicting...'
output = forest.predict(test_data).astype(int)


predictions_file = open(""/home/hkh/sources/kagglepy/titanic/output/myfirstforest.csv"", ""wb"")
open_file_object = csv.writer(predictions_file)
open_file_object.writerow([""PassengerId"",""Survived""])",titanic/refs/forestref.py,hkhpub/kagglepy,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans10_dt_only.py,diogo149/CauseEffectPairsPaper,1
"    import numpy as np
    
#    ntree = np.array([10])
    ntree = np.array(range(10,100,10))
    param_grid = dict(n_estimators = ntree)
    if user['Datatype'] == 'Regression':
        from sklearn.ensemble.forest import RandomForestRegressor
        grid = GridSearchCV(RandomForestRegressor(), param_grid, cv=kf)
    elif user['Datatype'] == 'Classification 2 classes':
        from sklearn.ensemble.forest import RandomForestClassifier
        estimator = RandomForestClassifier()
        grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=kf)
    grid.fit(X,Y)
    estimator = grid.best_estimator_
    YpredCV, Q2, RMSE_CV = OP.IterCV(X,Y,estimator,kf)
    return YpredCV, Q2, RMSE_CV, estimator
    
def SVM(X,Y,kf,user):
    from sklearn import svm
    from sklearn.grid_search import GridSearchCV",Optimize_Parameters.py,wiwatowasirikul/PCM,1
"                    train_data.append(coords + colors + label)
        train_data = np.asarray(train_data)
        return train_data

    def fit(self, n_estimators=10, filtername=""bilateral"", filter_d=3, filter_sigmacolor=1, filter_sigmaspace=1):
        self.filtername = filtername
        self.filter_d = filter_d
        self.filter_sigmacolor = filter_sigmacolor
        self.filter_sigmaspace = filter_sigmaspace
        train_data = self.get_training_data(filtername=filtername, filter_d=filter_d, filter_sigmacolor=filter_sigmacolor, filter_sigmaspace=filter_sigmaspace)
        rf = RandomForestClassifier(n_estimators=n_estimators)
        rf.fit(train_data[:,2:5], train_data[:,5])
        self.clf = rf

    def get_cv_score(self, n_estimators, filtername, filter_d, filter_sigmacolor, filter_sigmaspace, cv):
        train_data = self.get_training_data(filtername=filtername, filter_d=filter_d, filter_sigmacolor=filter_sigmacolor, filter_sigmaspace=filter_sigmaspace)
        rf = RandomForestClassifier(n_estimators=n_estimators)
        scores = cross_validation.cross_val_score(rf, train_data[:,2:5], train_data[:,5], cv=cv)
        return scores.mean()
",pixel_classifier.py,hunzikp/map-classification,1
"    training_label = [arr for idx_arr, arr in enumerate(label_bal)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = np.ravel(label_binarize(
        np.hstack(training_label).astype(int), [0, 255]))
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    crf_cv.append(crf.fit(training_data, training_label))

percentiles = [1., 2., 5., 10., 15., 20., 30.]

results_p = []
feat_imp_p = []
for p in percentiles:

    print 'Computing for percentile: {}'.format(p)",pipeline/feature-classification/exp-3/selection-extraction/rf/pipeline_classifier_t2w.py,I2Cvb/mp-mri-prostate,1
"    X_test = X[n_train:]
    y_test = y[n_train:]

    return X_train, X_test, y_train, y_test


ESTIMATORS = {
    ""dummy"": DummyClassifier(),
    'CART': DecisionTreeClassifier(),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),
    'RandomForest': RandomForestClassifier(n_estimators=100),
    'Nystroem-SVM': make_pipeline(
        Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'SampledRBF-SVM': make_pipeline(
        RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=100)),
    'LinearRegression-SAG': LogisticRegression(solver='sag', tol=1e-1, C=1e4),
    'MultilayerPerceptron': MLPClassifier(
        hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
        solver='sgd', learning_rate_init=0.2, momentum=0.9, verbose=1,
        tol=1e-4, random_state=1),",python/bench_mnist.py,suresh/notes,1
"                   'max_features': ['auto'],
                   'max_depth': [9],
                   'min_samples_split': [2],
                   'min_samples_leaf': [2]}
    grid_search = {'max_features': ['auto']}
    best = float('inf')
    best_params = None
    best_clf = None
    for params in cartesian_product(grid_search):
        print params
        rf = RandomForestClassifier(n_estimators=20*8, n_jobs=8, **params)
        score = test_clf(rf)
        if score < best:
            best = score
            best_params = params
            best_clf = rf
    print ""Best params: %s"" % best_params
    print ""Best score: %s"" % best
    
    # save best predictions for analysis",scripts/train_insertion_threshold.py,timpalpant/KaggleBillionWordImputation,1
"    model = kkeras.MLPC([X_train.shape[1],
                         param_d[""DNN:H1""], param_d[""DNN:H2""], nb_classes])
    model.fit(X_train, y_train, X_test, y_test, nb_classes)
    mlp_score = model.score(X_test, y_test)

    model = kkeras.CNNC(param_d[""n_cv_flt""], param_d[""n_cv_ln""], param_d[""cv_activation""],
                        l=[X_train.shape[1], param_d[""DNN:H1""], param_d[""DNN:H2""], nb_classes])
    model.fit(X_train, y_train, X_test, y_test, nb_classes)
    cnn_score = model.score(X_test, y_test)

    model = ensemble.RandomForestClassifier(n_estimators=param_d['RF:n_estimators'],
                                            oob_score=param_d['RF:oob_score'])
    # print(model)
    model.fit(X_train, y_train)
    rf_score = model.score(X_test, y_test)

    if disp:
        print(""DT-C:"", dt_score)
        print(""SVC:"", sv_score)
        print(""DNN:"", mlp_score)",kcellml.py,jskDr/jamespy_py3,1
"    print(""Data read for bots and humans from new_data"")
    
    dataset_X = [r[1:] for r in bots_data + humans_data]
    dataset_Y = [0] * len(bots_data) + [1] * len(humans_data)
    
    scale(dataset_X, with_max=True)
    print(""Data scaled"")
    print(""\n%d instances, where %g are bots\n"" % (len(dataset_X), dataset_Y.count(0)/len(dataset_Y)))
    
    kFold = KFold(n = len(dataset_X), n_folds = 10, shuffle = True)
    rf = RandomForestClassifier(criterion = 'entropy', n_estimators = 50)
    
    result_cv = cross_val(dataset_X, dataset_Y, rf, kFold)
    print(""Total accuracy: %g%% \nBots: %g%% \nHumans: %g%%\n"" % tuple([round(r, num_decimals)*100 for r in result_cv[:3]]))
    print(""f1 score: \nBots: %g%% \nHumans: %g%%\n"" % tuple([round(r, num_decimals)*100 for r in result_cv[3:5]]))
    print(result_cv[5])
    
    print(rf.feature_importances_)
    '''
    new = mClient['new_data']['new_users']",src/classify.py,tapilab/is-xhuang1994,1
"    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


results = []
for clf, name in (
        (RidgeClassifier(tol=1e-2, solver=""lsqr""), ""Ridge Classifier""),
        (Perceptron(n_iter=50), ""Perceptron""),
        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        (KNeighborsClassifier(n_neighbors=10), ""kNN""),
        (RandomForestClassifier(n_estimators=100), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,",classifier_university_title.py,denimalpaca/293n,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/autocause_default.py,diogo149/CauseEffectPairsPaper,1
"    def set_params(self, **params):
        return self


def test_rfe_features_importance():
    generator = check_random_state(0)
    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target

    clf = RandomForestClassifier(n_estimators=20,
                                 random_state=generator, max_depth=2)
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    rfe.fit(X, y)
    assert_equal(len(rfe.ranking_), X.shape[1])

    clf_svc = SVC(kernel=""linear"")
    rfe_svc = RFE(estimator=clf_svc, n_features_to_select=4, step=0.1)
    rfe_svc.fit(X, y)
",projects/scikit-learn-master/sklearn/feature_selection/tests/test_rfe.py,DailyActie/Surrogate-Model,1
"eigenfaces = pca.components_.reshape((n_components, h, w))

print(""Projecting the input data on the eigenfaces orthonormal basis"")
t0 = time()
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print(""done in %0.3fs"" % (time() - t0))


################################################################################
randomforest_clf = RandomForestClassifier(n_estimators=80,max_depth = 10)
""""""
class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)
""""""
decisiontree_clf = DecisionTreeClassifier(max_depth=10, min_samples_split=2,random_state=0)
""""""
class sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None, presort=False)
""""""
extratree_clf = ExtraTreesClassifier(n_estimators=20, max_depth=None,min_samples_split=2, random_state=0)
""""""",face_recognition_other_ensemble.py,zhangxd12/Lfw_face_recognition_svm_ensemble,1
"    plt.clf()
    plt.cla()
    plt.close() 
    return cm; 

def Rforest_learn_predict(gid, X, Y,weight, labels, k_folds, random_forest_trees ,plot_directory): 
    from sklearn.metrics import classification_report
    scaler =  preprocess_data(X); 
    
    #creating the random forest object
    clf = RandomForestClassifier(random_forest_trees, criterion=""entropy"" ,min_samples_leaf=20) ; 
    
    #cutting the set into 10 pieces, then propossing 10 partiion of 9(trainng)+1(test) data
    kf_total = cross_validation.KFold(len(X), n_folds = k_folds, shuffle = True, random_state = 4) ;
    result = pd.DataFrame() ;
    feature_importances = [] ;
    for i ,(train, test)  in enumerate(kf_total) :  
        result = train_RForest_with_kfold(i,train, test, gid,X,Y,weight,scaler,clf,result,feature_importances) ;
    
",script/loading benchmark/Rforest_on_patch.py,Remi-C/LOD_ordering_for_patches_of_points,1
"
    X_valid = X[N_TRAIN:N_TRAIN+N_VALID]
    y_valid = y[N_TRAIN:N_TRAIN+N_VALID]
    
    print X_train.shape
    print y_train.shape
    print X_valid.shape
    print y_valid.shape

    print 'training randomized forests'
    clf = RandomForestClassifier(n_estimators=150, criterion='gini', max_features=100, max_depth=2000)
    clf = clf.fit(X_train, y_train)

    print 'validation errors'
    valid_scores = cross_val_score(clf, X_valid, y_valid)
    print valid_scores.mean()

    # segment test images
    for im_name in TEST_IMAGES:
        ims = []",random_forest.py,saahil/MSSegmentation,1
"
import pandas as pd
print(""Pandas imported ok"")

from sklearn import datasets
print(""sklearn imported ok"")
iris = datasets.load_iris()
X, y = iris.data, iris.target

from sklearn.ensemble import RandomForestClassifier
rf1 = RandomForestClassifier()
rf1.fit(X,y)
print(""sklearn RandomForestClassifier: ok"")

from xgboost import XGBClassifier
xgb1 = XGBClassifier(n_estimators=3)
xgb1.fit(X[0:70],y[0:70])
print(""xgboost XGBClassifier: ok"")

import matplotlib.pyplot as plt",test_build.py,Kaggle/docker-python,1
"    #model = LogisticRegression(C=1.0,class_weight=None,penalty='l2',solver='lbfgs', multi_class='ovr' )#0.671
    #model = LogisticRegression(C=1E-1,class_weight=None,penalty='l1',solver='liblinear', multi_class='ovr' )#0.671
    #model = LogisticRegression(C=1E-1,class_weight=None,penalty='l1' )
    #model = Pipeline([('filter', GenericUnivariateSelect(chi2, param=97,mode='percentile')), ('model', LogisticRegression(C=1.0,solver='lbfgs', multi_class='ovr',class_weight='auto'))])
    #model = Pipeline([('filter', GenericUnivariateSelect(f_classif, param=95,mode='percentile')), ('model', LogisticRegression(C=10.0))])
    #model = OneVsRestClassifier(model,n_jobs=1)
    #model = Pipeline([('pca', PCA(n_components=20)),('model', LogisticRegression(C=1.0))])

    #model = SGDClassifier(alpha=1E-6,n_iter=250,shuffle=True,loss='log',penalty='l2',n_jobs=1,learning_rate='optimal',verbose=False)
    #model = SGDClassifier(alpha=1E-6,n_iter=800,shuffle=True,loss='modified_huber',penalty='l2',n_jobs=8,learning_rate='optimal',verbose=False)#mll=0.68
    #model =  RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=1,n_jobs=4,criterion='gini', max_features=20)
    #model = CalibratedClassifierCV(model, method='isotonic', cv=3)
    #model = KNeighborsClassifier(n_neighbors=5)
    #model =  RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=1,n_jobs=1,criterion='gini', max_features=20,oob_score=False,class_weight='auto')
    #model =  RandomForestClassifier(n_estimators=500,max_depth=None,min_samples_leaf=1,n_jobs=1,criterion='gini', max_features=20,oob_score=False,class_weight=None)
    #model = CalibratedClassifierCV(model, method='isotonic', cv=3)
    
    #model =  ExtraTreesClassifier(bootstrap=False,n_estimators=500,max_depth=None,min_samples_leaf=1,n_jobs=4,criterion='entropy', max_features=20,oob_score=False)
    
    #basemodel = GradientBoostingClassifier(loss='deviance',n_estimators=4, learning_rate=0.03, max_depth=10,subsample=.5,verbose=1)",competition_scripts/otto.py,chrissly31415/amimanera,1
"    #Random
    #DecisionTree
    #GradientBoosting

    rng = np.random.RandomState(1)

    classifiers = [
    LinearSVC(C=0.01, penalty=""l1"", dual=False),
    ensemble.ExtraTreesClassifier(n_estimators = 100, random_state = 0),
    DecisionTreeClassifier(max_depth=10),
    ensemble.RandomForestClassifier(max_depth=10, n_estimators=100),
    ensemble.AdaBoostClassifier(DecisionTreeClassifier(max_depth=10), n_estimators=300),
    ensemble.GradientBoostingClassifier(n_estimators = 500, learning_rate = 0.1, max_depth = 10 , random_state = 0)]

    scores = np.zeros((6, 6))

    i = 0 

    for clf in classifiers:
        lsvc = clf.fit(X_dummytrain,y_dummytrain)",classifyAkash_2.py,nmetts/sp2016-csci7000-bda-project,1
"    with open(fname) as f:
        for s in f:
            ss = s.split()
            labels.append(int(ss[-1]))
            data.append(map(float, ss[:-2]))
    return labels, data

trainset = read('./trainset')
testset  = read('./testset')

clf = RandomForestClassifier(n_estimators=10)
clf.fit(trainset[1], trainset[0])
print clf.predict(testset[1])
print testset[0]",tmp.py,Daiver/HRandomForest,1
"#clf = SGDClassifier(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'linearsvm accuracy ', accuracy_score(test_gt,pred)

#clf = LogisticRegression(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'logistic accuracy ', accuracy_score(test_gt,pred)

#clf = RandomForestClassifier(min_samples_leaf=20,n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'rfc accuracy ', accuracy_score(test_gt,pred)
#pred = clf.predict(raw_orig)
#with open('rfc.txt','w') as otf:
#    for p in pred:
#        otf.write(str(int(p)) + '\n')
from keras.utils.np_utils import to_categorical
",learning/converging-nn-basic.py,leonidk/centest,1
"    #create the training & test sets, skipping the header row with [1:]
    dataset = genfromtxt(open('dataset1.csv','r'), delimiter=',', dtype='f8')[1:]   
    target = [x[407] for x in dataset]
    train = [x[0:405] for x in dataset]

    #print target[:5]
    #print train[:5]
    # test = genfromtxt(open('test.csv','r'), delimiter=',', dtype='f8')[1:]
    
    #create and train the random forest
    #multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)
    rf = RandomForestClassifier(n_estimators=1000)
    rf.fit(train, target)


    nsymp = len(symp)            
    check = [0 for col in range(nsymp)]
    for i in range(len(train[0])):
        if(train[0][i] == 1):
            check[i] = 1",Rathi/rf.py,mayankiitg/YourDocChatBot,1
"            X_train = features[:cutoff]
            logging.info('X_train set size: {0}'.format(len(X_train)))
            X_test = features[cutoff:]
            logging.info('X_test set size: {0}'.format(len(X_test)))
            y_train = rewards[:cutoff]
            # logging.info('Train set size: {0}'.format(len(X_train)))
            y_test = rewards[cutoff:]

            # create classifier
            logging.info('Classifier: training...')
            # rfc = RandomForestClassifier(n_estimators=30)
            rfc = ExtraTreesClassifier(n_estimators=20, oob_score=True, bootstrap=True)
            rfc.fit(X_train, y_train)

            sample_test = sample_res[-len(X_test):]
            logging.info('Test set size: {0}'.format(len(sample_test)))
            sample_test['predict'] = rfc.predict(X_test)
            # print sample_test.tail()
    
            profits = []",06_randomforests/minmax/walkforward.py,Tjorriemorrie/trading,1
"from sklearn.ensemble import RandomForestClassifier

def randomForests(train,test):

	dataTarget = [x[0] for x in train]
	dataTrain = [x[1:] for x in train]
	rf = RandomForestClassifier(n_estimators=1500, n_jobs=3)
	rf.fit(dataTrain, dataTarget)
	predictions = rf.predict(test)
	archivo_prediccion = open(""/home/Alvarado/Documents/facultad/submissionRF.csv"",""a"")
	archivo_prediccion.write(""ImageId,Label\n"")
	imageId = 1
	for label in predictions:
		archivo_prediccion.write(str(imageId) + "","" + str(label) + ""\n"")
		imageId += 1
	archivo_prediccion.close()",randomforests.py,joaquinbf/Digit-Recognizer,1
"    # save 20% of data for performance evaluation
    X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, labels, test_size=0.2)

    param = [
        {
            ""max_depth"": [None, 10, 100, 1000, 10000],
            ""n_estimators"": [1, 10, 100]
        }
    ]

    forest = ensemble.RandomForestClassifier(random_state=0)

    # 10-fold cross validation, use 4 thread as each fold and each parameter set can be train in parallel
    clf = grid_search.GridSearchCV(forest, param,
            cv=10, n_jobs=20, verbose=3)

    clf.fit(X_train, y_train)

    if os.path.exists(model_output_path):
        joblib.dump(clf.best_estimator_, model_output_path)",sklearn_forest.py,mwleeds/android-malware-analysis,1
"    global defects_regr
    data = np.array(data).reshape(1, 2)
    predicted_test = defects_regr.predict(data)
    acceleration = data.item((0, 0))
    print ""Predicted %d"" % predicted_test[0]
    return predicted_test[0]

def init_defects_module(values, trees, data, labels):
    # Fit regression model
    global defects_regr
    defects_regr = RandomForestClassifier(n_estimators=trees)
    defects_regr.fit(data[:, [0,1]], labels)
    print ""init_defects_module: "", defects_regr.feature_importances_
    return

def predicted(data):
   return defects_regr.predict(data)",defects.py,dkdbProjects/server-result-sender,1
"    A = df_sub.as_matrix()
    X = A[:,6:116]
    X = X.astype(np.int64, copy=False)
    y = A[:,2]
    y = y.astype(np.int64, copy=False)

    # Split the data into a training set and a test set
    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=0)

    #Training data
    forest = RandomForestClassifier(n_estimators=10, max_depth=None, 
            min_samples_split=1, random_state=None, max_features=None)
    clf = forest.fit(X, y)
    scores = cross_val_score(clf, X, y, cv=5)
    print scores
    print ""Random Forest Cross Validation of %s: %s""%(subject,scores.mean())
    precision_rf[subject] = scores.mean()
    df_precision.loc[subject]=precision_rf[subject]
    
    ",pae/forcast/src/feature_eachSub_dropNaResult.py,wasit7/book_pae,1
"      i,
      split_count,
    )
    )
    
    # Test/train split
    training_data, test_data = final_vectorized_features.randomSplit([0.8, 0.2])
    
    # Instantiate and fit random forest classifier on all the data
    from pyspark.ml.classification import RandomForestClassifier
    rfc = RandomForestClassifier(
      featuresCol=""Features_vec"",
      labelCol=""ArrDelayBucket"",
      predictionCol=""Prediction"",
      maxBins=4896,
    )
    model = rfc.fit(training_data)
    
    # Save the new model over the old one
    model_output_path = ""{}/models/spark_random_forest_classifier.flight_delays.flight_time.bin"".format(",ch09/spark_model_with_flight_time.py,naoyak/Agile_Data_Code_2,1
"#from PyWiseRF import WiseRF

debug = False
verbose = False
bootstrap = False
n_estimators = 100

def benchmark_cuda(dataset, bfs_threshold = None):
  x_train, y_train = load_data(dataset)
  #Just use this forest to compile the code.
  throw_away = RandomForestClassifier(n_estimators = 1, bootstrap = bootstrap, verbose = False, 
        max_features = None, debug = debug)
  throw_away.fit(x_train, y_train, bfs_threshold = bfs_threshold)

  with timer(""%s benchmark cuda (bfs_threshold = %s)"" % (dataset, bfs_threshold)): 
    forest = RandomForestClassifier(n_estimators = n_estimators, bootstrap = bootstrap, verbose = verbose, 
        max_features = None, debug = debug)
    forest.fit(x_train, y_train, bfs_threshold = bfs_threshold)
  forest = None
",benchmark/benchmark_all.py,EasonLiao/CudaTree,1
"print(train.shape,test.shape)
id_test,test,train,y=preprocess_data(train,test)
print(train.shape,test.shape)

X=np.asarray(train)
y=np.asarray(y)
X_test=np.asarray(test)
X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.15,random_state=538)
from sklearn import linear_model,ensemble
#
clf=ensemble.RandomForestClassifier(n_estimators=100,random_state=100,max_depth=8,max_features=None,n_jobs=-1)
#clf =linear_model.SGDClassifier(loss='hinge', penalty='l2', alpha=0.00001, l1_ratio=0.15, fit_intercept=True, n_iter=200,
#                                shuffle=True, verbose=0, epsilon=0.1, n_jobs=-1, random_state=17, learning_rate='invscaling',eta0=1.0, power_t=0.5, class_weight=None, warm_start=False, average=False)
clf.fit(X_train,y_train)
predictions=clf.predict_proba(X_test)[:,1]
from sklearn.metrics import roc_auc_score
y_pred=clf.predict(X_val)
score=roc_auc_score(y_val, y_pred)

model='RF'",cervical-cancer-screening/models/src/RFonfeatures.py,paulperry/kaggle,1
"        X_test = vec.transform(X_test.to_dict(orient='record'))

        # 
        from sklearn.tree import DecisionTreeClassifier
        dtc = DecisionTreeClassifier()
        dtc.fit(X_train, y_train)
        dtc_y_pred = dtc.predict(X_test)

        # 
        from sklearn.ensemble import RandomForestClassifier
        rfc = RandomForestClassifier()
        rfc.fit(X_train, y_train)
        rfc_y_pred = rfc.predict(X_test)

        # 
        from sklearn.ensemble import GradientBoostingClassifier
        gbc = GradientBoostingClassifier()
        gbc.fit(X_train, y_train)
        gbc_y_pred = gbc.predict(X_test)
",ml/random_forest.py,SuperSaiyanSSS/SinaWeiboSpider,1
"    return df


# Function used to search best hyperparameters using ``hyperopt``.
def objective(space):
    pprint.pprint(space)
    # clf = LogisticRegression(C=space['C'],
    #                          class_weight={0: 1, 1: space['cw']},
    #                          random_state=1, max_iter=300, n_jobs=1,
    #                          tol=10.**(-5), penalty='l2')
    clf = RandomForestClassifier(n_estimators=space['n_estimators'],
                                 max_depth=space['max_depth'],
                                 max_features=space['max_features'],
                                 min_samples_split=space['mss'],
                                 min_samples_leaf=space['msl'],
                                 class_weight={0: 1, 1: space['cw']},
                                 verbose=1, random_state=1, n_jobs=4)
    # clf = SVC(C=space['C'], class_weight={0: 1, 1: space['cw']},
    #           probability=False, gamma=space['gamma'], random_state=1)
    # clf = KNeighborsClassifier(n_neighbors=space['k'], weights='distance',",block_schedule.py,ipashchenko/as,1
"gold_df = orfratings[gold_set].drop_duplicates(['chrom', 'gcoord', 'gstop', 'strand'])
# ORFs with same start and stop are guaranteed to have identical feature values and therefore bias cross-validation - so only keep one of them
gold_class = gold_df[['annot_start', 'annot_stop']].all(1).values.astype(np.int8)*2 - 1  # convert True/False to +1/-1
gold_feat = gold_df[feature_columns].values

if opts.verbose:
    logprint('Gold set contains %d annotated ORFs and %d unannotated ORFs' % ((gold_class > 0).sum(), (gold_class < 0).sum()))

mycv = StratifiedKFold(gold_class, opts.cvfold, shuffle=True)
if len(opts.minperleaf) > 1:
    currgrid = GridSearchCV(RandomForestClassifier(n_estimators=opts.numtrees), param_grid={'min_samples_leaf': opts.minperleaf},
                            scoring='accuracy', cv=mycv, n_jobs=opts.numproc)
    currgrid.fit(gold_feat, gold_class)

    if opts.verbose:
        logprint('Best estimator has estimated %f accuracy with %d minimum samples per leaf' %
                 (currgrid.best_score_, currgrid.best_params_['min_samples_leaf']))

    if currgrid.best_params_['min_samples_leaf'] == min(opts.minperleaf) and min(opts.minperleaf) > 1:
        sys.stderr.write('WARNING: Optimal minimum samples per leaf is minimum tested; recommended to test lower values\n')",rate_regression_output.py,alexfields/ORF-RATER,1
"        self.RelE = RelE

    def rfFit(self):
        self.traning = pd.read_table(self.asiteFn + "".txt"", header=0)
        # column names
        self.colNames = list(self.traning.columns.values)
        self.colNames.remove(""asite"")
        self.X = np.array(pd.get_dummies(self.traning[self.colNames]))
        self.y = np.array(self.traning[""asite""])
        ## feature selection
        self.clf = RandomForestClassifier(max_features=None, n_jobs=-1)
        self.clf = self.clf.fit(self.X, self.y)
        self.importances = self.clf.feature_importances_
        self.selector = RFECV(self.clf, step=1, cv=5)
        self.selector = self.selector.fit(self.X, self.y)
        self.sltX = self.selector.transform(self.X)
        print(""[result]\tOptimal number of features by recursive selection: %d"" % self.selector.n_features_, flush=True)
        ## define a new classifier for reduced features
        self.reducedClf = RandomForestClassifier(max_features=None, n_jobs=-1)
        self.reducedClf = self.reducedClf.fit(self.sltX, self.y)",scripts/asite_predict.py,hanfang/scikit-ribo,1
"    print ""pca""
    pca = PCA(n_components=23,whiten=True)
    pca.fit(train)
    train = pca.transform(train)
    test = pca.transform(test)

    #clf = LogisticRegression(penalty='l2',dual=True,fit_intercept=False,C=2,tol=1e-9,class_weight=None, random_state=None, intercept_scaling=1.0)
    clf = GaussianNB()
    #clf = MultinomialNB()
    #clf = GradientBoostingClassifier(n_estimators=400)
    #clf = RandomForestClassifier(n_estimators=400)
    #clf = RandomForestClassifier(n_estimators=100,max_depth=8,min_samples_leaf=4,n_jobs=3)
    #clf = SGDClassifier(loss=""log"", penalty=""l2"",alpha=0.1)
    #clf = svm.SVC(C = 1.0, kernel = 'rbf', probability = True)
    if ctype == ""cv"":
        print """"
        hehe = cross_validation.cross_val_score(clf,train,y,cv=3,scoring='roc_auc',n_jobs=-1)
        print hehe
        print np.mean(hehe)
",python/train_model.py,lavizhao/shopping,1
"            valid_dataset, [classification_metric], transformers)
    
  if model == 'rf':
    # Initialize model folder

    # Loading hyper parameters
    n_estimators = hyper_parameters['n_estimators']

    # Building scikit random forest model
    def model_builder(model_dir_rf):
      sklearn_model = RandomForestClassifier(
        class_weight=""balanced"", n_estimators=n_estimators,n_jobs=-1)
      return dc.models.sklearn_models.SklearnModel(sklearn_model, model_dir_rf)
    model_rf = dc.models.multitask.SingletaskToMultitask(
        tasks, model_builder)
    
    print('-------------------------------------')
    print('Start fitting by random forest')
    model_rf.fit(train_dataset)
    ",examples/benchmark.py,bowenliu16/deepchem,1
"import data
import process.bag_of_words
import submissions

if __name__ == '__main__':
    n_folds = 5

    skf = sklearn.cross_validation.StratifiedKFold(data.target, n_folds)
    clfs = [sklearn.ensemble.AdaBoostClassifier(n_estimators=100, random_state=process.seed),
            sklearn.ensemble.GradientBoostingClassifier(n_estimators=100, random_state=process.seed),
            sklearn.ensemble.RandomForestClassifier(n_estimators=100, random_state=process.seed),
            sklearn.linear_model.LogisticRegression(random_state=process.seed),
            sklearn.naive_bayes.MultinomialNB(),
            sklearn.svm.LinearSVC(random_state=process.seed)]

    blend_train = numpy.zeros((process.bag_of_words.train.shape[0], len(clfs)))
    blend_test = numpy.zeros((process.bag_of_words.test.shape[0], len(clfs)))

    for i, clf in enumerate(clfs):
        blend_test_i = numpy.zeros((process.bag_of_words.test.shape[0], len(skf)))",word2vec_nlp_tutorial/blend.py,wjfwzzc/Kaggle_Script,1
"    if test_idx:
        X_test, y_test = X[test_idx, :], y[test_idx]
        plt.scatter(X_test[:, 0], X_test[:, 1], c='', alpha=1.0, linewidth=1, marker='o', s=55, label='test set')


'''
We do not have to construct the random forest classifier from individual decision trees
by ourselves; there is already an implementation in scikit-learn that we can use:
''' 
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2)
forest.fit(X_train, y_train)
plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105, 150))
plt.xlabel('petal length')
plt.ylabel('petal width')
plt.legend(loc='upper left')
plt.show()

'''
Using the preceding code, we trained a random forest from 10 decision trees via the",self_practice/Chapter 3 Random Forest.py,wei-Z/Python-Machine-Learning,1
"
#drop the columns
df_train.drop(['Target','Id'],inplace = True,axis = 1)
df_test.drop(['Id'],inplace = True,axis = 1)

df_train.head()
df_train.columns
##################################################################################
#classifier 1 start
#train the Algorithm using RandomForestClassifier
clf = RandomForestClassifier(n_estimators=2000,min_samples_split=32,oob_score=True)
clf.fit(df_train,Y_train)

print(""the score of the RandomForestClassifier is : "",clf.score(df_train,Y_train) )

pred = clf.predict_proba(df_test)

#write submission file and submit
columns = ['Front','Left','Rear','Right']
sub = pd.DataFrame(data=pred, columns=columns)",Python/Data Science/HE_ML_Hackathon/predict_sign.py,vbsteja/code,1
"labels = df['num'].values
cols = list(df)
f1 = [x for x in cols if x not in set(['num','id'])]
features1 = df[list(f1)].values

X_train, X_test, y_train, y_test = train_test_split(features1,labels,test_size=0.50)

mlp = MLPClassifier(hidden_layer_sizes=(100,100,100))
clf1 = BaggingClassifier(n_estimators=10)
clf2 = BaggingClassifier(n_estimators=100)
clf3 = RandomForestClassifier(n_estimators=10,criterion='gini', min_samples_split=2,max_features=None)
clf4 = AdaBoostClassifier(n_estimators=100)
clf5 = VotingClassifier(estimators=[(""rf"",clf3),('bg',clf2),('ml',mlp),('ada',clf4)],voting='soft')

scaler = StandardScaler()
scaler.fit(X_train)
f_test = scaler.transform(f_test)
X_train = scaler.transform(X_train)

clf5.fit(X_train, y_train)",data/gary/heart_submit.py,isabellewei/deephealth,1
"    Examples
    --------
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.models import BayesMinimumRiskClassifier
    >>> from costcla.metrics import savings_score
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> f = RandomForestClassifier(random_state=0).fit(X_train, y_train)
    >>> y_prob_test = f.predict_proba(X_test)
    >>> y_pred_test_rf = f.predict(X_test)
    >>> f_bmr = BayesMinimumRiskClassifier()
    >>> f_bmr.fit(y_test, y_prob_test)
    >>> y_pred_test_bmr = f_bmr.predict(y_prob_test, cost_mat_test)
    >>> # Savings using only RandomForest
    >>> print(savings_score(y_test, y_pred_test_rf, cost_mat_test))
    0.12454256594
    >>> # Savings using RandomForest and Bayes Minimum Risk",costcla/models/directcost.py,albahnsen/CostSensitiveClassification,1
"
    Examples
    --------
    >>> from sklearn.datasets import load_breast_cancer
    >>> from sklearn.model_selection import train_test_split
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> raw_data = load_breast_cancer()
    >>> X_train, X_test, y_train, y_test = train_test_split(
        raw_data.data, raw_data.target, train_size=0.9,
        random_state=2017)
    >>> rf = RandomForestClassifier(
        n_estimators=3, random_state=random_state_classifier)
    >>> rf.fit(X=X_train, y=y_train)
    >>> estimator0 = rf.estimators_[0]
    >>> tree_dat0 = getTreeData(X_train = X_train,
                                dtree = estimator0,
                                root_node_id = 0)
    >>> tree_dat0['all_leaf_node_classes']
    ...                             # doctest: +SKIP
    ...",jupyter/utils/irf_utils.py,Yu-Group/scikit-learn-sandbox,1
"def roc_auc_avg_score(y_true, y_score):
    y_bin = label_binarize(y_true, classes=sorted(set(y_true)))
    return roc_auc_score(y_bin, y_score)


class TestModelEnsemble(unittest.TestCase):
    def test_simple(self):
        iris = load_iris()

        est = ModelEnsemble(
            assembly_estimator=RandomForestClassifier(n_estimators=50, random_state=1),
            intermediate_estimators=[
                LogisticRegression(random_state=1),
                GaussianNB()
            ]
        )

        scorer = make_scorer(roc_auc_avg_score, needs_proba=True)

        scores = cross_val_score(estimator=est, X=iris.data, y=iris.target, cv=3, scoring=scorer)",dstools/ml/ensemble_tests.py,dllllb/ds-tools,1
"test_dataset = fold_datasets[-1]

# Get supports on test-set
support_generator = dc.data.SupportGenerator(
    test_dataset, n_pos, n_neg, n_trials)

# Compute accuracies
task_scores = {task: [] for task in range(len(test_dataset.get_task_names()))}
for (task, support) in support_generator:
  # Train model on support
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=100)
  model = dc.models.SklearnModel(sklearn_model)
  model.fit(support)

  # Test model
  task_dataset = dc.data.get_task_dataset_minus_support(
      test_dataset, support, task)
  y_pred = model.predict_proba(task_dataset)
  score = metric.compute_metric(",examples/low_data/muv_rf_one_fold.py,bowenliu16/deepchem,1
"    train_inputs, train_targets, valid_inputs, valid_targets = load_data(include_mirror)
    clf = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=1, random_state=0, n_jobs=-1)
    scores = cross_val_score(clf, train_inputs, train_targets, n_jobs=-1)
    print scores.mean()

def run_RandFor(include_mirror=False):
    # train_inputs, train_targets, valid_inputs, valid_targets = load_data(include_mirror)
    inputs, targets, identities = load_data_with_identity(False)
    # inputs, targets, identities = reload_data_with_identity_normalized()
    lkf = LabelKFold(identities, n_folds=10)
    clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=1, random_state=0, n_jobs=-1)
    scores = cross_val_score(clf, inputs, targets, n_jobs=-1, cv=lkf)
    print scores
    print scores.mean()

def run_Bagging(num_estimator=10, num_iter=5, include_mirror=False, do_cv=False, reload=False):
    if not reload:
        train_inputs, train_targets, valid_inputs, valid_targets = load_data(include_mirror)
    else:
        train_inputs, train_targets, valid_inputs, valid_targets, test_inputs, test_targets = reload_data_with_test_normalized()",basicBoosting.py,Takonan/csc411_a3,1
"path = '../Data/'
print(""read training data"")
train = pd.read_csv(path+""train.csv"")
label = train['target']
trainID = train['id']
del train['id'] 
del train['target']
tsne = pd.read_csv(path+'train_tsne.csv')
train = train.join(tsne)

clf = RandomForestClassifier(n_jobs=-1, n_estimators=300, verbose=3, random_state=131)
iso_clf = CalibratedClassifierCV(clf, method='isotonic', cv=10)
iso_clf.fit(train.values, label)

print(""read test data"")
test  = pd.read_csv(path+""test.csv"")
ID = test['id']
del test['id']
tsne = pd.read_csv(path+'test_tsne.csv')
test = test.join(tsne)",new/src/1st_level/rf_retrain.py,puyokw/kaggle_Otto,1
"    bins = np.arange(0, 1.0, .05)
    bamboo.hist(score_groups[0].groupby('targets'), var='score', normed=True, alpha=0.5, bins=bins)


def feature_importances(importances, features):
    return pd.DataFrame(sorted(zip(features.columns, importances), key=lambda x: -x[1]),
                        columns=['feature', 'value'])


def get_importances(features, targets):
    fit = RandomForestClassifier(n_estimators=100).fit(features, targets)
    return feature_importances(fit.feature_importances_, features)",bamboo/modeling/modeling.py,ghl3/bamboo,1
"        X_train_cv, X_test_cv = X_train[train_index],X_train[test_index]
        y_train_cv, y_test_cv = y_train[train_index],y_train[test_index]
        clf = clf.fit(X_train_cv,y_train_cv)
        y_pred = clf.predict(X_test_cv)
        scores[test_index] = metrics.accuracy_score(y_test_cv.astype(int), y_pred.astype(int))
    print (""Mean score: {0:.3f} (+/-{1:.3f})"").format(np.mean(scores), sem(scores))

loo_cv(X_train, y_train,clf)

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=10, random_state=33)
clf = clf.fit(X_train, y_train)
loo_cv(X_train, y_train, clf)


clf_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=5)
clf_dt.fit(X_train, y_train)
measure_performance(X_test, y_test, clf_dt)",python/sklearn/learning_sklearn/decision_tree_titanic.py,qingkaikong/useful_script,1
"        param_dist = parameters['linear']
        estimator = ElasticNet()
    elif estimator == 'KNeighborsRegressor':
        param_dist = parameters['kneighbors']
        estimator = KNeighborsRegressor(algorithm='auto')

    # Classifiers
    elif estimator == 'RandomForestClassifier':
        param_dist = {**parameters['ensemble'], **parameters['bootstrap'],
                      **parameters['criterion']}
        estimator = RandomForestClassifier(
            n_jobs=n_jobs, n_estimators=n_estimators)
    elif estimator == 'ExtraTreesClassifier':
        param_dist = {**parameters['ensemble'], **parameters['bootstrap'],
                      **parameters['criterion']}
        estimator = ExtraTreesClassifier(
            n_jobs=n_jobs, n_estimators=n_estimators)
    elif estimator == 'GradientBoostingClassifier':
        param_dist = parameters['ensemble']
        estimator = GradientBoostingClassifier(n_estimators=n_estimators)",q2_sample_classifier/utilities.py,nbokulich/q2-sample-classifier,1
"    labels_train_neg = np.zeros((data_train_neg.shape[0], 1))
    x_train = np.concatenate((data_train_pos, data_train_neg), axis=0)
    y_train = np.concatenate((labels_train_pos, labels_train_neg), axis=0)

    x_train_features = hog_extraction(x_train)

    print(""Apprentissage."")
    error, clf = cross_validation_svm(x_train_features, y_train, N=3)
    # error, clf = cross_validation(x_train_features, y_train, svm.SVC(kernel='linear', C=0.05), N=5)
    # error, clf = cross_validation(x_train_features, y_train, AdaBoostClassifier(n_estimators=50))
    # error, clf = cross_validation(x_train_features, y_train, RandomForestClassifier(), N=0)

    window_w = SIZE_TRAIN_IMAGE[0]
    window_h = window_w

    print(""Predictions sur les donnes de test."")
    # The images in which a face is to detect.
    test_images = [given_data_test_path + file_name for file_name in os.listdir(given_data_test_path)]
    test_images.sort()
",tests.py,jjerphan/SY32FacialRecognition,1
"        else:
            newtesty = clf.predict(testx)
        # Transform them back if we need to
        if configs['benchmark_log_target']:
            newtesty = np.exp(newtesty)
        newtesty = np.transpose(np.array([newtesty]))
        return np.hstack((testy, newtesty))

    # Add a bunch of classifiers
    if configs['is_classification']:
        clfs = [RandomForestClassifier(n_estimators=25, n_jobs=4),
                LogisticRegression()]
    else:
        clfs = [RandomForestRegressor(n_estimators=25, n_jobs=4),
                linear_model.LinearRegression(),
                linear_model.Ridge()]
    for clf in clfs:
        testy = __clf_pred(clf, classification=False)

    # Average the models together",src/prototyper.py,mcraig2/py-analysis,1
"    ""Sliding scanner for variable length input samples""
    inputs, labels, instances = [], [], []
    instance_count = 0
    for sample, label in zip(X, y):
        sample_len = len(sample)
        for s in range(sample_len-window):
            inputs.append(sample[s: s+window].flatten())
            labels.append(label)
            instances.append(instance_count)
        instance_count += 1
    rf = RandomForestClassifier(**estimator_params)
    estimator_params.update({'max_features': 1})
    cf = RandomForestClassifier(**estimator_params)
    probas1 = cross_val_predict(rf, inputs, labels, cv=cv, method='predict_proba')
    probas2 = cross_val_predict(cf, inputs, labels, cv=cv, method='predict_proba')
    probas = []
    for instance in set(instances):
        mask = [i == instance for i in instances]
        p1 = probas1[mask]
        p2 = probas2[mask]",bleedml/utils.py,sig-ml/bleedml,1
"    @staticmethod
    def train(training_instances, training_classes, model_file):
        """""" Train a Random Forest classifier and store the model in a file.

            Args:
                training_instances (list): list of features.
                training_classes (list): list of binary values.
                model_file (str): the model output file.
        """"""

        clf = RandomForestClassifier(n_estimators=200,
                                     max_features=3,
                                     class_weight='balanced')

        # Down sampling the instances to 1:7

        # decompose instances into positives/negatives
        # positives = []
        # negatives = []
        # for i in range(len(training_instances)):",pke/supervised.py,boudinfl/pke,1
"        ClassificationModule.__init__(self, ""Readme Only Random Forest"", ""Ensemble Learner with 200 Decision-Trees as base-classifiers.\
        The text is encoded by a Tfidf-Vectorizer, containing a vocabulary of 5000 distinct words."")

        # Create vectorizer and fit on all available Descriptions
        self.vectorizer = getTextVectorizer(5000) # Maximum of different columns
        corpus = []
        for description in text_corpus:
            corpus.append(process_text(description))
        self.vectorizer.fit(corpus)

        self.clf = RandomForestClassifier(n_estimators=200)
        
        print ""\t-"", self.name


    def resetAllTraining(self):
        """"""Reset classification module to status before training""""""
        self.clf = sklearn.base.clone(self.clf)

    def trainOnSample(self, sample, nb_epoch=10, shuffle=True, verbose=True):",Application/Models/ClassificationModules/readmeonlyrandomforest.py,Ichaelus/Github-Classifier,1
"
h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",examples/plot_classifier_comparison.py,B3AU/waveTree,1
"    mwfl=0.0
    if(args[9].find(""None"")==-1):
        mwfl = float(args[9])

    if(args[10].find(""None"")!=-1):
        mln = None
    else:
        mln = int(args[10])

    if(args[0].find(""RandomForestClassifier"")!=-1):     
        return RandomForestClassifier(n_estimators=est, criterion=crit, max_depth=md, 
            min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=mwfl, max_features=mf, 
            max_leaf_nodes=mln, bootstrap=bstp, oob_score=os, n_jobs=1, random_state=42, 
            verbose=0, warm_start=ws, class_weight=cw)
    else:
        return ExtraTreesClassifier(n_estimators=est, criterion=crit, max_depth=md, 
            min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=mwfl, max_features=mf, 
            max_leaf_nodes=None, bootstrap=False, oob_score=False, n_jobs=1, random_state=42, verbose=0, ",recipe/classifiers/specificTree.py,RecipeML/Recipe,1
"from sklearn.linear_model import LogisticRegression
from settings import *

names = [""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]

def main():

	#set the timer
	start = time()
",UMKL/textData/classifiers.py,akhilpm/Masters-Project,1
"training = pd.read_csv(""protoAlpha_training.csv"")
testing = pd.read_csv(""protoAlpha_testing.csv"")

X = training.iloc[:,1:-1]
y = training['country_destination']

x_train,x_valid,y_train,y_valid = train_test_split(X,y,test_size=0.3,random_state=9372)

# Train classifier
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=200,n_jobs=1,verbose=10)
clf.fit(x_train,y_train)

# Run Predictions
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
y_preds = clf.predict(x_valid)
print( confusion_matrix(y_valid,y_preds) );
print( ""Accuracy: %f"" % (accuracy_score(y_valid,y_preds)) );
print( ""Precision: %f"" % (precision_score(y_valid,y_preds)) );
print( ""Recall: %f"" % (recall_score(y_valid,y_preds)) );",prototype_beta/randomForest_take1.py,valexandersaulys/airbnb_kaggle_contest,1
"
class modeltest(object):
     """"""docstring for modeltest""""""
     def __init__(self):
          super(modeltest, self).__init__()
     def test(self,train_seg,train_label):     
          vect = TfidfVectorizer(ngram_range=(1,1), min_df=1, max_features=1000)
          xvec = vect.fit_transform(train_seg)
          train_X, test_X, train_y, test_y = train_test_split(xvec, train_label, train_size=0.8, random_state=1)
          clf_lin = SVC(decision_function_shape='ovo')
          #clf_RFC = RandomForestClassifier(n_estimators=C)
          #clf_RFC.fit(train_X, train_y)
          #pre_result = clf_RFC.predict(test_X)
          clf_lin.fit(train_X, train_y)
          pre_result = clf_lin.predict(test_X)",classify_mode.py,huaijun-lee/classify,1
"        dataset = genfromtxt(open('../file/Train.csv','r'), delimiter=',', dtype='str')[1:] 
   	train=[]
	target=[]

	for x in dataset:

        	train.append([int(x[0]),long(x[1]),int(x[2]),int(x[3]),int(x[4]),int(x[5]),int(x[6]),float(x[7]),float(x[8])])
		target.append([x[9]])

        #create and train the random forest
        #multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)

        rf = RandomForestClassifier(n_estimators=100)
        rf.fit(train, target)
	
	# save the classifier
	with open('../file/Model.pkl', 'wb') as fid:
    		cPickle.dump(rf, fid)

Train()",cgi-bin/train.py,SaeedNajafi/ATM_Predictor,1
"from sklearn.ensemble import VotingClassifier, RandomForestClassifier


class Ensemble(Classifier):

    classifierName = 'Ensemble'

    def train(self):

        #TODO: Define ensemble
        cl1 = RandomForestClassifier(random_state=1)
        listOfClassifiers = [(""randomForest"", cl1)]

        clf = VotingClassifier(estimators=listOfClassifiers, voting='hard')
        clf.fit(self.Xtrain, self.ytrain)
            
        self.model = clf",infodens/classifier/ensemble.py,rrubino/B6-SFB1102,1
"    bounds = [[1, 25]]
    xmax = 10  # FIXME - should this not be optional?

    @staticmethod
    def _f(x):
        # iris = load_iris()
        X, y = X, y = make_hastie_10_2(random_state=0)
        x = np.ravel(x)
        f = np.zeros(x.shape)
        for i in range(f.size):
            clf = RandomForestClassifier(n_estimators=1, min_samples_leaf=int(np.round(x[i])), random_state=0)
            # scores = cross_val_score(clf, iris.data, iris.target)
            scores = cross_val_score(clf, X, y, cv=5)
            f[i] = -scores.mean()
        return f.ravel()

if __name__ == '__main__':
    objective = CV_RF()

    info = pybo.solve_bayesopt(",sandpit.py,jamesrobertlloyd/automl-phase-1,1
"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""rbf"", C=0.025, probability=True),
    NuSVC(probability=True),
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    AdaBoostClassifier(),
    GradientBoostingClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()]

def classify_test(X_train,y_train,X_test,y_test):
    for clf in classifiers:
        try:",learn/utils/classify_test.py,taotaocoule/stock,1
"### Please name your classifier clf for easy export below.
### Note that if you want to do PCA or other multi-stage operations,
### you'll need to use Pipelines. For more info:
### http://scikit-learn.org/stable/modules/pipeline.html

# Provided to give you a starting point. Try a variety of classifiers.
from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=5,
                            criterion=""entropy"",
                            random_state=20,
                            min_samples_split=8,
                            min_samples_leaf=2)

# evaluation
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np",poi_id.py,rjegankumar/enron_email_fraud_identification,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/no_test_instances )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X,Y, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 100, n_jobs = 12, random_state = 4)
rf.fit(X,Y)
result_rf = rf.predict(Z)
#print(Z[70])
#print('X', len(X),len(Y),len(X1[train_size:dataset_size]))
#print('RF prediction : ---> ',result_rf )
#print('actual ans: -->',test_class)
CM = confusion_matrix(test_class,result_rf) 
print(""Confusion Matrix : "")
print(CM)",sandbox/petsc/solvers/scripts/ScikitClassifiersRS1.py,LighthouseHPC/lighthouse,1
"            logging.info('Calculating data took:', dt.now() - tic)
            tic = dt.now()

            self._save_points()

            logging.info('Saving data took:', dt.now() - tic)

    def _train_rf(self):
        logging.info('Training the random forest')
        tic = dt.now()
        self.model_rf = sklearn.ensemble.RandomForestClassifier(n_estimators=self.num_tree, bootstrap=False, max_depth=self.max_depth, n_jobs=3, random_state=self.random_state, verbose=0)
        self.model_rf.fit(self.Xtrain_points, self.ytrain)
        logging.info('RF training took:', dt.now() - tic)

    def setup(self, freeup_mem=True):
        # setup the generator that we will be learning from
        self.learn_from.setup()

        # setup final storage
        self._setup_final_storage()",src/rects.py,yassersouri/omgh,1
"# Import the random forest package
from sklearn.ensemble import RandomForestClassifier 
from sklearn import cross_validation
import numpy as np
import cPickle as pickle

dataset = np.loadtxt('combined_training.csv', delimiter="","")

# Create the random forest object which will include all the parameters
# for the fit
forest = RandomForestClassifier(n_estimators = 100)

# Fit the training data to the Survived labels and create the decision trees
forest_fit = forest.fit(dataset[0::,1::],dataset[0::,0])

importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
             axis=0)

# Take the same decision trees and run it on the test data",classifier/random_forest.py,alexcritschristoph/VICA,1
"                dict(name=""Perceptron"",parameter_tunning=False, clf = Perceptron(n_iter=100)),
#                
                dict(name=""bnb"",parameter_tunning=False,clf=BernoulliNB(binarize=0.5)),
                dict(name=""sgd"",parameter_tunning=False,clf=SGDClassifier(loss=""hinge"", penalty=""l2"")),
                dict(name=""KNN"",parameter_tunning=False,tune_clf=GridSearchCV( KNeighborsClassifier(),[{'n_neighbors': [5,10,50,100],'metric':['euclidean','minkowski'],'p':[2,3,4,5]}],cv=5 ) ,clf=KNeighborsClassifier(n_neighbors=3,metric='euclidean')),
#                 dict(name=""KNN"",parameter_tunning=False,tune_clf=GridSearchCV( KNeighborsClassifier(),[{'n_neighbors': [5,10,50,100],'metric':['euclidean','minkowski'],'p':[2,3,4,5]}],cv=5 ) ,clf=KNeighborsClassifier(n_neighbors=10,metric='euclidean')),
#                 dict(name=""KNN"",parameter_tunning=False,tune_clf=GridSearchCV( KNeighborsClassifier(),[{'n_neighbors': [5,10,50,100],'metric':['euclidean','minkowski'],'p':[2,3,4,5]}],cv=5 ) ,clf=KNeighborsClassifier(n_neighbors=100,metric='euclidean')),
#                 dict(name=""KNN"",parameter_tunning=False,tune_clf=GridSearchCV( KNeighborsClassifier(),[{'n_neighbors': [5,10,50,100],'metric':['euclidean','minkowski'],'p':[2,3,4,5]}],cv=5 ) ,clf=KNeighborsClassifier(n_neighbors=1000,metric='euclidean')),                
#                 dict(name=""KNN"",parameter_tunning=False,tune_clf=GridSearchCV( KNeighborsClassifier(),[{'n_neighbors': [5,10,50,100],'metric':['euclidean','minkowski'],'p':[2,3,4,5]}],cv=5 ) ,clf=KNeighborsClassifier(n_neighbors=5000,metric='euclidean')),
 
                dict(name=""forest30"",parameter_tunning=False,clf=RandomForestClassifier(n_estimators=30,random_state=123,verbose=3)),
                dict(name=""lasso"",parameter_tunning=False, clf=Lasso(alpha=1.0,tol=1e-3,warm_start = True)),
                dict(name=""grad_boosting"",parameter_tunning=False, clf=GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, min_samples_split=2, min_samples_leaf=1, max_depth=3, init=None, random_state=None, max_features=None, verbose=0)),
                dict(name=""gaussian_process"",parameter_tunning=False, clf=GaussianProcess(theta0=5e-1)),
                dict(name=""ada_boost"",parameter_tunning=False, clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=""SAMME"",n_estimators=200)),
              ]
",python/Definations.py,mahmoudnabil/labr,1
"
# Add column for activity label
har_subject['activity'] = har_activity

################################
# Training and model selection
################################

# Perform our own train/test split, at test size 20%
X_train, X_test, S_train, S_test = cross_validation.train_test_split(X, S, test_size=0.2, random_state=42)
clf_feature_select = RandomForestClassifier(n_estimators=30, max_depth=None, min_samples_split=1, random_state=0)
clf_classify = RandomForestClassifier(n_estimators=30, max_depth=None, min_samples_split=1, random_state=0)

# Use a first pass classifier to pick features:
clf_feature_select = clf_feature_select.fit(X_train, S_train)

# Reduce the features
model = SelectFromModel(clf_feature_select, prefit=True)

###################################",src/HAR_analysis.py,nonabelian/di_har_proj,1
"test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1)


# The data is now ready to go. So lets fit to the train, then predict to the test!
# Convert back to a numpy array
train_data = train_df.values
test_data = test_df.values


print 'Training...'
forest = RandomForestClassifier(n_estimators=100)
forest = forest.fit( train_data[0::,1::], train_data[0::,0] )

print 'Predicting...'
output = forest.predict(test_data).astype(int)


predictions_file = open(""myfirstforest.csv"", ""wb"")
open_file_object = csv.writer(predictions_file)
open_file_object.writerow([""PassengerId"",""Survived""])",demos/myfirstforest.py,JakeCowton/titanic,1
"#data.head()

X = data.values[::, 1:14]
y = data.values[::, 0:1]
y = y.ravel()

from sklearn.cross_validation import train_test_split as train
X_train, X_test, y_train, y_test = train(X, y, test_size=0.8)

from sklearn.ensemble import RandomForestClassifier
#clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
#clf.fit(X_train, y_train)
#clf.score(X_test, y_test)
from sklearn.preprocessing import scale
X_train_draw = scale(X_train[::, 10:12])
X_test_draw = scale(X_test[::, 10:12])

clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
clf.fit(X_train_draw, y_train)
",wine/wine.py,kodopik/Machine-Learning,1
"    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/trial2.py,diogo149/CauseEffectPairsPaper,1
"    
    
# Evaluate Ensemble Algorithms
def evaluateEnsembleAlgorith(X_train, Y_train, outputPath):
    global imageidx
    print '\n === Evaluate Ensemble Algorithms ==='

    ensembles = []
    ensembles.append(('AB', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('AB', AdaBoostClassifier())])))
    ensembles.append(('GBM', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier())])))
    ensembles.append(('RF', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('RF', RandomForestClassifier())])))
    ensembles.append(('ET', Pipeline([('PCA', PCA()),('MinMaxScaler', MinMaxScaler(feature_range=(0, 1))),('Scaler', StandardScaler()),('ET', ExtraTreesClassifier())])))
    
    results = []
    names = []
    
    for name, model in ensembles:
        kfold = cross_validation.KFold(n=len(X_train), n_folds=NUM_FOLDS, random_state=RAND_SEED)
        cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=SCORING)
        results.append(cv_results)",lib/eda2.py,FabricioMatos/ifes-dropout-machine-learning,1
"        X: numpy array/dataframe with features
        y: true/false values matching feature rows, 1's and 0's
        kind: use 'classifier' or 'regressor' random forest
       Returns:
        random forest classifier fitted to X,y
    """"""

    X, y = get_training_data(known, neg)
    from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor)
    if kind == 'classifier':
        rf = RandomForestClassifier(n_estimators=100)
    else:
        rf = RandomForestRegressor(n_estimators=100)
    #print ('fitting..')
    rf.fit(X,y)
    return rf

def test_classifier(known=None, neg=None):

    from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor)",smallrnaseq/novel.py,dmnfarrell/smallrnaseq,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans10_knn_only.py,diogo149/CauseEffectPairsPaper,1
"    training_label = [arr for idx_arr, arr in enumerate(label)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = label_binarize(np.hstack(training_label).astype(int),
                                    [0, 255])
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    pred_prob = crf.fit(training_data, np.ravel(training_label)).predict_proba(
        testing_data)

    result_cv.append([pred_prob, crf.classes_])

# Save the information
path_store = '/data/prostate/results/mp-mri-prostate/exp-1/dce'
if not os.path.exists(path_store):
    os.makedirs(path_store)",pipeline/feature-classification/exp-1/pipeline_classifier_dce.py,I2Cvb/mp-mri-prostate,1
"                           converters={0:lambda s: ord(s.decode().split(""\"""")[1])})
    trainDataResponse = trainData[:,1]
    trainDataFeatures = trainData[:,0]

    # Train H2O GBM Model:
    #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")
    rf_h2o = h2o.random_forest(x=alphabet[['X']], y=alphabet[""y""], ntrees=1, max_depth=1, nbins=100)

    # Train scikit GBM Model:
    # Log.info(""scikit GBM with same parameters:"")
    rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
    rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

    # h2o
    rf_perf = rf_h2o.model_performance(alphabet)
    auc_h2o = rf_perf.auc()

    # scikit
    auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_DEPRECATED_smallcatRF.py,nilbody/h2o-3,1
"        
    data[i,:] = feature_row
    
end = time.time()
print ""Time Taken to extract all features : "", end-start

train_data,test_data,train_label,test_label = cross_validation.train_test_split(data,class_labels,test_size=0.3)

# Initializing the classifiers 

rf = RandomForestClassifier(n_estimators=101)
ada = AdaBoostClassifier(n_estimators=101)
gradboost = GradientBoostingClassifier(n_estimators=101)
svm = SVC()
gnb = GaussianNB()

classifiers = [rf,ada,gradboost,svm,gnb]
classifier_names = [""Random Forests"",""AdaBoost"",""Gradient Boost"",""SVM"",""Gaussian NB""]

print ""Starting Classification Performance Cycle ...""",PizzaCombinedModel.py,rupakc/Kaggle-Random-Acts-of-Pizza,1
"    tfidf = sklearn.feature_extraction.text.TfidfTransformer(use_idf=False)
    x_text_train_tfidf = tfidf.fit_transform(x_text_train_vect)

    mutual_info = sklearn.feature_selection.SelectKBest(sklearn.feature_selection.mutual_info_classif, k=K_BEST)
    x_text_train_k_best = mutual_info.fit_transform(x_text_train_tfidf, y_train)

    all_train_features = scipy.sparse.hstack((x_text_train_k_best, x_features_train)).A

    from sklearn.ensemble import *

    clf = RandomForestClassifier(n_estimators=500).fit(all_train_features, y_train)
    predicted = clf.predict(all_train_features)
    train_error = 1 - sklearn.metrics.accuracy_score(y_train, predicted)

    x_text_test_vect = vect.transform(x_text_test)
    x_text_test_tfidf = tfidf.transform(x_text_test_vect)
    x_text_test_k_best = mutual_info.transform(x_text_test_tfidf)
    all_test_features = scipy.sparse.hstack((x_text_test_k_best, x_features_test)).A
    predicted = clf.predict(all_test_features)
    test_error = 1 - sklearn.metrics.accuracy_score(y_test, predicted)",step_4/scripts/train_sentiment_model.py,chuajiesheng/twitter-sentiment-analysis,1
"df_3 = pd.read_csv('C:/LearningMaterials/Kaggle/Mlsp/train_labels.csv')
df_train_labels = df_3.ix[:,1]
y = df_train_labels.values
print ""Dimensions of train X and Y""
print X.shape
print y.shape

#Get a linear model from the sklearn
#clf = linear_model.LogisticRegression(C=0.16,penalty='l1', tol=0.001, fit_intercept=True)
tuned_parameters = [{'max_features': ['sqrt'], 'n_estimators': [1000]}]
clf = GridSearchCV( RandomForestClassifier(min_samples_split=1), tuned_parameters, cv=3, verbose=2 ).fit(X, y)
#clf = linear_model.LogisticRegression(C=0.16,penalty='l2', tol=0.001, fit_intercept=True)
#clf.fit(X, y)
print 'Best parameters set found on development set:'
print clf.best_estimator_
y_predict_train = clf.predict_proba(X)
#print 'prediction accuracy: %.4f' % (1 - (1. / len(y) * sum( y_predict_train != y )))
pickle.dump( clf, open( output_model_file, 'wb' ))
print 'Model Saved and working on test set'
y_predict_train = y_predict_train[:,1]",benchmark.py,muthujothi/Kaggle-schizophrenia-classification,1
"
        if not SK18:
            custom_cv = KFold(n=y_train.shape[0], n_folds=3, shuffle=True, random_state=42)
        else:
            custom_cv = KFold(n_splits=3, shuffle=True, random_state=42)

        # define the pipe
        pipe = Pipeline([
            ('scaler', SelectiveScaler()),
            ('pca', SelectivePCA(weight=True)),
            ('rf', RandomForestClassifier(random_state=42))
        ])

        # define hyper parameters
        hp = {
            'scaler__scaler': [StandardScaler(), RobustScaler(), MinMaxScaler()],
            'pca__whiten': [True, False],
            'pca__weight': [True, False],
            'pca__n_components': uniform(0.75, 0.15),
            'rf__n_estimators': randint(5, 10),",skutil/tests/test_big.py,tgsmith61591/skutil,1
"
names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes""
]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)",from_sklearn_website.py,Abhijith1995/tumor_classifier,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.ml.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = spark.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> model.featureImportances
    SparseVector(1, {0: 1.0})
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0",python/pyspark/ml/classification.py,Panos-Bletsos/spark-cost-model-optimizer,1
"clf_names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA"", ""Logistic Regression"", ""Logistic Regression CV""]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025, probability=True),
    SVC(gamma=2, C=1, probability=True),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
    LogisticRegression(),
    LogisticRegressionCV()]

def merge_row(nparr1, nparr2):
    return np.append(nparr1, nparr2, axis=1)",motifwalk/classification/simple_clf.py,gear/motifwalk,1
"

def get_x_point(context, data, ticker, move_return):
    date = data[ticker][""dt""].strftime(""%Y-%m-%d"")
    pytrader_data = context.pytrader_data[ticker].loc[date].values
    return append(pytrader_data, move_return)


def initialize(context):
    context.pytrader_data = {}
    context.model = RandomForestClassifier()
    context.StockTuple = namedtuple(
        ""StockTuple"", [""ticker"", ""days_after"", ""close"", ""move_return"", ""prediction""]
    )
    context.x = []
    context.y = []
    context.yesterday_price = {}
    context.number_days_after = 1
    context.data_points_necessary = 50
    context.data_countdowns = []",pytrader/algorithms/large_move_diff.py,hahnicity/pytrader,1
"
print(""Test Error = %g "" % (1.0 - accuracy_dt))
'''
Test Error = 0.0697674 
'''

#########################
## Random Forest Model ##
#########################

rf = RandomForestClassifier(labelCol='label',
	maxDepth=4,
	impurity=""gini"",
	numTrees=500,
	seed=42)

model_rf = rf.fit(trainingSet)

predictions_rf = model_rf.transform(testSet)
",pySpark/breastCancerPySparkDF.py,raviolli77/machineLearning_breastCancer_Python,1
"# drop NaNs
traindf = traindf.dropna()
validationdf = validationdf.dropna()
X_train = traindf[traindf.columns[0:-1]].values
Y_train = traindf[traindf.columns[-1]].values
X_test = validationdf[validationdf.columns[0:-1]].values
Y_test = validationdf[validationdf.columns[-1]].values

# train a random forest
print(""Beginning random forest classification"")
clf = RandomForestClassifier(n_estimators=1000)
clf.fit(X_train, Y_train)
scores = cross_val_score(clf, X_train, Y_train, cv=5)
Y_test_RFC = clf.predict(X_test)

print(""Results from cross-validation on training set:"")
print(scores, scores.mean(), scores.std())

testscore = accuracy_score(Y_test, Y_test_RFC)
",Code/learnvehicle.py,sbussmann/sensor-fusion,1
"            
        elif(self.strategy=='l1'):
            model = LogisticRegression(C=0.01, penalty='l1',n_jobs=-1, random_state = 0)  #to be tuned 
            model.fit(df_train, y_train)
            coef = np.mean(np.abs(model.coef_),axis=0)
            abstract_threshold = np.percentile(coef,100.*self.threshold)
            self.__to_discard = df_train.columns[coef<abstract_threshold]
            self.__fitOK = True
            
        elif(self.strategy=='rf_feature_importance'):
            model = RandomForestClassifier(n_estimators=50,n_jobs=-1, random_state = 0) #to be tuned 
            model.fit(df_train, y_train)
            coef = model.feature_importances_
            abstract_threshold = np.percentile(coef,100.*self.threshold)        
            self.__to_discard = df_train.columns[coef<abstract_threshold]
            self.__fitOK = True
            
        else:
            raise ValueError(""Strategy invalid. Please choose between 'variance', 'l1' or 'rf_feature_importance'"")            
            ",python-package/mlbox/model/supervised/classification/feature_selector.py,AxeldeRomblay/MLBox,1
"        if self.setting not in ['both_only', 'all', 'tfidf_only', 'grammar_only']:
            raise NameError('Incorrect feature setting.')

    def define_clfs_params(self):
        '''
        Defines all relevant parameters and classes for classfier objects.
        Edit these if you wish to change parameters.
        '''
        # These are the classifiers
        self.clfs = {
            'RF': RandomForestClassifier(n_estimators = 50, n_jobs = -1),
            'ET': ExtraTreesClassifier(n_estimators = 10, n_jobs = -1, criterion = 'entropy'),
            'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth = [1, 5, 10, 15]), algorithm = ""SAMME"", n_estimators = 200),
            'LR': LogisticRegression(penalty = 'l1', C = 1e5),
            'SVM': svm.SVC(kernel = 'linear', probability = True, random_state = 0),
            'GB': GradientBoostingClassifier(learning_rate = 0.05, subsample = 0.5, max_depth = 6, n_estimators = 10),
            'NB': GaussianNB(),
            'DT': DecisionTreeClassifier(),
            'SGD': SGDClassifier(loss = 'log', penalty = 'l2'),
            'KNN': KNeighborsClassifier(n_neighbors = 3)",pipeline/model_loop.py,aldengolab/fake-news-detection,1
"        i,
        split_count,
      )
    )
  
    # Test/train split
    training_data, test_data = final_vectorized_features.randomSplit([0.8, 0.2])
  
    # Instantiate and fit random forest classifier on all the data
    from pyspark.ml.classification import RandomForestClassifier
    rfc = RandomForestClassifier(
      featuresCol=""Features_vec"",
      labelCol=""ArrDelayBucket"",
      predictionCol=""Prediction"",
      maxBins=4657,
    )
    model = rfc.fit(training_data)
  
    # Save the new model over the old one
    model_output_path = ""{}/models/spark_random_forest_classifier.flight_delays.baseline.bin"".format(",ch09/baseline_spark_mllib_model.py,naoyak/Agile_Data_Code_2,1
"    df = pd.read_csv('Data/train.csv', dtype={'is_booking':bool,'srch_destination_id':np.int32, 'hotel_cluster':np.int32},parse_dates=True, infer_datetime_format=True)
    e=time.time()
    print e-s

    df['date_time'] = pd.to_datetime(df.date_time,errors='coerce')
    df['book_year'] = df['date_time'].dt.year
    from sklearn import tree
    # X = df[['hotel_market', 'hotel_country','hotel_continent','srch_destination_id','user_location_city','user_id','book_year','user_location_country','user_location_region','is_package','srch_destination_type_id']]
    X = df[['hotel_market', 'hotel_country','hotel_continent','srch_destination_id','user_location_city','user_id','book_year','is_package']]
    target = df.hotel_cluster
    ctreeModel = RandomForestClassifier(n_estimators=5,min_samples_leaf=150)
    ctreeModel = ctreeModel.fit(X, target)

    s=time.time()
    print ""START test""
    df_val = pd.read_csv('Data/test.csv', dtype={'is_booking':bool,'srch_destination_id':np.int32, 'hotel_cluster':np.int32},parse_dates=True, infer_datetime_format=True)
    e=time.time()
    print e-s
    df_val['date_time'] = pd.to_datetime(df_val.date_time,errors='coerce')
    df_val['book_year'] = df_val['date_time'].dt.year",hybridRandomForest.py,zhilongz/expedia,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans10_rf_only.py,diogo149/CauseEffectPairsPaper,1
"

from sklearn import preprocessing
for col in train.columns:
    scaler = preprocessing.StandardScaler()
    train[col] = scaler.fit_transform(train[col])
    test[col] = scaler.transform(test[col])


from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 500, random_state = 2543)
forest = forest.fit(train, target)
probs = forest.predict_proba(test)
output = [""%f"" % x[1] for x in probs]


df = pd.DataFrame()
df[""ID""] = testid
df[""target""] = output
df.to_csv('output1.csv', index = False)",spring.py,DEK11/Spring-Leaf-Competition,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.ml.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = spark.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> model.featureImportances
    SparseVector(1, {0: 1.0})
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0",python/pyspark/ml/classification.py,bOOm-X/spark,1
"
# Create Test Array
vTrain = np.zeros(shape=(len(vFare), 6))
vTrain[:, 0] = vFare
vTrain[:, 1] = vAges
vTrain[:, 2] = vGend
vTrain[:, 3] = vStrt
vTrain[:, 4] = vFamP
vTrain[:, 5] = vFamS

clf = RandomForestClassifier(n_estimators=1000, max_depth=None).fit(vTrain, vSurv)

# Finally normalize all items: zscore --> crop outliers (top 5%) --> min-max
#for v in np.arange(np.shape(vDat1M)[1]):
#	vDat1M[:, v] = (vDat1M[:, v] - np.mean(vDat1M[:, v])) / np.std(vDat1M[:, v])
#	vDat1M[np.where(vDat1M[:, v]) > optSD, v] = optSD
#	vDat1M[:, v] = (vDat1M[:, v] - np.min(vDat1M[:, v])) / (np.max(vDat1M[:, v]) - np.min(vDat1M[:, v]))

##########################################################################
##",titanic/titanic_analysis_2.py,josephdviviano/intro-data-science,1
"
    pred_v = []
    acc = []
    for x in np.nditer(limit_comp):
        indices = sample(range(days_previous), int(np.round(days_previous * train_split)))
        X_TRAIN = X_TEST_B.ix[indices]
        Y_TRAIN = Y_TEST_B.ix[indices]
        X_TEST = X_TEST_B.drop(X_TEST_B.index[indices])
        Y_TEST = Y_TEST_B.drop(Y_TEST_B.index[indices])
        # Fit the training data
        fluc_m.append(RandomForestClassifier(n_estimators=n))
        fluc_m[-1].fit(X_TRAIN, 1*(Y_TRAIN > x))
        # See how well we did
        a = fluc_m[-1].score(X_TEST, 1*(Y_TEST > x))
        acc.append(a)
        # Predict the future
        pred_v.append(fluc_m[-1].predict(PREDICT_X)[0])

    # Make an estimate of the daily change
    change = 0",ML_predict.py,simonward86/MySJcLqwwx,1
"        for penalty in ['l1', 'l2']:
            log_regs.append([sklm.LogisticRegression(penalty=penalty, C=c, fit_intercept=False, multi_class='ovr'),
                             c, penalty])
    return log_regs


def get_rand_fors():
    rand_fors = []
    for trees in [25, 100, 200]:
        for depth in [5, 10, 20]:
            rand_fors.append([sken.RandomForestClassifier(n_estimators=trees, max_depth=depth,
                                                          max_features='sqrt', n_jobs=-1), trees, depth])
    return rand_fors


def record_run(matrix, matrix_index, y_true, alg, alg_name, params):
    f_result = open(""results.txt"", mode='a')
    f_result.write('Matrix:' + str(matrix_index) + os.linesep)
    f_result.write(alg_name + os.linesep)
    for param in params:",whats-cooking.py,CURigel/Whats-Cooking,1
"traindata = pd.read_csv('C:/Users/sound/Desktop/Kaggle/Leaf Classfication/data/train.csv')
x_train = traindata.values[:, 2:]
y_train = traindata.values[:, 1]

#set the number of trees in random forest
num_trees = [10, 50, 100, 200, 300, 400, 500]
#calculate the cross validation scores and std
cr_val_scores = list()
cr_val_scores_std = list()
for n_tree in num_trees:
  recognizer = RandomForestClassifier(n_tree)
  cr_val_score = cross_val_score(recognizer, x_train, y_train)
  cr_val_scores.append(np.mean(cr_val_score))
  cr_val_scores_std.append(np.std(cr_val_score))
  
#plot cross_val_score and std
sc_array = np.array(cr_val_scores)
std_array = np.array(cr_val_scores_std) 
plt.plot(num_trees, cr_val_scores)
plt.plot(num_trees, sc_array + std_array, 'b--')",Leaf Classfication/Leaf Classfication in Random Forest/Random Forest Benchmark.py,0Steve0/Kaggle,1
"#	PREDICT
###
#####

correlation_values = {}

for nw in networks:
	print ""$"", nw

	# Create the random forest object
	forest = RandomForestClassifier(n_estimators = 100)

	#####
	#	TRAIN phase
	#####
	## Load data
	node_ids, train_data, score = load_data(""./SimuData/""+nw.replace('epinions', 'slashdot')+""/e0_v0_sum.csv"", ""./SimuData/""+nw.replace('epinions', 'slashdot')+""/e0_v0_src.tgf"")
	print ""TRAINING DATA loaded""

	## Train the random forest",learn_anonymity_rf.py,gaborgulyas/predict-anonymity,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.ml.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = spark.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> model.featureImportances
    SparseVector(1, {0: 1.0})
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0",python/pyspark/ml/classification.py,jianran/spark,1
"            tmp = train[train['case_length'] >= nr_events].groupby(case_id_col).head(nr_events)
            tmp[case_id_col] = tmp[case_id_col].apply(lambda x: ""%s_%s""%(x, nr_events))
            train_prefixes = pd.concat([train_prefixes, tmp], axis=0)
        
        
        if dataset_name not in best_params or method_name not in best_params[dataset_name]:
            continue
        rf_max_features = best_params[dataset_name][method_name]['rf_max_features']
        hmm_n_states = best_params[dataset_name][method_name]['hmm_n_states']

        cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=rf_max_features, random_state=random_state)
        feature_combiner = FeatureUnion([(method, init_encoder(method)) for method in methods])
        pipeline = Pipeline([('encoder', feature_combiner), ('cls', cls)])

        # fit pipeline
        train_y = train_prefixes.groupby(case_id_col).first()[label_col]

        print(""Fitting pipeline..."")
        sys.stdout.flush()
        start = time()",experiments_final/run_all_single.py,irhete/predictive-monitoring-benchmark,1
"
# select feature
model = SelectFromModel(clf, prefit=True)
train_new = model.transform(train)
train_new.shape

test_new = model.transform(test)
test_new.shape

# Hyperparameters tuning
forest = RandomForestClassifier(max_features='sqrt')

parameter_grid = {
                 'max_depth' : [4,5,6,7,8],
                 'n_estimators': [200,210,240,250],
                 'criterion': ['gini','entropy']
                 }

cross_validation = StratifiedKFold(targets, n_folds=5)
",python/pipeline/titanic_tutorial.py,trhongbinwang/data_science_journey,1
"
class Classifier(BiPlot):
    '''
    To hold methods and data to support classification of measurements in a STOQS database.
    See http://scikit-learn.org/stable/auto_examples/plot_classifier_comparison.html
    '''
    classifiers = { 'Nearest_Neighbors': KNeighborsClassifier(3),
                    'Linear_SVM': SVC(kernel=""linear"", C=0.025),
                    'RBF_SVM': SVC(gamma=2, C=1),
                    'Decision_Tree': DecisionTreeClassifier(max_depth=5),
                    'Random_Forest': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
                    'AdaBoost': AdaBoostClassifier(),
                    'Naive_Bayes': GaussianNB(),
                    'LDA': LDA(),
                    'QDA': QDA()
                  }
    def getActivity(self, mpx, mpy):
        '''
        Return activity object which MeasuredParameters mpx and mpy belong to
        '''",stoqs/contrib/analysis/classify.py,danellecline/stoqs,1
"Y_training = Y[0:last_training_index]
Y_testing = Y[last_training_index:]

print ""there are %d records, %d training and %d testing"" % (len(data), len(X_training), len(X_testing))
print ""a priori   'inject newline' rate is %3d/%4d = %f"" % (sum(Y), len(Y), sum(Y)/float(len(Y)))

# transform categorical values
index_of_cat_features = [0, 3, 4, 5]
# todo

forest = RandomForestClassifier(n_estimators = 100)
forest = forest.fit(X_training, Y_training)

Y_predictions = forest.predict(X_testing)

print ""expected   'inject newline' rate is %3d/%4d = %f"" % \
      (sum(Y_testing), len(Y_testing), sum(Y_testing)/float(len(Y_testing)))
print ""prediction 'inject newline' rate is %3d/%4d = %f"" % \
      (sum(Y_predictions), len(Y_predictions), sum(Y_predictions)/float(len(Y_predictions)))
",python/play/random_forest_on_token_features.py,antlr/codebuff,1
"    
if __name__ == ""__main__"":
    print ""Load Dataset""
    Arffhandler = Arffhandler()
    Arffhandler.Load(sys.argv[1])
    Arffhandler.OneHotEncode()
    print ""Setup data""
    inputs = Arffhandler.inputs
    output = Arffhandler.output
    print ""Setup and fit RandomForest""
    randomForest = RandomForestClassifier(n_estimators=30, \
        criterion=""gini"", \
        max_features=""auto"", \
        max_depth=3, \
        min_samples_split=2, \
        min_samples_leaf=1, \
        min_weight_fraction_leaf=0, \
        max_leaf_nodes=None, \
        bootstrap=True, \
        oob_score=True, \",other/handlers/ForestHandler.py,bcraenen/KFClassifier,1
"		
		
		Y_train = np.copy(sd.LD.data['Y_train'])
		X_train = np.copy(sd.LD.data['X_train'])
		X_valid = np.copy(sd.LD.data['X_valid'])
		X_test = np.copy(sd.LD.data['X_test'])
		
		split = int(len(Y_train)*0.5)
		Lnum = -1
		for model in [linear_model.LogisticRegression(random_state=101),
					ensemble.RandomForestClassifier(n_estimators=16,  max_depth=3, random_state=102),
					linear_model.LogisticRegression(random_state=103),
					ensemble.GradientBoostingClassifier(n_estimators=100, max_depth=4, warm_start=False, random_state=104),
					ensemble.GradientBoostingClassifier(n_estimators=100, warm_start=False, learning_rate=0.1, random_state=105),
					]:
			Lnum += 1
			
			if Ltime_budget < 500 and (time.time() - Lstart) / Ltime_budget > 0.5 and Lnum > 0:
				break
				",lib/engine_serial.py,djajetic/AutoML3,1
"        self.criterion = 'entropy'
    
if __name__ == ""__main__"":
    print ""Load Dataset""
    Arffhandler = Arffhandler()
    Arffhandler.Load(sys.argv[1])
    Arffhandler.OneHotEncode()
    inputs = Arffhandler.inputs
    output = Arffhandler.output
    print ""Setup and fit RandomForest""
    randomForest = RandomForestClassifier(n_estimators=30, \
                                          criterion=""gini"", \
                                          max_features=""auto"", \
                                          max_depth=3, \
                                          min_samples_split=2, \
                                          min_samples_leaf=1, \
                                          min_weight_fraction_leaf=0, \
                                          max_leaf_nodes=None, \
                                          bootstrap=True, \
                                          oob_score=True, \",other/handlers/KnowledgeFragmentHandler.py,bcraenen/KFClassifier,1
"    report.scatter([(X.columns[0], X.columns[2])], mask=mask).plot()
    report.learning_curve(RocAuc(), mask=mask).plot()
    report.metrics_vs_cut(significance, mask=mask).plot()


def test_own_classification_reports():
    """"""
    testing clf.test_on
    """"""
    X, y, sample_weight = generate_classification_data()
    clf = SklearnClassifier(RandomForestClassifier())
    clf.fit(X, y, sample_weight=sample_weight)
    report = clf.test_on(X, y, sample_weight=sample_weight)
    roc1 = report.compute_metric(RocAuc())

    lds = LabeledDataStorage(X, y, sample_weight=sample_weight)
    roc2 = clf.test_on_lds(lds=lds).compute_metric(RocAuc())
    assert roc1 == roc2, 'Something wrong with test_on'

",tests/test_factory.py,vkuznet/rep,1
"    actionIDs,taxonomy,database = readannos()
    print 'getting training data.... '
    xtrain,ytrain = getC3Ddata(database,indexs,gtlabels,'training')
    print 'got it!! and shape is ',np.shape(xtrain)
    print 'getting validation data.... '
    xval,yval = getC3Ddata(database,indexs,gtlabels,'validation')
    print 'got it!! and shape is ',np.shape(xval)
    
    numSamples = np.shape(xval)[0]

    clf = RandomForestClassifier(n_estimators=256,n_jobs=2)
    clf = clf.fit(xtrain, ytrain)
    preds = clf.predict(xval)
    correctPreds = preds == yval;
    print 'Overall Accuracy is ',100*float(np.sum(correctPreds))/numSamples, '% ', ' with RF'
    
    saveName = '{}data/BWtrainingRF-{}.pkl'.format(baseDir,'C3D')
    with open(saveName,'w') as f:
            pickle.dump(clf,f)
            ",processing/trainC3dFramewise.py,gurkirt/actNet-inAct,1
"  if Normalize:
    Trx = 2.*Trx - 1.
    Try = 2.*Try - 1.
  

  # SVM fitting
  Models = [
 
  [svm.SVC(), ""SVM""],
 
#  [ensemble.RandomForestClassifier(n_estimators=256,),""RF"",],
 
  [linear_model.LogisticRegression(),""LR"",], 
 
#  [linear_model.Perceptron(), ""PCP""] 
    
#  [svm.SVC(C=3.16, cache_size=200, class_weight=None, coef0=0.0, degree=7, gamma=.1, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=0),""SVM2"",],

#  [linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_inteept=True, intercept_scaling=1, class_weight=None, random_state=None),""LR2"",], 
  ",svmtest.py,rodrigosurita/freepuf,1
"    predictions = predict(test[features], clf)

    cm = confusion_matrix(test[target], predictions)
    accuracy = accuracy_score(test[target], predictions)

    df_out = pd.DataFrame(cm, index=labels, columns=labels)
    return df_out.astype(float).to_dict(), accuracy


def train_model(data, target):
    rf = RandomForestClassifier()
    rf.fit(data, target)
    return rf


def predict(data, clf):
    return clf.predict(data)


def split_data(data, ratio_train=0.7):",iris-server/ml/classification.py,andygoldschmidt/iris-webapp,1
"    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
  
    dataset = dc.data.NumpyDataset(X, y, w, ids)
    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

    sklearn_model = RandomForestClassifier()
    model_dir = tempfile.mkdtemp()
    model = dc.models.SklearnModel(sklearn_model, model_dir)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Load trained model
    reloaded_model = dc.models.SklearnModel(None, model_dir)",deepchem/models/tests/test_reload.py,bowenliu16/deepchem,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_pca1.py,diogo149/CauseEffectPairsPaper,1
"	start = datetime.now()
	
	#Run at least once the reduce_data on train and test
	#reduce_data(loc_train, loc_train, loc_train_reduced)
	#reduce_data(loc_test, loc_train, loc_test_reduced)
	""""""
	X_train = load_data(loc_train_reduced, nrows=max_train_lines_read)
	y = load_data(loc_labels, nrows=max_train_lines_read)
	y = y.drop(""id"", axis=1) # dont need ID in labels

	clf = ensemble.RandomForestClassifier(n_estimators=nr_estimators_rf, n_jobs=n_cpu_jobs, random_state=random_state, verbose=verbosity)
	print(""\nTraining classifier:\n\t%s\n\nPuny humans are instructed to wait...\n""%clf)
	clf.fit(X_train,y)

	#We don't need no education
	del X_train
	
	# Predicting and writing Kaggle submission
	print(""\nWriting Kaggle submission to %s""%loc_kaggle_submission)
	X_test = load_data(loc_test_reduced)",others/kaggle_tradeshift.py,timpalpant/KaggleTSTextClassification,1
"output = pd.DataFrame( data={""PassengerId"":test[""PassengerId""], ""Survived"":result} )
output.to_csv( ""gnb.csv"", index=False, quoting=3 )


# #### kaggle0.74163

# ### 

# In[21]:

forest = RandomForestClassifier( n_estimators=500, criterion='entropy', max_depth=5, min_samples_split=1,
  min_samples_leaf=1, max_features='auto', bootstrap=False, oob_score=False, n_jobs=4,
  verbose=0)

get_ipython().magic(u'time forest = forest.fit( train_data, label )')
print ""10: "", np.mean(cross_val_score(forest, train_data, label, cv=10, scoring='roc_auc'))

result = forest.predict( test_data )
output = pd.DataFrame( data={""PassengerId"":test[""PassengerId""], ""Survived"":result} )
output.to_csv( ""rf.csv"", index=False, quoting=3 )",competitions/Titanic/titanic.py,lijingpeng/kaggle,1
"
classifiers = {
		'SVCP': svm.SVC(gamma = 0.001, C = 10),
		'SVCR': svm.SVC(gamma = 0.0001, C = 50),
		'NB ': GaussianNB(),
		'BNB': BernoulliNB(),
		'NBU': neighbors.KNeighborsClassifier(5, weights = 'uniform'),
		'NBD': neighbors.KNeighborsClassifier(5, weights = 'distance'),
		'TRE': tree.DecisionTreeClassifier(),
		'GBC': GradientBoostingClassifier(n_estimators = 100, learning_rate = 1.0, max_depth = 1, random_state = 0),
		'RFC': RandomForestClassifier()
	}

scores = [(n, clf.fit(training_data, training_target).score(test_data, test_target)) for n, clf in classifiers.iteritems()]

for name, score in sorted(scores, key = lambda t: t[1], reverse = True):",score_classifiers.py,paolo-torres/Sign-Language-Translator,1
"vecQ = np.zeros((n_kernels,1))
gamma = 0.01

names = [""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"", ""Logistic Regression""]


classifiers = [
	SVC(kernel=""linear"", C=3.4,gamma=0.1),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4)]


def compute_J(N, theta):

	if N == 0:
		return np.pi - theta
	elif N == 1:",UMKL/textData/add_kernels.py,akhilpm/Masters-Project,1
"

def test_impute():
    from pyimpute import load_training_rasters, load_targets, impute

    # Load training data
    train_xs, train_y = load_training_rasters(response_raster, explanatory_rasters)

    # Train a classifier
    from sklearn.ensemble import RandomForestClassifier
    clf = RandomForestClassifier(n_estimators=10, n_jobs=1)
    clf.fit(train_xs, train_y)

    # Load targets
    target_xs, raster_info = load_targets(explanatory_rasters)

    # Go...
    impute(target_xs, clf, raster_info, outdir=TMPOUT,
           linechunk=400, class_prob=True, certainty=True)
",tests/test_impute.py,perrygeo/pyimpute,1
"    # (even if the labels are only 0 or 1)

    # Load the sparse validation data :
    X_test, y_test = load_svmlight_file(
        os.path.join(data_dir, 'test_preprocessed.data'))
    # Now X_test contains a 'scipy.sparse.csr.csr_matrix'
    # And y_test is a simple numpy array of float64
    # (even if the labels are only 0 or 1)

    # Declaration of the model algorithm
    clf = RandomForestClassifier()
    # Most of the classifiers can take sparse matrix as inputs
    #  (they automatically convert it if necessary)
    # fit() function is used to train the classifier
    clf.fit(X_train, y_train)

    # Computes the prediction thanks to the predict() function
    # for the validation set
    valid_predictions = clf.predict(X_valid)
    # and the test set",starting kit/python/minimal_preprocessed.py,tgensol/projet,1
"    df = pd.read_pickle(filename)
    df = df.drop(['Mostly Cloudy', 'Clear', 'Partly Cloudy', 'Flurries', \
                  'Light Snow', 'Foggy', 'Snow', 'Rain'], axis=1)
    y = df.pop('prediction').values
    X = df.values
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)
    return df, X_train, X_test, y_train, y_test


def fit_model(X_train, X_test, y_train, y_test):
    model = RandomForestClassifier(n_estimators=50, random_state=0)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print ""Accuracy: "", accuracy_score(y_test, y_pred)
    print ""Precision: "", precision_score(y_test, y_pred)
    print ""Recall: "", recall_score(y_test, y_pred)
    return model


def plot_importances(model, df):",build_model.py,THEdavehogue/punxsutawney_phil_predictor,1
"    Examples
    --------
    >>> from costcla.probcal import ROCConvexHull
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.metrics import brier_score_loss
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> f = RandomForestClassifier()
    >>> f.fit(X_train, y_train)
    >>> y_prob_test = f.predict_proba(X_test)
    >>> f_cal = ROCConvexHull()
    >>> f_cal.fit(y_test, y_prob_test)
    >>> y_prob_test_cal = f_cal.predict_proba(y_prob_test)
    >>> # Brier score using only RandomForest
    >>> print(brier_score_loss(y_test, y_prob_test[:, 1]))
    0.0577615264881
    >>> # Brier score using calibrated RandomForest",costcla/probcal/probcal.py,albahnsen/CostSensitiveClassification,1
"    scoring=""accuracy"",
    n_jobs=1)

plot.validation_curve(train_scores, test_scores, param_range, param_name,
                      semilogx=True)
plt.show()

param_range = np.array([1, 10, 100])
param_name = ""n_estimators""
train_scores, test_scores = validation_curve(
    RandomForestClassifier(), X, y,
    param_name=param_name,
    param_range=param_range,
    cv=10, scoring=""accuracy"", n_jobs=1)

plot.validation_curve(train_scores, test_scores, param_range, param_name,
                      semilogx=False)
plt.show()",examples/validation_curve.py,edublancas/sklearn-evaluation,1
"from sklearn.ensemble import RandomForestClassifier


class Random_forest(Classifier):
    
    classifierName = 'Random Forest'
    n_estimators = 20
        
    def train(self):

        clf = RandomForestClassifier(n_estimators=20, n_jobs=self.threadCount)
        # Set the parameters by cross-validation
        # specify parameters and distributions to sample from
        param_dist = {""bootstrap"": [True, False],
                          ""criterion"": [""gini"", ""entropy""]}

        n_iter_search = 4
        estimatorClass = RandomizedSearchCV(clf, param_distributions=param_dist,
                                  n_iter=n_iter_search, n_jobs=self.threadCount, cv=5)
        estimatorClass.fit(self.Xtrain, self.ytrain)",infodens/classifier/random_forest.py,rrubino/B6-SFB1102,1
"    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


results = []
for clf, name in (
        (RidgeClassifier(tol=1e-2, solver=""lsqr""), ""Ridge Classifier""),
        (Perceptron(n_iter=50), ""Perceptron""),
        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        (KNeighborsClassifier(n_neighbors=10), ""kNN""),
        (RandomForestClassifier(n_estimators=100), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,",projects/scikit-learn-master/examples/text/document_classification_20newsgroups.py,DailyActie/Surrogate-Model,1
"from sklearn import ensemble

class RandomForest(MLClassifier):

  def __init__(self, train_file, test_file, output_folder):
    MLClassifier.__init__(self, ""rf"", train_file, test_file, output_folder)

  def run(self):
    (train_instances, train_classes, test_instances, test_ids) = self.parse_data()

    clf = ensemble.RandomForestClassifier()

    clf.fit(train_instances, train_classes)

    result_class_with_labels = clf.predict(test_instances)

    self.output_result_to_file(test_ids, result_class_with_labels)
",classifiers/random_forest.py,tdopires/forest-cover-group6,1
"    import IPython
    IPython.embed()
    return df

def fit_models(df, X, y):
   # Classifiers to test
    classifiers = [('logistic_regression', LogisticRegression())] 
                   #('k_nearest_neighbors', KNeighborsClassifier()),
                   #('decision_tree', DecisionTreeClassifier()),
                   #('SVM', LinearSVC()),
                   #('random_forest', RandomForestClassifier()),
                   #('boosting', GradientBoostingClassifier()),
                   #('bagging', BaggingClassifier())]

    ml.build_classifiers(df, X, y, classifiers)
    #ml.test_classifier(df, X, y, classifiers)
 

#-------------------------------------------------------
",pipeline/mcps_pipeline.py,BridgitD/school-dropout-predictions,1
"def plot_forest(max_depth=1):
    plt.figure()
    ax = plt.gca()
    h = 0.02

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    if max_depth != 0:
        forest = RandomForestClassifier(n_estimators=20, max_depth=max_depth,
                                        random_state=1).fit(X, y)
        Z = forest.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, alpha=.4)
        ax.set_title(""max_depth = %d"" % max_depth)
    else:
        ax.set_title(""data set"")
    ax.scatter(X[:, 0], X[:, 1], c=np.array(['b', 'r'])[y], s=60)
    ax.set_xlim(x_min, x_max)",day3-machine-learning/plots/plot_interactive_forest.py,dalya/AstroHackWeek2015,1
"from sklearn.linear_model import LogisticRegression
logreg_grid = GridSearchCV( LogisticRegression( multi_class = ""ovr"" ), param_grid = {
		""C"" : np.logspace( -1, 2, num = 4 ),
	}, cv = 10, n_jobs = -1, verbose = 50, scoring = ""log_loss"" ).fit( X_train_0, y_train_0 )

ensemble_clf.append( ( ""Logistic"", logreg_grid.best_estimator_ ) )
scores_to_df( logreg_grid.grid_scores_ )

## 2.Random Forest
from sklearn.ensemble import RandomForestClassifier
rf_grid = GridSearchCV( RandomForestClassifier( n_estimators = 256 ), param_grid = {
## max_depth -- the maximum allowed number of levels in the decision tree.
		""max_depth"" : [ 1, 3, 5, 7, 10, ],
	}, cv = 10, scoring = 'log_loss', verbose = 10 ).fit( X_train_0, y_train_0 )

ensemble_clf.append( ( ""Forest"", rf_grid.best_estimator_ ) )
scores_to_df( rf_grid.grid_scores_ )

## 3. k - nearest neighbour
from sklearn.neighbors import KNeighborsClassifier",data_study/otto_group/main.py,ivannz/study_notes,1
"    return X, y


def risk_estimation(risk_X, risk_y):
    """"""
    estimate risk first and then predict accuracy on the estimated risk level
    the risk learner is selected by the brier score loss
    """"""
    risk_learners = [  # candidate risk learners
        linear_model.LogisticRegression(),
        ensemble.RandomForestClassifier(n_estimators=10),
        ensemble.AdaBoostClassifier(n_estimators=10),
        ensemble.GradientBoostingClassifier(n_estimators=10)]

    best_brier_score_loss = 100
    best_y_pred_cali_prob = None
    for risk_learner in risk_learners:
        cali_learner = CalibratedClassifierCV(risk_learner, cv=3, method='isotonic')
        k_fold = KFold(3)
        y_pred_cali_prob = np.zeros((len(risk_y), ))",risk_estimation.py,wangleye/age_risk_estimation_book,1
"
# Initialize several learners, to be compared momentarily
logistic1 = LogisticRegression(C=.1)
logistic2 = LogisticRegression(C=.3)
logistic3 = LogisticRegression(C=1)
logistic4 = LogisticRegression(C=5)
logistic5 = LogisticRegression(C=10)
linsvc = svm.LinearSVC()
knn = KNeighborsClassifier(4, weights='distance')
mytree = tree.DecisionTreeClassifier()
myforest = RandomForestClassifier()

for clf,name in [(logistic1, ""Logistic Regression C=.1""), (logistic2, ""Logistic Regression C=.3""), (logistic3, ""Logistic Regression C=1""), (logistic4, ""Logistic Regression C=5""), (logistic5, ""Logistic Regression C=10""), (linsvc, ""Linear SVC""), (knn, ""K-nearest Neighbors""), (mytree, ""Decision Tree""), (myforest, ""Random Forest"")]:

    scores = cross_validation.cross_val_score(clf, X_scaled, y, scoring='recall', cv=400)
    print(name + "":"")
    print(""Accuracy: {:0.4f} - {:0.4f}"".format(scores.mean()-scores.std()/2, scores.mean()+scores.std()/2))
    print()",crossvalidate.py,nrpeterson/mse-closure-predictor,1
"        #print datasets_expand
        datasets = sv.preprocess(datasets_expand, scale=True, max_samples=10000, nshuffle=nshuffle)
        #print datasets
        print len(datasets)
        clf = [
            LogisticRegression(),
            KNeighborsClassifier(3),
            SVC(kernel=""linear"", C=0.025),
            SVC(gamma=2, C=1),
            DecisionTreeClassifier(max_depth=5),
            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            AdaBoostClassifier(),
            GaussianNB(),
            LDA(),
            QDA()]
        data_names = [ 'X' + str(i) + '#' + str(j) for i in indicies for j in range(nshuffle) ]
        clf_names = ['LR', 'KNN', 'LSVM', 'RBF', ""Tree"",
                     ""RForest"", ""AdaBoost"", ""NB"", ""LDA"", ""QDA""]
        #sv.plot_classifiers(datasets, clf, data_names=data_names, clf_names=clf_names)
        sv.plot_classifiers_roc(datasets, clf, data_names=data_names, clf_names=clf_names)",myquant/strategy/LogisticStrategy.py,goldfire9/stock-ml,1
"    ----------
    classes_ : array-like, shape = [n_predictions]

    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.linear_model import LogisticRegression
    >>> from sklearn.naive_bayes import GaussianNB
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
    >>> y = np.array([1, 1, 1, 2, 2, 2])
    >>> eclf1 = VotingClassifier(estimators=[
    ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
    >>> eclf1 = eclf1.fit(X, y)
    >>> print(eclf1.predict(X))
    [1 1 1 2 2 2]
    >>> eclf2 = VotingClassifier(estimators=[",sklearn/ensemble/voting_classifier.py,CforED/Machine-Learning,1
"            rules= fpgrowth([itemMatrix[i] for i in pindex],supp = supp,zmin = 1,zmax = maxlen)
            rules = [tuple(np.sort(rule[0])) for rule in rules]
            rules = list(set(rules))
            start_time = time.time()
            print '\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules))
        else:
            rules = []
            start_time = time.time()
            for length in xrange(1,maxlen+1,1):
                n_estimators = min(pow(df.shape[1],length),4000)
                clf = RandomForestClassifier(n_estimators = n_estimators,max_depth = length)
                clf.fit(self.df,self.Y)
                for n in xrange(n_estimators):
                    rules.extend(extract_rules(clf.estimators_[n],df.columns))
            rules = [list(x) for x in set(tuple(x) for x in rules)]
            print '\tTook %0.3fs to generate %d rules' % (time.time() - start_time, len(rules))
        self.screen_rules(rules,df,N) # select the top N rules using secondary criteria, information gain
        self.getPatternSpace()

    def screen_rules(self,rules,df,N):",BOAmodel.py,wangtongada/BOA,1
"# -*- coding: utf-8 -*-

from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = RandomForestClassifier(n_estimators=15, max_depth=None,
                             min_samples_split=2, random_state=0)
clf.fit(X, y)

output = Porter(clf, language='js').export()
print(output)

""""""
var Brain = function() {
",examples/classifier/RandomForestClassifier/js/basics.py,nok/sklearn-porter,1
"
#
# Cross validate, train and evaluate classifier
#

# Test/train split
training_data, test_data = final_vectorized_features.randomSplit([0.7, 0.3])

# Instantiate and fit random forest classifier
from pyspark.ml.classification import RandomForestClassifier
rfc = RandomForestClassifier(
  featuresCol=""Features_vec"",
  labelCol=""ArrDelayBucket"",
  maxBins=4657,
  maxMemoryInMB=1024
)
model = rfc.fit(training_data)

# Evaluate model using test data
predictions = model.transform(test_data)",ch07/train_spark_mllib_model.py,rjurney/Agile_Data_Code_2,1
"
# ----------------
# Random Forest
# ----------------

# null values are bad for decision trees -> fill them with 0
data.fillna(0, inplace=True)
#print('Nonzero :', np.count_nonzero(pd.isnull(data.ix[train_idx,feats1])))

# take random forest model
rf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1)
rf1.fit(data.ix[train_idx,feats1], data['tipped'].ix[train_idx])
print('Random Forest classifier: ', rf1)

# calculate predictions with probabilities
preds1 = rf1.predict_proba(data.ix[test_idx,feats1])

fpr1, tpr1, thr1 = roc_curve(data['tipped'].ix[test_idx], preds1[:,1])
auc1 = roc_auc_score(data['tipped'].ix[test_idx], preds1[:,1])
print(' AUC for RandomForest: ', auc1)",examples/taxi-example.py,remigius42/code_camp_2017_machine_learning,1
"X = pd.DataFrame(train_x)
y = pd.DataFrame(target_x, columns=['target'])
test_x = pd.DataFrame(test_x)
test_y = pd.DataFrame(target_y, columns=['target'])
########################################################################################################################

# Test out Gestalt.
skf = KFold(n_splits=3, random_state=42, shuffle=True)
# Base estimators come in the form of a dictionary of {estimator1:'name1', estimator2:'name2'}
# This makes life easy when naming the meta-learner dataset.
estimators = {RandomForestClassifier(n_estimators=10, n_jobs=8, random_state=42): 'RFR1',
              XGBClassifier(num_round=50,
                            verbose_eval=False,
                            params={'objective': 'binary:logistic',
                                    'silent': 1}): 'XGB1'}

print(""\nPandas Test"")
for stype in ['t', 'cv', 'st', 's']:
    print('\n')
    b_cancer = GeneralisedStacking(base_estimators_dict=estimators,",examples/breast_cancer_binary.py,mpearmain/gestalt,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    # RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans10_gbm_only.py,diogo149/CauseEffectPairsPaper,1
"    rough idea of parameters for the classifiers. (Does not address pre-processing)
    More classifiers can be added as desired, and parameters expanded.

    Later: Add options for RBM + Logit; PCA; ICA; LDA.
     See also
    http://scikit-learn-laboratory.readthedocs.org/en/latest/_modules/skll/learner.html

    TODO: Add parameters + put classifiers/""pipeline_#"" in a list. (To allow checking only some params)
    '''

#    pipeline1 = Pipeline('clf', RandomForestClassifier() )
#
#    pipeline2 = Pipeline(
#    ('clf', KNeighborsClassifier()),)
    pipeline1 = RandomForestClassifier(n_jobs=-1)
    pipeline2 = KNeighborsClassifier()

    pipeline3 = SVC(cache_size=1500)
    # pipeline3 = NuSVC(cache_size=1500)
",ProFET/feat_extract/PipeTasks.py,ddofer/ProFET,1
"    assert np.array_equal(result['guess'].values, dtc.predict(testing_features))


def test_random_forest():
    """"""Ensure that the TPOT random forest method outputs the same as the sklearn random forest when""""""

    tpot_obj = TPOT()
    result = tpot_obj._random_forest(training_testing_data, 0.1)
    result = result[result['group'] == 'testing']

    rfc = RandomForestClassifier(n_estimators=500, min_weight_fraction_leaf=0.1, random_state=42, n_jobs=-1)
    rfc.fit(training_features, training_classes)

    assert np.array_equal(result['guess'].values, rfc.predict(testing_features))


def test_random_forest_2():
    """"""Ensure that the TPOT random forest method outputs the same as the sklearn random forest when min_weight>0.5""""""

    tpot_obj = TPOT()",tests.py,bartleyn/tpot,1
"# The input to fit_transform should be a list of strings.
train_data_features = vectorizer.fit_transform(clean_train_reviews)

# Numpy arrays are easy to work with, so convert the result to an array
train_data_features = train_data_features.toarray()

## Train classifier
print ""Training the random forest...""

# Initialize a Random Forest classifier with 100 trees
forest = RandomForestClassifier(n_estimators = 100)

# Fit the forest to the training set, using the bag of words as features
# and the sentiment labels as the response variable
#
# This may take a few minutes to run
forest = forest.fit(train_data_features, train[""sentiment""])

# Read the test data
test = pd.read_csv(""/home/hkh/sources/kagglepy/popcorn/data/testData.tsv"", header=0, delimiter=""\t"", quoting=3)",popcorn/src/bagofwords.py,hkhpub/kagglepy,1
"y_str = labels
y = le.fit_transform(y_str)
print('Label encoding done')


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=13)

print('Initializing classifier')
#clf = SVC(C=1000.0) #88.3 0:00:03.069055
print('Classifier initialized')
#clf = RandomForestClassifier() #91.84 0:00:00.019845
#clf = GradientBoostingClassifier() #94.07 0:00:00.108704
#clf = GaussianNB() #69.78 0:00:00.106093
#clf = KNeighborsClassifier(3) #71.90 0:00:16.034729
#clf = AdaBoostClassifier() #85.55 0:00:00.142586
clf = LogisticRegression() #87.42 0:00:00.062891
print('Initializing learning')
clf.fit(X_train, y_train)
print('Learning complete')
",classification/classificationSVM.py,pcomputo/webpage-classifier,1
"vocab = vectorizer.get_feature_names()
print vocab

dist = np.sum(train_data_features, axis=0)
for tag, count in zip(vocab, dist):     
    print count, tag

# Feed it into a supervised learning algorithm
print ""Training the random forest (this may take a while)...\n""
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 100)
forest = forest.fit(train_data_features,train[""sentiment""])

end = time.time() 
elapsed = end - start
print ""Time taken to extract features and fit model: "", elapsed, ""seconds.""

# Get test results.  NOTE that we do NOT call ""fit"" for the test set
test_data_features = vectorizer.transform(clean_test_reviews).toarray() 
result = forest.predict(test_data_features)",BoW.py,angelachapman/Kaggle-DeepLearning-Tutorial,1
"        training_label = [arr[idx_imb] for idx_arr, arr in enumerate(label_bal)
                         if idx_arr != idx_lopo_cv]
        # Concatenate the data
        training_data = np.vstack(training_data)
        training_label = label_binarize(np.hstack(training_label).astype(int),
                                        [0, 255])
        print 'Create the training set ...'

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_bal.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/balancing/pipeline_classifier_mrsi_spectra.py,I2Cvb/mp-mri-prostate,1
"
        # Make predictors
        pred = self.pred
        if y_colums is None:
            pred[""switch""] = np.where((pred.make_splits(5, inplace=False).shift(1) / pred.make_splits(5, inplace=False))
                                  > (1.0025 / 0.9975) + self.min_shift, 1, 0)
        else:
            pred[""switch""] = pred[y_colums]
        self.indicators = p
        # Make model
        clf = RandomForestClassifier(n_estimators=n)
        results = pd.DataFrame()
        accuracy = []
        # Backtest all data using a rolling look forward method.
        for i in range(ndays, len(pred.index)-forward_look):
            # We perform a 80/20 split on the data
            ind = int(np.round(ndays*0.8))
            X_TRAIN = pred.ix[(i - ndays):(i - ndays + ind),p]
            if forward_look > 0:
                idx = pred.ix[(i - ndays):(i - ndays + ind+forward_look)].index",Methods/ML.py,simonward86/MySJcLqwwx,1
"         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""Linear Discriminant Analysis"",
         ""Quadratic Discriminant Analysis"", 
         ""MLP""]


classifiers = [
    KNeighborsClassifier(weights='distance', n_neighbors=121),
    SVC(kernel=""linear"", C=1, probability=True),
    SVC(C=1, probability=True),
    DecisionTreeClassifier(max_depth=10),
    RandomForestClassifier(max_depth=10, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(solver='lsqr', shrinkage=""auto""),
    QuadraticDiscriminantAnalysis(),
    MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100,75,50,25,15), max_iter=10000, random_state=1)
    ]


param_grid = {",2-AlexNet/MLPClassifier.py,cs60050/TeamGabru,1
"targets_tr = traindf['cuisine']

predictors_ts = tfidfts


# classifier = LinearSVC(C=0.80, penalty=""l2"", dual=False)
# parameters = {'C':[1, 10]}
# parameters = {""max_depth"": [3, 5,7]}
# clf = LinearSVC()
# clf = LogisticRegression()
# clf = RandomForestClassifier(n_estimators=100, max_features=""auto"",random_state=50)

# classifier = grid_search.GridSearchCV(clf, parameters)
# classifier = GridSearchCV(clf, parameters)
# classifier = RandomForestClassifier(n_estimators=200)
classifier = xgb.XGBClassifier(max_depth=3, n_estimators=100, learning_rate=0.05) #0.69328

classifier=classifier.fit(predictors_tr,targets_tr)

predictions=classifier.predict(predictors_ts)",deep_learn/whatiscooking/test1.py,zhDai/CToFun,1
"    return (data, result)

def cross_validation_test():
    data = get_train_data()
    target = data.Cover_Type
    train = data.drop(['Cover_Type'], axis = 1)
    kfold = 10
    cross_val_final = {}

    print 'Cross validation test...'
    model_rfc = RandomForestClassifier(n_estimators = 1024, criterion='entropy', n_jobs = -1)
    model_knc = KNeighborsClassifier(n_neighbors = 128)
    model_lr = LogisticRegression(penalty='l1', C=1e5)

    scores = cross_validation.cross_val_score(model_rfc, train, target, cv = kfold)
    cross_val_final['RFC'] = scores.mean()
    print 'RFC: ', scores.mean()

    scores = cross_validation.cross_val_score(model_knc, train, target, cv = kfold)
    cross_val_final['KNC'] = scores.mean()",forest_cover_type.py,zhzhussupovkz/forest-cover-type-prediction,1
"    ax.set_xlabel(""true label"")
    ax.set_ylabel(""predicted label"")
    ax.set_title(label)
    i += 1

# random forest results
from sklearn.ensemble import RandomForestClassifier

for max_features in [1.0, 0.3]:
    for n_estimators in [10, 100]:
        clf = RandomForestClassifier(max_features=max_features,
                                     n_estimators=n_estimators,
                                     random_state=0).fit(Xtrain, ytrain)
        ypred = clf.predict(Xtest)
        label = ""RF: max_features = {0}, n_estimators = {1}"".format(
            max_features, n_estimators)
        print('{0}: {1}'.format(label, metrics.f1_score(ytest, ypred))
        ax = fig.add_subplot(3, 2, i + 1, xticks=[], yticks=[])
        ax.imshow(metrics.confusion_matrix(ypred, ytest),
                   interpolation='nearest', cmap=plt.cm.binary)",notebooks/solutions/04_svm_rf.py,pprett/sklearn_pycon2014,1
"def random_forest(dataset, DV, max_dep):
	start = time.time()
	# Load Data to Pandas
	data = pd.read_csv(dataset, index_col=0)
	data.columns = [camel_to_snake(col) for col in data.columns]

	#DV
	y = data[str(DV)]
	X = data[data.columns - [str(DV)]]

	clf = RandomForestClassifier(n_jobs=2, max_depth=max_dep)
	model = clf.fit(X, y)

	end = time.time()
	print ""Classifier: Random Forest,"", ""Depth = "", max_dep
	print ""Runtime, base model: %.3f"" % (end-start), ""seconds.""
	return model 

# Unhash to test:
#random_forest('data/cs-training#3B.csv', 'serious_dlqin2yrs', 5)",pipeline/__5_MCPS_Classifier.py,BridgitD/school-dropout-predictions,1
"            from sklearn.neighbors import KNeighborsClassifier
            estimator = KNeighborsClassifier(
                **self.kwargs)
        elif self.estimator == 'decision-tree':
            from sklearn.tree import DecisionTreeClassifier
            estimator = DecisionTreeClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'random-forest':
            from sklearn.ensemble import RandomForestClassifier
            estimator = RandomForestClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'adaboost':
            from sklearn.ensemble import AdaBoostClassifier
            estimator = AdaBoostClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'gradient-boosting':
            from sklearn.ensemble import GradientBoostingClassifier",imbalanced-learn-master/imblearn/under_sampling/instance_hardness_threshold.py,RPGOne/Skynet,1
"test_data[""Embarked""].fillna('S',inplace=True)

train_data[""Sex""] = map(lambda x : sex_map[x],train_data[""Sex""])
test_data[""Sex""] = map(lambda x : sex_map[x],test_data[""Sex""])

train_data[""Embarked""] = map(lambda x : embarked_map[x],train_data[""Embarked""])
test_data[""Embarked""] = map(lambda x : embarked_map[x],test_data[""Embarked""])

#Initializing Classifiers

rf = RandomForestClassifier(n_estimators=51)
ada = AdaBoostClassifier(n_estimators=51)
bag = BaggingClassifier(n_estimators=51)
gradboost = GradientBoostingClassifier()
classifier_list = [rf,ada,bag,gradboost]
classifier_names = [""Random Forests"",""AdaBoost"",""Bagging"",""Gradient Boost""] 

#Iterating over classifiers in order to find the performance metrics

for classifier,classifier_name in zip(classifier_list,classifier_names): ",Titanic.py,rupakc/Kaggle-Titanice-Machine-Learning-for-Disaster,1
"# print('Linear classifier: ', sgd)

# calculate roc and auc
# prediction_p = sgd.predict_proba(X_test)
# print(""Prediction Probabilities"")
# print(prediction_p[:,1])
# plot_data(y_test, prediction_p[:,1], ""Linear Classifier"")


# Random Forest
rf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1)
rf1.fit(X_train, y_train)
#print('Random Forest classifier: ', rf1)

# calculate predictions with probabilities
#prediction_p = rf1.predict_proba(X_test)
#plot_data(y_test, prediction_p[:,1], ""Random Forest"")

print('**** Results ****')
train_predictions = rf1.predict(X_test)",examples/leaf/leaf-febi.py,remigius42/code_camp_2017_machine_learning,1
"for (n_estimators, min_weight_fraction_leaf, max_features, criterion) in itertools.product([10, 50, 100, 500, 1000],
                                                                                           np.arange(0., 0.51, 0.05),
                                                                                           [0.1, 0.25, 0.5, 0.75, 'sqrt', 'log2', None],
                                                                                           ['gini', 'entropy']):
    features = input_data.drop('class', axis=1).values.astype(float)
    labels = input_data['class'].values

    try:
        # Create the pipeline for the model
        clf = make_pipeline(StandardScaler(),
                            RandomForestClassifier(n_estimators=n_estimators,
                                                   min_weight_fraction_leaf=min_weight_fraction_leaf,
                                                   max_features=max_features,
                                                   criterion=criterion,
                                                   random_state=324089))
        # 10-fold CV score for the pipeline
        cv_predictions = cross_val_predict(estimator=clf, X=features, y=labels, cv=10)
        accuracy = accuracy_score(labels, cv_predictions)
        macro_f1 = f1_score(labels, cv_predictions, average='macro')
        balanced_accuracy = balanced_accuracy_score(labels, cv_predictions)",model_code/RandomForestClassifier.py,rhiever/sklearn-benchmarks,1
"earth_classifier = Pipeline([('earth', Earth(max_degree=3, penalty=1.5)),
                             ('logistic', LogisticRegression())])

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""Naive Bayes"", ""LDA"", ""QDA"", ""Earth""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025, probability=True),
    SVC(gamma=2, C=1, probability=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    GaussianNB(),
    LDA(),
    QDA(),
    earth_classifier]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",examples/plot_classifier_comp.py,DucQuang1/py-earth,1
"from sklearn.cross_validation import train_test_split

from sklearn_evaluation import plot

data = datasets.make_classification(200, 10, 5, class_sep=0.65)
X = data[0]
y = data[1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

est = RandomForestClassifier()
est.fit(X_train, y_train)

plot.feature_importances(est, top_n=5)",examples/feature_importances.py,edublancas/sklearn-evaluation,1
"from sklearn.qda import QDA
from collections import OrderedDict

__author__ = 'Alex Rogozhnikov'


def test_feature_splitter(size=2000):
    X, y = commonutils.generate_sample(size, 10, distance=0.5)
    X['column0'] = numpy.clip(numpy.array(X['column0']).astype(numpy.int), -2, 2)
    trainX, testX, trainY, testY = commonutils.train_test_split(X, y)
    base_estimators = {'rf': RandomForestClassifier()}
    splitter = FeatureSplitter('column0', base_estimators=base_estimators, final_estimator=RandomForestClassifier())
    splitter.fit(trainX, trainY)

    print(splitter.score(testX, testY))
    print(RandomForestClassifier().fit(trainX, trainY).score(testX, testY))
    print(DumbSplitter('column0', base_estimator=RandomForestClassifier()).fit(trainX, trainY).score(testX, testY))
    chain = OrderedDict()
    chain['QDA'] = QDA()
    chain['LDA'] = LDA()",tests/test_expclassifiers.py,anaderi/lhcb_trigger_ml,1
"    estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                          axis=0, verbose=0)))
    estimators.append(('scaler', StandardScaler()))
    estimators.append(('mlp', KerasClassifier(build_fn=create_baseline,
                                              nb_epoch=125,
                                              batch_size=1024,
                                              verbose=0)))
    pipeline_nn = Pipeline(estimators)

    # Create model for RF
    clf = RandomForestClassifier(n_estimators=1200,
                                 max_depth=17,
                                 max_features=3,
                                 min_samples_split=2,
                                 min_samples_leaf=3,
                                 class_weight='balanced_subsample',
                                 verbose=0, random_state=1, n_jobs=4)
    estimators = list()
    estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                          axis=0, verbose=2)))",ml4vs/stacking.py,ipashchenko/ml4vs,1
"  threshold = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, round(cm[i, j], 2), horizontalalignment=""center"", color=""white"" if cm[i, j] > threshold else ""black"")
  plt.xticks(np.arange(2), [0, 1])
  plt.yticks(np.arange(2), [0, 1])
  plt.show()
  # plt.savefig('figures/rf_test_cm.png')

# build RF model
def predict_rf(train_features, test_features, train_labels, test_labels):
  model = RandomForestClassifier(n_estimators=1000)
  model.fit(train_features, train_labels)
  predictions = model.predict(train_features)
  print get_accuracy(predictions, train_labels)
  predictions = model.predict(test_features)
  print get_accuracy(predictions, test_labels)
  
  # create confusion matrix of prediction results
  # cm = confusion_matrix(test_labels, predictions)
  # plot_confusion_matrix(cm, title='Normalized Confusion Matrix (Test Data)')",clean/classification.py,BIDS-collaborative/EDAM,1
"- k_fold         : number of folds to be use in k-fold
'''
def classify_RandomForests(XTrain, yTrain, XTest, yTest, 
    k_fold, out_prefix, verbose = False):
  print("":::Starting Random Forest Classifier:::"")
  # Dict witht he SVM parameters
  parameters = [{'criterion': ['gini', 'entropy'],
      'n_estimators': [10,25,50,75,100,150,200]}]

  # Run the hypertuning Parameter and k-fold for
  rf = RandomForestClassifier(n_jobs = -1)
  clf = GridSearchCV(rf, parameters, cv = k_fold, scoring = 'accuracy')
  clf.fit(XTrain, yTrain)

  # Evaluate the selected model on the validation dataset
  evaluate_gridsearch(clf, XTest, yTest, ""Random Forest"", out_prefix, verbose)

'''
@Description
  - Method to perform the audio-feature classification using Neural Network.",musicgenre/audio_features/classifiers_comparison.py,Hguimaraes/torch-musicgenre,1
"    ]
    polarities = [
        [""Negative"", 0],
        [""Positive"", 1]
    ]
    crossed = [
        [""Negative -> Positive"", 0],
        [""Positive -> Negative"", 1],
    ]
    path = ""inf_spec_ner_liwc_speciteller.csv""
    rf = RandomForestClassifier(n_estimators=200, criterion='entropy')
    svc = LinearSVC(penalty=""l1"", dual=False, tol=1e-3)
    lr = LogisticRegression()
    for feature_set in feature_sets:
        print(feature_set[0])
        for polarity in polarities:
            print(polarity[0])
            X, C = get_data(path, feature_set[1], polarity[1])
            # run k-fold cv
            print(""--------- RF --------"")",ml/mlapp.py,ben-aaron188/information_specificity,1
"    train_data = train_df.values
    test_data = test_df.values
    
    X_train = train_data[:,1:]
    y_train = train_data[:,0]
    
    X_test = test_data[:,1:]
    idx = test_data[:,0].astype(np.int32)
    
    #random forest classifier
    rfc = RandomForestClassifier(n_estimators=100)  
    rfc.fit(X_train, y_train)
    score_rfc = rfc.score(X_train, y_train)
    out_rfc = rfc.predict(X_test)
    print ""random forest classifier score: %f"" %score_rfc
    
    #logistic regression
    logreg = LogisticRegression()
    logreg.fit(X_train, y_train)
    score_logreg = logreg.score(X_train, y_train)",titanic/titanic_kernel.py,vsmolyakov/kaggle,1
"    
    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    sklearn_model = RandomForestClassifier()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9",deepchem/models/tests/test_overfit.py,rbharath/deepchem,1
"    clean_test_reviews.append( review_to_wordlist( review, \
        remove_stopwords=True ))

testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )


# ****** Fit a random forest to the training set, then make predictions
#
# Fit a random forest to the training data, using 100 trees
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier( n_estimators = 100 )

print ""Fitting a random forest to labeled training data...""
forest = forest.fit( trainDataVecs, train[""sentiment""] )

# Test & extract results 
result = forest.predict( testDataVecs )

# Write the test results 
output = pd.DataFrame( data={""id"":test[""id""], ""sentiment"":result} )",TutorialCode_Final/Word2Vec_AverageVectors.py,angelachapman/Kaggle-DeepLearning-Tutorial,1
"plt.scatter(grade_slow, bumpy_slow, color = ""r"", label=""slow"")
plt.legend()
plt.xlabel(""bumpiness"")
plt.ylabel(""grade"")
#plt.show()
#################################################################################

### your code here!  name your classifier object clf if you want the
### visualization code (prettyPicture) to show you the decision boundary
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=10)
t0 = time()
clf.fit(features_train, labels_train)
print ""training time:"", round(time()-t0, 3), ""s""

t0 = time()
pred = clf.predict(features_test)
print ""precition time:"", round(time()-t0, 3), ""s""

from sklearn.metrics import accuracy_score",random_forest/hemant_randForest.py,CoderHam/Machine_Learning_Projects,1
"		Xdev = model.transform(X_dev)
		Xeval = model.transform(X_eval)
		
		return model, Xdev, Xeval
		
		
	# Make sure to perform feature selection on X before using this method
	def grid_optimization(self, X, Y):
		
		# Initialize a random forest classifier
		rf = RandomForestClassifier()
		
		# Define a parameter grid to search over
		param_dist = {""n_estimators"": range(50, 550, 50), 
		              ""max_depth"": range(3, 17, 2), 
		              ""criterion"": ['gini', 'entropy']}	
																
		# Setup 10-fold stratified cross validation
		cross_validation = StratifiedKFold(Y, n_folds=10)
		",Lepton-Number-Violation-at-100-TeV/Data_Analysis/MLpipeline.py,pwinslow/Machine-Learning-Projects,1
"dischaps=dischaps.T
validhaps=validhaps.T
dischaps=np.array(dischaps)
validhaps=np.array(validhaps)

label=pd.read_csv(""discovery.double.label"",header=None)
label=np.array(label)

#c refers to PH. Each haplotype is treated as an observation and then the evidence is combined
#to create a new variable.
rfc = RandomForestClassifier(n_estimators=500, max_features=""auto"",min_samples_leaf=min_samples_leaf,oob_score=True)
rfc.fit(dischaps,np.ravel(label))
predc=rfc.oob_decision_function_
predc=predc[:,1]
predc=map(evi,predc)
predc=np.array([predc[i] for i in range(0,len(predc),2)]) +np.array([predc[i] for i  in range(1,len(predc),2)])
predc=pd.DataFrame(predc)
predc.to_csv(""c.disc.bloc""+str(blocn),na_rep='NA',sep="" "",line_terminator="" "",header=False,index=False)

validc=rfc.predict_proba(validhaps)",randfor.py,FelBalazard/Prediction-with-Haplotypes,1
"print metrics.classification_report(y_,y_rf)
print metrics.accuracy_score(y_,y_rf)

# print sgd.score( train_X, train_y )
# print ( lb.inverse_transform( sgd.predict( train_X ) ) <> lb.inverse_transform( train_y ) ).sum( )

# Split the training set into training and validation sets
# X,X_,y,y_ = cross_validation.train_test_split(train_X,train_y,test_size=0.2)

# Train and predict with the random forest classifier
rf = ensemble.RandomForestClassifier()
y = lb.inverse_transform( y )
rf.fit(X,y)
y_rf = rf.predict(X_)
print metrics.classification_report(y_,y_rf)
print metrics.accuracy_score(y_,y_rf)

# Retrain with entire training set and predict test set.
# rf.fit(train_X,train_y)
# y_test_rf = rf.predict(test_X)",ForestCoverType/forestcovertype.py,RomainBrault/OVFM,1
"
#and finally. make it into a numpy array
training_set = np.array(training_set)




########################
#   Instantiate RFCs   #
########################
RFC_SA = RandomForestClassifier(max_depth=RFC_depth, n_estimators=RFC_estimators)
RFC_LB = RandomForestClassifier(max_depth=RFC_depth, n_estimators=RFC_estimators)



########################
#      Train RFCs      #
########################
#training one RFC on SA
RFC_SA.fit(training_set[0::,3:9], training_set[0::,0])",RFC-RH_Script_1.py,buckinha/gravity,1
"
    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
    dataset = dc.data.NumpyDataset(X, y, w, ids)

    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)
    sklearn_model = RandomForestClassifier()
    model = dc.models.SklearnModel(sklearn_model)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Eval model on train
    scores = model.evaluate(dataset, [classification_metric])
    assert scores[classification_metric.name] > .9",deepchem/models/tests/test_overfit.py,joegomes/deepchem,1
"
def createKNN():
    clf = KNeighborsClassifier(n_neighbors=13,algorithm='kd_tree',weights='uniform',p=1)
    return clf

def createDecisionTree():
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createRandomForest():
    clf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createExtraTree():
    clf = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createAdaBoost():
    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    clf = AdaBoostClassifier(dt, n_estimators=300)",libs/classifiers.py,KellyChan/Kaggle,1
"
__author__ = 'Emanuele Tamponi'


# CLASSIFIER_NAMES = [""ab"", ""gb"", ""ba"", ""rf"", ""et""]
CLASSIFIER_NAMES = [""ab"", ""ba"", ""rf""]
CLASSIFIERS = {
    ""ab"": AdaBoostClassifier(n_estimators=100),
    # ""gb"": GradientBoostingClassifier(n_estimators=100),
    ""ba"": BaggingClassifier(n_estimators=100),
    ""rf"": RandomForestClassifier(n_estimators=100),
    # ""et"": ExtraTreesClassifier(n_estimators=100)
}
N_FOLDS = 5

PROBE_NAMES = [""imb"", ""lin""]
PROBES = {
    ""imb"": Imbalance(),
    ""lin"": LinearBoundary(),
    # ""lda"": LDAProbe()",mrca/evaluation/__init__.py,etamponi/mrca,1
"    print(""The best classifier is: "", grid.best_estimator_)
    
    res = np.transpose(grid.predict_proba(X_test))[1]
    auc = sklearn.metrics.roc_auc_score(y_test, res)
    print ""svmrbf AUC: "", auc
    

def myRF_AUC(data, target):
    X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, target, test_size=0.4, random_state=42)
    cv = StratifiedKFold(y=y_train, n_folds=3)
    grid = GridSearchCV(RandomForestClassifier(min_samples_split=1, bootstrap=False), param_grid=dict(n_estimators=np.array([1500,3000,5000])), cv=cv)    
    
    grid.fit(X_train, y_train)
    print(""The best classifier is: "", grid.best_estimator_)
    
    res = np.transpose(grid.predict_proba(X_test))[1]
    auc = sklearn.metrics.roc_auc_score(y_test, res)
    print ""rforest AUC: "", auc
    
    '''",sklearnClassifiers/simpleClassifier2.py,rampasek/seizure-prediction,1
"                except TypeError:
                    failed = True
                assert failed

                # type error for non-h2o estimators
                failed = False
                try:
                    _ = H2OPipeline([
                        ('nzv', H2ONearZeroVarianceFilterer()),
                        ('mc', H2OMulticollinearityFilterer(threshold=0.9)),
                        ('est', RandomForestClassifier())
                    ],
                        feature_names=F.columns.tolist(),
                        target_feature='species'
                    )

                    # won't even get here...
                    # pipe.fit(train)
                except TypeError:
                    failed = True",skutil/h2o/tests/test_h2o.py,tgsmith61591/skutil,1
"    # Numpy arrays are easy to work with, so convert the result to an
    # array
    train_data_features = train_data_features.toarray()

    # ******* Train a random forest using the bag of words
    #
    print ""Training the random forest (this may take a while)...""


    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators = 200)

    # Fit the forest to the training set, using the bag of words as
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit( train_data_features, train[""tema_id""] )


",BagOfWords.py,D4D-Mexico/sac,1
"    label0 = [0 for k in range(len(pos))]
    label1 = [1 for k in range(len(neg))]
    samples = np.array(pos + neg)
    labels = np.array(label0 + label1)
    paths = pathpos + pathneg
    imgs = imgspos + imgsneg
    com_num = np.minimum(300, samples.shape[0] - 10)
    clf = PCA(com_num)
    samples = clf.fit_transform(samples)
    print 'after pca : ', samples.shape
    clf = RandomForestClassifier()
    clf.fit(samples,labels)
    cnf = clf.predict_proba(samples)[:,0]
    X = []

    for k in range(len(paths)):
        X.append((paths[k], cnf[k], imgs[k]))
    X = sorted(X, key = lambda a : a[1], reverse=True)
    line = """"
    lineA = """" #sometimes, the positive set is split into two parts",class_help/a_rf.py,z01nl1o02/tests,1
"pred = clf.predict(U_test)
metrics.f1_score(newsgroups_test.target, pred, average='macro') #0.054655141360509529

# ------------------------------------------------------------------------------
# truncated SVD library (Works as Latent semantic analysis)
from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=1000, n_iter=7, random_state=42)
svd_transformed = svd.fit_transform(vectors) # from (11314, 101631) to (11314, 1000)
svd_transformed_test = svd.fit_transform(vectors_test)

clf = RandomForestClassifier(n_estimators=100)
clf.fit(svd_transformed, newsgroups_train.target)
pred = clf.predict(svd_transformed_test)
metrics.f1_score(newsgroups_test.target, pred, average='macro') #0.


# ------------------------------------------------------------------------------
# Random Forest
clf = RandomForestClassifier(n_estimators=100)
clf.fit(vectors, newsgroups_train.target)",reuters_documents_classification.py,Saftophobia/word2vec_reuters_classification,1
"print(""Features for trainig: successfully"")

# Assign data for validation
amount = int(0.8*len(train))
validation = train[amount:]
train = train[:amount]
print(""Assign data for validation: successfully"")

# Classifier
# clf = tree.DecisionTreeClassifier()
clf = RandomForestClassifier(n_estimators = 700, n_jobs = -1)
print(""Classifier: successfully"")

# Traning
clf.fit(train[column_labels], train[""status_group""])
print(""Traning: successfully"")

# Accuracy
accuracy = accuracy_score(clf.predict(validation[column_labels]), validation[""status_group""])
print(""Accuracy = "" + str(accuracy))",ml_v2.py,BhagyeshVikani/Pump-it-Up-Data-Mining-the-Water-Table,1
"
    Examples
    --------
    >>> grab = sklearn.datasets.fetch_20newsgroups
    >>> D, Di = grab(subset='train'), grab(subset='test')

    >>> vec = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)
    >>> X = vec.fit_transform(D.data)
    >>> Xi = vec.transform(Di.data)

    >>> rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)
    >>> rf.fit(X, D.target)

    >>> import omesa.tools.lime_eval as le
    >>> le.LimeEval(rf, vec, class_names=list(set(Di.target)))
    >>> exps = le.explain(Di[:5])
    >>> graph_to_file(exps, '/some/file/wherever')

    Notes
    -----",omesa/tools/lime_eval.py,cmry/omesa,1
"from reverse_game_of_life import *
from sklearn.ensemble import RandomForestClassifier
from sys import argv


example_size = int(argv[1])
examples = create_examples(example_size)
rf_params = {'max_depth':[4,8,12,16,20],'max_features':[4,8,12,16,20,24,28,32,36]}

lc_rf10_w3_1k = LocalClassifier(window_size=3,off_board_value=-1,clf=RandomForestClassifier(n_estimators=10,n_jobs=-1))
lc_rf10_w3_1k.tune_and_train(examples[0:int(example_size/100)],rf_params,use_transformations=True,verbosity=1)
lc_rf10_w3_1k.test(examples[int(example_size/2):],detailed_output=True)",quick_setup.py,valisc/reverse-game-of-life,1
"random.shuffle(test_data_list)

for item in train_data_list:
	train_data.append([item[0], item[1]])
	train_data_label.append(d[item[2]])
for item in test_data_list:
	test_data.append([item[0], item[1]])
	test_data_label.append(d[item[2]])


regr = AdaBoostRegressor(RandomForestClassifier(),n_estimators=100)
regr.fit(train_data,train_data_label)
print ""fitting is done""
y_predict = regr.predict(test_data)
print y_predict
correct = 0
for a,b in zip(y_predict, test_data_label):
	if a == b:
		correct = correct+1
",boosted_randomF.py,siddharthhparikh/INFM750-project,1
"dataset[ 'target' ] = np.array( target_parse['target'] )
dataset[ 'target_names' ] = np.array( target_parse['names'] )



###########
#random forest classification
###########

#setup inital params
clf = RandomForestClassifier( n_estimators=500 )
#run RFC on dataset with target classifiers; runs the model fit
clf = clf.fit( dataset['data'], dataset['target'] )


######
#run some sanity checks here. 
######
training_stats = clf.feature_importances_ #array of variable importances for model.
",utils/classify_WHAM_vcf.py,zeeev/wham,1
"test_index  = index[N:]

X = df.ix[:, 1:9]
y = df[10]

imp = Imputer(strategy='mean', axis=0)
X = imp.fit_transform(X)

#print 'Training..'
#clf = SVC()
clf = RandomForestClassifier()
clf.fit(X[train_index, :], y[train_index])

print 'Test'
#print clf.predict(X[test_index, :])
#print y[test_index]",src/python-lesson/glass.py,minhouse/python-lesson,1
"svm_model.fit(training_set, training_labels)

svm_test_predict = svm_model.predict(test_set)
svm_rw_predict = svm_model.predict(rw_set)

print(""\nSVM\n"")
print(""Test set confusion matrix:\n%s"" % metrics.confusion_matrix(test_labels, svm_test_predict))
print(""Real world confusion matrix:\n%s"" % metrics.confusion_matrix(rw_labels, svm_rw_predict))

# Random forest
forest_model = RandomForestClassifier(n_estimators=100)
forest_model.fit(training_set, training_labels)

rf_test_predict = forest_model.predict(test_set)
rf_rw_predict = forest_model.predict(rw_set)

print(""\nRandom forest\n"")
print(""Test set confusion matrix:\n%s"" % metrics.confusion_matrix(test_labels, rf_test_predict))
print(""Real world confusion matrix:\n%s"" % metrics.confusion_matrix(rw_labels, rf_rw_predict))
",Exercises/06_wheres_wally_traditional.py,peterwittek/qml-rg,1
"  trainingSet = newFeatures[r, :]; trainLabels = labels[r]
  testingSet = newFeatures[r2, :]; testLabels = labels[r2]
      
  if not equalClassSize:
    testingSet, testLabels = balanceClasses(testingSet, testLabels)
    clf = LogisticRegression(C=C, class_weight='auto', intercept_scaling=B, penalty='l2')
#     clf = svm.SVC(C=C, kernel='rbf', class_weight='auto', probability=returnProb)
  else:
    clf = LogisticRegression(C=C, intercept_scaling=B, penalty='l2')
#     clf = svm.SVC(C=C, kernel='rbf', class_weight='auto', probability=returnProb)
#     clf = RandomForestClassifier()
#     clf = KNeighborsClassifier(n_neighbors=15)
#   print np.arange(20)[clf2.get_support()]
#     clf = AdaBoostClassifier()
#   clf = GradientBoostingClassifier(init=LogisticRegression)
#     clf = GaussianNB()
#   clf = DecisionTreeClassifier()
  
  if featureSelect:
    rfecv = RFE(estimator=clf, step=1,  n_features_to_select=8)",classify_old.py,jasonzliang/kick_classifier,1
"            if k == 'normal-1':
                X_ref = X
                y_ref = y

            scaler = pre.StandardScaler()
            # scaled = scaler.fit_transform(X)
            # X = pd.DataFrame(scaled, columns=X.columns)

            # ref
            clf = LogisticRegression(C=1,penalty='l1', class_weight={0:1,1:8})
            # clf = RandomForestClassifier(n_estimators=10, max_features=.5)
            scores = sl.cross_validation.cross_val_score(clf, X, y, scoring='roc_auc')
            mscores = np.mean(scores)
            print(k + ': xval scores=' + str(scores) + ' | mean=' + str(mscores))
            if mscores > best_auc:
                best_auc = mscores
            clf.fit(X,y)
            z = clf.predict_proba(X)
            probs = []
            for r in z:",cnct_graph2_analyze.py,ecodan/kaggle-connectomix,1
"        self.pca = PCA(n_components=n_components, svd_solver='randomized',
          whiten=True).fit(self.X)

    def pca_transform(self,X_input):
        X_input_pca = self.pca.transform(X_input)
        return X_input_pca

    def classifier_fit(self):
        print(""Fitting the classifier to the training set."")
        X_pca = self.pca_transform(self.X)
        self.clf = RandomForestClassifier(n_estimators=64,max_depth=None,min_samples_split=16, random_state=0)
        self.clf = self.clf.fit(X_pca,self.Y)
        
    def classifier_predict(self,X_test):
        X_test_pca=self.pca_transform(X_test)
        y_pred = self.clf.predict(X_test_pca)
        return y_pred

    def cross_validation(self,k_fold):
        X_pca=self.pca_transform(self.X)",auto.py,jhneo/animalClassificationPca,1
"
    features_train = ll.transform(features_train)
    features_test = ll.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))
",nytimes/step4_analysis_supervised_4(LocallyLinearEmbedding).py,dikien/Machine-Learning-Newspaper,1
"	ct_train_data = TfidfVectorizer()
	ct_train_labels = CountVectorizer(tokenizer=lambda t: t.split(""|""))
	X = ct_train_data.fit_transform(open(pre.data_file, 'r'))
	print ""Train passages vectorized""
	Y = ct_train_labels.fit_transform(open(pre.label_file, 'r'))	
	Y = Y.todense()
	print ""Train labels vectorized""
	if not os.path.exists(randomforest_model_file):
		# svc = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, 
		# 	multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=100)
		# rfModel = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, n_jobs=-1))
		rfModel = OneVsRestClassifier(RandomForestClassifier(n_estimators=25, n_jobs=4, verbose=1, max_depth=5), n_jobs=2)
		# rfModel = OneVsRestClassifier(LogisticRegression())
		# rfModel = OneVsRestClassifier(LinearSVC())
		# rfModel = KNeighborsClassifier(n_neighbors=10, n_jobs=4)
		# rfModel = MLPClassifier(hidden_layer_sizes=(100,), batch_size=100 ,activation='logistic', max_iter=25, alpha=1e-4, solver='sgd', verbose=True, \
		# tol=0.0001, random_state=1, learning_rate_init=.1)
		rfModel.fit(X[:split], Y[:split])
		# pickle.dump(rfModel, open(randomforest_model_file, ""wb""))
	else:",Task 1/part1.py,vipmunot/Yelp-Dataset-Challenge,1
"            #save_object(rf, r'train_Matrix.pkl')
            save_object(rf, self.outfile)
            elapsed = timeit.default_timer() - start_time
            print ""ELAPSED="", elapsed

        count_vectors=True
        if count_vectors:
            self.obtain_trainig_set_size()

    def do_training(self):
        rf = RandomForestClassifier(n_estimators=self.Nestimators, oob_score=True)
        rf.fit(self.train_signals, self.train_vals)

        return rf


    def obtain_trainig_set_size(self):
        print np.count_nonzero(train_vals)
        sbar=set(np.ravel(train_vals).astype(int))
",rfVextract/VsRFtrain03.py,dnolivieri/RFVextract,1
"for x in range(TRAIN_SIZE):
	data_T = np.reshape(train_data[x], [-1, 1])
	fake_train_data[x] = pca.transform(data_T)
for x in range(EVAL_SIZE):
	data_T = np.reshape(eval_data[x], [-1, 1])
	fake_eval_data[x] = pca.transform(data_T)

train_data = fake_train_data
eval_data = fake_eval_data

clf = RandomForestClassifier(n_estimators=250)
clf.fit(train_data, train_label)

print (train_data.shape)
print (eval_data.shape)
print (eval_label)

train_predict = clf.predict(train_data)
eval_predict = clf.predict(eval_data)
eval_result = np.sum(eval_predict == eval_label) / float(eval_label.shape[0])",work/ML/tensorflow/separa/pca_rf.py,ElvisLouis/code,1
"    svm=grid_search.GridSearchCV(svr, parameters, cv=kf)
    svm=grid_search.GridSearchCV(svr, parameters)
    svm.fit(x,y)
    print svm.best_score_
    print svm.best_params_
    # ~65%

def ran_forest_model(x,y):
    # Random Forest Modeling
    parameters={'n_estimators':[5,10,20,30,40,50], 'criterion':['gini', 'entropy']}
    rfm=RandomForestClassifier()
    rfm_g=grid_search.GridSearchCV(rfm, parameters, cv=kf)
    rfm_g.fit(x,y)
    print rfm_g.best_score_
    print rfm_g.best_params_
    # ~65%


if __name__ == '__main__':
    # Read in the data",Classification/Winner_Model.py,mprego/NBA,1
"import sys
import numpy as np

X1=np.array(pickle.load(open('X2g_train.p')))
X2=np.array(pickle.load(open('X3g_train.p')))
X3=np.array(pickle.load(open('X4g_train.p')))
X4=np.array(pickle.load(open('Xhead_train.p')))

X=np.hstack((X2,X1,X3,X4))
y=np.array(pickle.load(open('y.p')))
rf=RandomForestClassifier(n_estimators=200)
Xr=rf.fit_transform(X,y)
pickle.dump(Xr,open('X33_train_reproduce.p','w'))
print Xr.shape
del X,X1,X2,X3,X4,Xr

X1=np.array(pickle.load(open('X2g_test.p')))
X2=np.array(pickle.load(open('X3g_test.p')))
X3=np.array(pickle.load(open('X4g_test.p')))
X4=np.array(pickle.load(open('Xhead_test.p')))",microsoft malware/Malware_Say_No_To_Overfitting/kaggle_Microsoft_malware_full/getfea.py,bikash/kaggleCompetition,1
"# print(""Best estimator found by grid search:"")
# print(clf.best_estimator_)




#==============================================================================
# RandomForest (Classifier Variant) Model Fitting parameters
#==============================================================================

estimator = Pipeline([(""forest"", RandomForestClassifier(random_state=0, n_estimators=100))])
estimator.fit(predictors, outcomes)

predicted = estimator.predict(predictors)
prediction_scores  = accuracy_score(outcomes, predicted) #This code will change, fi we cross validate

# print (type(predicted[2]))
# print (predicted[2])
lst_predicted = []
for i in predicted:",Visual_Game/CreatingBaseDF.py,georgetown-analytics/nba-tracking,1
"
# Parameters
NEST = 100      # Number of estimators
CRIT = ""entropy""

# Import Data
X, Y, x_test, test_id = get_data()
x_train, y_train, x_valid, y_valid = train_test_split(X,Y,test_size=0.333)

# Make our model
model = RandomForestClassifier(n_estimators=NEST, criterion=CRIT)
model.fit(x_train,y_train)

# Make metrics on ze data
y_preds = model.predict(x_valid)
cr = classification_report(y_valid,y_preds)
cm = confusion_matrix(y_valid,y_preds)
ac = accuracy_score(y_valid,y_preds)
print(cr); print(cm); print(""Accuracy Score: %f"" % ac);
",rf_run_1.py,valexandersaulys/prudential_insurance_kaggle,1
"                           'penalty': ['l1', 'l2']}}

    if 'Decision Tree' not in exclude:
        classifiers['Decision Tree'] = {
            'clf': DecisionTreeClassifier(max_depth=None,
                                          max_features='auto'),
            'parameters': {}}

    if 'Random Forest' not in exclude:
        classifiers['Random Forest'] = {
            'clf': RandomForestClassifier(max_depth=None,
                                          n_estimators=10,
                                          max_features='auto'),
            'parameters': {'n_estimators': list(range(5, 20))}}

    if 'Logistic Regression' not in exclude:
        classifiers['Logistic Regression'] = {
            'clf': LogisticRegression(fit_intercept=True, solver='lbfgs',
                                      penalty='l2'),
            'parameters': {'C': [0.001, 0.1, 1]}}",polyssifier/poly_utils.py,alvarouc/polyssifier,1
"
    data.append(features)
    target.append(imagePath.split(""_"")[-2])

targetNames = np.unique(target)
le = LabelEncoder()
target = le.fit_transform(target)

(trainData, testData, trainTarget, testTarget) = train_test_split(data, target, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=25, random_state=84)
model.fit(trainData, trainTarget)
print(classification_report(testTarget, model.predict(testData), target_names=targetNames))

for i in np.random.choice(np.arange(0, len(imagePaths)), 10):
    imagePath = imagePaths[i]
    maskPath = maskPaths[i]

    image = cv2.imread(imagePath)
    mask = cv2.imread(maskPath)",hoshiwomiru/030_classify.py,guybrush007/hoshiwomiru,1
"            np.hstack(training_label).astype(int), [0, 255]))
        print 'Create the training set ...'

        # Learn the PCA projection
        pca = PCA(n_components=sp, whiten=True)
        training_data = pca.fit_transform(training_data)
        testing_data = pca.transform(testing_data)

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_sp.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/selection-extraction/pca/pipeline_classifier_dce.py,I2Cvb/mp-mri-prostate,1
"				# 'n_iter':30
			# }),
			# ('Extra Trees', ExtraTreesClassifier(random_state=0), {
				# 'param_dist':dict(
					# n_estimators=[50, 100] + range(200, 1001, 200),
					# max_features=np.linspace(0.5, 1, 6).tolist()+['sqrt', 'log2'],
					# min_samples_leaf=[1]+range(10, 101, 10),
					# class_weight=['balanced', None]),
				# 'n_iter':30
			# }),
			('Random Forest', RandomForestClassifier(random_state=0), {
				'param_dist':dict(
					n_estimators=[50, 100] + range(200, 1001, 200),
					max_features=np.linspace(0.5, 1, 6).tolist()+['sqrt', 'log2'],
					max_depth=[None] + range(10,101,10),
					min_samples_leaf=[1]+range(10, 101, 10),
					class_weight=['balanced', None]),
				'n_iter':30
			}),
			# ('Bagging LinearSVC', BaggingClassifier(base_estimator=build_model(LinearSVC, 'Classifier', 'LinearSVC', tuned=opts.best, pr=pr, mltl=opts.mltl, loss='squared_hinge', dual=False), random_state=0), {",bin/chm_annot.py,cskyan/chmannot,1
"counter = 0.
for review in clean_train_reviews:
    if (counter % 5000) == 0:
        print ""review %d of %d"" % (counter, len(clean_train_reviews))
    train_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )
    counter = counter+1


# Fit a simple classifier such as logreg or RF 
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 100)
print ""Fitting a random forest to labeled training data...""
forest = forest.fit(train_centroids,train[""sentiment""])


# Convert test reviews to bags of centroids
test_centroids = np.zeros((test[""review""].size, num_clusters), dtype=""float32"")
counter = 0.
for review in clean_test_reviews:
    if (counter % 5000) == 0:",BagOfCentroids.py,angelachapman/Kaggle-DeepLearning-Tutorial,1
"    except:
        # load all feature matrices, AttributeAnalyzer, identify seeds
        return
        sys.argv = ['embed', path]
        (context_features, attr_features_by_type) = embed.main()  # use sim, delta, embedding, etc. from params.py file
        assert ((context_features is not None) and (len(attr_features_by_type) == 4))
        other_attr_types = [at for at in gplus_attr_types if (at != attr_type)]
        n = context_features.shape[0]

        # construct classifiers
        clf_dict = {'logreg' : LogisticRegression(), 'naive_bayes' : GaussianNB(), 'randfor' : RandomForestClassifier(n_estimators = pm.num_trees), 'boost' : AdaBoostClassifier(n_estimators = pm.num_trees), 'kde' : TwoClassKDE()}
        prec_df = pd.DataFrame()  # for storing mean & stdev topN_save precisions for each parameter combo

        # run nomination
        for embedding_info in embedding_info_vals:
            for sphere_content in sphere_content_vals:
                print(""\nembedding_info = %s, sphere_content = %s"" % (embedding_info, str(sphere_content)))
                # stack all desired feature matrices, with or without projecting to sphere
                embedding_mats = []
                if (embedding_info != 'NPMIs'):",test_gplus.py,jeremander/AttrVN,1
"    dataframe = pd.read_csv(INPUT_FILE, delimiter='|', header=None)
    dataset = dataframe.values

    X = dataset[:, 1:].astype(float)
    y = dataset[:, 0]
    # X = StandardScaler().fit_transform(X)     # makes no difference
    with open(FEATURES_FILE, 'r') as ifile:
        names = [line for line in ifile]

    # Build a forest and compute the feature importances
    forest = RandomForestClassifier(n_estimators=25,
                                    random_state=0, verbose=True)

    forest.fit(X, y)
    importance_list = forest.feature_importances_
    std = np.std([tree.feature_importances_ for tree in forest.estimators_],
                 axis=0)
    indices = np.argsort(importance_list)[::-1]
    with open(FEATURES_FILE, 'r') as f:
        feature_names = [line.strip() for line in f.readlines()]",src/ml/feature_importance_impurity.py,seokjunbing/cs75,1
"                              'n_wiki_3': n_wiki_3[0]
                              })
    summary_pd = summary_pd.fillna(0)
    return summary_pd


# random forest
def random_forest(x_train, y_train):
    x_train_f, x_train_c, y_train_f, y_train_c = train_test_split(x_train, y_train, test_size = 0.1)
    for k in range(5, 30):
        clf = RandomForestClassifier(n_estimators = 200, max_features = k)
        cv = cross_validation.KFold(len(x_train_f), n_folds = 5, shuffle = True)
        results = []
        for traincv, testcv in cv:
            probas = clf.fit(x_train_f[traincv], y_train_f[traincv]).predict_proba(x_train_f[testcv])
            fpr, tpr, thresholds = metrics.roc_curve(y_train_f[testcv], probas[:,1])
            results.append(metrics.auc(fpr, tpr))
        print ""Results: "" + str(k) + "" "" + str(np.array(results).mean())
    
    k = 11",kddcup15.py,ctozlm/KDDCUP15,1
"from ..what_sklearn import whatamise_sklearn, _check_all_monkeypatched

whatamise_sklearn(check=True, log=True)


def test_monkeypatch():
    assert _check_all_monkeypatched()


def test_non_ids():
    rfc = RandomForestClassifier()
    assert 'n_jobs' not in rfc.what().id()
    assert 'n_jobs' in str(rfc.what())


def test_pipeline():
    norm = Normalizer(norm='l1')
    norm_id = norm.what().id()
    assert norm_id == ""Normalizer(norm='l1')""
    kmeans = KMeans(n_clusters=12)",whatami/wrappers/tests/test_what_sklearn.py,sdvillal/whatami,1
"from sklearn.linear_model import LogisticRegression

# Split the global matrix ""result"" into a training and a testing set
df = pd.read_csv('../matrix.csv', sep=',')
df = df.fillna(0)
train, test = fitter.split(df,0.5)
variables = ['MatchDico', 'TextBlob', 'delta_sale', 'delta_at']
target = ['Actual']

# MODEL 1 - Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100)
print('RANDOM FOREST CLASSIFIER')
fitter.fit_model(train, test, target, variables, rf)

# MODEL 2 - Gradient Boosting Classifier
gb = GradientBoostingClassifier(n_estimators=100)
print
print('GRADIENT BOOSTING CLASSIFIER')
fitter.fit_model(train, test, target, variables, gb)
",FinancialAnalystV3/4.fitModels/fitModels.py,CedricVallee/pythonFinancialAnalyst,1
"    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


results = []
for clf, name in (
        (RidgeClassifier(tol=1e-2, solver=""lsqr""), ""Ridge Classifier""),
        (Perceptron(n_iter=50), ""Perceptron""),
        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        (KNeighborsClassifier(n_neighbors=10), ""kNN""),
        (RandomForestClassifier(n_estimators=100), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,",scripts/document_classification_20newsgroups.py,giacbrd/ShallowLearn,1
"                           converters={0:lambda s: ord(s.split(""\"""")[1])})
    trainDataResponse = trainData[:,1]
    trainDataFeatures = trainData[:,0]

    # Train H2O GBM Model:
    #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")
    rf_h2o = h2o.random_forest(x=alphabet[['X']], y=alphabet[""y""], ntrees=1, max_depth=1, nbins=100)

    # Train scikit GBM Model:
    # Log.info(""scikit GBM with same parameters:"")
    rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
    rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

    # h2o
    rf_perf = rf_h2o.model_performance(alphabet)
    auc_h2o = rf_perf.auc()

    # scikit
    auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_smallcatRF.py,slchen2014/h2o-3,1
"
    features, labels, vectorizer, selector, le, features_data = preprocess(""pkl/article_2_people.pkl"", ""pkl/lable_2_people.pkl"", k)
    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=42)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))
",nytimes/step4_analysis_supervised_1.py,dikien/Machine-Learning-Newspaper,1
"
#################################################################
# Initialize a Random Forest classifier with 100 trees          #
# Fit the forest to the training set, using the bag of words as #
# features and the sentiment labels as the response variable    #
# This may take a few minutes to run                            #
#################################################################

print ""Training the random forest wait it will take time to train ...""

forest = RandomForestClassifier( n_estimators = 100, n_jobs = -1, verbose = 1 )

forest = forest.fit( train_data_features, train[""sentiment""] )

##############################################################
# Use the random forest to make sentiment label predictions  #
##############################################################

print ""Predicting test labels...\n""
rf_p = forest.predict_proba( test_data_features )",MovieReviewSentimentAnalysis/MovieReveiw/RandomForest.py,anilcs13m/Projects,1
"def plot_BestKFeatures (X_train, y_train):
    '''
    http://nbviewer.ipython.org/github/gmonce/scikit-learn-book/blob/master/Chapter%204%20-%20Advanced%20Features%20-%20Feature%20Engineering%20and%20Selection.ipynb
    Find the best percentile of features to use,
    using cross-validation on the training set and get K best feats
    '''
    from sklearn import cross_validation
    from sklearn import feature_selection
    from sklearn import tree
    dt = tree.DecisionTreeClassifier(criterion='entropy')
    dt = RandomForestClassifier(n_jobs=2, bootstrap=True, n_estimators=250, criterion='gini')
    dt = dt.fit(X_train, y_train)

    percentiles = range(1, 95, 5)
    results = []
    for i in range(1, 95, 5):
        fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=i) #Original
        fs = feature_selection.SelectPercentile(feature_selection.f_classif, percentile=i) # alt
        X_train_fs = fs.fit_transform(X_train, y_train)
        scores = cross_validation.cross_val_score(dt, X_train_fs, y_train, cv=4)",ProFET/feat_extract/Model_Parameters_CV.py,ddofer/ProFET,1
"
# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)

exported_pipeline = make_pipeline(
    make_union(VotingClassifier([(""est"", BernoulliNB(alpha=0.41000000000000003, binarize=0.43, fit_prior=True))]), FunctionTransformer(lambda X: X)),
    StandardScaler(),
    RandomForestClassifier(n_estimators=500)
)

exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)",LA_Team/FinalPipeline_LM_long_2.py,seg/2016-ml-contest,1
"    rands = np.random.rand(X.shape[0])
    mask = rands > 0.85
    X['species'].iloc[mask] = np.nan

    # define imputer, assert no missing
    imputer = BaggedCategoricalImputer(cols=['species'])
    y = imputer.fit_transform(X)
    assert y['species'].isnull().sum() == 0, 'expected no null...'

    # now test with a different estimator
    imputer = BaggedCategoricalImputer(cols=['species'], base_estimator=RandomForestClassifier())
    y = imputer.fit_transform(X)
    assert y['species'].isnull().sum() == 0, 'expected no null...'


def test_selective_imputer():
    a = pd.DataFrame.from_records([
        [1, 2, 3],
        [np.nan, 2, 2],
        [2, np.nan, np.nan]",skutil/preprocessing/tests/test_impute.py,tgsmith61591/skutil,1
"    models = [(""LR"", LogisticRegression()), 
              (""LDA"", LDA()), 
              (""QDA"", QDA()),
              (""LSVC"", LinearSVC()),
              (""RSVM"", SVC(
              	C=1000000.0, cache_size=200, class_weight=None,
                coef0=0.0, degree=3, gamma=0.0001, kernel='rbf',
                max_iter=-1, probability=False, random_state=None,
                shrinking=True, tol=0.001, verbose=False)
              ),
              (""RF"", RandomForestClassifier(
              	n_estimators=1000, criterion='gini', 
                max_depth=None, min_samples_split=2, 
                min_samples_leaf=1, max_features='auto', 
                bootstrap=True, oob_score=False, n_jobs=1, 
                random_state=None, verbose=0)
              )]

    # Iterate through the models
    for m in models:",SAT eBook/chapter11/forecast.py,Funtimezzhou/TradeBuildTools,1
"

dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

clfs = [
	('ab', ab, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('dt', dt,  {'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('gb', gb, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],'max_depth':[5,10,25,50,75,100]}),
	('rf', rf, {'n_estimators':[10,25,50,75,100],'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('svcl', svcl, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('lr', lr, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]})",scripts/histograms/non-normalised-ml-search.py,jmrozanec/white-bkg-classification,1
"
	#pro-processing train features
	label_encoder= LabelEncoder()
	PdDistrict_transform_train_data=label_encoder.fit_transform(train_data['PdDistrict'])
	Dow_transform_train_data=label_encoder.fit_transform(train_data['DayOfWeek'])
	train_data=pd.DataFrame({'X':train_data['X'],'Y': train_data['Y'],'PdDistrict_transform':PdDistrict_transform_train_data,'Dow_transform':Dow_transform_train_data})
	train_features=train_data.values
	train_target=train_data['Category'].astype('category')

	# training random forest
	model=RandomForestClassifier()

	# Grid search for model evaluation 
	grid_search_params={'criterion':['gini','entropy'],'n_estimators':[16,32,64,128],'max_features':['auto','log2','sqrt']}
	gs=GridSearchCV(estimator=model,param_grid=grid_search_params,scoring='log_loss',n_jobs=-1)

	#fitting the model
	gs.fit(train_features,train_data_target)

	#loading test_data data",San Francisco Crime Classification/main/first_attempt.py,tranlyvu/kaggle,1
"        beta1=.9,
        beta2=.999)

  elif model_name == 'rf':
    # Loading hyper parameters
    n_estimators = hyper_parameters['n_estimators']
    nb_epoch = None

    # Building scikit random forest model
    def model_builder(model_dir_rf):
      sklearn_model = RandomForestClassifier(
          class_weight=""balanced"", n_estimators=n_estimators, n_jobs=-1)
      return deepchem.models.sklearn_models.SklearnModel(sklearn_model,
                                                         model_dir_rf)

    model = deepchem.models.multitask.SingletaskToMultitask(tasks,
                                                            model_builder)

  elif model_name == 'xgb':
    # Loading hyper parameters",deepchem/molnet/run_benchmark_models.py,miaecle/deepchem,1
"    """"""
    Concrete optimizer for sklearn random forest -> sklearn.ensemble.RandomForestClassifier
    """"""
    def getClf(self, individual):
        """"""
        Builds a classifier object from an individual one

        :param individual: individual to create classifier
        :return: classifier sklearn.ensemble.RandomForestClassifier
        """"""
        clf = RandomForestClassifier(n_estimators=individual[3],
                                     criterion=""gini"",
                                     max_depth=None,
                                     min_samples_split=individual[0],
                                     min_samples_leaf=individual[1],
                                     min_weight_fraction_leaf=0,
                                     max_features=individual[2],
                                     max_leaf_nodes=None,
                                     min_impurity_split=1e-7,
                                     bootstrap=True,",optimize.py,Caparrini/pyGenreClf,1
"submission_file = ""submission.csv""

def main():
    print(""Reading the data"")
    data = cu.get_dataframe(train_file)

    print(""Extracting features"")
    fea = features.extract_features(data)

    print(""Training the model"")
    rf = RandomForestClassifier(n_estimators=50, verbose=2, compute_importances=True, n_jobs=1)
    rf.fit(fea, data[""OpenStatus""])

    print(""Reading test file and making predictions"")
    data = cu.get_dataframe(test_file)
    test_features = features.extract_features(data)
    probs = rf.predict_proba(test_features)

    print(""Calculating priors and updating posteriors"")
    new_priors = cu.get_priors(full_train_file)",src/model.py,coreyabshire/stacko,1
"
if __name__ == '__main__':
    categories = ['alt.atheism', 'soc.religion.christian']
    newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)
    newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)
    class_names = ['atheism', 'christian']

    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)
    train_vectors = vectorizer.fit_transform(newsgroups_train.data)
    test_vectors = vectorizer.transform(newsgroups_test.data)
    rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500)
    rf.fit(train_vectors, newsgroups_train.target)
    pred = rf.predict(test_vectors)
    sklearn.metrics.f1_score(newsgroups_test.target, pred, average='binary')
    c = make_pipeline(vectorizer, rf)
",benchmark/text_perf.py,marcotcr/lime,1
"        training_label = np.ravel(label_binarize(
            np.hstack(training_label).astype(int), [0, 255]))
        print 'Create the training set ...'

        # Perform the classification for the current cv and the
        # given configuration
        # Feature selector
        sel = SelectPercentile(f_classif, p)
        training_data = sel.fit_transform(training_data, training_label)
        testing_data = sel.transform(testing_data)
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data, training_label).predict_proba(
            testing_data)

        results_cv.append([pred_prob, crf.classes_])
        feat_imp_cv.append(sel.get_support(indices=True))

    results_p.append(results_cv)
    feat_imp_p.append(feat_imp_cv)
",pipeline/feature-classification/exp-3/selection-extraction/anova/pipeline_classifier_t2w.py,I2Cvb/mp-mri-prostate,1
"        self.predict(clf, X, Y)
        #  feature importance
        self.plot(clf, self.features)

    def get_classifier(self, X, Y):
        """""" 
        :param X: 
        :param Y: 
        :return: 
        """"""
        clf = RandomForestClassifier(n_estimators=10)
        clf.fit(X, Y)
        return clf

    def predict(self, clf, X, Y):
        """""" 
        :param clf: 
        :param X: 
        :param Y: 
        :return: ",machinelearning/randomforestclassifier/model.py,ideaplat/Tback,1
"    print(""The best classifier is: "", grid.best_estimator_)
    
    res = np.transpose(grid.predict_proba(X_test))[1]
    auc = sklearn.metrics.roc_auc_score(y_test, res)
    print ""svmrbf AUC: "", auc
    

def myRF_AUC(data, target):
    X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, target, test_size=0.4, random_state=42)
    cv = StratifiedKFold(y=y_train, n_folds=3)
    grid = GridSearchCV(RandomForestClassifier(min_samples_split=1, bootstrap=False), param_grid=dict(n_estimators=np.array([1500,3000,5000])), cv=cv)    
    
    grid.fit(X_train, y_train)
    print(""The best classifier is: "", grid.best_estimator_)
    
    res = np.transpose(grid.predict_proba(X_test))[1]
    auc = sklearn.metrics.roc_auc_score(y_test, res)
    print ""rforest AUC: "", auc
    
    '''",sklearnClassifiers/simpleClassifier3.py,rampasek/seizure-prediction,1
"    (1) model : RandomForestClassifier object
    (2) X : array
    (3) y : array
    """"""
    def __init__(self,
                 X,
                 y,
                 max_features='auto',
                 depth=None,
                 n_estimators=300):
        self.model = RandomForestClassifier(max_features=max_features,
                                            n_estimators=n_estimators,
                                            max_depth=None,
                                            oob_score=False,
                                            n_jobs=-1)
        self.X = X
        self.y = y

    def train(self):
        """"""",code/stat159lambda/classification/random_forest/rf.py,berkeley-stat159/project-lambda,1
"    # Numpy arrays are easy to work with, so convert the result to an
    # array
    train_data_features = train_data_features.toarray()

    # ******* Train a random forest using the bag of words
    #
    print ""Training the random forest (this may take a while)...""


    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators = 100)

    # Fit the forest to the training set, using the bag of words as
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit(train_data_features, train[""sentiment""])


",backup/BagOfWords.py,ddboline/kaggle_imdb_sentiment_model,1
"    f.write(""ID,Category\n"")

    for i, res in enumerate(pred):
        f.write(""%d,%d\n"" % (i+1,res))

    f.close()

clfs = []

# Through cv testing, I found the optimal number of estimators to be 15
clfs.append(ensemble.RandomForestClassifier(n_estimators=150))
clfs.append(ensemble.GradientBoostingClassifier(n_estimators=200))
clfs.append(ensemble.AdaBoostClassifier(n_estimators=135))
#clfs.append(neighbors.KNeighborsClassifier(n_neighbors=10))
#clfs.append(svm.SVC())

#predictificate(data, target, test, clfs)

clf = ensemble.RandomForestRegressor()
clf.fit(train_data, target)",Honors/code.py,bcspragu/Machine-Learning-Projects,1
"    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.sampling import cost_sampling, undersampling
    >>> from costcla.metrics import savings_score
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> X_cps_o, y_cps_o, cost_mat_cps_o =  cost_sampling(X_train, y_train, cost_mat_train, method='OverSampling')
    >>> X_cps_r, y_cps_r, cost_mat_cps_r =  cost_sampling(X_train, y_train, cost_mat_train, method='RejectionSampling')
    >>> X_u, y_u, cost_mat_u = undersampling(X_train, y_train, cost_mat_train)
    >>> y_pred_test_rf = RandomForestClassifier(random_state=0).fit(X_train, y_train).predict(X_test)
    >>> y_pred_test_rf_cps_o = RandomForestClassifier(random_state=0).fit(X_cps_o, y_cps_o).predict(X_test)
    >>> y_pred_test_rf_cps_r = RandomForestClassifier(random_state=0).fit(X_cps_r, y_cps_r).predict(X_test)
    >>> y_pred_test_rf_u = RandomForestClassifier(random_state=0).fit(X_u, y_u).predict(X_test)
    >>> # Savings using only RandomForest
    >>> print savings_score(y_test, y_pred_test_rf, cost_mat_test)
    0.12454256594
    >>> # Savings using RandomForest with cost-proportionate over-sampling
    >>> print savings_score(y_test, y_pred_test_rf_cps_o, cost_mat_test)
    0.192480226286",costcla/sampling/cost_sampling.py,madjelan/CostSensitiveClassification,1
"-----------------------
-- Models
-----------------------
'''
def listModels():
    models = list()
    models.append(KNeighborsClassifier(n_neighbors=20))
    models.append(LogisticRegression(penalty='l1'))
    models.append(AdaBoostClassifier())
    models.append(GradientBoostingClassifier())
    models.append(RandomForestClassifier())
    models.append(LDA())
    models.append(DecisionTreeClassifier())
    models.append(SVC(kernel='linear'))
    return models

'''
-----------------------
-- Useful
-----------------------",src/models.py,franblas/eegChallenge,1
"def _eval_classifier_cv(classifier_name, clf, X, Y, cv=5):
  acc = np.mean(sklearn.cross_validation.cross_val_score(clf, X, Y, cv = cv))
  print classifier_name, ""cross-validation accuracy"", acc 
  auc = np.mean(sklearn.cross_validation.cross_val_score(clf, X, Y, cv = cv, scoring='roc_auc'))
  print classifier_name, ""cross-validation AUC"", auc
  return acc, auc
  
def eval_cv(X, Y, logistic_regression = True, n_trees=50, cv = 5):
  lr = sklearn.linear_model.LogisticRegression()
  _eval_classifier_cv(""Logistic Regression"", lr, X, Y, cv)
  rf = sklearn.ensemble.RandomForestClassifier(n_trees)
  _eval_classifier_cv(""Random Forest"", rf, X, Y, cv)
  
def _eval_classifier(classifier_name, clf, X_train, Y_train, X_test, Y_test):
  clf.fit(X_train, Y_train)
  Y_pred = clf.predict(X_test)
  acc = np.mean(Y_test == Y_pred)
  print classifier_name, ""Accuracy"", acc
  Y_prob = clf.predict_proba(X_test)
  if len(Y_prob.shape) > 1:",eval_dataset.py,iskandr/immuno,1
"        last_iter = (i == n_iters - 1)
        
        
        if last_iter and last_iter_rf:
            drop_mask = np.abs(model_weights) < 0.0000001
            print ""Keeping %d / %d features: %s"" % (
                (~drop_mask).sum(), len(drop_mask), (~drop_mask).nonzero()
            )
            X_train = X_train[:, ~drop_mask]
            X_test = X_test[:, ~drop_mask]
            model = sklearn.ensemble.RandomForestClassifier(200)

            
        model.fit(X_train, train_lte)
            
        pred = model.predict_proba(X_test)[:, 1]
        preds.append(pred)

        if last_iter and last_iter_average:
            pred = np.zeros_like(pred)",old/train.py,iskandr/mhcpred,1
"    print 'Iteration #{}'.format(k)
    # Extract the data
    ### Training
    training_data = data[training_idx, :]
    training_label = label[training_idx]
    ### Testing 
    testing_data = data[testing_idx, :]
    testing_label = label[testing_idx]

    # Declare the random forest
    #crf = RandomForestClassifier(n_estimators=100, n_jobs=n_jobs)
    #crf = AdaBoostClassifier(n_estimators=100)
    #crf = LinearSVC()
    crf = LDA()

    # Train the classifier
    crf.fit(training_data, training_label)

    # Test the classifier
    pred_labels = crf.predict(testing_data)",pipeline/feature-classification/test_classification.py,I2Cvb/data_balancing,1
"valid_data = np.nan_to_num(valid_data)

cat_features = np.where(feat_type == 'Categorical')[0]
ohe = preprocessing.OneHotEncoder(categorical_features = cat_features)
ohe.fit(train_data)
train_data = ohe.transform(train_data).toarray()
test_data = ohe.transform(test_data).toarray()
valid_data = ohe.transform(valid_data).toarray()


clf = ensemble.RandomForestClassifier(n_jobs = -1, verbose = 2, n_estimators = 7000, random_state=42)
clf.fit(train_data, labels)
test_preds = clf.predict(test_data)
valid_preds = clf.predict(valid_data)

np.savetxt('res/adult_test_001.predict', test_preds, '%1.5f')
np.savetxt('res/adult_valid_001.predict', valid_preds, '%1.5f')


",Phase0/adult_main.py,abhishekkrthakur/AutoML,1
"        explanation = explainer.explain_instance(X[r_idx, :], func)
        times.append(time.time() - start_time)
        scores.append(explanation.score)
        print('...')

    return times, scores


if __name__ == '__main__':
    X_raw, y_raw = make_classification(n_classes=2, n_features=1000, n_samples=1000)
    clf = RandomForestClassifier()
    clf.fit(X_raw, y_raw)
    y_hat = clf.predict_proba(X_raw)

    times, scores = interpret_data(X_raw, y_hat, clf.predict_proba)
    print('%9.4fs %9.4fs %9.4fs' % (min(times), sum(times) / len(times), max(times)))",benchmark/table_perf.py,marcotcr/lime,1
"

def test_impute():
    from pyimpute import load_training_rasters, load_targets, impute

    # Load training data
    train_xs, train_y = load_training_rasters(response_raster, explanatory_rasters)

    # Train a classifier
    from sklearn.ensemble import RandomForestClassifier 
    clf = RandomForestClassifier(n_estimators=10, n_jobs=1)
    clf.fit(train_xs, train_y)

    # Load targets
    target_xs, raster_info = load_targets(explanatory_rasters)

    # Go...
    impute(target_xs, clf, raster_info, outdir=TMPOUT,
           linechunk=400, class_prob=True, certainty=True)
",tests/test_impute.py,ritviksahajpal/pyimpute,1
"
def main():
  '''
  Test different models with generated features
  '''
  df_train = pd.read_csv('data/train.csv', encoding=""ISO-8859-1"")
  trainData = pd.read_csv('data/train_features.csv', encoding=""ISO-8859-1"")
  X_train, X_test, y_train, y_test = train_test_split(trainData, df_train['relevance'], test_size=0.3, random_state=42)
  
  # Classifiers: weaker results than Regressors
  # model = ensemble.RandomForestClassifier(n_estimators=50, criterion='entropy')
  # model = ensemble.GradientBoostingClassifier()
  # model = svm.SVC()
  # model.fit(X_train, [str(n) for n in y_train])
  # print(""Train RMSE: %.3f"" % np.sqrt(np.mean(([float(n) for n in model.predict(X_test)] - y_test) ** 2)))
  
  # Regressors
  # model = linear_model.SGDRegressor()
  # model = ensemble.RandomForestRegressor()
  # model = svm.LinearSVR()",build_models.py,CSC591ADBI-TeamProjects/Product-Search-Relevance,1
"            "" "".join([str(x) for x in y2]), "" "".join([ str(x) for x in clf.predict(X2)])
            ))
    cscore = clf.score(X2, y2)
    pval = prob( len(y2)-round(cscore*len(y2)) + 1, 0.5, len(y2) )
    verbalise(""G"", ""score: %.2f (p=%.5f)\n"" % (cscore, pval ))
    return clf.coef_, clf.score(X2, y2)

def random_forest_engine(X1, y1, X2, y2):
    verbalise(""C"", ""Size of training set: %d\nSize of test set: %d"" % (len(y1), len(y2)))
    verbalise(""C"", ""Number of features: %d"" % X1.shape[1])
    clf = RandomForestClassifier(max_features='log2', n_estimators=120, random_state=125)
    #print clf
    clf.fit(X1, y1)
    verbalise(""Y"", ""real: %s\npred: %s"" % (
            "" "".join([str(x) for x in y2]), "" "".join([ str(x) for x in clf.predict(X2)])
            ))
    cscore = clf.score(X2, y2)
    pval = prob( len(y2)-round(cscore*len(y2)) + 1, 0.5, len(y2) )
    verbalise(""G"", ""score: %.2f (p=%.5f)\n"" % (cscore, pval ))
    return clf.score(X2, y2)",brain_machine.py,oxpeter/small_fry,1
"
h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",playground/clustering/landsat.py,geobricks/Playground,1
"def bow_train(vectorizer):

    data = pd.read_csv(get_file(""labeled""), header=0,
                       delimiter=""\t"", quoting=3)

    clean_reviews = clean_data(data)

    train_features = vectorizer.fit_transform(clean_reviews)
    train_features = train_features.toarray()

    forest = RandomForestClassifier(n_estimators=100)
    forest = forest.fit(train_features, data[""sentiment""])

    return forest


def dump_result(data, result):
    output = pd.DataFrame(data={""id"": data[""id""], ""sentiment"": result})
    output.to_csv(""bow.csv"", index=False, quoting=3)
",src/old_scrips/dump/tutorial.py,johnthebrave/sentiment-mining,1
"    return filenames, features, groundtruths

def column(matrix, i):
    return [row[i] for row in matrix]

def process_results(train, test):
    train_fn, train_features, train_groundtruths = read_file_bayle(train)
    test_fn, test_features, test_groundtruths = read_file_bayle(test)
    step = 0.1
    # for weight in np.arange(0.0, 1.0, step):
    # inside_clf = RandomForestClassifier(random_state=2)
    inside_clf = DecisionTreeClassifier(random_state=2)
        # class_weight={""i"":weight, ""s"":1-weight})
    clf = AdaBoostClassifier(
        random_state=2,#with 4 98%precision song class
        base_estimator=inside_clf)
    clf.fit(train_features, train_groundtruths)
    predictions = clf.predict(test_features)
    print(""Accuracy "" + str(accuracy_score(test_groundtruths, predictions)))
    print(""F-Measure "" + str(f1_score(test_groundtruths, predictions, average=""weighted"")))",src/bayle.py,ybayle/ReproducibleResearchIEEE2017,1
"        self.update_experiment_info()
        if 'SVM' in self.use:
            from sklearn.svm import LinearSVC
            self.classifier_type = LinearSVC(
                                       class_weight='balanced',C=self.svm_c,
                                       multi_class='ovr',
                                            dual=False)
        else:
            from sklearn.ensemble import RandomForestClassifier
            self.classifier_type =\
                RandomForestClassifier(10)
        self.unified_classifier = None
        if use_sparse:
            if not(use_sparse == 'Features' or use_sparse == 'Buffer'):
                raise Exception('Invalid use_sparse, its value shoud be '
                                + 'None/False/Buffer/Features')
        self.sparsecoded = use_sparse
        self.decide = None
        # Training variables
        self.training_data = None",classifiers.py,VasLem/KinectPainting,1
"        (tp_norm,fn_norm,rec)
    return rec,tp_norm,fn_norm

def fraction_correct(predict,ans):
    tp= float( len(np.where(predict.astype('bool') & ans.astype('bool'))[0]) )
    tn= float( len(np.where( (predict.astype('bool')==False) & (ans.astype('bool')==False) )[0]) )
    return (tp+tn)/len(ans)

def best_machine_learn_NoRandOrd(TrainX,TrainY,TestX,\
                                n_estim=100,min_samples_spl=2,scale=False):
    forest1 = RandomForestClassifier(n_estimators=n_estim, max_depth=None,
                                     min_samples_split=min_samples_spl, random_state=0,
                                    compute_importances=True)
    forest1.fit(TrainX,TrainY)
    forestOut1 = forest1.predict(TestX)
    # precision(forestOut1,TestY)
#     recall(forestOut1,TestY)
#     print sum(forestOut1 == TestY)/float(len(forestOut1))

    # forest2 = ExtraTreesClassifier(n_estimators=n_estim, max_depth=None,",machine_learn/Blob/machine_learn.py,kaylanb/SkinApp,1
"    ## get_model_name testing
    ##////////////////////////////////////////////////////////////////////

    def test_real_model(self):
        """"""
        Test that model name works for sklearn estimators
        """"""
        model1 = LassoCV()
        model2 = LSHForest()
        model3 = KMeans()
        model4 = RandomForestClassifier()
        self.assertEqual(get_model_name(model1), 'LassoCV')
        self.assertEqual(get_model_name(model2), 'LSHForest')
        self.assertEqual(get_model_name(model3), 'KMeans')
        self.assertEqual(get_model_name(model4), 'RandomForestClassifier')

    def test_pipeline(self):
        """"""
        Test that model name works for sklearn pipelines
        """"""",tests/test_utils/test_helpers.py,pdamodaran/yellowbrick,1
"                'num_attempts_within7',
                'num_contacts_within15',
                'num_attempts_within15',
                'p_all_needs_disconnect',
                'sum_median_days',
                ]


# In[14]:

rf_clf = RandomForestClassifier(n_estimators=500, max_depth=None)


# ### A2: Manually Separating Test and Training Sets

# We separate the testing and training data by a particular date, using
# 80% of the data to predict the future 20% of cases.

# In[6]:
",Modeling.py,dssg/healthleads-public,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/len(X1) )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X1,target, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 100, n_jobs = 12, random_state = 4)
result_rf = cross_val_predict(rf,X1,target, cv = 10)
#print(Z[70])
#print('X', len(X),len(Y),len(X1[train_size:dataset_size]))
#print('RF prediction : ---> ',result_rf )
#print('actual ans: -->',test_class)
CM = confusion_matrix(target,result_rf) 
print(""Confusion Matrix : "")
print(CM)
for i in range(0,len(X1)):",sandbox/petsc/solvers/scripts/ScikitClassifiersRS2CV.py,LighthouseHPC/lighthouse,1
"
bm_data = (X_train, y_train, X_test, y_test)
# ******************* MODEL FITTING *********************

results = []
for clf, name in (
        (RidgeClassifier(tol=1e-2, solver='lsqr'), 'Ridge Classifier'),
        (Perceptron(n_iter=50), 'Perceptron'),
        (PassiveAggressiveClassifier(n_iter=50), 'Passive-Aggressive'),
        (KNeighborsClassifier(n_neighbors=10), 'kNN'),
        (RandomForestClassifier(n_estimators=100), 'Random forest')):
    print('_' * 80)
    print(name)
    results.append(benchmark(clf, *bm_data))

for penalty in ['l2', 'l1']:
    print('_' * 80)
    print('%s penalty' % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,",News/NewsClassification.py,craymichael/News,1
"        adaClf = AdaBoostClassifier(n_estimators=60)
        print ""Classifier:""
        print adaClf
        print ""Training"", subject
        adaClf.fit(xx, yy)
        classifiers.append(adaClf)
        print ""Time:"", time.time() - start_time'''

        '''start_time = time.time()
        print
        rfClf = RandomForestClassifier(n_estimators=20, n_jobs=-1)
        print ""Classifier:""
        print rfClf
        print ""Training"", subject
        rfClf.fit(xx, yy)
        classifiers.append(rfClf)
        print ""Time:"", time.time() - start_time

        start_time = time.time()
        print",src/split_multiclassification_stacked_w_initials.py,LooseTerrifyingSpaceMonkey/DecMeg2014,1
"    df_train = df_all.iloc[:n_train]
    df_test = df_all.iloc[n_train:]

    X = df_train.values
    X_test = df_test.values
    feature_names = df_all.columns.values.tolist()
    
    print(X.shape)
    
    print(""build the model"")
    clf1 = RandomForestClassifier(n_estimators=100,random_state=571,max_features=8,max_depth=13,n_jobs=1)
    clf2 = KNeighborsClassifier(n_neighbors=250, p=1, weights=""distance"")
    clf3 = ExtraTreesClassifier(n_estimators=200,max_depth=14, max_features=12,random_state=571,n_jobs=1)
    clf4 = GaussianNB()
    clf5 = GradientBoostingClassifier(n_estimators=100,random_state=571,max_depth=6, max_features=7)
    
    clf6 = RandomForestClassifier(n_estimators=1000,max_features=10,max_depth=14,n_jobs=1) # feats = 10
    clf7 = GradientBoostingClassifier(n_estimators=100,max_depth=9, max_features=7)  # feats = 7
    
    first_stage = [",stack_script.py,tsterbak/scikit-stack,1
"                 ""AdaBoost"",
                 ""Naive Bayes"",
                 ""QDA""]

        classifiers = [KNeighborsClassifier(3),
                       SVC(kernel=""linear"", C=0.025),
                       SVC(gamma=2, C=1),
                       linear_model.LogisticRegression(C=1e5),
                       # GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
                       # DecisionTreeClassifier(max_depth=5),
                       RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
                       MLPClassifier(alpha=1),
                       AdaBoostClassifier(),
                       GaussianNB(),
                       QuadraticDiscriminantAnalysis()]

        # iterate over classifiers
        for name, clf in zip(names, classifiers):
            clf.fit(train_vectors, train_labels)
            score = clf.score(test_vectors, test_labels)",models/src/classification.py,Semen52/nlp4u,1
"

class TestLimeTabular(unittest.TestCase):
    def test_lime_explainer_bad_regressor(self):
        iris = load_iris()
        train, test, labels_train, labels_test = (
            sklearn.cross_validation.train_test_split(iris.data,
                                                      iris.target,
                                                      train_size=0.80))

        rf = RandomForestClassifier(n_estimators=500)
        rf.fit(train, labels_train)
        lasso = Lasso(alpha=1, fit_intercept=True)
        i = np.random.randint(0, test.shape[0])
        with self.assertRaises(TypeError):
            explainer = LimeTabularExplainer(
                train,
                feature_names=iris.feature_names,
                class_names=iris.target_names,
                discretize_continuous=True)",lime/tests/test_lime_tabular.py,marcbllv/lime,1
"            tmp = LBPUMultiBlockAndHOGFeatures(x)
            feat = tmp.getFeatures()
            myfeat_test.append(feat)
            mylabel_test.append(i)

        print ""Features obtained for test class"", i

    # Train
    # Create a classifier: a support vector classifier
    svml = LinearSVC()
    rf = RandomForestClassifier()
    gnb = GaussianNB()
    tr = tree.DecisionTreeClassifier()
    dummy = DummyClassifier()

    print ""Training ...""
    # Train
    # Compute traing time
    ttime = []
    tt = time.time()",handwriting_recognition/main.py,eusebioaguilera/cvsamples,1
"        return self.pack_data(training_ids = training_ids, testing_ids = testing_ids, training_samples = training_samples, testing_samples = testing_samples, \
                training_labels = training_labels, testing_labels = testing_labels, testing_predicted_labels = testing_predicted_labels)

    def random_forest(self, data, params):
        training_ids, testing_ids         = self.unpack_ids(data)
        training_samples, testing_samples = self.unpack_samples(data)
        training_labels, testing_labels   = self.unpack_labels(data)

        n_estimators      = 10 if (not 'n-estimators' in params or not params['n-estimators'])else int(params['n-estimators'])
        criterion         = ""gini"" if not 'criterion' in params else params['criterion']
        random_forest_clf = ensemble.RandomForestClassifier(n_estimators = n_estimators, criterion = criterion)
        random_forest_clf.fit(training_samples, training_labels)
        if testing_samples.any():
            testing_predicted_labels = random_forest_clf.predict(testing_samples)
        return self.pack_data(training_ids = training_ids, testing_ids = testing_ids, training_samples = training_samples, testing_samples = testing_samples, \
                training_labels = training_labels, testing_labels = testing_labels, testing_predicted_labels = testing_predicted_labels)

    def kmeans(self, data, params):
        training_ids, testing_ids         = self.unpack_ids(data)
        training_samples, testing_samples = self.unpack_samples(data)",utils/Algorithms.py,hzxie/Sharp-V,1
"rocksVMinesNames = numpy.array(['V' + str(i) for i in range(ncols)])

#break into training and test sets.
xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.30, random_state=531)

auc = []
nTreeList = range(50, 2000, 50)
for iTrees in nTreeList:
    depth = None
    maxFeat  = 8 #try tweaking
    rocksVMinesRFModel = ensemble.RandomForestClassifier(n_estimators=iTrees, max_depth=depth, max_features=maxFeat,
                                                 oob_score=False, random_state=531)

    rocksVMinesRFModel.fit(xTrain,yTrain)

    #Accumulate auc on test set
    prediction = rocksVMinesRFModel.predict_proba(xTest)
    aucCalc = roc_auc_score(yTest, prediction[:,1:2])
    auc.append(aucCalc)
",machine-learning/python_codes/07/rocksVMinesRF.py,nafis/Data-Science,1
"    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for support vector machine: ', auc)

    #plot_curve(fpr, tpr, 'SVM ' + str(auc))
    return clf


def train_random_forest(x_train, y_train, x_cv, y_cv):
    clf = RandomForestClassifier(n_estimators=100)
    clf.fit(x_train, y_train)

    preds = clf.predict_proba(x_cv)
    fpr, tpr, thr = roc_curve(y_cv, preds[:, 1])
    auc = roc_auc_score(y_cv, preds[:, 1])
    print('AUC for knn: ', auc)

    plot_curve(fpr, tpr, 'KNN ' + str(auc))
    return clf",examples/sara/titanic_sara_6.py,remigius42/code_camp_2017_machine_learning,1
"    Load an sklearn model by key.
    :param key: Model key string
    :param regressor: Regression flag
    :return: sklearn model object
    '''
    if key == KNN_MODEL_KEY:
        return KNeighborsRegressor() if regressor else KNeighborsClassifier()
    elif key == SVM_MODEL_KEY:
        return SVR() if regressor else SVC()
    elif key == RF_MODEL_KEY:
        return RandomForestRegressor() if regressor else RandomForestClassifier()
    raise ConfigurationError('[!] Invalid model key specified [%s]' % key)

def load_default_grid_by_key(key):
    '''
    Load default map of grid parameters for hyperparameter tuning.
    :param key: Model key string
    :return: dict of parameters
    '''
    if key == KNN_MODEL_KEY:",toy_train/trainer_utils.py,stevedicristofaro/toy_trainer,1
"        analyzer=""word"", tokenizer=None, preprocessor=None, stop_words=None, max_features=5000)

    train_tweets = [tweet[0] for tweet in train]
    train_sentiment = [tweet[1] for tweet in train]

    # Learns the vocabulary and returns term document matrix
    train_data_features = vectorizer.fit_transform(train_tweets)
    train_data_features = train_data_features.toarray()

    # Fit training data to Random Forest classifier
    forest = RandomForestClassifier(n_estimators=100)
    forest = forest.fit(train_data_features, train_sentiment)

    test_tweets = [tweet[0] for tweet in test]
    test_sentiment = [tweet[1] for tweet in test]

    # Gets document term matrix
    test_data_features = vectorizer.transform(test_tweets)
    test_data_features = test_data_features.toarray()
",sentimental_analysis.py,ManrajGrover/Sentimental-Analysis,1
"import numpy as np
from cudatree import load_data, RandomForestClassifier, timer
from cudatree import util

x, y = load_data(""iris"")

def test_iris_memorize():
  with timer(""Cuda treelearn""):
    forest = RandomForestClassifier(bootstrap = False)
    forest.fit(x, y)
  with timer(""Predict""):
    diff, total = util.test_diff(forest.predict(x), y)  
    print ""%s(Wrong)/%s(Total). The error rate is %f."" % (diff, total, diff/float(total))
  assert diff == 0, ""Didn't perfectly memorize, got %d wrong"" % diff

from helpers import compare_accuracy
def test_iris_accuracy():
  compare_accuracy(x,y)",test/test_iris.py,EasonLiao/CudaTree,1
"    slice_scores = []
    full_scores = []

    # class from config
    c = parse.Bunch(**config)

    n_slices = data.shape[c.slice_axis]
    if c.start_slice == ""center"":
        c.start_slice = int(n_slices/2)

    forest = RandomForestClassifier(**parse.config_for_function(RandomForestClassifier.__init__, config))
    training_data = []
    training_labels = []

    visited_slices = []
    current_slice = c.start_slice

    # loop over desired number of cycles
    for i in range(c.max_slices):
",MultiVolumeActiveLearning/HypothesisRandomForestSlicePerformance.py,jenspetersen/Experiments,1
"data_train_neg = load_data(new_train_data_path, type=""neg/"")
labels_train_neg = np.zeros((data_train_neg.shape[0], 1))
x_train = np.concatenate((data_train_pos, data_train_neg), axis=0)
y_train = np.concatenate((labels_train_pos, labels_train_neg), axis=0)

x_train_features = hog_extraction(x_train)

print(""Apprentissage."")
# error, clf = cross_validation(x_train_features, y_train, svm.SVC(kernel='linear', C=0.05), N=5)
# error, clf = cross_validation(x_train_features, y_train, AdaBoostClassifier(n_estimators=50))
# error, clf = cross_validation(x_train_features, y_train, RandomForestClassifier(), N=0)
error, clf = cross_validation_svm(x_train_features, y_train, N=3)

window_w = SIZE_TRAIN_IMAGE[0]
window_h = window_w

print(""Predictions sur les donnes de test."")
# The images in which a face is to detect.
test_images = [given_data_test_path + file_name for file_name in os.listdir(given_data_test_path)]
test_images.sort()",toto.py,jjerphan/SY32FacialRecognition,1
"					print('shuffling safe train data')
					np.random.shuffle(traind[1])
				#obtain training dataset by combining all fail data points with a subset of the safe data points
				trainx=np.vstack((traind[0],traind[1][start:end,:]))
				
				trainTTR=trainx[:,3]
				trainy=trainx[:,0] #training labels
				trainx=trainx[:,cols] #training features
				
				#create classifier
				clf =en.RandomForestClassifier(n_estimators=tc)
				#train
				clf.fit(trainx, trainy)
				print('testing..')
				#test on training data
				testing(str(set)+'trainRF'+str(tc)+'_forward'+str(train_days)+'d'+str(test_days)+'dxval'+str(i)+'_corr'+str(cthreshold)+'_nsafe'+str(ns)+'run'+str(l),trainx,trainy,clf)
				#test on individual test data
				testing(str(set)+'test0RF'+str(tc)+'_forward'+str(train_days)+'d'+str(test_days)+'dxval'+str(i)+'_corr'+str(cthreshold)+'_nsafe'+str(ns)+'run'+str(l),testx[0],testy[0],clf)
				#test on ensemble test data
				testing(str(set)+'test1RF'+str(tc)+'_forward'+str(train_days)+'d'+str(test_days)+'dxval'+str(i)+'_corr'+str(cthreshold)+'_nsafe'+str(ns)+'run'+str(l),testx[1],testy[1],clf)",classification.py,alinasirbu/google_cluster_failure_prediction,1
"# In[10]:

d = pandas.read_csv(""data/mnist_small.csv"")
d_train = d[:int(0.8*len(d))]
d_test = d[int(0.8*len(d)):]


# In[11]:

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(d_train.drop('label', axis=1), d_train['label'])


# In[12]:

from sklearn.metrics import confusion_matrix
preds = rf.predict(d_test.drop('label', axis=1))
cm = confusion_matrix(d_test['label'], preds)
",examples/realWorldMachineLearning/Chapter+4+-+Evaluation+and+Optimization.py,remigius42/code_camp_2017_machine_learning,1
"# clf = KNeighborsClassifier()

# adaboost
# algo = ""Adaboost""
# from sklearn.ensemble import AdaBoostClassifier
# clf = AdaBoostClassifier()

# random forrest
algo = ""Random Forest""
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# fit
clf.fit(features_train, labels_train)

# predict
pred = clf.predict(features_test)

# print accuracy
from sklearn.metrics import accuracy_score",p5/choose_your_own/your_algorithm.py,stefanbuenten/nanodegree,1
"vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
                                 stop_words='english')


X2 = vectorizer.fit_transform(data_text)
clf2 = SGDClassifier(loss='log', penalty='l2',alpha=1e-5, n_iter=25, random_state=42,shuffle=True)
clf2.fit(X2, labels)
vocab = my_dict2 = {y:x for x,y in vectorizer.vocabulary_.iteritems()}


#clf = RandomForestClassifier(n_estimators=750)

allTextInOrders = data_text + data_text + unhelpful_exp_text + helpful_exp_text
trainingDistributions = []

# for reviewT in data_text:
# 	trainingDistributions.append(lda[corpus.dictionary.doc2bow(corpus.proc(reviewT))])


",src/old_pipeline/treesRedditReviews.py,cudbg/Dialectic,1
"if options.TRAINING_INPUT_DIRECTORY is not None:
    TRAINING_INPUT_DIRECTORY = options.TRAINING_INPUT_DIRECTORY

# From demonstration , we only used  4 of 5 histrical data for training
train_mat = genfromtxt(TRAINING_INPUT_DIRECTORY+'/training_matrix.csv', delimiter=',')
# First column is the lable column
y = train_mat[:, 0]
X = train_mat[:, 1:]

# build a classifier, in this demo we use Random Forest, you can switch to any other classifier
clf = RandomForestClassifier(n_estimators=50)

# Utility function to report best scores


def report(grid_scores, n_top=3):
    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]
    for i, score in enumerate(top_scores):
        print(""Model with rank: {0}"".format(i + 1))
        print(""Mean validation score: {0:.3f} (std: {1:.3f})"".format(",model/random-forest-model.py,BrakeValve/dataflow,1
"        'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', \
        'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', \
        'Proline']

X = df_wine.iloc[:, 1:].values
y = df_wine.iloc[:, 0].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

feat_labels = df_wine.columns[1:]
forest = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)
forest.fit(X_train, y_train)
importances = forest.feature_importances_
indices = np.argsort(importances)[::-1]

for f in range(X_train.shape[1]):
    print(""%2d) %-*s %f"" % (f+1, 30, feat_labels[f], importances[indices[f]]))

plt.title('Feature Importances')
plt.bar(range(X_train.shape[1]), importances[indices], color='lightblue', \",DataPreprocessing/feature_importance.py,southpaw94/MachineLearning,1
"            test.loc[test_series.isnull(), test_name] = -999


kf = KFold(train.shape[0], n_folds=3, random_state=1)
#alg = svm.SVC(max_iter=300)
alg=Pipeline([
  #('feature_selection', SelectFromModel(ExtraTreesClassifier(n_estimators=100,criterion= 'entropy'))),
  ('classification',svm.SVC(max_iter=500) )
])
 
#RandomForestClassifier(n_estimators=100,criterion= 'entropy')
for c in train.columns:
    predictions = []
    for trainkf, test in kf:
    	print trainkf
    	train_predictors = (train.iloc[trainkf,:])
    	train_target = target.iloc[trainkf]
    	print train_predictors.shape[0],train_predictors.shape[1]
    	print train_target.shape[0]
    	print train.iloc[test,:].shape[0],train.iloc[test,:].shape[1]",svm.py,souravsarangi/BNPParibasKaggle,1
"    #     dummies = pd.get_dummies(df[f], prefix=f)
    #     for dummy in dummies.columns:
    #         df[dummy] = dummies[dummy]
    #         features.append(dummy)
    #     df = df.drop(f, 1)
    #     features.remove(f)

    clf = Pipeline([
        (""imputer"", Imputer(strategy=""mean"", axis=0)),
        ('feature_selection', SelectKBest(k=5)),
        (""forest"", RandomForestClassifier())])
    clf.fit(train[features], train[target])
    score = clf.score(test[features], test[target])
    predicted = clf.predict(test[features])

    cm = confusion_matrix(test[target], predicted)
    print ""Random Forest score: %f"" % score
    print ""confusion_matrix : \n%s"" % cm
    return clf
",delay/model.py,datamindedbe/train-occupancy,1
"FPMAT = numpy.zeros((all_senaryo_sets.__len__(), alphaStep))
s = 0
a = 0

f = h5py.File('../randomforest_results_wg.h5', 'w')
allpositives = 0
for senaryo in all_senaryo_sets:
    allpositives += sum(senaryo['batch_features_set'].labels==1)
    avg_tp_rate = 0.0
    avg_fp_number = 0
    rf = ensemble.RandomForestClassifier(n_estimators=treeCount, random_state=(roc_iterator+1)*10)
    sde = senaryo['all_features_set'].balanceOnLabel(multiplier=mult)
    sample_weight = numpy.array([1/mult if i == 0 else 1.0 for i in sde.labels])
    rf.fit(sde.data, numpy.ravel(sde.labels), sample_weight)
    for x in range(0, treeCount):
        rf_ext = ensemble.RandomForestClassifier(n_estimators=treeCount, random_state=(roc_iterator+1)*10)
        sde = senaryo['all_features_set'].balanceOnLabel(multiplier=mult)
        sample_weight = numpy.array([1/mult if i == 0 else 1.0 for i in sde.labels])
        rf_ext.fit(sde.data, numpy.ravel(sde.labels), sample_weight)
        rf.estimators_.extend(rf_ext.estimators_)",objectclassifier/plot_roc_forCT.py,ilkerc/noduledetector,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.ml.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = spark.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> model.featureImportances
    SparseVector(1, {0: 1.0})
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0",GenesAssociation/spark-2.0.0-bin-hadoop2.7/python/pyspark/ml/classification.py,DataReplyUK/datareplyuk,1
"def WriteFile(name, out):
    predictions_file = open(name, ""wb"")
    open_file_object = csv.writer(predictions_file)
    open_file_object.writerow([""PassengerId"",""Survived""])
    global ids
    open_file_object.writerows(zip(ids, out))
    predictions_file.close()


def RandomForest(train_data_x, train_data_y, test_data):
    forest = RandomForestClassifier(n_estimators=100)
    forest = forest.fit(train_data_x, train_data_y)
    out = forest.predict(test_data).astype(int)
    joblib.dump(forest, './result/model.pkl')
    # WriteFile('./result/randomforest.csv', out)
    return out

def Linear(train_data_x, train_data_y, test_data):
    linear = linear_model.LinearRegression()
    linear.fit(train_data_x, train_data_y)",predict.py,kdqzzxxcc/TitanicPredict,1
"        labels: list of strings
        features: tuple of ints

    Returns:
        clf_forest: scikit-learn object
        accuracy: float
    """"""
    # Split data into train and test
    X_train, X_test, y_train, y_test = data_split(data)

    clf_forest = RandomForestClassifier(n_estimators=n_estimators,max_features=max_features,max_depth = max_depth)
    clf_forest = clf_forest.fit(X_train, y_train)
    accuracy = clf_forest.score(X_test,y_test)
    print(""Predictions:\n{}"".format(clf_forest.predict(X_test)))
    print(""Actual:\n{}"".format(y_test[:10]))
    print(""Score for {}x{}:\n{}"".format(features[0],features[1], accuracy))
    return clf_forest, accuracy

def adaboost(data,max_depth=3,n_estimators=10):
    """"""Adaboost classifier.",helper_functions.py,JustinShenk/sonic-face,1
"
    if shuffle:
        idx = np.random.permutation(y.size)
        X = X[idx]
        y = y[idx]
    
    skf = list(StratifiedKFold(y, n_folds))
    

    clfs = [
            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'), 
            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            xgb.XGBClassifier(objective='multi:softprob',silent =False, n_estimators=40)]
    
    dataset_blend_train_list = []
    for j, clf in enumerate(clfs):
        dataset_blend_train_list.append(np.zeros((X.shape[0], num_class-1 )))
    ",code/skip-thoughts/TomKenter-siamese-cbow-faf752ef6a99/stack5.py,UKPLab/semeval2017-scienceie,1
"

def single_clf_run(X_train, y_train, X_test, clf_str):
    if clf_str == 'SVM':
        svm = SVC(kernel='linear', probability=True)
        svm.fit(X_train, y_train)
        y_pred = svm.predict(X_test)
        y_score = svm.predict_proba(X_test)[:, svm.classes_ == 1]
        meta_data = svm.coef_
    elif clf_str == 'RF':
        rf_clf = RandomForestClassifier()
        params_rf = {'n_estimators': np.arange(10, 200, 20), 'max_features': ['sqrt', 'log2', 0.25, 0.5, 0.75]}
        cv = get_cross_validator(n_folds=5)
        grid_search = GridSearchCV(rf_clf, params_rf, scoring='accuracy', cv=cv, refit=True, verbose=1, n_jobs=15)
        grid_search.fit(X_train, y_train)
        rf_clf = grid_search.best_estimator_
        y_pred = rf_clf.predict(X_test)
        y_score = rf_clf.predict_proba(X_test)[:, 1]
        meta_data = rf_clf.feature_importances_
    else:",FTD_classification.py,PaulZhutovsky/ftd_project,1
"from sklearn.datasets import load_breast_cancer as load_data


X, y = load_data(return_X_y=True)
rf = classifier_factory(RandomForestClassifier())
rf.plot_learning_curve(X, y)
plt.show()

# Using the more flexible functions API
from scikitplot import plotters as skplt
rf = RandomForestClassifier()
skplt.plot_learning_curve(rf, X, y)
plt.show()",examples/plot_learning_curve.py,reiinakano/scikit-plot,1
"        if info['task']=='regression':
            if info['is_sparse']==True:
                self.name = ""BaggingRidgeRegressor""
                self.model = BaggingRegressor(base_estimator=Ridge(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            else:
                self.name = ""GradientBoostingRegressor""
                self.model = GradientBoostingRegressor(n_estimators=1, verbose=verbose, warm_start = True)
        else:
            if info['has_categorical']: # Out of lazziness, we do not convert categorical variables...
                self.name = ""RandomForestClassifier""
                self.model = RandomForestClassifier(n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            elif info['is_sparse']:                
                self.name = ""BaggingNBClassifier""
                self.model = BaggingClassifier(base_estimator=BernoulliNB(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...                          
            else:
                self.name = ""GradientBoostingClassifier""
                self.model = eval(self.name + ""(n_estimators=1, verbose="" + str(verbose) + "", min_samples_split=10, random_state=1, warm_start = True)"")
            if info['task']=='multilabel.classification':
                self.model = models.MultiLabelEnsemble(self.model)  
                          ",autokit/hyper.py,tadejs/autokit,1
"                ]

def main():
    print(""Reading the data"")
    data = cu.get_dataframe(train_file)

    print(""Extracting features"")
    fea = extract_features(feature_names, data)

    print(""Training the model"")
    rf = RandomForestClassifier(n_estimators=50, verbose=2, compute_importances=True, n_jobs=-1)
    rf.fit(fea, data[""OpenStatus""])

    print(""Reading test file and making predictions"")
    data = cu.get_dataframe(test_file)
    test_features = extract_features(feature_names, data)
    probs = rf.predict_proba(test_features)

    print(""Calculating priors and updating posteriors"")
    new_priors = cu.get_priors(full_train_file)",submission/basic_benchmark.py,coreyabshire/stacko,1
"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.externals import joblib
import matplotlib.pyplot as plt


def plotroc(traindata, trainlabel, testdata, testlabel, labels, rocfilename, cmfilename):
    print('# plot ROC curve')
    print('## train data shape: %s' % (traindata.shape,))
    #clf = LogisticRegression(C=0.0005)
    clf = RandomForestClassifier(10, oob_score=True, n_jobs=-1)
    clf.fit(traindata, trainlabel)
    print('## test data shape: %s' % (testdata.shape,))
    predlabel = clf.predict(testdata)
    predprob = clf.predict_proba(testdata)
    cm = confusion_matrix(testlabel, predlabel)
    print(cm)
    plotconfusionmatrix(cm, labels, cmfilename)
    print(classification_report(testlabel, predlabel, target_names=labels))
",classifier/bin/report.py,wellflat/cat-fancier,1
"    # Numpy arrays are easy to work with, so convert the result to an
    # array
    train_data_features = train_data_features.toarray()

    # ******* Train a random forest using the bag of words
    #
    print ""Training the random forest (this may take a while)...""


    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators = 100)

    # Fit the forest to the training set, using the bag of words as
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit( train_data_features, train[""sentiment""] )


",Sentiment Analysis/BagOfWords.py,prabhatsaini91/NLP-TA,1
"        return accuracy

    def apply_knn(self, X_train, y_train, X_test, y_test):
        clf = neighbors.KNeighborsClassifier()
        clf.fit(X_train, y_train)
        accuracy = clf.score(X_test, y_test)
        print(""Accuracy for KNN Classifier %s"" % accuracy)
        return accuracy

    def apply_random_forest(self, X_train, y_train, X_test, y_test):
        clf = RandomForestClassifier(n_estimators=5, n_jobs=-1)
        clf.fit(X_train, y_train)
        accuracy = clf.score(X_test, y_test)
        print(""Accuracy for RF Classifier %s"" % accuracy)
        return accuracy

    def select_best_param_svc(self, X_train, y_train, parameters):
        svr = SVC()
        clf = GridSearchCV(svr, parameters)
        clf.fit(X_train, y_train)",Capstone/DataProcessor.py,abhipr1/DATA_SCIENCE_INTENSIVE,1
"  
  # Drop the index columns
  for column in index_columns:
    final_vectorized_features = final_vectorized_features.drop(column)
  
  # Inspect the finalized features
  final_vectorized_features.show()
  
  # Instantiate and fit random forest classifier on all the data
  from pyspark.ml.classification import RandomForestClassifier
  rfc = RandomForestClassifier(
    featuresCol=""Features_vec"",
    labelCol=""ArrDelayBucket"",
    predictionCol=""Prediction"",
    maxBins=4657,
    maxMemoryInMB=1024
  )
  model = rfc.fit(final_vectorized_features)
  
  # Save the new model over the old one",ch08/train_spark_mllib_model.py,naoyak/Agile_Data_Code_2,1
"
# ## 
# 
# ### 
# 
# 

# In[37]:

from sklearn.ensemble import RandomForestClassifier
get_ipython().magic(u'time rfc = RandomForestClassifier(n_estimators = 500, max_features = 50, max_depth=None)')
rfc.fit(xTrain, yTrain)


# #### 
# 

# In[31]:

predTest = rfc.predict(xTest)",competitions/image_recognize/image_recognize.py,lijingpeng/kaggle,1
"            if W is not None:
                W_false_sub = W_false[false_idx]

            X_sub = np.vstack([X_true_sub, X_false_sub])
            Y_sub = np.concatenate([Y_true_sub, Y_false_sub])
            if W is not None:
                W_sub = np.concatenate([W_true_sub, W_false_sub])
            if self.logistic_regression:
                clf = sklearn.linear_model.LogisticRegression()
            else:
                clf = sklearn.ensemble.RandomForestClassifier(
                    n_estimators = self.n_estimators)
            if W is None:
                clf.fit(X_sub, Y_sub)
            else:
                clf.fit(X_sub, Y_sub, sample_weight = W_sub)
            self.models.append(clf)

    def _predict_counts(self, X):
        Y = np.zeros(len(X), dtype=int)",balanced_ensemble.py,hammerlab/immuno_research,1
"from sklearn.ensemble import RandomForestClassifier
from techson_server.settings import BASE_DIR

path = BASE_DIR + '/db/dataset/data.csv'

train_data = pd.read_csv(path)

y_train = train_data['label']
x_train = train_data.drop('label', axis=1)

RFC = RandomForestClassifier(n_estimators=100, n_jobs=-1)

RFC.fit(x_train, y_train)

path = BASE_DIR + '/classifiers/gradient_boosting_classifier.pkl'

with open(path, 'wb') as f:
    pickle.dump(RFC, f)",mlscripts/gradient_boosting_classifier.py,KirovVerst/techson_server,1
"
    XPcaList = {}

    for i in range(len(k_mers)):
        XKmer = GetMatrix(X, k_mers[i])   
        pca = PCA(n_components=pca_components[k_mers[i]])
        XPcaList[k_mers[i]] = pca.fit_transform(XKmer)

    # Training Examples
    for index in range(len(k_mers)):
        ClfList.append(RandomForestClassifier(n_estimators=94, max_depth=25,min_samples_split=2, random_state=0))
        X_train = XPcaList[k_mers[index]][test_num:]
        y_train = y[test_num:]

        ClfList[index].fit(X_train,y_train)
    
    y_predict = []
    y_test = y[:test_num]
    for data_index in range(test_num):
        value = 0",Experiment/Sklearn/K-Mer-Essemble-PCA.py,mushroom-x/piRNA,1
"    count_enrollment = df_sub['3COURSEID'].value_counts()
    #print ""Number of %s enrollment: %s""%(subject,count_enrollment)
    
    A = df_sub.as_matrix()
    X = A[:,6:]
    X = X.astype(np.int64, copy=False)
    y = A[:,5]
    y = y.astype(np.int64, copy=False)
    
    #Training data
    forest = RandomForestClassifier(n_estimators=10, max_depth=None, 
                min_samples_split=1, random_state=None, max_features=None)
    clf = forest.fit(X, y)
    with_scores = cross_val_score(clf, X, y, cv=5)
    print with_scores
    print ""Random Forest Cross Validation of %s: %s""%(subject,with_scores.mean())
    precision_rf[subject] = with_scores.mean()
    df_precision_without.loc[subject]=precision_rf[subject]
    print ""-----------------------------------""
        ",pae/final_code/src/compare_merge(schpv).py,wasit7/book_pae,1
"
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)

exported_pipeline = RandomForestClassifier(bootstrap=False, max_features=0.4, min_samples_leaf=1, min_samples_split=9)

exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)",tutorials/tpot_titanic_pipeline.py,weixuanfu2016/tpot,1
"        (""Linear"", LinearRegression()),
        (""DTree"", DecisionTreeClassifier()),
        ('Knn 5', KNeighborsClassifier(5)),
        ('Knn 10', KNeighborsClassifier(10)),
        ('Knn 15', KNeighborsClassifier(15)),
        ('Knn 20', KNeighborsClassifier(20)),
        ('Logistic Regression', LogisticRegression()),
        ('Linear SVM', SVC(kernel='linear', probability=True)),
        ('Poly SVM', SVC(kernel='poly', degree=2, probability=True)),
        ('RBF SVM', SVC(kernel='rbf', gamma=2, C=1, probability=True)),
        ('Random Forest', RandomForestClassifier())]

def compute_score(model):
    # Warning : score computation over 1 class, and not ten most likely ?
    score_dir = ""../database""
    images = listdir(score_dir)
    images.sort()
    
    score = 0
    for x in images:",src/exps.py,rmonat/princess-or-frog,1
"	market_earn=0
	pred_invest=0

	X,y,Z=buildDataSet()
	y=np.array(y)
	print(len(X))

	###algortihm selection for testing and benchmarking

	#clf=svm.SVC(kernel=""poly"",degree=10,C=1)
	clf=RandomForestClassifier(max_features=None, oob_score=True)
	#clf=GradientBoostingClassifier()
	#kf_total = cross_validation.KFold(len(X), n_folds=2,  shuffle=True, random_state=4)
	#clf=NearestCentroid(metric='euclidean', shrink_threshold=None)
	#clf=LogisticRegression()
	#scores=[]
	#scores = cross_validation.cross_val_score(clf, X[:-test_size],y[:-test_size], cv=5)
	#print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
	clf.fit(X[:-test_size],y[:-test_size])
",testing.py,aditya-pai/stockPrediction,1
"        min_trees = args.min_trees
        max_trees = args.max_trees
        step = 0
        divisor = 10

        while step < 1:
            step = int((max_trees - min_trees)/divisor)
            divisor -= 1

        for trees in range(min_trees, max_trees+1, step):
            clf1 = ensemble.RandomForestClassifier(bootstrap=False,
                                                   n_estimators=trees)
            clf2 = ensemble.ExtraTreesClassifier(bootstrap=False,
                                                 n_estimators=trees)

            if select == 'on' or select == 'both':
                pipe1 = Pipeline([('selection', ensemble
                .ExtraTreesClassifier()),
                                  ('classification', clf1)])
                pipe2 = Pipeline([('selection', ensemble",lird.py,vsaw/LIRD,1
"
# remove the feature indices which are in the form 'feature_no:feature_value'
for i in xrange(0, len(train_inputs)):
	for j in xrange(0, len(train_inputs[i])):
		train_inputs[i][j] = float(train_inputs[i][j].split(':')[1])

for i in xrange(0, len(test_inputs)):
	for j in xrange(0, len(test_inputs[i])):
		test_inputs[i][j] = float(test_inputs[i][j].split(':')[1])

rf = RandomForestClassifier(n_estimators = 20, max_features = 'auto')
print '... training random forest'
rf.fit(train_inputs, train_outputs)

print '... testing'
confusion_matrix = numpy.zeros([2,2])
correct = 0
total = len(test_inputs)
for i in xrange(0, len(test_inputs)):
	prediction = rf.predict(test_inputs[i])",rf_fileIO.py,therainmak3r/quora-AnswerClassifier,1
"trans = LinearDiscriminantAnalysis(n_components=3)
trans.fit(X,y)
X = trans.transform(X)

# Split Up Data
x_train,x_valid,y_train,y_valid = train_test_split(X,y,test_size=0.3,random_state=None)

# Train classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsOneClassifier
clf = OneVsOneClassifier(RandomForestClassifier(n_estimators=100,n_jobs=5))
clf.fit(x_train,y_train)

# Run Predictions
from sklearn.metrics import confusion_matrix, accuracy_score
y_preds = clf.predict(x_valid)
print( confusion_matrix(y_valid,y_preds) );
print( ""Accuracy: %f"" % (accuracy_score(y_valid,y_preds)) );
f = open('randomForest_take3.txt', 'w')
f.write( str(confusion_matrix(y_valid,y_preds)) );",prototype_alpha/randomForest_take3.py,valexandersaulys/airbnb_kaggle_contest,1
"    # names = ['NearestNeighbors', 'DecisionTree', ""RandomForest"", ""AdaBoost""]
    hyper_parameter = PIPELINE_CONFIG.HYPER_PARAMETER
    if verbose:
        print(hyper_parameter, end=', ')
    names = [""NearestNeighbors"", ""RBF SVM"",
             ""DecisionTree"", ""RandomForest"", ""AdaBoost"", ""KNN"", ""RadiusNeighbors""]
    classifiers = [
        KNeighborsClassifier(hyper_parameter),
        SVC(gamma=hyper_parameter, C=1),
        DecisionTreeClassifier(max_depth=hyper_parameter),
        RandomForestClassifier(max_depth=hyper_parameter, n_estimators=10, max_features=1),
        AdaBoostClassifier(n_estimators=hyper_parameter),
        KNN(num_neighbors=hyper_parameter, weight='distance'),
        RadiusNeighborsClassifier(hyper_parameter)]
    classifier_pool = dict(zip(names, classifiers))

    selected = PIPELINE_CONFIG.MODEL_NAME

    results = dict(zip(selected, [0 for _ in range(len(selected))]))
    counter = 0",decoding/pipeline.py,colpain/NeuralDecoding,1
"    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


results = []
for clf, name in (
        (RidgeClassifier(tol=1e-2, solver=""lsqr""), ""Ridge Classifier""),
        (Perceptron(n_iter=50), ""Perceptron""),
        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        #(KNeighborsClassifier(n_neighbors=10, n_jobs=-1, leaf_size=120), ""kNN""),
        (RandomForestClassifier(n_estimators=100, n_jobs=-1), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,",classifier_university_comments.py,denimalpaca/293n,1
"            train.loc[train_series.isnull(), train_name] = -999
        #and Test
        tmp_len = len(test[test_series.isnull()])
        if tmp_len>0:
            test.loc[test_series.isnull(), test_name] = -999


kf = KFold(train.shape[0], n_folds=3, random_state=1)
alg = tree.DecisionTreeRegressor()
 
#RandomForestClassifier(n_estimators=100,criterion= 'entropy')

predictions = []
for trainkf, test in kf:
	print trainkf
	# The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.
	train_predictors = (train.iloc[trainkf,:])
	# The target we're using to train the algorithm.
	train_target = target.iloc[trainkf]
	print train_predictors.shape[0],train_predictors.shape[1]",other methods with not so great accuracy/decisiontree.py,souravsarangi/BNPParibasKaggle,1
"    print ""Cross Domain""
    # documents_stack=stack_data.values() 
    # documents_wiki=wiki_data.values()
    PolitenessFeatureVectorizer.generate_bow_features(documents_stack, bow)

    X_stack, y_stack = documents2feature_vectors(documents_stack)
    X_wiki, y_wiki = documents2feature_vectors(documents_wiki)

    print ""Fitting""
    clf = svm.SVC(C=0.02, kernel='linear', probability=True)
    # clf = RandomForestClassifier(n_estimators=50)
    clf.fit(X_stack, y_stack)
    y_pred = clf.predict(X_wiki)
    print ""Trained on Stack and results predicted for wiki"" 
    # Test
    #print(classification_report(y_wiki, y_pred))
    print(clf.score(X_wiki, y_wiki))

    print ""------------------------------------------------------""
",train_and_test.py,nidishrajendran/computational-politeness,1
"
        return relations
        
    def _create_classifier(self):
        if self._meta[""method""] == ""LogisticRegressionL2"": clf = LogisticRegression(penalty='l2', tol=0.0001, C=1.0)  #, class_weight='auto')
        elif self._meta[""method""] == ""LogisticRegressionL1"": clf = LogisticRegression(penalty='l1', tol=0.0001, C=1.0)  #, class_weight='auto')
        elif self._meta[""method""] == ""MultinomialNB"": clf = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)
        elif self._meta[""method""] == 'LinearSVC': clf = LinearSVC(C=1.0)
        elif self._meta[""method""] == 'SVC':  clf = SVC(C=1.0, probability=True)
        elif self._meta[""method""] == 'SVCLinear': clf = SVC(kernel='linear', C=1.0, probability=True)
        elif self._meta[""method""] == 'RandomForest': clf = RandomForestClassifier(n_estimators = 100)
        elif self._meta[""method""] == 'Dummy': clf = DummyClassifier(strategy='most_frequent', random_state=0)
        elif self._meta[""method""] == 'GradientBoosting': clf = GradientBoostingClassifier()
        elif self._meta[""method""] == 'AdaBoost': clf = AdaBoostClassifier()
        elif self._meta[""method""] == 'RandomTreesEmbedding': clf = RandomTreesEmbedding()
        elif self._meta[""method""] == 'Bagging': clf = BaggingClassifier()  
        else: clf = LogisticRegression(penalty='l2', tol=0.0001, C=1.0)  # , class_weight='auto')
        return clf 
    
    def _print_clf_info(self):",jnt/isas/supervised.py,tudarmstadt-lt/taxi,1
"    
    data_train_size_mb = size_mb(data_train)
    data_test_size_mb = size_mb(data_test)
    print('data loaded - train: {0}Mb test: {1}Mb'.format(data_train_size_mb, data_test_size_mb))
    
    g2v = GensimDoc2Vec(cores=2)
    g2v.fit(data_train)
    
    # Make a pipeline with the classifiers
    pipeline = Pipeline([ ('vectorizer', g2v),
                           ('clf', RandomForestClassifier(max_features='auto', n_estimators=100))
                        ])
    
    # Fit the pipeline
    pipeline.fit(data_train, target_train)
    
    # Now measure the precision
    pred = pipeline.predict(data_test)
    print ""ACCURACY "",accuracy_score(target_test, pred)
    ",gensim_sklearn.py,linucks/textclass,1
"    },
    'decision_tree': {
        'clf': DecisionTreeClassifier(),
        'param_grid': {'max_depth': range(3, 11)},
    },
    'knn': {
        'clf': KNeighborsClassifier(),
        'param_grid': {'n_neighbors': range(1, 11)},
    },
    'random_forest': {
        'clf': RandomForestClassifier(),
        'param_grid': {
            'max_depth': np.linspace(4, 10, num=4, dtype=int),
            'max_features': np.linspace(4, 8, num=3, dtype=int),
            'n_estimators': np.linspace(50, 250, num=5, dtype=int),
        },
    },
    'svm_linear': {
        'clf': LinearSVC(),
        'param_grid': {'C': np.logspace(-2, 4, num=7)},",main.py,jeremyn/kaggle-titanic,1
"    
    scores= {'train': [], 'test': []}
    for i in range(25):
        X_train= np.array([data_dict['data'][inner_cv['X_train'][i][j]]\
                          for j in range(len(inner_cv['X_train'][i]))])
        X_test= np.array([data_dict['data'][inner_cv['X_test'][i][j]]\
                         for j in range(len(inner_cv['X_test'][i]))])
        y_train= inner_cv['y_train'][i]
        y_test= inner_cv['y_test'][i]

        est = ensemble.RandomForestClassifier()
        est.fit(X_train, y_train)
        scores['train'].append(est.score(X_train, y_train))
        scores['test'].append(est.score(X_test, y_test))
    
    with open('rf_scores.pickle','wb') as f:
        pickle.dump(scores, f, pickle.HIGHEST_PROTOCOL) 

    return
",estimators.py,jrabenoit/shopvec,1
"        return clf
    
class TSRandomForestClassifier(TSClassifier):
    '''
    A RandomForest model
    ''' 
    criterion = 'entropy'
    n_estimators = 48
    
    def classifier(self):
        clf = RandomForestClassifier(
            n_estimators=self.n_estimators, 
            min_samples_split=1, max_depth=None,
            criterion=self.criterion,
            n_jobs=-1, random_state=42, verbose=2)
        return clf

class TSSVCRandomForestClassifier(TSClassifier):
    '''
    A RandomForest model with features first selected by LinearSVC",scripts/predictors.py,timpalpant/KaggleTSTextClassification,1
"
end = time.time()
print ""Time Taken for Feature Extraction : "", end-start

train_feature_list = [train_data_bag,train_data_tfidf]
test_feature_list = [test_data_bag,test_data_tfidf]
feature_names = [""Bag Of Words"", ""Tf-Idf""]

#Initializing the classifiers 

rf = RandomForestClassifier(n_estimators=51,random_state=1)
svm = SVC(kernel=""linear"",probability=True)
mnb = MultinomialNB(fit_prior=True)
ada = AdaBoostClassifier(random_state=1)

#Creating an estimator list for Voting Classifiers 

classifier_names = [""Random Forests"",""SVM"",""Multinomial NB"",""Adaboost""]
classifiers = [rf,svm,mnb,ada]
estimator_list = zip(classifier_names,classifiers)",PizzaTextModel.py,rupakc/Kaggle-Random-Acts-of-Pizza,1
"
print np.shape(training_array)

# Create final training data set and test data set
training_array = training_array[:-100]
test_dataset = training_array[-100:]

print training_category

# Create random forest
cfr = RandomForestClassifier(n_estimators=500, n_jobs=-1)
the_forest = cfr.fit(training_array[2000:2100, 1:], training_category[2000:2100])

predictions = the_forest.predict(test_dataset[:, 1:])
predictions = np.vstack(predictions)

test_fb_id = np.vstack(test_dataset[:, 0])

print np.concatenate((test_fb_id, predictions), axis=1)
",Category_detection.py,Lothilius/oiPy,1
"    # Remove rows with non-numeric data in any rogue field.
    train_x = train_x[train_x.applymap(lambda x: isinstance(x, (int, float))).all(1)].fillna(value=0)
    test_x = test_x[test_x.applymap(lambda x: isinstance(x, (int, float))).all(1)].fillna(value=0)
    log.debug('Testing and Training data have removed all non int and float rows.')

    classifier_models = [{'name': 'Logistic Regression Classifier',
                          'object': linear_model.LogisticRegression()},
                         {'name': 'Nearest Neighbors Classifier',
                          'object': KNeighborsClassifier()},
                         {'name': 'Random Forest Classifier',
                          'object': RandomForestClassifier(n_estimators=80, random_state=87, n_jobs=-1)}]

    #Create Default Models
    for model in classifier_models:
        #Get Trained Model from File\
        if os.path.isfile('pickled_objects/%s_classifier' % model['name']):
            clf = pickle.load(open('pickled_objects/%s_classifier' % model['name'], 'rb'))
        else:
            clf = model['object']
",modeling.py,mcrowson/predict-kiva,1
"X_des = X_des_norm.astype(int) 

results = []
# 
sample_leaf_options = list(range(1, 50, 3))
# 
n_estimators_options = list(range(1, 10, 5))
for leaf_size in sample_leaf_options:
    for n_estimators_size in n_estimators_options:
        
        rfc = RandomForestClassifier(min_samples_leaf=leaf_size, n_estimators=n_estimators_size, random_state=50)
        rfc.fit(X_train,y_train)

        y_pred_class = rfc.predict(X_test)
        results.append((leaf_size, n_estimators_size, (y_test == y_pred_class).mean()))        

print(max(results, key=lambda x: x[2]))

rfc = RandomForestClassifier(min_samples_leaf=6, n_estimators=6, random_state=50)
rfc.fit(X_train,y_train)",Digit Recognizer/Digit Recognizer.py,rickyhan83/ml_kaggle,1
"    p3 = df.iloc[i, 4]
    p4 = df.iloc[i, 5]
    p5 = df.iloc[i, 6]
    p6 = df.iloc[i, 7]
    p7 = df.iloc[i, 8]
    # save dict of model hyperparameters
    clf_dict = {'criterion': p1, 'bootstrap': p2, 'n_estimators': p3,
    'max_depth': p4, 'max_features': p5, 'min_samples_split': p6,
    'min_samples_leaf': p7}
    # set up model using hyperparameters
    clf = RandomForestClassifier(n_jobs=-1, random_state=0, criterion=p1,
                                 bootstrap=p2, n_estimators=p3, max_depth=p4, max_features=p5, min_samples_split=p6,
                                 min_samples_leaf=p7)
    # get training and test set from list
    xtrain = x_train_sets[i]
    xtest = x_test_sets[i]
    # train model
    clf.fit(xtrain, y_train)
    # get training and test predictions
    preds_train = clf.predict(xtrain)",src/models/ml_test.py,mworles/capstone_one,1
"    def __init__(self):
        # images_train=data_train[:,1:]
        # trainX, _trainX, trainY, _trainY = train_test_split(images_train/255.,values_train,test_size=0.5)

        # #load test.csv
        # test = pd.read_csv(""data/test.csv"")
        # data_test=test.as_matrix()
        # testX, _testX = train_test_split(data_test/255.,test_size=0.99)
        
        # Random Forest
        # self.clf = RandomForestClassifier()
        
        # Stochastic Gradient Descent
        # self.clf = SGDClassifier()
        
        # Support Vector Machine
        # self.clf = LinearSVC()
        
        # Nearest Neighbors
        # self.clf = KNeighborsClassifier(n_neighbors=13)",redigit/clf.py,osgee/redigit,1
"### visualization code (prettyPicture) to show you the decision boundary
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from time import time
t0 = time()
t1 = time()
print ""start""
print ""training""
clf_dt = DecisionTreeClassifier(min_samples_split = 40).fit(features_train,labels_train)
clf_rf = RandomForestClassifier(n_estimators=10).fit(features_train,labels_train)
clf_svc = SVC(kernel=""rbf"", C=80000).fit(features_train,labels_train)

print ""training time:"", round(time()-t0, 3), ""s""

print ""predicting""
pred_dt = clf_dt.predict(features_test)
pred_rf = clf_rf.predict(features_test)
pred_svc = clf_svc.predict(features_test)
print ""predicting time:"", round(time()-t1, 3), ""s""",choose_your_own/your_algorithm.py,junhua/udacity_intro_to_ml,1
"    # Generate dummy dataset
    np.random.seed(123)
    ids = np.arange(n_samples)
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(2, size=(n_samples, n_tasks))
    w = np.ones((n_samples, n_tasks))
  
    dataset = dc.data.NumpyDataset(X, y, w, ids)
    classification_metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

    sklearn_model = RandomForestClassifier()
    model_dir = tempfile.mkdtemp()
    model = dc.models.SklearnModel(sklearn_model, model_dir)

    # Fit trained model
    model.fit(dataset)
    model.save()

    # Load trained model
    reloaded_model = dc.models.SklearnModel(None, model_dir)",deepchem/models/tests/test_reload.py,rbharath/deepchem,1
"
        expected_eval_outputs = eval_df.Survived.values
        eval_df = eval_df.drop([""PassengerId"", ""Survived"",\
                                ""Ticket"", ""Cabin""],
                                axis=1)

        train_data = normalise_data(train_df).values
        eval_data = normalise_data(eval_df).values


        forest = RandomForestClassifier(n_estimators=200,
                                        n_jobs=-1,
                                        criterion=""entropy"")

        forest = forest.fit(train_data, expected_training_outputs)


        evaluation = forest.predict(eval_data)

        em = EvaluationMetrics(evaluation, expected_eval_outputs)",src/classifiers.py,JakeCowton/titanic,1
"
    def train_test_val_RFs(self, xTrain, yTrain, xVal, yVal, experiments_folder, save_prefix = ''):

        #INITIALISE THE VALIDATION FORESTS
        if useOMAForest:
            self.val_RFs_OMA = [rf.Forest(rf_params_obj) for x in range(self.n_val_forests)]
            self.val_RFs_OMA_AUC = np.zeros((self.n_val_forests))
            self.val_RFs_OMA_TE = np.zeros((self.n_val_forests))

        if useSKLForest:
            self.val_RFs_SKL = [RandomForestClassifier(n_estimators=rf_max_num_trees, criterion=rf_criterion, max_depth=rf_max_depth,
                                                       min_samples_split=rf_min_sample_count, max_features=rf_no_active_vars, oob_score=False)
                                for x in range(self.n_val_forests)]
            self.val_RFs_SKL_AUC = np.zeros((self.n_val_forests))
            self.val_RFs_SKL_TE = np.zeros((self.n_val_forests))


        for rf_idx, rf_name in enumerate(self.val_RFs_names):

            print 'training forest', rf_idx+1, 'of', self.n_val_forests, ':', rf_name, ', with cost:', self.val_RFs_costs[rf_idx]",run_CSERF.py,lionelBytes/CSERF,1
"Run with:
  bin/spark-submit examples/src/main/python/ml/random_forest_example.py
""""""


def testClassification(train, test):
    # Train a RandomForest model.
    # Setting featureSubsetStrategy=""auto"" lets the algorithm choose.
    # Note: Use larger numTrees in practice.

    rf = RandomForestClassifier(labelCol=""indexedLabel"", numTrees=3, maxDepth=4)

    model = rf.fit(train)
    predictionAndLabels = model.transform(test).select(""prediction"", ""indexedLabel"") \
        .map(lambda x: (x.prediction, x.indexedLabel))

    metrics = MulticlassMetrics(predictionAndLabels)
    print(""weighted f-measure %.3f"" % metrics.weightedFMeasure())
    print(""precision %s"" % metrics.precision())
    print(""recall %s"" % metrics.recall())",examples/src/main/python/ml/random_forest_example.py,pronix/spark,1
"    #clf = LogisticRegression()
    y_pred = clf.fit(tr_x,tr_y).predict(te_x)
    print 'predicted ',sum(y_pred),' positive item.'
    print 'precision_score : ',precision_score(te_y,y_pred)
    print 'recall_score : ',recall_score(te_y,y_pred)
    print 'f1_score : ',f1_score(te_y,y_pred)
    save_pred(y_pred)

def RF_predictor(tr_x,tr_y,te_x,te_y):
    '''Random Forest'''
    #clf = RandomForestClassifier(n_estimators=200,max_features=""auto"",max_depth=8,min_samples_split=10,min_samples_leaf=2,n_jobs=2)
    clf = RandomForestClassifier(n_estimators=15,max_depth=8,n_jobs=2)
    print ""start to fit the metricx""
    clf.fit(tr_x,tr_y)
    print ""start to predict ..""
    y_pred = clf.predict(te_x)
    print '************************'
    print 'predict ',sum(y_pred),' positive item.'
    print 'p_score : ',precision_score(te_y,y_pred)
    print 'r_score : ',recall_score(te_y,y_pred)",pycode/prediction.py,CharLLCH/jianchi_alimobileR,1
"        traxelIdPerTimestepToUniqueIdMap, uuidToTraxelMap = hytra.core.jsongraph.getMappingsBetweenUUIDsAndTraxels(self.model)
        # there might be empty frames. We want them as output too.
        timesteps = [str(t).decode(""utf-8"") for t in range(int(min(traxelIdPerTimestepToUniqueIdMap.keys())), max([int(idx) for idx in traxelIdPerTimestepToUniqueIdMap.keys()]) + 1)]
                
        # compute new object features
        objectFeatures = self._computeObjectFeatures(timesteps)

        # load transition classifier if any
        if transition_classifier_filename is not None:
            getLogger().info(""\tLoading transition classifier"")
            transitionClassifier = probabilitygenerator.RandomForestClassifier(
                transition_classifier_path, transition_classifier_filename)
        else:
            getLogger().info(""\tUsing distance based transition energies"")
            transitionClassifier = None

        # run min-cost max-flow to find merger assignments
        getLogger().info(""Running min-cost max-flow to find resolved merger assignments"")

        nodeFlowMap, arcFlowMap = self._minCostMaxFlowMergerResolving(objectFeatures, transitionClassifier)",hytra/core/ilastikmergerresolver.py,chaubold/hytra,1
"# and randomly permute it
boston = datasets.load_boston()
perm = rng.permutation(boston.target.size)
boston.data = boston.data[perm]
boston.target = boston.target[perm]


def test_classification_toy():
    """"""Check classification on a toy dataset.""""""
    # Random forest
    clf = RandomForestClassifier(n_estimators=10, random_state=1)
    clf.fit(X, y)
    assert_array_equal(clf.predict(T), true_result)
    assert_equal(10, len(clf))

    clf = RandomForestClassifier(n_estimators=10, max_features=1,
                                 random_state=1)
    clf.fit(X, y)
    assert_array_equal(clf.predict(T), true_result)
    assert_equal(10, len(clf))",python/sklearn/sklearn/ensemble/tests/test_forest.py,seckcoder/lang-learn,1
"df = pd.read_csv(url, names=names)
array = df.values
X = array[:,0:8]
y = array[:,8]

seed = 21
num_trees = 100
max_features = 3

kfold = model_selection.KFold(n_splits=10, random_state=seed)
model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)
results = model_selection.cross_val_score(model, X, y, cv=kfold)
print('results: ')
print(results)
print()",Python/Machine Learning/ScikitClassifiers/Classifiers/Random_Forrest_Classification.py,sindresf/The-Playground,1
"
    print(trainX.shape, trainY.shape)
    print(""..."")

    
#    clf = KNeighborsClassifier() 
    from sklearn.linear_model import LogisticRegression  
    clf = OneVsOneClassifier(LogisticRegression(penalty='l2'))
    clf = LogisticRegression(penalty='l2')
    from sklearn.ensemble import RandomForestClassifier  
    # clf = RandomForestClassifier(n_estimators=8)  
    # clf = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,gamma=0.0, kernel='rbf', probability=False, shrinking=True, tol=0.001,verbose=False)
#    clf = OneVsRestClassifier(svm.SVC(0.1))
#    clf = OneVsOneClassifier(svm.SVC(C=0.07, kernel = 'linear'))
    
    clf.fit(trainX, trainY)

    print(""..."")
#    for i in range(trainFlieNum,fileNum):
    for i in range(0,fileNum):        ",Uniwalk_pre/1. create_data.py,songjs1993/DeepSimRank,1
"                    missing_data)

    def validate_rf(self, n_trees):
        score = 0.0
        for train_index, test_index in LeaveOneOut(len(self.class_labels)):
            X_train = self.feature_matrix[train_index]
            X_test = self.feature_matrix[test_index]
            y_train = np.array(self.class_labels)[train_index]
            y_test = np.array(self.class_labels)[test_index]
            
            rf = RandomForestClassifier(n_estimators=n_trees)
            rf.fit(X_train, y_train)
            score += rf.score(X_test, y_test)
            
        return score / len(self.class_labels)

    def select_from_snps(self, snps):
        snp_labels = set(snps.labels)
        selected_indices = []
        selected_feature_labels = []",aranyani/models.py,rnowling/aranyani,1
"
codon_identifier = dict(("""".join(v), k)
                        for k, v in enumerate(itertools.product('ATGC', repeat=3)))


class Classifier(object):
    """"""This is a classifier for codon reassignment""""""

    def __init__(self, method, classifier_spec={}, scale=False, n_estimators=1000):
        if method == 'rf':
            self.clf = RandomForestClassifier(
                n_estimators=n_estimators, n_jobs=-1, max_leaf_nodes=1000, **classifier_spec)
        elif method == 'svc':
            self.clf = svm.SVC(probability=True, **classifier_spec)
        elif method == 'etc':
            self.clf = ExtraTreesClassifier(
                n_estimators=n_estimators, **classifier_spec)
        elif method == 'gnb':
            self.clf = GaussianNB()
        else:",coretracker/classifier/classifier.py,UdeM-LBIT/CoreTracker,1
"#Training classifier (first argument to the script) against a train_path (second argument)

from my_fun import *
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.externals import joblib
import sys

train_path = sys.argv[1]

clfRandom = RandomForestClassifier(n_estimators=95)
print ""clfRandom created""

clfRandom = train_obj(train_path, clfRandom)
#9 stands for maximum compression
joblib.dump(clfRandom, ""forest.xml"", 9)",python/classifiers_train.py,parloma/robotcontrol,1
"
    def predict(self, xhat, yhat):
        pred = self.model.predict(xhat)
        print 'Precision, recall, F1:'
        for f in [precision_score, recall_score, f1_score]:
            print f(yhat, pred),
        print

    def run_model_tests(self, models=None):
        if not models:
            models = [LogisticRegression(), LogisticRegressionCV(), SVC(), LinearSVC(), RandomForestClassifier()]
        x, y, xhat, yhat = self.load_data()

        for model in models:
            print model
            print 'Training...'
            self.train_model(x, y, model=model)
            self.predict(xhat, yhat)",python/document_predictor.py,kiankd/events,1
"                            dt_train = feature_combiners[cl].fit_transform(dt_train_cluster)
                            encode_time_train = time() - start
                        else:
                            dt_train = feature_combiners[cl].transform(dt_train_cluster)

                        start = time()
                        dt_test = feature_combiners[cl].transform(dt_test_cluster)
                        encode_time_test = time() - start

                        #### FIT CLASSIFIER ####
                        cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=rf_max_features, random_state=random_state)
                        cls.fit(dt_train, train_y)

                        #### PREDICT ####
                        start = time()
                        if len(train_y.unique()) == 1:
                            hardcoded_prediction = 1 if train_y[0] == pos_label else 0
                            current_cluster_preds = [hardcoded_prediction] * len(relevant_test_cases)
                        else:
                            # make predictions",experiments_param_optim_cv/run_state_optim_cv.py,irhete/predictive-monitoring-benchmark,1
"        df = pd.read_csv(path)
    X = df.values
    X_test, ids = X[:, 1:], X[:, 0]
    return X_test.astype(float), ids.astype(str)


def train():
    X_train, X_valid, y_train, y_valid = load_train_data()
    # Number of trees, increase this to beat the benchmark ;)
    n_estimators = 10
    clf = RandomForestClassifier(n_estimators=n_estimators)
    print("" -- Start training Random Forest Classifier."")
    clf.fit(X_train, y_train)
    y_prob = clf.predict_proba(X_valid)
    print("" -- Finished training."")

    encoder = LabelEncoder()
    y_true = encoder.fit_transform(y_valid)
    assert (encoder.classes_ == clf.classes_).all()
",benchmark.py,basvb/Ottogroup,1
"        else:
            train_sizes = []

        train_scores = []
        valid_scores = []

        for size in train_sizes:
            x_sample, y_sample = self._create_learning_curve_sample(x, y, size)
            x_train, x_test, y_train, y_test = cross_validation.train_test_split(x_sample, y_sample, test_size=0.1)

            clf = RandomForestClassifier(n_jobs=-1)

            clf.fit(x_train, y_train)

            y_test_predicted = clf.predict(x_test)
            y_train_predicted = clf.predict(x_train)

            valid_scores.append(metrics.accuracy_score(y_test, y_test_predicted))
            train_scores.append(metrics.accuracy_score(y_train, y_train_predicted))
",soothsayer/classifiers.py,shawnhermans/soothsayer,1
"
h = .05  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]
'''
X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
			random_state=1, n_clusters_per_class=1)
'''   
# X is 1084 x 2",scikit_code/plot_classifier_comparison.py,chakpongchung/RIPS_2014_BGI_source_code,1
"pred_test = lm.predict(X_test)


# In[18]:

#Random Forest
y_random = df_filled['multiple']
X_random = df_filled[['month', 'day','region','property','propextent','attacktype1','weaptype1','nperps','specificity' ]]
features_train, features_test,target_train, target_test = train_test_split(X_random,y_random, test_size = 0.2,random_state=0)
#Random Forest
forest=RandomForestClassifier(n_estimators=10)
forest = forest.fit( features_train, target_train)
output = forest.predict(features_test).astype(int)
forest.score(features_train, target_train )

# In[15]:

from patsy import dmatrices
from sklearn.linear_model import LogisticRegression
#Logistic Regression",Terrorisks/flaskr/flaskr/BT4221.py,chloeyangu/BigDataAnalytics,1
"        model = LogisticRegression()
    elif modeltype == 'rbf': ## SVM RBF kernel
        print('# SVM (RBF kernel) model')
        tuned_params = [{'kernel':['rbf'], 'C':np.logspace(0, 2, 20),
                         'gamma':np.logspace(-5, -3, 10)},]
        model = SVC(probability=False)
    elif modeltype == 'rf':  ## Random Forest
        print('# Random Forest model')
        tuned_params = [{'n_estimators': range(100, 200, 10),
                         'max_features': ['auto', 'log2']}]  ## auto == sqrt
        model = RandomForestClassifier(oob_score=True, n_jobs=jobs)
    else:
        print('model type: [lr|rbf|rf]')
        sys.exit(-1)
        
    print('# number of folds: %s' % (cv,))
    print('# params grid: %s' % (tuned_params,))

    print(""# Tuning hyper-parameters for accuracy\n"")
",classifier/bin/train_model.py,wellflat/cat-fancier,1
"
    Arguments:
    - `data`:
    """"""

    predictors = [""overlapping"",""reoccur1"", ""reoccur2"", ""reoccur3"",""reoccur4"", ""reoccur5"", ""euclidean"",""refuting_feature_count"",""char_length_headline"",""char_length_body""]#,""cosine""]#,""wmdistance"", ""euclidean""]
    response = train.Stance

    _test = test[[""overlapping"", ""reoccur1"", ""reoccur2"", ""reoccur3"",""reoccur4"", ""reoccur5"",""euclidean"",""refuting_feature_count"",""char_length_headline"",""char_length_body""]]#,""cosine""]] #,""wmdistance"", ""euclidean""]]

    clf = RandomForestClassifier(n_jobs=2)
    clf.fit(train[predictors],response)
    _predictions = clf.predict(_test)

    predictions = pd.Series(_predictions.tolist())
    test[""predicted_RF""] = predictions.values

    test[""is_correct_prediction_RF""] = test[""Stance""] == test[""predicted_RF""]
    correctly_predicted_rows = test[test['is_correct_prediction_RF'] == True]
",code/python/classifier.py,srjit/fakenewschallange,1
"    raw_train_data = normaliseAndScale(raw_train_data)

    print len(raw_train_data), len(raw_train_data[""CCA_f1""])
    print len(raw_test_data), len(raw_test_data[""CCA_f1""])
    print len(train_labels)
    print len(test_labels)

    if multiclass:
        # model = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=1000, max_samples=0.2, max_features=1.0)
        # model = AdaBoostClassifier(n_estimators=1000, learning_rate=0.1, base_estimator=DecisionTreeClassifier(max_depth=1))
        model = RandomForestClassifier(n_estimators=1000, max_depth=10)
        # model = LinearSVC()
        svm = LinearSVM(C=1)
        # svm = SGDClassifier(shuffle=True, loss=""hinge"")
        # svm = LogisticRegression()

        svm.classes_ = [1, 2, 3]

        pre_model = BernoulliRBM(learning_rate=0.1, n_components=5, n_iter=20)
        # pipeline = MyPipeline(steps=[(""rbm"", pre_model), (""svm"", svm)])",src/main.py,kahvel/MAProject,1
"
		

		inc = size/cv
		res = []
		for k in range(cv):
			#divide data
			traintrips, target, testtrips, testtarget = self.splitData(dataset, labels, k)
			
			#set up classifier
			clf = RandomForestClassifier(n_estimators=500)
			clf.fit(traintrips, target)
			predLabels = clf.predict (testtrips)
			#print predLabels
			#print testtarget

			#save results
			res.append(self.calculateResults(predLabels, testtarget))
			#print self.calculateResults(predLabels, testtarget)
		return res",Driver.py,mkery/CS349-roads,1
"    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    # LogisticRegression(random_state=0),
    # DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    # GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsClassifier(),
    # GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/trial1.py,diogo149/CauseEffectPairsPaper,1
"Using it is fairly simple and does not require changing anything that is already using sklearn,
but beware that we are monkey-patching sklearn BaseEstimator to add the what method.

Example
-------

>>> # Monkey-patch sklearn estimators
>>> whatamise_sklearn()
>>> # Use what() to retrieve the id of a model
>>> from sklearn.ensemble import RandomForestClassifier
>>> rfc = RandomForestClassifier(n_jobs=8, n_estimators=23)
>>> print(rfc.what().id())
rfc(bootstrap=True,class_weight=None,criterion='gini',max_depth=None,max_features='auto',max_leaf_nodes=None,min_impurity_split=1e-07,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,n_estimators=23,random_state=None,warm_start=False)
>>> print(rfc.what())
rfc(bootstrap=True,class_weight=None,criterion='gini',max_depth=None,max_features='auto',max_leaf_nodes=None,min_impurity_split=1e-07,min_samples_leaf=1,min_samples_split=2,min_weight_fraction_leaf=0.0,n_estimators=23,n_jobs=8,oob_score=False,random_state=None,verbose=0,warm_start=False)

Implementation Notes
--------------------

scikit-learn, in the good tradition of machine learning libraries (e.g. good old weka)",whatami/wrappers/what_sklearn.py,sdvillal/whatami,1
"from text.sentence import Sentence

pp = pprint.PrettyPrinter(indent=4)
text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(7,20), min_df=0.2, max_df=0.5)),
                             #('vect', CountVectorizer(analyzer='word', ngram_range=(1,5), stop_words=""english"", min_df=0.1)),
                             #     ('tfidf', TfidfTransformer(use_idf=True, norm=""l2"")),
                                  #('tfidf', TfidfVectorizer(analyzer='char_wb', ngram_range=(6,20))),
                                  #('clf', SGDClassifier(loss='hinge', penalty='l1', alpha=0.01, n_iter=5, random_state=42)),
                                  #('clf', SGDClassifier())
                                  #('clf', svm.SVC(kernel='rbf', C=10, verbose=True, tol=1e-5))
                                  #('clf', RandomForestClassifier(n_estimators=10))
                                    #('feature_selection', feature_selection.SelectFromModel(LinearSVC(penalty=""l1""))),
                                  ('clf', MultinomialNB(alpha=0.1, fit_prior=False))
                                  #('clf', DummyClassifier(strategy=""constant"", constant=True))
                                 ])
class SeeDevCorpus(Corpus):
    """"""
    Corpus for the BioNLP SeeDev task
    self.path is the base directory of the files of this corpus.
    """"""",src/reader/seedev_corpus.py,AndreLamurias/IBEnt,1
"    Validate = self.data[self.data['is_train'] == False]

    self.generate_features()
    self.x_train = Train[list(self.features)].values
    self.y_train = Train['Bootcamp'].values
    self.x_validate = Validate[list(self.features)].values
    self.y_validate = Validate['Bootcamp'].values

  def set_classifier(self):
    random.seed(100)
    random_forest = RandomForestClassifier(n_estimators=1000)
    random_forest.fit(self.x_train, self.y_train)
    self.classifier = random_forest

  def set_roc(self):
    status = self.get_status(self.x_validate)
    fpr, tpr, _ = roc_curve(self.y_validate, status[:,1], pos_label=1)
    self.roc_auc = auc(fpr, tpr)

  def get_status(self, data):",crystal_gazer/gazer.py,mfalade/Crystal-Gazer,1
"
    estimated_hgts = learner.predict(estmat)
    print estimated_hgts
    print learner
    return estimated_hgts


def run(ngenes=50, ntaxa=5):
    testdata = gen_test_data(ntaxa=ntaxa, ngenes=ngenes, nreps=1)

#    learner = ensemble.RandomForestClassifier(n_estimators=100)
    learner = svm.SVC()

    ehgt = pipeline(testdata[0][0], testdata[1][0], testdata[2][0], testdata[3][0], testdata[4], learner = learner)

    print ehgt

    print testdata[3]
    sources = [i[0] for i in testdata[3][0]]
",simhgt.py,pranjalv123/hgt-nn,1
"    train_X, train_y, train_pairs = generate_X_y(train_pairs)
    print(train_X, file=sys.stderr)
    print(train_y, file=sys.stderr)
    print(train_X.shape, train_y.shape, file=sys.stderr)
    test_X, test_y, test_pairs = generate_X_y(test_pairs)
    print(test_X, file=sys.stderr)
    print(test_y, file=sys.stderr)
    print(test_X.shape, test_y.shape, file=sys.stderr)
    
    # random forest training
    rfc = RandomForestClassifier(n_estimators=500, n_jobs=-1)
    rfc.fit(train_X, train_y)
    
    # save and load back the pickled model
    joblib.dump(rfc, model)
    rfc = joblib.load(model) 

    # random forest test
    test_p = rfc.predict_proba(test_X)
    ",train_test_random_forest.py,zhujianwei31415/deepfr,1
"                                  max_depth=[2**i for i in xrange(1, 10)],
                                  class_weight=['balanced']
                                  )
            rfc_parameters = dict(n_estimators=[2**i for i in xrange(1, 10)],
                                  criterion=('gini', 'entropy'),
                                  max_features=('sqrt', 'log2', None),
                                  class_weight=['balanced']
                                  )
            # train DT and RF models using grid search cross validation
            dtc = GridSearchCV(DecisionTreeClassifier(), dtc_parameters)
            rfc = GridSearchCV(RandomForestClassifier(), rfc_parameters)
            dtc.fit(train_data[:, :-1], train_data[:, -1].astype(int))
            rfc.fit(train_data[:, :-1], train_data[:, -1].astype(int))

            # predict with cv'ed models
            dt_y_test_hat = dtc.predict(test_data[:, :-1])
            rf_y_test_hat = rfc.predict(test_data[:, :-1])

            # compute objective function
            dt_obj_val = (sum(dt_y_test_hat != test_data[:, -1]) /",predict_with_best_dt_and_rf.py,rafaelvalle/MDI,1
"def training_and_test(token, train_data, test_data, num_classes, result):
    """"""Train and test

    Args:
        token (:obj:`str`): token representing this run
        train_data (:obj:`tuple` of :obj:`numpy.array`): Tuple of training feature and label
        test_data (:obj:`tuple` of :obj:`numpy.array`): Tuple of testing feature and label
        num_classes (:obj:`int`): Number of classes
        result (:obj:`pyActLearn.performance.record.LearningResult`): LearningResult object to hold learning result
    """"""
    model = RandomForestClassifier(n_estimators=20, criterion=""entropy"")
    model.fit(train_data[0], train_data[1].flatten())
    # Test
    predicted_y = model.predict(test_data[0])
    predicted_proba = model.predict_proba(test_data[0])
    # Evaluate the Test and Store Result
    confusion_matrix = get_confusion_matrix(num_classes=num_classes,
                                            label=test_data[1].flatten(), predicted=predicted_y)
    result.add_record(model.get_params(), key=token, confusion_matrix=confusion_matrix)
    # In case any label is missing, populate it",examples/CASAS_single/casas_randforest.py,TinghuiWang/pyActLearn,1
"    neg_x = np.vstack(neg_x)

    pos_y = np.ones(pos_x.shape[0], dtype=np.int)
    neg_y = np.zeros(neg_x.shape[0], dtype=np.int)

    X = np.vstack((pos_x, neg_x))
    y = np.concatenate((pos_y, neg_y))

    print 'started learning'
    l_start = dt.now()
    model = sklearn.ensemble.RandomForestClassifier(n_estimators=10, max_depth=20, n_jobs=1, random_state=912)
    model.fit(X, y)

    preds = model.predict_proba(X)
    print sklearn.metrics.auc(y, preds[:, 0])

    joblib.dump(model, 'detectors/%s_model.mdl' % part_name, compress=3)
    print 'learned in', (dt.now() - l_start)
    print 'finsihed in', (dt.now() - start)
",create_rf_model.py,yassersouri/fast-bird-part-localization,1
"def GetMatrix(data,k = 3):
    X = []
    for i in range(len(data)):
        X.append(GetKMerFeature(data[i],k))
    return X

for k in range(1,7):
    X = GetMatrix(data,k)
    y = target

    clf = RandomForestClassifier(n_estimators=94, max_depth=25,
        min_samples_split=2, random_state=0)
    scores = cross_val_score(clf,X,y) ",Experiment/Sklearn/K-mer.py,mushroom-x/piRNA,1
"

def main():
    options = get_options()

    train_data = np.load(options.traindata)

    X_train = train_data['X']
    y_train = train_data['y']

    clf = RandomForestClassifier(max_depth=10, n_estimators=10)
    clf.fit(X_train, y_train)

    cv_data = np.load(options.cvdata)

    X_cv = cv_data['X']
    y_cv = cv_data['y']

    score = clf.score(X_cv, y_cv)
    print 'Cv score is {}'.format(score)",scikit/train.py,blindmotion/detector,1
"    total   = 0

    for p, r in zip(predicted, reference):
        total += 1
        if p == r:
            correct += 1
        
    return (correct * 100.0) / total

def cats_dogs_classifier(path, x, y, trainingfiles):
    random_forest1 = RandomForestClassifier(n_estimators=max_random_trees, max_features=total_features, n_jobs=4)
    random_forest2 = RandomForestClassifier(n_estimators=max_random_trees, max_features=total_features, n_jobs=4)
    gnb = GaussianNB()    
    pipe = Pipeline(steps=[('random_forest1', random_forest1), ('random_forest2', random_forest2), ('gnb', gnb)])
    
    extractor = image_extractor()
    samples = extractor.extractImages(path, x, y, trainingfiles)
    (X, Y) = getXY(samples)
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)
",cat_dog_classifier.py,sudnya/binary-image-classifier,1
"import scipy.sparse as sp

from skml.problem_transformation import LabelPowerset
from skml.datasets import load_dataset

X, y = load_dataset('yeast')


class TestLP(Chai):
    def test_lp_fit_predict(self):
        clf = LabelPowerset(RandomForestClassifier())
        clf.fit(X, y)
        y_pred = clf.predict(X)
        hamming_loss(y, y_pred)

    def test_lp_pipeline(self):
        pl = Pipeline([(""lp"", LabelPowerset(RandomForestClassifier()))])
        pl.fit(X, y)

    def test_lp_gridsearch(self):",test/test_lp.py,ChristianSch/skml,1
"                          concatenate(feature_parameters, {'clf__C': [1/x for x in [0.01, 0.1, 0.3, 1.0, 3.0, 10.0]]})),
            'svm_gaussian':(svm.SVC(tol=1e-6, kernel='rbf'),
                            use_spare_array,
                            not use_binary_features,
                            concatenate(feature_parameters, {'clf__gamma': [.01, .03, 0.1],
                                                     'clf__C': [1/x for x in [0.01, 0.1, 0.3, 1.0, 3.0, 10.0]]})),
            'decision_tree':(tree.DecisionTreeClassifier(criterion='entropy', random_state=RandomState(seed)),
                             not use_spare_array,
                             not use_binary_features,
                             concatenate(feature_parameters,{'clf__max_depth': [2, 3, 4, 5, 6, 7 , 8, 9, 10, 15, 20]})),
            'random_forest':(RandomForestClassifier(criterion='entropy', random_state=RandomState(seed)),
                             not use_spare_array,
                             not use_binary_features,
                             concatenate(feature_parameters,{'clf__max_depth': [2, 3, 4, 5],
                                                             'clf__n_estimators': [5, 25, 50, 100, 150, 200]})),
            'naive_bayes':(BernoulliNB(alpha=1.0, binarize=None, fit_prior=True, class_prior=None),
                           use_spare_array,
                           use_binary_features,
                           {'vect__ngram_range':((1,1),(1,2),(1,3)),
                            'vect__analyzer':('word', 'char_wb')})",model_analysis.py,chop-dbhi/arrc,1
"    BusTrainFeatureIndices = list(xrange(19,22))
    logging.debug(""generic features = %s"" % genericFeatureIndices)
    logging.debug(""advanced features = %s"" % AdvancedFeatureIndices)
    logging.debug(""location features = %s"" % LocationFeatureIndices)
    logging.debug(""time features = %s"" % TimeFeatureIndices)
    logging.debug(""bus train features = %s"" % BusTrainFeatureIndices)
    return genericFeatureIndices + BusTrainFeatureIndices

  def buildModelStep(self):
    from sklearn import ensemble
    forestClf = ensemble.RandomForestClassifier()
    model = forestClf.fit(self.selFeatureMatrix, self.cleanedResultVector)
    return model

  def generateFeatureMatrixAndIDsStep(self, sectionQuery):
    toPredictSections = self.Sections.find(sectionQuery)
    logging.debug(""Predicting values for %d sections"" % toPredictSections.count())
    featureMatrix = np.zeros([toPredictSections.count(), len(self.featureLabels)])
    sectionIds = []
    sectionUserIds = []",emission/analysis/classification/inference/mode.py,e-mission/e-mission-server,1
"        myfile.close()

        #Initialize training models
        mod_1 = SVC(kernel='linear', C=1, gamma=1)
        mod_2 = LogisticRegression()
        mod_3 = GaussianNB()
        mod_4 = MultinomialNB()
        mod_5 = BernoulliNB()

        #Ensemble classifiers
        mod_6 = RandomForestClassifier(n_estimators=50)
        mod_7 = BaggingClassifier(mod_2, n_estimators=50)
        mod_8 = GradientBoostingClassifier(loss='deviance', n_estimators=100)

        mod_9 = VotingClassifier(
            estimators=[(""SVM"", mod_1), (""LR"", mod_2), (""Gauss"", mod_3), (""Multinom"", mod_4), (""Bernoulli"", mod_5),
                        (""RandomForest"", mod_6), (""Bagging"", mod_7), (""GB"", mod_8)], voting='hard')
        mod_10 = VotingClassifier(estimators=[(""SVM"", mod_1), (""LR"", mod_2), (""Multinom"", mod_4),(""Bernoulli"", mod_5),(""Bagging"",mod_7)], voting='hard',weights=[1, 2, 3, 2,1])

        #Vectorizers for feature extraction",informationRetrival/classification/classify.py,BhavyaLight/information-retrival-search-engine,1
"    X_train, X_test, y_train, y_test = train_test_split(
        features, responses, train_size=train_split_propn)

    rf_time = np.array([])
    metrics_tmp = {}
    feature_importances = {}
    for i in range(n_trials):
        t0 = time.time()

        # run random forest and time
        rf = RandomForestClassifier(n_estimators=n_estimators)
        rf.fit(X=X_train, y=y_train)
        rf_time = np.append(rf_time, time.time() - t0)

        # get metrics
        metrics_tmp[i] = irf_utils.get_validation_metrics(rf, y_test, X_test)

        # get feature importances
        feature_importances[i] = rf.feature_importances_
",benchmarks/deleteme/py_irf_benchmarks.py,Yu-Group/scikit-learn-sandbox,1
"
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75
df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)
df.head()

train, test = df[df['is_train']==True], df[df['is_train']==False]

features = df.columns[:4]
clf = RandomForestClassifier(n_jobs=2)
y, _ = pd.factorize(train['species'])
clf.fit(train[features], y)

preds = iris.target_names[clf.predict(test[features])]

rst = pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds'])
",numpy/SKLearnRF.py,Ginkgo-Biloba/Misc-Python,1
"
	return fpr, tpr, auc_knn, predictions


def randomForest(dataFrame, printStats = True):
	""""""
	Function performs a random forest 
	by running (on terminal):
	$ python breastCancerWisconsinDataSet_MachineLearning.py RF 
	""""""
	fit_RF = RandomForestClassifier(random_state = 42, 
		bootstrap=True,
		max_depth=4,
		criterion='entropy',
		n_estimators = 500)

	fit_RF.fit(training_set, 
		class_set['diagnosis'])

	importancesRF = fit_RF.feature_importances_",breastCancerWisconsinDataSet_MachineLearning.py,raviolli77/machineLearning_breastCancer_Python,1
"Y15 = 0*nu.ones([12]);
Y14 = 0*nu.ones([12]);
Y4 = 1*nu.ones([12]);
Y8 = 1*nu.ones([12]);
# define input and output matrix
X = nu.concatenate((X15, X14, X4, X8), axis=0)
y = nu.concatenate((Y15, Y14, Y4, Y8), axis=0)

# define classification model.
dtree = DecisionTreeClassifier()
rfore = RandomForestClassifier(n_estimators=1000)
perce = linear_model.Perceptron(penalty='l1')
rlasso = linear_model.RandomizedLasso()

# evaluate classification model
y_dtree_est = nu.zeros([12,4])
y_rfore_est = nu.zeros([12,4])
y_perce_est = nu.zeros([12,4])
y_rlasso_est = nu.zeros([12,4])
y_test = nu.zeros([12,4])",src/load_data_mat_and_apply_rf.py,lamotriz/sistemas-de-aterramento,1
"# Import the random forest package
from sklearn.ensemble import RandomForestClassifier

# Create the random forset object which will include all the parameters for the fit
forest = RandomForestClassifier(n_estimators = 50)

# Fit the training data to the Supervised labels and create the decision trees
forest = forest.fit(train_data[0::, 1::], train_data[0::, 0])

# Take the same decision trees and run it on the test data
output = forest.predict(test_data).astype(int)

predictions_file = open(""/home/hkh/sources/kagglepy/titanic/output/myfirstforest.csv"", ""wb"")
open_file_object = csv.writer(predictions_file)",titanic/randforest.py,hkhpub/kagglepy,1
"# List of models for data preprocessing
#Damir Jajetic, 2015
def get_models(sd):
	models = [
			'""raw_data""',
			'logt()',
			'preprocessing.StandardScaler()',
			'preprocessing.Normalizer()',
			'ensemble.RandomForestClassifier(n_estimators=200,  max_depth=8,random_state=0)',
			'preprocessing.MinMaxScaler()', 
			'Pipeline([(""rf"", ensemble.RandomForestClassifier(n_estimators=200, max_depth=8,random_state=Lnum)),(""nor"", preprocessing.Normalizer())])',
			'Pipeline([(""rf"", ensemble.RandomForestClassifier(n_estimators=200, max_depth=8,random_state=Lnum)),(""nor"", preprocessing.StandardScaler())])',
			'Pipeline([(""rf"", ensemble.RandomForestClassifier(n_estimators=200, max_depth=8,random_state=Lnum)),(""pca"", decomposition.PCA())])',
			'Pipeline([(""rf"", ensemble.RandomForestClassifier(n_estimators=200, max_depth=8,random_state=Lnum)),(""pca"", decomposition.PCA()), (""nor"", preprocessing.StandardScaler())])',
			#'preprocessing.PolynomialFeatures()', check mem
			'decomposition.PCA()',
			'Pipeline([(""mm"", preprocessing.StandardScaler()), (""bin"", preprocessing.Binarizer())])',
			'random_projection.GaussianRandomProjection()',",lib/engine_prep_models.py,djajetic/AutoML3,1
"    X = X[:, :-1]
    return X, y

xtrain, ytrain = get_data('train')
xtest, ytest = get_data('test')

estimators = {'subsemble': Subsemble(),
              'super_learner': SuperLearner(),
              'blend_ensemble': BlendEnsemble()}

base_learners = [RandomForestClassifier(n_estimators=500,
                                        max_depth=10,
                                        min_samples_split=50,
                                        max_features=0.6),
                 LogisticRegression(C=1e5),
                 GradientBoostingClassifier()]

for clf in estimators.values():
    clf.add([RandomForestClassifier(), LogisticRegression(), MLPClassifier()])
    clf.add_meta(SVC())",benchmarks/ensemble_comp.py,flennerhag/mlens_dev,1
"
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

""""""
  Random Forest. Take into parameters the training dataset x_train and y_train, the test dataset x_test and y_test, the depth maximum of the trees(optional, default = 7) and n the number of trees in the forest (optional, default = 10).
  Return [accuracy, y_predict], where y_predict are the predicted labels from the test dataset.
""""""
def do_RandomForest(x_train, y_train, x_test, y_test, max_depth=7, n=10):
  rf = RandomForestClassifier(n_estimators=n, max_depth=max_depth)
  rf.fit(x_train, y_train)
  score = rf.score(x_test, y_test)
  y_predict = rf.predict(x_test)
  return score, y_predict

if __name__ == ""__main__"":
    x_train = [[1,1],[1,0],[0,1],[0,0],[1,1],[1,0],[0,1],[0,0],[1,1],[1,0],[0,1],[0,0],[1,1],[1,0],[0,1],[0,0]]  
    y_train = [1,1,1,0,1,1,1,0,1,1,1,0,1,1,1,0]
    x_test = [[1,1],[1,0],[0,1],[0,0],[1,1],[1,0],[0,1],[0,0]]",tools/python/random_forest.py,jajoe/machine_learning,1
"X_train['Age'].fillna(X_train['Age'].mean(), inplace=True)
X_train['Fare'].fillna(X_train['Fare'].mean(), inplace=True)

X_test['Age'].fillna(X_test['Age'].mean(), inplace=True)
X_test['Fare'].fillna(X_test['Fare'].mean(), inplace=True)

dict_vec=DictVectorizer(sparse=False)
X_train=dict_vec.fit_transform(X_train.to_dict(orient='record'))
X_test=dict_vec.fit_transform(X_test.to_dict(orient='record'))

rfc = RandomForestClassifier()

rfc.fit(X_train, y_train)
rfc_y_predict = rfc.predict(X_test)
rfc_submission = pd.DataFrame({
    'PassengerId': X_test['passenger'], 'Survived': rfc_y_predict
})
rfc_submission.to_csv('./rfc_submission.csv', index=False)

",kaggle/test.py,quoniammm/happy-machine-learning,1
"#    estimators = [('normalizer', TfidfTransformer(norm='l1', use_idf=True)),
#                  ('svm', sklearn.svm.LinearSVC(C=130, class_weight='auto'))]
#    clf = Pipeline(estimators)
    
#    tuned_parameters = {'svm__C': np.logspace(0,3,10)}
    
    #clf = DummyClassifier(strategy='most_frequent',random_state=0)
    #clf = DummyClassifier(strategy='stratified')
#    clf = DummyClassifier(strategy='uniform')
    #clf = LDA(n_components=3)
    #clf = RandomForestClassifier(n_estimators=10, max_features=30)    
    
#    clf = Pipeline(steps=[
#                          ('rbm', BernoulliRBM(learning_rate =0.005, n_iter=30, n_components=30, verbose=True)), 
#                          ('logistic', LogisticRegression(C = 100.0))])
    

    # simple test
#    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y,
#                                                                         test_size=0.1)",learning/old/test1_cv.py,fcchou/CS229-project,1
"        x[index, :] = np.reshape(img, (1, imageSize))
    return x


def train(xTrain, yTrain):
    """"""
    :param xTrain: training data features
    :param yTrain: training data labels
    :return: model object
    """"""
    RFC = ensemble.RandomForestClassifier()
    RFC.fit(xTrain, yTrain)
    return RFC


def test(xTest, model):
    """"""
    :param xTest: testing data features
    :param model: model object
    :return: predictions list",project/CharRec/RF_CharRec.py,DrigerG/MachineLearning,1
"
### Helpers to quantify classifier's quality.


def baseline(train_data, train_labels, test_data, test_labels, omit=[]):
    """"""Train various classifiers to get a baseline.""""""
    clf, train_accuracy, test_accuracy, train_f1, test_f1, exec_time = [], [], [], [], [], []
    clf.append(sklearn.neighbors.KNeighborsClassifier(n_neighbors=10))
    clf.append(sklearn.linear_model.LogisticRegression())
    clf.append(sklearn.naive_bayes.BernoulliNB(alpha=.01))
    clf.append(sklearn.ensemble.RandomForestClassifier())
    clf.append(sklearn.naive_bayes.MultinomialNB(alpha=.01))
    clf.append(sklearn.linear_model.RidgeClassifier())
    clf.append(sklearn.svm.LinearSVC())
    for i,c in enumerate(clf):
        if i not in omit:
            t_start = time.process_time()
            c.fit(train_data, train_labels)
            train_pred = c.predict(train_data)
            test_pred = c.predict(test_data)",lib/utils.py,mdeff/cnn_graph,1
"
class TestRandomForestClassifierParity(TestCase, JPMMLClassificationTest):

    @classmethod
    def setUpClass(cls):
        if JPMMLTest.can_run():
            JPMMLTest.init_jpmml()


    def setUp(self):
        self.model = RandomForestClassifier()
        self.init_data()
        self.converter = RandomForestClassifierConverter(
            estimator=self.model,
            context=self.ctx
        )

",sklearn_pmml/convert/test/test_randomForestConverter.py,kod3r/sklearn-pmml,1
"    plt.savefig('images/ROC_curve.png')

    return [precision, recall, fscore]
    pass


def run_RandomForest(train_data, train_labels, test_data, test_labels, text):
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

    if run_mode == 1: # Classifion mode
        clf = RandomForestClassifier()

        predict_Y = clf.fit(train_data, train_labels).predict(test_data)
        predict_Y_proba = clf.predict_proba(test_data)[:,1]

        [precision, recall, fscore] = compute_metrics(predict_Y, predict_Y_proba, test_labels, text)

        return predict_Y, predict_Y_proba, [precision, recall, fscore]

    else: # Regression Mode",main.py,ducminhkhoi/PrecipitationPrediction,1
"
seed = random.seed(1990)

X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.70, random_state=seed)

features = 10

num_trees = int(X_train.shape[1]/features)


clf = RandomForestClassifier(n_estimators=num_trees, max_features=features)

start_time = time.time()

clf = clf.fit(X_train, y_train)
      
y_pred = clf.predict(X_test)
        
np.mean(1 - np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1)))
",Covertype/DT.py,Knight13/Exploring-Deep-Neural-Decision-Trees,1
"training_set = [ [clean[i][0]>=0.995,  clean[i][0]<=0.005] + clean[i]    for i in range(len(clean))]

#and finally. make it into a numpy array
training_set = np.array(training_set)





#instantiate random forest classfiers
RFC_SA = RandomForestClassifier(max_depth=RFC_depth, n_estimators=RFC_estimators)
RFC_LB = RandomForestClassifier(max_depth=RFC_depth, n_estimators=RFC_estimators)

#train one RFC on SA
RFC_SA.fit(training_set[0::,3::], training_set[0::,0])

#train another RFC on LB
RFC_LB.fit(training_set[0::,3::], training_set[0::,1])

",SWMv1_3_RandomForest SCRIPT.py,buckinha/gravity,1
"                           converters={0:lambda s: ord(s.split(""\"""")[1])})
    trainDataResponse = trainData[:,1]
    trainDataFeatures = trainData[:,0]

    # Train H2O GBM Model:
    #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")
    rf_h2o = h2o.random_forest(x=alphabet[['X']], y=alphabet[""y""], ntrees=1, max_depth=1, nbins=100)

    # Train scikit GBM Model:
    # Log.info(""scikit GBM with same parameters:"")
    rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
    rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

    # h2o
    rf_perf = rf_h2o.model_performance(alphabet)
    auc_h2o = rf_perf.auc()

    # scikit
    auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_smallcatRF.py,weaver-viii/h2o-3,1
"# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division

import sklearn.ensemble

import submissions
from data import *

rf = sklearn.ensemble.RandomForestClassifier(n_estimators=20, oob_score=True, n_jobs=2)
rf.fit(train, target)
pred = rf.predict_proba(test)

submissions.save_csv(pred, ""random_forest.csv"")",sf_crime/random_forest.py,wjfwzzc/Kaggle_Script,1
"    df = load_sim()
    feature_list = np.array(['reco_log_energy', 'ShowerPlane_cos_zenith', 'InIce_log_charge',
                             'NChannels', 'NStations', 'reco_radius', 'reco_InIce_containment', 'log_s125'])
    num_features = len(feature_list)
    X_train_std, X_test_std, y_train, y_test = get_train_test_sets(
        df, feature_list)

    pipeline = RF_pipeline()
    pipeline.fit(X_train_std, y_train)

    # forest = RandomForestClassifier(
    #     n_estimators=500, max_depth=6, criterion='gini', max_features=None, n_jobs=-1, verbose=1)
    # # Train forest on training data
    # forest.fit(X_train_std, y_train)

    name = pipeline.steps[1][1].__class__.__name__
    importances = pipeline.steps[1][1].feature_importances_
    indices = np.argsort(importances)[::-1]

    fig, ax = plt.subplots()",analysis/random-forest-feature-importance.py,jrbourbeau/composition,1
"    return x


def train(x_train, y_train, params):
    """"""
    :param x_train: training data features
    :param y_train: training data labels
    :param params: keyword parameters for RF algo
    :return: model object
    """"""
    cla = ensemble.RandomForestClassifier(**params)
    # One Vs Rest multi class strategy
    cla = multiclass.OneVsRestClassifier(cla)
    cla.fit(x_train, y_train)
    return cla


def test(x_test, model):
    """"""
    :param x_test: test data",project/CharRec/CharRecRF.py,DrigerG/IIITB-ML,1
"from sklearn import preprocessing
trainColumns = train.select_dtypes(['object']).columns
for col in trainColumns:
    le = preprocessing.LabelEncoder()
    le.fit(train[col].append(test[col]))
    train[col] = le.transform(train[col])
    test[col] = le.transform(test[col])


from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 500, random_state = 1104)
forest = forest.fit(train, target)
probs = forest.predict_proba(test)
output = [""%f"" % x[1] for x in probs]


df = pd.DataFrame()
df[""ID""] = testid
df[""target""] = output
df.to_csv('output.csv', index = False)",leaf.py,DEK11/Spring-Leaf-Competition,1
"            model = Lasso(alpha= alpha, normalize=normalize)
        if modelType == ""huber"":
            model = HuberRegressor(fit_intercept=fitInt, alpha=alpha, epsilon=epsilon)
        if modelType == ""treeReg"":
            model = DecisionTreeRegressor(max_depth= max_depth, max_features=max_features, min_samples_split = min_samples_split)
        if modelType == ""treeClass"":
            model = DecisionTreeClassifier(max_depth = max_depth, max_features=max_features, min_samples_split = min_samples_split)
        if modelType == ""forestReg"":
            model = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, max_features= max_features, min_samples_split = min_samples_split, n_jobs=n_jobs)
        if modelType == ""forestClass"":
            model = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, max_features= max_features, min_samples_split = min_samples_split, n_jobs=n_jobs)
    
    return model


def SetupModel(modelInit, parOptional={}):
    #model selection and hyperparameters
    modelType = modelInit[0]
    kernel = modelInit[1]
    poly= modelInit[2]",src/models/model_utils.py,Kleurenprinter/prompred,1
"             labels_val) = batch_load_manipulate(validation_batchnum,
                                                 leftright=False, updown=False,
                                                 batch_loc=training_folder)

        # Turn the input data for each image into a single number, which equals
        # the percentage of red pixels in the image
        input_train = images_to_percentage_red(all_train_data)
        input_val = images_to_percentage_red(val_data)

        # Fit random forest
        rand_forest = RandomForestClassifier(n_estimators=1000)
        rand_forest.fit(input_train, labels_train)
        self.trained_model = rand_forest

        # Compute accuracy, training loss and validation loss
        (accuracy,
         train_loss,
         val_loss) = self.get_stats(input_train, labels_train, input_val,
                                    labels_val,
                                    agnosticic_average=agnosticic_average)",workflow_classes/benchmark_model.py,dangall/Kaggle-MobileODT-Cancer-Screening,1
"    Args:
        params (dict): Dictionary of hyperparameters.

    Returns:
        model (sklearn.ensemble.RandomForestClassifer): Selected model.

    Raises:
        None
    """"""

    model = RandomForestClassifier(
        criterion=params['criterion'],
        max_features=params['max_features'],
        max_depth=params['max_depth'],
        min_samples_split=params['min_samples_split'],
        max_leaf_nodes=params['max_leaf_nodes'],
        min_samples_leaf=params['min_samples_leaf'],
        n_estimators=params['n_estimators'],
        verbose=0
    )",models/sklearn_launcher.py,wwunlp/sner,1
"X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X_new,Y,test_size=0.2)

# Correcting the ordering of features_new
features = []
for f in sorted(np.argsort(fsel.feature_importances_)[::-1][:features_new]):
	features.append(data.columns[2+f])

# Choosing the best algorithm
algos = {
	'DecisionTree': tree.DecisionTreeClassifier(),
	'RandomForest': ensem.RandomForestClassifier(),
	'GradientBoosting': ensem.AdaBoostClassifier(),
	'AdaBoost': ensem.AdaBoostClassifier(),
	'GNB': GaussianNB()
}
results={}
for algo in algos:
	clf = algos[algo]
	clf.fit(X_train,Y_train)
	score = clf.score(X_test,Y_test)",Virus Detector/learn.py,iamharshit/ML_works,1
"
###########################################################################
train_x = X[trainInd,:]
train_y = y[trainInd]
print ""Train set contains %s outcomes "" %(np.sum(train_y))

mtry = int(np.round(np.sqrt(train_x.shape[1])))
print ""Training random forest with mtry= %s"" %(mtry)
# feature selection
print ""Applying variable importance feature selection...""
rf = RandomForestClassifier(max_features=mtry, n_estimators=ntrees,max_depth=max_depth,min_samples_split=2, random_state=0, n_jobs=-1, bootstrap=False)
rf = rf.fit(train_x, train_y)
feat_sel = SelectFromModel(rf,threshold='mean', prefit=True)
train_x = feat_sel.transform(train_x)
print ""Selected %s number of features"" %(train_x.shape[1])
	

",inst/python/rf_var_imp.py,OHDSI/PatientLevelPrediction,1
"    """"""
    for idx, learner in enumerate(learners):
        print(""======= {} ======="".format(str(learner_names[idx])))
        cross_validation_sklearner(learner, learner_names[idx], X, Y, K)


if __name__ == ""__main__"":
    book_y, book_X = load_training_data(""training_data/feature_avg_filtered.txt"")
    learner_lr = linear_model.LogisticRegression()
    learner_svm = svm.SVC(probability=True)
    learner_rf = ensemble.RandomForestClassifier(n_estimators=50)
    learner_dt = tree.DecisionTreeClassifier()
    learner_gbc = ensemble.GradientBoostingClassifier(n_estimators=50)
    learner_ada = ensemble.AdaBoostClassifier(n_estimators=50)
    cross_validation_multi_learners((learner_lr, learner_svm, learner_rf, learner_dt, learner_gbc),
                                    (""lr"", ""svm"", ""rf"", ""dt"", ""gbc""), book_X, book_y)",predict_age.py,wangleye/age_risk_estimation_book,1
"

    train_review_subset_x = clean_labeledtrain_reviews[::2]
    train_review_subset_y = labeledtrain_data['sentiment'][::2]
    test_review_subset_x = clean_labeledtrain_reviews[1::2]
    test_review_subset_y = labeledtrain_data['sentiment'][1::2]

    train_data_features = vectorizer.fit_transform(train_review_subset_x).toarray()


    forest = RandomForestClassifier(n_estimators = 100)
    forest = forest.fit(train_data_features, train_review_subset_y)

    test_data_features = vectorizer.transform(test_review_subset_x).toarray()


    print forest.score(test_data_features, test_review_subset_y)

    del train_review_subset_x, train_review_subset_y, test_review_subset_x, test_review_subset_y, test_data_features, train_data_features
",backup/bag_of_words_model.py,ddboline/kaggle_imdb_sentiment_model,1
"  trainDataFeatures = trainData[:,0]

  # Train H2O GBM Model:
  #Log.info(""H2O GBM (Naive Split) with parameters:\nntrees = 1, max_depth = 1, nbins = 100\n"")

  rf_h2o = H2ORandomForestEstimator(ntrees=1, max_depth=1, nbins=100)
  rf_h2o.train(x='X', y=""y"", training_frame=alphabet)

  # Train scikit GBM Model:
  # Log.info(""scikit GBM with same parameters:"")
  rf_sci = ensemble.RandomForestClassifier(n_estimators=1, criterion='entropy', max_depth=1)
  rf_sci.fit(trainDataFeatures[:,np.newaxis],trainDataResponse)

  # h2o
  rf_perf = rf_h2o.model_performance(alphabet)
  auc_h2o = rf_perf.auc()

  # scikit
  auc_sci = roc_auc_score(trainDataResponse, rf_sci.predict_proba(trainDataFeatures[:,np.newaxis])[:,1])
",h2o-py/tests/testdir_algos/rf/pyunit_smallcatRF.py,spennihana/h2o-3,1
"
print (""rf ready"")
train_data, train_label = tool.random_shuffle(train_data, train_label)
eval_data, eval_label = tool.random_shuffle(eval_data, eval_label)

""""""
argv = int(sys.argv[1])
feature = argv
""""""

clf = RandomForestClassifier(n_estimators=20)
clf.fit(train_data, train_label)

print (train_data.shape)
print (eval_data.shape)
print (eval_label)

train_predict = clf.predict(train_data)
eval_predict = clf.predict(eval_data)
eval_result = np.sum(eval_predict == eval_label) / float(eval_label.shape[0])",work/ML/tensorflow/separa/rf.py,ElvisLouis/code,1
"	time.sleep(5)
	bar.numerator = result.completed_count()
	print(""Waiting for return results "", bar, end='\r')
	sys.stdout.flush()

results = result.join() #wait for jobs to finish

df_full = pd.DataFrame(list(results))

print('--- Training random forest')
clf = RandomForestClassifier(n_estimators=150, n_jobs=-1, random_state=0)
train_data = df_full[df_full.sponsored.notnull()].fillna(0)
test = df_full[df_full.sponsored.isnull() & df_full.file.isin(test_files)].fillna(0)
clf.fit(train_data.drop(['file', 'sponsored'], 1), train_data.sponsored)

print('--- Create predictions and submission')
submission = test[['file']].reset_index(drop=True)
submission['sponsored'] = clf.predict_proba(test.drop(['file', 'sponsored'], 1))[:, 1]
submission.to_csv('native_btb_basic_submission.csv', index=False)",celery_run.py,carlsonp/kaggle-TrulyNative,1
"train = train.toDF(schema_train)
train = train.withColumn('label',train.label.cast(DoubleType()))
test = test.toDF(schema_test)
            
#merge bytes and asm features
assembler = VectorAssembler(inputCols=[""bytes"", ""asm""],outputCol=""features"")
train = assembler.transform(train)
test = assembler.transform(test)

#rf classifier
rf = RandomForestClassifier(numTrees=100,maxDepth=12,maxBins=32,maxMemoryInMB=512,seed=1)
model = rf.fit(train)
result = model.transform(test)

#save to csv
result.select(""hash"",""prediction"").toPandas().to_csv('prediction.csv',header=False,index=False)

spark.stop()",Distributed_ML/Malware_Classification/pipeline_large.py,iamshang1/Projects,1
"    models = [
        ('Logistic Regression (L1)', linear_model.LogisticRegression(penalty='l1')),
        ('Logistic Regression (L2)', linear_model.LogisticRegression(penalty='l2')),
        # ('logistic_regression-L2-C100', linear_model.LogisticRegression(penalty='l2', C=100.0)),
        # ('randomized_logistic_regression', linear_model.RandomizedLogisticRegression()),
        # ('SGD', linear_model.SGDClassifier()),
        ('Perceptron', linear_model.Perceptron(penalty='l1')),
        # ('perceptron-L2', linear_model.Perceptron(penalty='l2')),
        # ('linear-svc-L2', svm.LinearSVC(penalty='l2')),
        # ('linear-svc-L1', svm.LinearSVC(penalty='l1', dual=False)),
        # ('random-forest', ensemble.RandomForestClassifier(n_estimators=10, criterion='gini',
        #     max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='auto',
        #     bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0,
        #     min_density=None, compute_importances=None)),
        ('Naive Bayes', naive_bayes.MultinomialNB()),
        # ('Naive Bayes (Bernoulli)', naive_bayes.BernoulliNB()),
        # ('knn-10', neighbors.KNeighborsClassifier(10)),
        # ('neuralnet', neural_network.BernoulliRBM()),
        # ('qda', qda.QDA()),
        # ('knn-3', neighbors.KNeighborsClassifier(3)),",tsa/analyses/grid.py,chbrown/tsa,1
"            self.transformer = NBLogCountRatioTransformer(**transformer_kwargs)
        elif text_transformer_type == ""PVTransformer"":
            self.transformer = PVTransformer(**transformer_kwargs)

        else:
            print(""Transformer type not known"")
        
        if cls_method == ""logit"":
            self.cls = LogisticRegression(**cls_kwargs) 
        elif cls_method == ""rf"":
            self.cls = RandomForestClassifier(**cls_kwargs)
        else:
            print(""Classifier method not known"")
        
        self.hardcoded_prediction = None
        self.test_encode_time = None
        self.test_preproc_time = None
        self.test_time = None
        self.nr_test_cases = None
        ",PredictiveModel.py,irhete/PredictiveMonitoringWithText,1
"import pandas as pd
from sklearn.ensemble import RandomForestClassifier

from helpers.log_config import log_to_info


class RFClassifier(object):
    def classify(self, mp, train_centroids, training_data_sentiment, test_centroids, testing_data_ids):
        # ****** Fit a random forest and extract predictions
        clf = RandomForestClassifier(n_estimators=mp.random_forest_estimators)

        # Fitting the forest may take a few minutes
        log_to_info('Fitting a random forest to labeled training data...')
        clf = clf.fit(train_centroids, training_data_sentiment)
        result = clf.predict(test_centroids)

        # Write the test results
        return pd.DataFrame(data={'id': testing_data_ids, 'sentiment': result})",code/classifiers/rfclassifier.py,lukaselmer/hierarchical-paragraph-vectors,1
"
    df1=pd.DataFrame(X_std,columns=ListaColumnas)
    df2=pd.DataFrame(pca,columns=columnas_pca)
    
   
    df_PCA=pd.concat([df1,df2],axis=1)
    
    y = df[objetivo]
   
    
    forest = RandomForestClassifier(n_estimators=250, random_state=0)
    forest.fit(df_PCA, y)
    importances = forest.feature_importances_
    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)
    indices = np.argsort(importances)[::-1]

    # Obtenemos el ranking de los mejores 30
    print(""TOP 30:"")
    
    for f in range(30):",prusontchm/RatioSelection.py,ciffcesarhernandez/AF5_PRACTICA3,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/a_to_b_only.py,diogo149/CauseEffectPairsPaper,1
"    95%
    """"""
    for feature in feature_list[1:-1]:
        feature_num = list(sample[feature].value_counts())[0]
        all_num = (len(sample[feature])+0.0)
        if feature_num/(all_num+0.0) > 0.95:
            print feature,feature_num/(all_num+0.0)

def feature_important(sample,feature_list,feature_data,label):
    scores = []
    rf = RandomForestClassifier(n_estimators=200, max_depth=4)
    for feature in feature_list[1:-1]:
        score = cross_val_score(rf, feature_data[[feature]], label, scoring=""roc_auc"", cv=ShuffleSplit(len(feature_data), 3, .3))
        scores.append((round(numpy.mean(score), 3), feature))
    print sorted(scores, reverse=True)

def data_explore(sample,feature_list,feature_data,label):
    des = sample.describe()
    for feature in feature_list:
        min_num = str(des[3:4][feature])",data_analysis.py,ericxk/santander_no,1
"Y_pred = sgd.predict(X_test)
acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)

# Decision Tree
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, Y_train)
Y_pred = decision_tree.predict(X_test)
acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)

# Random Forest
random_forest = RandomForestClassifier(n_estimators=13)
random_forest.fit(X_train, Y_train)
Y_pred = random_forest.predict(X_test)
before = Y_pred
random_forest.score(X_train, Y_train)
acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)
print(""--Before--"")
print(acc_random_forest)

models = pd.DataFrame({",titanic_twomodels_S.py,michael-hoffman/titanic,1
"




def classifiers(x_test,x_train,y_test2,y_train2):
    clf,train_accuracy, test_accuracy = [], [], []

    clf.append(sklearn.svm.LinearSVC()) # linear SVM classifier
    clf.append(sklearn.linear_model.LogisticRegression()) # logistic classifier
    #clf.append(sklearn.ensemble.RandomForestClassifier())


    for c in clf:
        c.fit(x_train, y_train2)
        train_pred = c.predict(x_train)
        test_pred = c.predict(x_test)
        train_accuracy.append('{:5.2f}'.format(100*sklearn.metrics.accuracy_score(y_train2, train_pred)))
        test_accuracy.append('{:5.2f}'.format(100*sklearn.metrics.accuracy_score(y_test2, test_pred)))
        print(test_pred.sum())",project/reports/stock_market/myutil.py,mdeff/ntds_2016,1
"
    def add_estimator(self, name, instance, params):
        self.grid_search.add_estimator(name, instance, params)

    def fit(self, X, y=None):
        if self.default:
            self.grid_search = GridSearchEstimatorSelector(X, y, self.cv)
            self.grid_search.add_estimator('SVC', SVC(), {'kernel': [""linear"", ""rbf""],
                                                          'C': [1, 5, 10, 50],
                                                          'gamma': [0.0, 0.001, 0.0001]})
            self.grid_search.add_estimator('RandomForestClassifier', RandomForestClassifier(),
                                       {'n_estimators': [5, 10, 20, 50]})
            self.grid_search.add_estimator('ExtraTreeClassifier', ExtraTreesClassifier(),
                                       {'n_estimators': [5, 10, 20, 50]})
            self.grid_search.add_estimator('LogisticRegression', LogisticRegression(),
                                       {'C': [1, 5, 10, 50], 'solver': [""lbfgs"", ""liblinear""]})
            self.grid_search.add_estimator('SGDClassifier', SGDClassifier(),
                                       {'n_iter': [5, 10, 20, 50], 'alpha': [0.0001, 0.001],
                                        'loss': [""hinge"", ""modified_huber"",
                                                 ""huber"", ""squared_hinge"", ""perceptron""]})",tinylearn.py,llvll/beaconml,1
"iris = datasets.load_iris()
rng = check_random_state(0)
perm = rng.permutation(iris.target.size)
iris.data = iris.data[perm]
iris.target = iris.target[perm]

# Stage 1 model
bclf = LogisticRegression(random_state=1)

# Stage 0 models
clfs = [RandomForestClassifier(n_estimators=40, criterion = 'gini', random_state=1),
        ExtraTreesClassifier(n_estimators=30, criterion = 'gini', random_state=3),
        GradientBoostingClassifier(n_estimators=25, random_state=1),
        GradientBoostingClassifier(n_estimators=30, random_state=2),
        #GradientBoostingClassifier(n_estimators=30, random_state=3),
        KNeighborsClassifier(),
        RidgeClassifier(random_state=1),
        Ridge(),
        TSNE(n_components=2)
        ]",stacked_generalization/example/cross_validation_for_iris.py,fukatani/stacked_generalization,1
"selected_columns = matrix[['VehYear', 'VehicleAge', 'Make', 'Model', 'Trim', 'SubModel', 'Color', 'Transmission',
                           'WheelType', 'VehOdo', 'Nationality', 'Size', 'TopThreeAmericanName', 'VehBCost',
                           'IsOnlineSale', 'WarrantyCost']]

# transform categorical attributes into numerical attributes
dummy_matrix = pd.get_dummies(selected_columns)

columns = dummy_matrix.columns.values

# determine most important features
clf = sklearn.ensemble.RandomForestClassifier()  # ExtraTreesClassifier, RandomForestClassifier
important_dummy_matrix = clf.fit(dummy_matrix, matrix['IsBadBuy']).transform(dummy_matrix)

table = PrettyTable(['Attribute', 'Importance (Asc)'])

importances = {}

for i in range(0, len(clf.feature_importances_)):
    column = columns[i]
    importance = clf.feature_importances_[i]",analysis/analysis.py,timothymiko/CarAuctionEvaluator,1
"test_dataset = fold_datasets[-1]

# Get supports on test-set
support_generator = dc.data.SupportGenerator(
    test_dataset, n_pos, n_neg, n_trials)

# Compute accuracies
task_scores = {task: [] for task in range(len(test_dataset.get_task_names()))}
for (task, support) in support_generator:
  # Train model on support
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=100)
  model = dc.models.SklearnModel(sklearn_model)
  model.fit(support)

  # Test model
  task_dataset = dc.data.get_task_dataset_minus_support(
      test_dataset, support, task)
  y_pred = model.predict_proba(task_dataset)
  score = metric.compute_metric(",examples/low_data/tox_rf_one_fold.py,rbharath/deepchem,1
"        all_output = """"
        h = .02  # step size in the mesh
        self.names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
                      ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""Linear Discriminant Analysis"",
                      ""Quadratic Discriminant Analysis""]
        classifiers = [
            KNeighborsClassifier(3),
            SVC(kernel=""linear"", C=0.025),
            SVC(gamma=2, C=1),
            DecisionTreeClassifier(max_depth=5),
            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            AdaBoostClassifier(),
            GaussianNB(),
            LinearDiscriminantAnalysis(),
            QuadraticDiscriminantAnalysis()]

        for i in range(0, len(self.names)):
            if self.names[i] == self.name:
                clf = classifiers[i]
",history/models.py,igorpejic/pytrader,1
"                                     rf))
        stat_list = numpy.mean(stat_list, axis=0)
        print '\t'.join(str(s) for s in [i] + list(stat_list))

def train_rf(target, control, n_est, rand_state):
    np_fps = []
    for fp in target + control:
        arr = numpy.zeros((1,))
        DataStructs.ConvertToNumpyArray(fp, arr)
        np_fps.append(arr)
    rf = RandomForestClassifier(n_estimators=n_est,
                                random_state=rand_state)
    ys_fit = [1] * len(target) + [0] * len(control)
    rf.fit(np_fps, ys_fit)
    return rf


def test_rf(target, control, rf):
    p = len(target)
    n = len(control)",training_methods/classifier/random_forest_analysis.py,dkdeconti/PAINS-train,1
"    Y_test = []
    for i, y in enumerate(Y_train.T):
        if args.col and i not in args.col:
            y_hat = y.mean()
            Y_test.append(y_hat*np.ones(args.test.shape[0]))
            clfs.append(y_hat)
            continue
            
        logging.info(i)
        kwargs = CFG[i]
        clf = RandomForestClassifier(
            n_estimators=128, criterion='entropy', max_depth=None,
            max_leaf_nodes=None, min_samples_split=1,
            n_jobs=-1, random_state=42, verbose=2,
            **kwargs)
        if len(np.unique(y)) == 1:
            Y_test.append(y[0]*np.ones(args.test.shape[0]))
            clfs.append(y[0])
        else:
            logging.info(""Fitting"")",scripts/predictors/random_forest.tuned.py,timpalpant/KaggleTSTextClassification,1
"    label_encoder = sklearn.preprocessing.LabelEncoder()
    y = label_encoder.fit_transform(y)

    if run_settings['classifier'] == 'dummy':
        # just a dummy uniform probability classifier for working purposes
        clf = sklearn.dummy.DummyClassifier(strategy='uniform')
    elif run_settings['classifier'] == 'logistic regression':
        clf = sklearn.linear_model.SGDClassifier(n_jobs=-1,
                                                 loss='log')
    elif run_settings['classifier'] == 'random forest':
        forest = sklearn.ensemble.RandomForestClassifier(n_jobs=-1,
                                                  n_estimators=100,
        #                                          verbose=1,
                                                  max_depth=5)
        scaler = sklearn.preprocessing.StandardScaler()
        clf = sklearn.pipeline.Pipeline(((""scl"",scaler),(""clf"",forest)))

    # only supporting stratified shuffle split for now
    cv = sklearn.cross_validation.StratifiedShuffleSplit(y,
                                    **run_settings['cross validation'])",train.py,Neuroglycerin/neukrill-net-work,1
"            property_list_list.append(property_list)
        dataDescrs_array = np.asarray(property_list_list)
        dataActs_array   = np.array(TL_list)

        for randomseedcounter in range(1,11):
                if self.verbous: 
                    print(""################################"")
                    print(""try to calculate seed %d"" % randomseedcounter)
                X_train,X_test,y_train,y_test = cross_validation.train_test_split(dataDescrs_array,dataActs_array,test_size=.4,random_state=randomseedcounter)
#            try:
                clf_RF     = RandomForestClassifier(n_estimators=100,random_state=randomseedcounter)
                clf_RF     = clf_RF.fit(X_train,y_train)

                cv_counter = 5

                scores = cross_validation.cross_val_score( clf_RF, X_test,y_test, cv=cv_counter,scoring='accuracy')

                accuracy_CV = round(scores.mean(),3)
                accuracy_std_CV = round(scores.std(),3)
   ",Contrib/pzc/p_con.py,greglandrum/rdkit,1
"        clf = svm.SVC(kernel = 'linear', probability = True)
    elif name == 'rbfSVM':
        clf = svm.SVC(kernel = 'rbf', probability = True)
    elif name == 'sigmoidSVM':
        clf = svm.SVC(kernel = 'sigmoid', probability = True)
    elif name == 'polySVM':
        clf = svm.SVC(kernel = 'poly', probability = True)
    elif name == 'decisiontree':
        clf = DecisionTreeClassifier()
    elif name == 'randomforest':
        clf = RandomForestClassifier()
    elif name == 'extratree':
        clf = ExtraTreesClassifier()
    elif name == 'adaboost':
        clf = AdaBoostClassifier()
    elif name == 'gradientboost':
        clf = GradientBoostingClassifier()
    return clf

",InstrumentRecognition/InstrumentRecognition.py,tian-zhou/Surgical-Instrument-Dataset,1
"                   ylim=(X[:, 1].min(), X[:, 1].max()))


# In[6]:

interact(fit_randomized_tree, random_state=[0, 100]); 


# In[7]:

clf = RandomForestClassifier(n_estimators=100, random_state=0)
visualize_tree(clf, X, y, boundaries=False); 


# ## Regression 

# In[8]:

x = 10 * np.random.rand(100)
",notebooks/02-Exploratory_(Interactive)_Data_Analysis.py,balmandhunter/jupyter-tips-and-tricks,1
"perceptron_score = cross_val_score(perceptron, X_train, Y_train, cv=cv, scoring='f1').mean()

sgd = SGDClassifier()
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=45)
sgd_score = cross_val_score(sgd, X_train, Y_train, cv=cv, scoring='f1').mean()

decision_tree = DecisionTreeClassifier()
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=45)
Dtree_score = cross_val_score(decision_tree, X_train, Y_train, cv=cv, scoring='f1').mean()

random_forest = RandomForestClassifier(n_estimators=100)
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=45)
random_forest_score = cross_val_score(random_forest, X_train, Y_train, cv=cv, scoring='f1').mean()

adaboost = AdaBoostClassifier(n_estimators=100)
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=45)
ada_score = cross_val_score(adaboost, X_train, Y_train, cv=cv, scoring='f1').mean()

XGB = xgb.XGBClassifier()
cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=45)",Online_Marketing_Analysis/dataset-classification.py,MichaelMKKang/Projects,1
"        
        if math.isinf(difference) or math.isnan(difference): 
            difference = 1.0
            
        feature_row.append(difference)
        
    data[i,:] = feature_row

train_data,test_data,train_label,test_label = cross_validation.train_test_split(data,class_labels,test_size=0.3)

rf = RandomForestClassifier(n_estimators=101)
ada = AdaBoostClassifier(n_estimators=101)
gradboost = GradientBoostingClassifier(n_estimators=101)
svm = SVC()
gnb = GaussianNB()

classifiers = [rf,ada,gradboost,svm,gnb]
classifier_names = [""Random Forests"",""AdaBoost"",""Gradient Boost"",""SVM"",""Gaussian NB""]

for classifier,classifier_name in zip(classifiers,classifier_names):",PizzaDataModel.py,rupakc/Kaggle-Random-Acts-of-Pizza,1
"        return SVMClass(X_train, y_train, X_test, y_test)

    elif method == 'LDA':
        return LinearDA(X_train, y_train, X_test, y_test)

    elif method == 'QDA':
        return QuadDA(X_train, y_train, X_test, y_test)


def RF(X_train, y_train, X_test, y_test):
    clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)
    clf.fit(X_train, y_train)
    accuracy = clf.score(X_test, y_test)
    return accuracy


def KNN(X_train, y_train, X_test, y_test):
    clf = neighbors.KNeighborsClassifier()
    clf.fit(X_train, y_train)
    accuracy = clf.score(X_test, y_test)",ML_based/machineLearning.py,Tingguo/stock,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/categorical_kmeans3.py,diogo149/CauseEffectPairsPaper,1
"    trainx, trainy, testx, testy, columns, imputer, scaler = load(force_update=True) # to do the prediction force_update must be True in order to get the imputer and the scaler
    _trainy = trainy
    _testy = testy
    v = np.vectorize(lambda x: 0 if x == 0 else 1) #Transform: 0 non-default - 1 default
    trainy = v(trainy)
    testy=v(testy)
    
    print ""Training Model""
    #bestfive (268, 1, 520, 521, 770)
    goldenfeatures=[774, 329, 268, 218, 1, 4, 520, 521, 769, 770, 769, 775, 770, 773, 767, 776, 198, 714, 772, 199, 777]
    _clf = ensemble.RandomForestClassifier(n_estimators=8, criterion='entropy', max_depth=None, min_samples_split=6, min_samples_leaf=7, max_features=None, bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, verbose=3, min_density=None, compute_importances=None)#tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=10, min_samples_split=8, min_samples_leaf=6, max_features=None, random_state=None, min_density=5124, compute_importances=None)#linear_model.LogisticRegression(C=1e20,penalty='l2')
    _clf.fit(trainx[:,goldenfeatures], trainy)
    pred1 = _clf.predict_proba(testx[:,goldenfeatures])[:,1]
    print ""CLASS1 AUC: "", metrics.roc_auc_score(testy, pred1)
    precision, recall, threshold = metrics.precision_recall_curve(testy, pred1)
    #find max f1 score threshold
    f1 = 2 * ((precision[:-1]*recall[:-1])/(precision[:-1]+recall[:-1]))
    f1max = np.where(f1 == f1.max())[0]
    t1 = threshold[f1max]
    probToClass = np.vectorize(lambda x: 1 if x >= t1 else 0)",run.py,eriksson/loan-kaggle,1
" 
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75
df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)
df.head()
 
train, test = df[df['is_train']==True], df[df['is_train']==False]
 
features = df.columns[:4]
clf = RandomForestClassifier(n_jobs=2)
y, _ = pd.factorize(train['species'])
clf.fit(train[features], y)
 
preds = iris.target_names[clf.predict(test[features])]
x = pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds'])
print x",Classification/iris_rforest.py,tayebzaidi/snova_analysis,1
"        with open(param_path, 'r') as f:
            self.params = pickle.load(f)
        self.outfold_pred = np.load(outfold_path)

    def get_fitted(self):
        return np.array([self.outfold_pred]).T

    def get_feature_list(self, X_train, y_train):
        weight = y_train.shape[0] / (2 * np.bincount(y_train))
        sample_weight = np.array([weight[i] for i in y_train])
        clf = RandomForestClassifier(**self.params)
        clf.fit(X_train, y_train, sample_weight)
        score = np.array(clf.feature_importances_)
        sel_ix = np.arange(score.shape[0])[score > np.mean(score)]
        return {args.data: list(sel_ix)}

    def predict(self, X_test):
        pred_list = []
        for cclf in self.clf:
            pred = cclf.predict_proba(X_test)[:, 1]",model/level2_data.py,jingxiang-li/kaggle-yelp,1
"            aspect='auto', cmap='cool')

if __name__ == '__main__':
    #add your models here
    models = ((""No model"", None),
              (""Logistic Regression"", LogisticRegression(C = 1e6, penalty = 'l2')),
              (""Logistic Regression (L1)"", LogisticRegression(C = 1e6, penalty = 'l1')),
              (""Linear SVM"", svm.SVC(kernel='linear', C=1e3)),
              (""Kernel SVM (polynomial)"", svm.SVC(kernel='poly', C=1e6)),
              (""Kernel SVM (RBF)"", svm.SVC(kernel='rbf', C=1e6)),
              (""Random Forest"", RandomForestClassifier()))
    i = InteractiveML(models)
    plt.show()
    while (True):
        time.sleep(1)",interactive_ml.py,ecgeil/mlworkshop,1
"	predict_result = SVM_classifier.predict(test_set)
	return predict_result

def get_dt_prediction(train_set, train_label, test_set):
	DT_classifier = tree.DecisionTreeClassifier()
	DT_classifier.fit(train_set, train_label)
	predict_result = DT_classifier.predict(test_set)
	return predict_result

def get_randomForest_prediction(train_set, train_label, test_set):
	clf = RandomForestClassifier()
	clf.fit(train_set, train_label)
	predict_result = clf.predict(test_set)
	return predict_result

def get_logistic_prediction(train_set, train_label, test_set):
	logistic_classifier = LogisticRegression()
	logistic_classifier.fit(train_set, train_label)
	predict_result = logistic_classifier.predict(test_set)
	return predict_result",stage5/src/classification_task.py,samfu1994/cs838webpage,1
"

# TF-IDF

# ### +Word2vec

# In[37]:

from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier( n_estimators = 100, n_jobs=2)

print(""Fitting a random forest to labeled training data..."")
get_ipython().magic(u'time forest = forest.fit( trainDataVecs, label )')
print ""10: "", np.mean(cross_val_score(forest, trainDataVecs, label, cv=10, scoring='roc_auc'))

# 
result = forest.predict( testDataVecs )

output = pd.DataFrame( data={""id"":test[""id""], ""sentiment"":result} )",competitions/Bag_of_Words/bags_of_words.py,lijingpeng/kaggle,1
"			thresh = 0.5
		# thresholds = np.concatenate((thresholds,thresh_vec),axis=0)
		# thresh = threshold_estimate(x_train,y_train)
		print(""%d %f"" % (x_train.shape[0], thresh))
		weight = float(len(y_train[y_train == 0]))/float(len(y_train[y_train == 1]))
		w1 = np.array([1]*y_train.shape[0])
		w1[y_train==1]=weight
		weight1 = float(len(y_test[y_test == 0]))/float(len(y_test[y_test == 1]))
		clf = xgb.XGBClassifier(max_depth=10, learning_rate=0.1, n_estimators=1000, nthread=50)
		#clf = ensemble.GradientBoostingClassifier(**params)
		#clf = RandomForestClassifier(n_estimators=1000)
		clf.fit(x_train, y_train, sample_weight=w1)
		prob = clf.predict_proba(x_test)
		#yfit1 = clf.predict(x_test)
		#score = clf.score(x_test,y_test,sample_weight=w2)
		yfit1 = (prob[:,1]>thresh)
		index = np.concatenate((index,test_index),axis=0)
		label = np.concatenate((label,y_test),axis=0)
		yfit = np.concatenate((yfit,yfit1),axis=0)
		precision, recall, f1, mcc = score_function(y_test,yfit1)",mainPEP.py,ma-compbio/PEP,1
"            DecisionTreeClassifier or DecisionTreeRegressor."")

if __name__ == ""__main__"":
    # test
    from sklearn.datasets import load_iris
    iris = load_iris()
    idx = range(len(iris.data))
    np.random.shuffle(idx)
    X = iris.data[idx]
    Y = iris.target[idx]
    dt = RandomForestClassifier(max_depth=3)
    dt.fit(X[:len(X)/2], Y[:len(X)/2])
    testX = X[len(X)/2:len(X)/2+5]
    base_prediction = dt.predict_proba(testX)
    pred, bias, contrib = _predict_forest(dt, testX)
    print ""Prediction: "" + str(pred)
    print ""Bias: "" + str(pred)
    print ""Contribution: "" + str(pred)

    assert(np.allclose(base_prediction, pred))",other/handlers/treeinterpreter.py,bcraenen/KFClassifier,1
"	f.close()

# def main():
fn = sys.argv[1]
X,Y = load_svmlight_file(fn)

rf_parameters = {
	""n_estimators"": 2000,
	""n_jobs"": 8
}
clf = RandomForestClassifier(**rf_parameters)
X = X.toarray()

print clf

print ""Starting Training""
t0 = time()
clf.fit(X, Y)
train_time = time() - t0
print ""Training on %s took %s""%(fn, train_time)",benchmarks/sklrf.py,lytics/CloudForest,1
"    knn.fit(train_features, train_target)
    pred = knn.predict(test_features)
    kaccuracy = metrics.accuracy_score(test_target, pred)
    accuracy_dict[""knn""]=kaccuracy
    print kaccuracy

    """"""
       RandomForestClassifier classifier
    """"""
    print("" Accuracy by RandomForestClassifier "")
    Rf = RandomForestClassifier(n_estimators=200)
    Rf.fit(train_features, train_target)
    pred = Rf.predict(test_features)
    Rfaccuracy = metrics.accuracy_score(test_target, pred)
    accuracy_dict[""RF""] = Rfaccuracy
    print Rfaccuracy

    """"""
       LogisticRegression classifier
    """"""",QuoraAnswersClr/quora_answerclassifier.py,anilcs13m/Projects,1
"############Gross Fitting: Primaries only.
#==============================================================================
# Modelling class (Classifier Variant) Model Fitting parameters
#==============================================================================

class MultModels:
    def __init__(self, df):
        self.predictors, self.outcomes = gen_predictors_outcomes(df)
      
    def rf(self):
        self.estimator = Pipeline([(""forest"", RandomForestClassifier(random_state=0, n_estimators=50))])
        #add type
        self.scor_vis()
        
    def adaboost(self):
        self.estimator = Pipeline([(""AdaBoost"", AdaBoostClassifier())])
        self.scor_vis()
                
    def svm(self):
        #train_test_split",ModelFitting/master_pipeline_python.py,georgetown-analytics/nba-tracking,1
"X_train = X[:,li_train,:]
X_validate = X[:,li_validate,:]
y_train = y[li_train]
y_validate = y[li_validate]
XX_train = X_train.reshape((X_train.shape[0]*X_train.shape[1],X_train.shape[2]))
XX_validate = X_validate.reshape((X_validate.shape[0]*X_validate.shape[1],X_validate.shape[2]))
yy_train = np.tile(y_train, n_augments)
yy_validate = np.tile(y_validate, n_augments)


clf = sklearn.ensemble.RandomForestClassifier(n_estimators=1000, max_depth=25, min_samples_leaf=3, n_jobs=16, random_state=42)
pcfilter = sklearn.feature_selection.SelectPercentile(sklearn.feature_selection.f_classif, percentile=100)

pipeline = sklearn.pipeline.Pipeline([('filter', pcfilter), ('clf', clf)])

parameters = {'filter__percentile': [50, 60, 40, 70, 30, 80, 20]}

scorer = sklearn.metrics.make_scorer(sklearn.metrics.log_loss, greater_is_better=False, needs_proba=True)

grid_search = sklearn.grid_search.GridSearchCV(pipeline, parameters, scoring=scorer, n_jobs=1, cv=4, verbose=1)",crossval_cache3.py,Neuroglycerin/neukrill-net-work,1
"                }]


grid = [{'criterion': ['gini', 'entropy'],
                 'n_estimators': [100, 1000],
                 'max_features' : ['auto', 'log2'],
                 'bootstrap' : [True],#, False],
                 'oob_score': [False, True],
                }]

model = ensemble.RandomForestClassifier(n_jobs=-1)



#Create grid search object optimized for accuracy
model = grid_search.GridSearchCV(model, grid, scoring='accuracy')

#Perform rf grid search
model.fit(train_x, train_y)
print model.best_params_",pipeline/training.py,edublancas/titanic,1
"    # Encode the labels
    label_encoder = sklearn.preprocessing.LabelEncoder()
    y = label_encoder.fit_transform(y)
    
    # just a dummy uniform probability classifier for working purposes
    #clf = sklearn.dummy.DummyClassifier(strategy='uniform')
    
    #clf = sklearn.linear_model.SGDClassifier(n_jobs=-1,
    #                                         loss='log')
    
    #clf = sklearn.ensemble.RandomForestClassifier(n_jobs=-1,
    #                                              n_estimators=100,
    #                                              verbose=1)
    
    # clf = sklearn.svm.SVC(probability=True)
    
    clf = sklearn.linear_model.LogisticRegression()
    
    cv = sklearn.cross_validation.StratifiedShuffleSplit(y)
    ",train_attr.py,Neuroglycerin/neukrill-net-work,1
"    # Numpy arrays are easy to work with, so convert the result to an
    # array
    train_data_features = train_data_features.toarray()

    # ******* Train a random forest using the bag of words
    #
    print ""Training the random forest (this may take a while)...""


    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators = 100)

    # Fit the forest to the training set, using the bag of words as
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit( train_data_features, train[""sentiment""] )


",analysis/BagOfWords.py,weiwang/popcorn,1
"trees classifier (third column) and by an AdaBoost classifier (fourth column).

In the first row, the classifiers are built using the sepal width and the sepal
length features only, on the second row using the petal length and sepal length
only, and on the third row using the petal width and the petal length only.

In descending order of quality, when trained (outside of this example) on all
4 features using 30 estimators and scored using 10 fold cross validation, we see::

    ExtraTreesClassifier()  # 0.95 score
    RandomForestClassifier()  # 0.94 score
    AdaBoost(DecisionTree(max_depth=3))  # 0.94 score
    DecisionTree(max_depth=None)  # 0.94 score

Increasing `max_depth` for AdaBoost lowers the standard deviation of the scores (but
the average score does not improve).

See the console's output for further details about each model.

In this example you might try to:",projects/scikit-learn-master/examples/ensemble/plot_forest_iris.py,DailyActie/Surrogate-Model,1
"        1 Find best clf with default param
        2 vary param of best clf and find best param
        3 use best param and best clf to find recall for 100 percent precision
    """"""
    utils.print_success(""Find Recall for best Precision for each tag"")
    train = utils.abs_path_file(train)
    test = utils.abs_path_file(test)
    train_features, train_groundtruths = read_file(train)
    test_features, test_groundtruths = read_file(test)
    classifiers = {
        # ""RandomForest"": RandomForestClassifier(),#n_estimators=5
        ""DecisionTree"":DecisionTreeClassifier()#,#max_depth=10
        # ""SVM"":SVC(kernel=""linear"", C=0.0205),
        # ""ExtraTreesClassifier"":ExtraTreesClassifier(n_estimators=5, criterion=""entropy"", max_features=""log2"", max_depth=9),
        # ""LogisticRegression"":LogisticRegression()
    }
    tags = list(set(test_groundtruths))
    nb_tag = len(tags)
    step = 0.01
    # for index, tag in enumerate([""i""]):",src/classify.py,ybayle/ReproducibleResearchIEEE2017,1
"    ids = df['PassengerId'].values
    df = df.drop('PassengerId', axis=1)
    return df, ids


if __name__ == '__main__':
    # Training data
    df_train, _ = read_and_clean('train.csv')
    df_train.info()
    train = df_train.values
    forest = RandomForestClassifier(n_estimators=100)
    fit = forest.fit(train[:, 1:], train[:, 0])

    # Test data
    df_test, ids = read_and_clean('test.csv')
    df_test.info()

    # Prediction
    prediction = np.array(fit.predict(df_test.values), dtype=int)
    out = np.stack((np.array(ids, dtype=int), prediction), axis=-1)",01-titanic/model.py,ambimanus/kaggle,1
"        positions = sorted(bin_by_pos.keys())

        self.piece = piece
        self.integral = {}
        nf = note_frequency()
        for pos in positions:
            for n in bin_by_pos[pos]:
                nf.add(n)

def fetch_classifier():
    rforest = RandomForestClassifier(n_estimators=100)
    cc = chord_classifier(rforest)

    try:
        from sklearn.externals import joblib
        c = joblib.load('cached/chord-classifier.pkl')
        cc.classifier = c
    except Exception, e:
        print e
        print ""Retraining classifier...""",chords.py,fabeschan/midigeneration,1
"# Parameters for trees
random_state = 5342
n_jobs = 8
verbose = 1
n_estimators = 89
# ExtraTreesClassifier - classifier 1
clf1 = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf2 = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
# RandomForestClassifier - classifier 2
n_estimators = 89
clf3 = RandomForestClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf4 = RandomForestClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)

# Start training
print('training started')
clf1.fit(X, y)
X_new1 = clf1.transform(X, '1.25*median')
clf3.fit(X, y)
X_new2 = clf3.transform(X)
",solution4b.py,canast02/microsoft-malware-classification-challenge,1
"    sel = RFECV()
elif (selector == ""lasso""):
    sel = SelectFromModel(LassoCV(), threshold=0.005)
elif (selector == ""rlregr""):
    sel = RandomizedLogisticRegression()
elif (selector == ""svm""):
    sel = eval( ""SelectFromModel(LinearSVC(%s))"" % (args.selector_params) )
elif (selector == ""extra_trees""):
    sel = SelectFromModel(ExtraTreesClassifier())
elif (selector == ""random_forest""):
    sel = SelectFromModel(RandomForestClassifier())

print sel.estimator
if (type(sel) == SelectFromModel and type(sel.estimator) == LassoCV):
    sel.fit(Xtr, Ytr)
    top_ranked = sorted(enumerate(sel.estimator_.coef_), key=lambda x:x[1], reverse=True)
    top_indices = map(list,zip(*top_ranked))[0]
    for feat,pval in zip(np.asarray(vectorizer.get_feature_names())[top_indices],sel.estimator_.coef_[top_indices]):
        print ""%s\t%s"" % (feat, pval)
elif (type(sel) == SelectFromModel and (type(sel.estimator) == ExtraTreesClassifier or type(sel.estimator) == RandomForestClassifier)):",mlfix/scripts/scikit-f_select.py,varisd/school,1
"    return np.random.randint(min_value, max_value, shape)


class EstimatorFactory(DjangoModelFactory):

    class Meta:
        model = Estimator
        django_get_or_create = ('object_hash',)

    estimator = factory.Iterator([
        RandomForestClassifier(),
    ])

    create_date = factory.LazyFunction(datetime.now)
    object_hash = factory.LazyAttribute(lambda o: compute_hash(o.estimator))
    object_file = DjangoFileField(
        filename=lambda o: '%s/%s' % (Estimator.DIRECTORY, o.object_hash))


class DataSetFactory(DjangoModelFactory):",estimators/tests/factories.py,fridiculous/django-estimators,1
"from sklearn.metrics import roc_auc_score

__author__ = 'jake'


class RandomForest:
    """"""
    Iterative random forest with the ability to stop early.
    """"""
    def __init__(self, **kwargs):
        self.clf = RandomForestClassifier(warm_start=True, **kwargs)

    def fit(self, X, y, validation_data):
        X_val, y_val = validation_data
        step_size = self.clf.n_estimators
        early_stopping_rounds = 5
        margin = .001

        score_hist = [-1]
        while max(score_hist) in score_hist[-early_stopping_rounds:]:",berserker/estimators/core.py,jpopham91/berserker,1
"    return rescaledX

    
def trainAndSaveLRModel(X_train, Y_train, outputFileNameForModel):
    print '\nTraining ...'

    rescaledX = rescaleData(X_train)

    # From the EDA4 we found:
    # Best: 0.853451 using {'max_features': 'log2', 'n_estimators': 1000, 'criterion': 'gini'}    
    model = RandomForestClassifier(max_features='log2', n_estimators=1000, criterion='gini')
    
    #train
    model.fit(rescaledX, Y_train)
    
    #save the trained model to file
    joblib.dump(model, outputFileNameForModel)
    print 'Model saved to ""%s""' % outputFileNameForModel
    
    return model",lib/ifes_2009_to_2014_model.py,FabricioMatos/ifes-dropout-machine-learning,1
"
#spot checking of machine learning algorithms begins

""""""
#all models are stored in list 'models'
models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('DTC', DecisionTreeClassifier(max_depth = 6, min_samples_split = 4)))
models.append(('SVC', SVC()))
models.append(('RFC', RandomForestClassifier(n_estimators=100)))
models.append(('KNC', KNeighborsClassifier(n_neighbors = 3)))
models.append(('MLP', MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',
       beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(10, 5), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,
       warm_start=False)))
# evaluate each model in turn",titanic_predictor.py,Euler10/Titanic-Machine-Learning-from-Disaster,1
"                , .

            ref. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
            """"""
            r_pars = [{
                        ""max_depth"" : [3, None],
                        ""bootstrap"" : [True, False],
                        ""criterion"" : [""gini"", ""entropy""]
                        }]
            len_xy = Y.shape[0]
            est = RandomForestClassifier(
                    n_estimators=2000,
                    max_features='sqrt') 

            # predictionGridSearch
            estimator = GridSearchCV(est, r_pars)
        else:
            # GridSearch, 200
            estimator = RandomForestClassifier(
                    n_estimators=200,",programs/machine_learning/create_cpssnps.py,CORDEA/analysis_of_1000genomes-data,1
"        '''
        Function for fitting a Knowledge Fragment Classifier to a set of input examples, based on a set of classifications.
        
        First a Random Forest Classifier is used to create a set of fragments.
        These are extracted, evaluated, and sorted.
        They are then grown by repeated, and iteratively merging two fragments together based on their fitness.
        Each iteration, the number of fragments is reduced.
        The final list of fragments is then used to select a ruleset.
        '''
        if self.verbose > 0: timing.log(""Instantiate Random Forest Classifier"")
        forest = RandomForestClassifier(n_estimators=self.n_estimators_forest, \
                                        criterion=self.criterion_forest, \
                                        max_features=self.max_features_forest, \
                                        max_depth=self.max_depth_forest, \
                                        min_samples_split=self.min_samples_split_forest, \
                                        min_samples_leaf=self.min_samples_leaf_forest, \
                                        min_weight_fraction_leaf=self.min_weight_fraction_leaf_forest, \
                                        max_leaf_nodes=self.max_leaf_nodes_forest, \
                                        bootstrap=self.bootstrap_forest, \
                                        oob_score=self.oob_score_forest, \",src/root/main/KnowledgeFragmentClassifier.py,bcraenen/KFClassifier,1
"    if weighting_power is None:
        weighting_power = 1
        multiply_by_weight = False
    else:
        multiply_by_weight = True

    return ResilientEnsemble(
        pipeline=pipeline,
        n_estimators=n_estimators,
        training_strategy=TrainingStrategy(
            base_estimator=RandomForestClassifier(
                bootstrap=False,
                n_estimators=inner_estimators,
                max_features=max_features,
                criterion=criterion,
                max_depth=max_depth
            ),
            train_set_generator=
            train_set_generators.RandomCentroidPDFTrainSetGenerator(
                pdf=pdfs.DistanceGeneralizedExponential(precision=precision,",resilient/configs/ensembles.py,etamponi/resilient-protocol,1
"    # Split it into coordinates.
    x_train = train[:int(len(train) * sampling)][['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]
    y_train = train[:int(len(train) * sampling)][['Survived']]
    x_test = train[int(len(train) * sampling):][['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]
    y_test = train[int(len(train) * sampling):]['Survived']

    # See how it scores as a dependent on the number of trees.
    scores = []
    values = range(1, 200)
    for trees in values:
        model = RandomForestClassifier(n_estimators=trees)
        model.fit(x_train, np.ravel(y_train))
        scores.append(model.score(x_test, y_test))

    # Plot the score as a function of the trees.
    plt.plot(values, scores, '-r')
    plt.xlabel('Trees')
    plt.ylabel('Score')
    plt.title('Correct Predictions by Number of Trees')
    plt.show()    ",random_forest2.py,AbnerZheng/Titanic_Kaggle,1
"    #####################################################
    training_data = prune_nonnumber(training_data)
    #for row in training_data:
        #del row[0]
    #####################################################
    threshold = int(len(training_labels) * 0.8)
    validation_data = np.array(training_data[threshold:], float)
    validation_labels = np.array(training_labels[threshold:], float)
    training_data = np.array(training_data[:threshold], float)
    training_labels = np.array(training_labels[:threshold], float)
    clf = ensemble.GradientBoostingClassifier(n_estimators=1000, max_depth=2) #ensemble.ExtraTreesClassifier()#ensemble.RandomForestClassifier() #tree.DecisionTreeClassifier()  #skl.linear_model.SGDClassifier(penalty='elasticnet') #svm.NuSVC() #GaussianNB()
    clf.fit(training_data, training_labels)
    validation_results = clf.predict(validation_data)
    fpr, tpr, thresholds = metrics.roc_curve(validation_labels, validation_results, pos_label=1)
    auc = metrics.auc(fpr,tpr)
    print auc 
    testing_data = read_csv(""test.csv"", category_dict, False, False)
    """"""
    for row in testing_data:
        del row[9]",classify.py,AmrARaouf/StumbleUpon-Classification,1
"        logger.info(params)

        # cross validation here
        scores = []
        for train_ix, test_ix in makeKFold(5, self.y, self.reps):
            X_train, y_train = self.X[train_ix, :], self.y[train_ix]
            X_test, y_test = self.X[test_ix, :], self.y[test_ix]
            weight = y_train.shape[0] / (2 * np.bincount(y_train))
            sample_weight = np.array([weight[i] for i in y_train])

            clf = RandomForestClassifier(**params)
            cclf = CalibratedClassifierCV(base_estimator=clf,
                                          method='isotonic',
                                          cv=makeKFold(3, y_train, self.reps))
            cclf.fit(X_train, y_train, sample_weight)
            pred = cclf.predict(X_test)
            scores.append(f1_score(y_true=y_test, y_pred=pred))

        print(scores)
        score = np.mean(scores)",model/randomForest.py,jingxiang-li/kaggle-yelp,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/no_aggregate.py,diogo149/CauseEffectPairsPaper,1
"    livedoor = json.load(f)

start = time.time()
#stopwords = get_stop_words(livedoor['data'], n=100, min_freq=20)
livedoor['data'] = [' '.join(doc) for doc in livedoor['data']]
X_train, X_test, y_train, y_test = train_test_split(livedoor['data'], livedoor['label'], test_size=0.4)

parameters = {'n_estimators': [10, 30, 50, 70, 90, 110, 130, 150], 'max_features': ['auto', 'sqrt', 'log2', None]}
text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', GridSearchCV(RandomForestClassifier(), parameters, cv=2, scoring='accuracy', verbose=10, n_jobs=-1)),
                     ])

text_clf = text_clf.fit(X_train, y_train)
print(text_clf.score(X_test, y_test))
y_pred = text_clf.predict(X_test)
print(classification_report(y_test, y_pred))",tests/classification.py,Hironsan/natural-language-preprocessings,1
"
    for k in ui_feat_train:
        ui_train.append(list(k))
        feat_train.append(ui_feat_train[k])

print ""load finished with entries: "", len(ui_train), len(feat_train)
feat_data = np.array(feat_train)

print ""feat dimension: "", len(feat_data[0,1:])
logistic = linear_model.LogisticRegression(penalty='l1')
#logistic = RandomForestClassifier(n_estimators=8, random_state=1)
logistic.fit(feat_data[:, 1:], feat_data[:, 0])
print 'Fit finished'

# ""deal with day %d"" % date2
date2 = int(sys.argv[-1])
ui_feat_pre = load_raw_feature(date2)
add_global_feature(ui_feat_pre, date2)
add_u_i_feature(ui_feat_pre, date2)
",predict_merge.py,southerncross/tianchi-monster,1
"#Use stacking to combine multiple forests/runs into a single prediction
num_forests = 90
widgets = [SimpleProgress(), ' | ', Percentage(), ' | ', ETA()]
pbar = ProgressBar(widgets=widgets, maxval=num_forests).start()
forests = []

for i in range(0, num_forests):
	pbar.update(i)

	#https://www.youtube.com/watch?v=0GrciaGYzV0
	clf = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=i)
	train_data = df_full[df_full.sponsored.notnull()].fillna(0)
	test = df_full[df_full.sponsored.isnull() & df_full.file.isin(test_files)].fillna(0)
	clf.fit(train_data.drop(['file', 'sponsored'], 1), train_data.sponsored)

	submission = test[['file']].reset_index(drop=True)
	submission['sponsored'] = clf.predict_proba(test.drop(['file', 'sponsored'], 1))[:, 1]

	#add the forest to our list
	forests.append(submission)",scikit_generate_prediction_stacking.py,carlsonp/kaggle-TrulyNative,1
"        return performKNNClass(X_train, y_train, X_test, y_test, parameters, fout, savemodel)
    elif method == 'SVM':   
        return performSVMClass(X_train, y_train, X_test, y_test, parameters, fout, savemodel)
    elif method == 'ADA':
        return performAdaBoostClass(X_train, y_train, X_test, y_test, parameters, fout, savemodel)

def performRFClass(X_train, y_train, X_test, y_test, parameters, fout, savemodel):
    """"""
    Random Forest Binary Classification
    """"""
    clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)
    clf.fit(X_train, y_train)
    
    if savemodel == True:
        #fname_out = '{}-{}.pickle'.format(fout, datetime.now())
        fname_out = fout + '.pickle'
        with open(fname_out, 'wb') as f:
            pickle.dump(clf, f, -1)    
    
    accuracy = clf.score(X_test, y_test)",smap_nepse/prediction/classify.py,samshara/Stock-Market-Analysis-and-Prediction,1
"
# Define the model
classifiers = [
  DecisionTreeClassifier(max_depth=5),
  MLPClassifier(algorithm='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, learning_rate_init=0.001, batch_size=64, max_iter=100, verbose=False),
  MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1),
  MLPClassifier(algorithm='adam', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1),
  KNeighborsClassifier(2),
  SVC(kernel=""linear"", C=0.025),
  SVC(gamma=2, C=1),
  RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
  AdaBoostClassifier(),
  GaussianNB(),
  LinearDiscriminantAnalysis(),
  QuadraticDiscriminantAnalysis()]

if len(sys.argv) > 1:
  classifier_index = int(sys.argv[1])
else:
  classifier_index = 0",sklearn_exmaples/cancer_classifier.py,tobegit3hub/deep_recommend_system,1
"            clf['config'].update(dict(nthreads=n_cpu, random_state=meta_config.randint()))
            clf_tmp = XGBoostClassifier(**clf.get('config'))
        elif clf['clf_type'] == 'tmva':
            serial_clf = True
            clf_tmp = TMVAClassifier(**clf.get('config'))
        elif clf['clf_type'] == 'gb':
            serial_clf = True
            clf_tmp = SklearnClassifier(GradientBoostingClassifier(**clf.get('config')))
        elif clf['clf_type'] == 'rdf':
            clf['config'].update(dict(n_jobs=n_cpu, random_state=meta_config.randint()))
            clf_tmp = SklearnClassifier(RandomForestClassifier(**clf.get('config')))
        elif clf['clf_type'] == 'ada':
            serial_clf = True
            clf['config'].update(dict(random_state=meta_config.randint()))
            clf_tmp = SklearnClassifier(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(
                random_state=meta_config.randint()), **clf.get('config')))
        elif clf['clf_type'] == 'knn':
            clf['config'].update(dict(random_state=meta_config.randint(), n_jobs=n_cpu))
            clf_tmp = SklearnClassifier(KNeighborsClassifier(**clf.get('config')))
        elif clf['clf_type'] == 'rdf':",raredecay/analysis/ml_analysis.py,mayou36/raredecay,1
"    """""" Use entirety of provided X, Y to train random forest

    Arguments
    Xtrain -- Training data
    Ytrain -- Training prediction

    Returns
    classifier -- A random forest of n estimators, fitted to Xtrain and Ytrain
    """"""
    if grid == True:
        forest = RandomForestClassifier(max_depth=None, random_state=0, min_samples_split=1,max_features=38)
        parameters = {
            'n_estimators': [200,250,300],
        }

        # Classify over grid of parameters
        classifier = GridSearchCV(forest, parameters)
    else:
        classifier = RandomForestClassifier(n_estimators=n)
",src/random_forest.py,bravelittlescientist/kdd-particle-physics-ml-fall13,1
"        # Get the spatial information
        training_data_spa = [arr for idx_arr, arr in enumerate(data[-1])
                             if idx_arr in idx_patient_training]
        # Concatenate the data
        training_data_mod = np.vstack(training_data_mod)
        training_data_spa = np.vstack(training_data_spa)
        # Concatenate spatial information and modality information
        training_data = np.hstack((training_data_mod, training_data_spa))

        # Create the current RF
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        crf.fit(training_data, training_label)
        # Add the classifier
        rf_ensemble.append(crf)

    # Get the labels
    validation_label = [arr for idx_arr, arr in enumerate(label)
                        if idx_arr in idx_patient_validation]
    validation_label = np.ravel(label_binarize(
        np.hstack(validation_label).astype(int), [0, 255]))",pipeline/feature-classification/exp-2/pipeline_classifier_stacking_adaboost.py,I2Cvb/mp-mri-prostate,1
"    SVC(kernel=""linear"", C=0.025),                                              #  3
    SVC(kernel=""linear"", C=1),                                                  #  4
    SVC(kernel=""linear"", C=100),                                                #  5
    SVC(gamma=0.5, C=0.1),                                                      #  6
    SVC(gamma=2, C=1),                                                          #  7
    SVC(gamma=50, C=100),                                                       #  8
    DecisionTreeClassifier(max_depth=5),                                        #  9
    DecisionTreeClassifier(max_depth=10),                                       # 10 - !
    SVC(gamma=2, C=1000),                                                       # 11
    SVC(gamma=2, C=100),                                                        # 12
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),       # 13
    RandomForestClassifier(max_depth=10, n_estimators=10, max_features=1),      # 14 - !
    RandomForestClassifier(max_depth=15, n_estimators=50, max_features=5),      # 15 - !
    AdaBoostClassifier(),                                                       # 16
    GaussianNB(),                                                               # 17
    MultinomialNB(),                                                            # 18
    BernoulliNB(),                                                              # 19
    LDA(),                                                                      # 20
    QDA()                                                                       # 21
    ]",KForest/KForest/KGump.py,Andrew414/KForest,1
"
    if len(data_shape) != 2:
        raise ValueError(""Only 2-d data allowed (samples by dimension)."")

    classifiers = {
        ""Nearest Neighbors"": KNeighborsClassifier(3),
        ""Linear SVM"": SVC(kernel=""linear"", C=1, probability=True),
        ""RBF SVM"": SVC(gamma=2, C=1, probability=True),
        ""Decision Tree"": DecisionTreeClassifier(max_depth=None,
                                                max_features=""auto""),
        ""Random Forest"": RandomForestClassifier(max_depth=None,
                                                n_estimators=10,
                                                max_features=""auto"",
                                                n_jobs=PROCESSORS),
        ""Logistic Regression"": LogisticRegression(),
        ""Naive Bayes"": GaussianNB(),
        ""LDA"": LDA()
        }

    params = {",polyssifier.py,pliz/polyssifier,1
"
    def __init__(self, doc2vec):
        super(MultipClassifiers, self).__init__(doc2vec)
        self.classifiers = [
            #KNeighborsClassifier(3), #*
            #KNeighborsClassifier(6), #*
            SVC(kernel=""linear"", C=0.025, probability=True),
            #SVC(gamma=2, C=1), #*
            #GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),#need memory
            #DecisionTreeClassifier(max_depth=5),
            #RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
            #MLPClassifier(alpha=1),
            #AdaBoostClassifier(),
            #GaussianNB(),
            #QuadraticDiscriminantAnalysis() #*
            ]

    def get_data(self, train_docs):
        Log.info(self, 'Get vectorized data for corpus...')
        X = []",main.py,VNGResearch/doc2vec,1
"    
	goldy = goldy.read().replace(""\r"","""").split(""\n"")[1:-1]
	
	#populate a list of classifiers 
	support_vector_c      = SVC(C=1)
	decision_tree_c       = DecisionTreeClassifier(max_depth=5) #with 4 features, depth of 2 is best; with 6 features depth of 4
	nearest_neighbors_c   = KNeighborsClassifier(6)
	naive_bayes_c         = GaussianNB()
	lda_c                 = LDA()
	qda_c                 = QDA()
	random_forests_c      = RandomForestClassifier(max_features=2, max_depth=2)
	ada_boost_c           = AdaBoostClassifier()
	logistic_regression_c = LogisticRegression()
	
	for classifier in [support_vector_c, decision_tree_c, nearest_neighbors_c, naive_bayes_c, lda_c, qda_c, random_forests_c, logistic_regression_c]:

		#each time you load a new classifier, reset all predictions values to zero with this little trick
		tp, fp, tn, fn = (0,)*4
		
		for i in xrange(50):",classify/multi_classifier.py,duhaime/detect_reuse,1
"test = np.load('test_distribute.npy')[:,1:]

data = train[:,1:]
target = train[:,0]

# Originally, I thought removing these fields would be good, but I actually do better by not removing them, tragic
#data = np.delete(data, np.s_[[x for x in range(990) if x % 15 in [3,4,5,10,11,12,13,14]]], 1)
#test = np.delete(test, np.s_[[x for x in range(990) if x % 15 in [3,4,5,10,11,12,13,14]]], 1)

#clf = svm.LinearSVC()
pred = RandomForestClassifier(n_estimators=100).fit(data, target).predict(test)

f = open('predictions.csv', 'w')
f.write(""ID,Category\n"")

for i, res in enumerate(pred):
    f.write(""%d,%d\n"" % (i+1,res))

f.close()",Sign-Language/code.py,bcspragu/Machine-Learning-Projects,1
"
    learner = []

    pipeline = []

    ############################################################################

    if clf_or_regr == ""clf"":
        learner = GaussianNB()
        learner = DecisionTreeClassifier()
        #learner = RandomForestClassifier()
        #learner = SVC()
        #learner = LogisticRegression()

    if clf_or_regr == ""regr"":
        learner = LinearRegression()
        #learner = Ridge()
        #learner = Lasso()
        #learner = BayesianRidge()
        #learner = SGDRegressor()",python/py/quant/clf_or_regr.py,austinjalexander/sandbox,1
"def train(x_train, y_train, algo=""DT""):
    """"""
    :param x_train: training data features
    :param y_train: training data labels
    :param algo: choice of learning algorithm [DT, RF, KNN, MLP]
    :return: model object
    """"""
    if algo == ""DT"":
        cla = tree.DecisionTreeClassifier()
    elif algo == ""RF"":
        cla = ensemble.RandomForestClassifier(max_features=40,
                                              n_estimators=500,
                                              n_jobs=1,
                                              max_depth=150)
    elif algo == ""KNN"":
        cla = neighbors.KNeighborsClassifier()
    elif algo == ""MLP"":
        cla = neural_network.MLPClassifier()

    # Enable one of the optimization methods",project/DigiRec/DigiRec.py,DrigerG/IIITB-ML,1
"#y_t=np.asarray(y_t)

#y_pos=y_pos.tolist()
#y_1500=y_1500.tolist()
#y_train=y_1500+y_pos
y_t=np.array(y)
X_s=pd.DataFrame.as_matrix(X_test)
y_s=np.array(y_test)


estimator = RandomForestClassifier(n_estimators=55,
                                           criterion='gini', 
                                           bootstrap=False,
                                           n_jobs=2)
pu_estimator = PUAdapter(estimator, hold_out_ratio=0.2)
pu_estimator.fit(X_t,y_t)
y_pred = pu_estimator.predict(X_s)
diff=y_s[y_pred!=y_s]
print n,list(diff).count(-1),list(diff).count(1)
falseneg=list(diff).count(1)",static/scripts/TCGApuAdaptedv2.py,raysinensis/tcgaAPP,1
"from sklearn.cross_validation import train_test_split

from sklearn_evaluation import plot

data = datasets.make_classification(200, 10, 5, class_sep=0.65)
X = data[0]
y = data[1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

est = RandomForestClassifier()
est.fit(X_train, y_train)

y_pred = est.predict(X_test)
y_score = est.predict_proba(X_test)
y_true = y_test

plot.confusion_matrix(y_true, y_pred)",examples/confusion_matrix.py,edublancas/sklearn-evaluation,1
"
    features_train = lda.transform(features_train)
    features_test = lda.transform(features_test)

    for name, clf in [
        ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R')),
        ('BernoulliNB', BernoulliNB(alpha=1)),
        ('GaussianNB', GaussianNB()),
        ('DecisionTreeClassifier', DecisionTreeClassifier(min_samples_split=100)),
        ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=50, algorithm='ball_tree')),
        ('RandomForestClassifier', RandomForestClassifier(min_samples_split=100)),
        ('SVC', SVC(kernel='linear', C=1))
    ]:

        if not data.has_key(name):
            data[name] = []

        print ""*"" * 100
        print('Method: {}'.format(name) + ' the number of feature is {}'.format(k))
",nytimes/step4_analysis_supervised_4(lda).py,dikien/Machine-Learning-Newspaper,1
"    }, {
        'clf': SGDClassifier(loss='log', penalty='l2', alpha=1e-3, n_iter=5, random_state=42),
        'name': 'SGDClassifier',
    }, {
        'clf': SGDClassifier(loss='modified_huber', penalty='l2', alpha=1e-3, n_iter=5, random_state=42),
        'name': 'SGDClassifier',
    }, {
        'clf': DecisionTreeClassifier(),
        'name': 'DecisionTreeClassifier',
    }, {
        'clf': RandomForestClassifier(n_estimators=10),
        'name': 'RandomForestClassifier',
    }, {
        'clf': MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1),
        'name': 'Neural Network :)',
    }]

    models = [{
        'clf': OneVsRestClassifier(model[""clf""]),
        'name': 'OneVsRest(%s)' % model['name'],",src/data_cross_validation.py,AntoineToubhans/trellearn,1
"from pandas import DataFrame
import numpy as np
import pandas as pd

class pseudo_labeler(object):
    def __init__(self, estimater):
        """"""
        Define a pseudo-labeler for semi-supervised learning.
        
        -----Parameters-----
        estimater:    Estimater such as XGBClassifier() or RandomForestClassifier(), which have ""predict_proba"" method.
        """"""
        
        self.estimater = estimater
    
    def pseudo_labeling(self, X_train, y_train, X_test, max_iter=100, th_confidence=0.95, fit_params=None):
        """"""
        Extract test data with enough confidence and conduct pseudo-labeling.
        
        -----Parameters-----",semi_supervised_learner.py,nejumi/tools_for_kaggle,1
"					if center:
						b = MatrixUtils.get_reduced_vector(b, center)
					X.append(b)
					Y.append(f.replace('.p',''))
					classes.append(f)
		classes = list(set(classes))	
		print '# training posts: ', len(X)
		print '# classes: ', len(classes)

		clf_params = {'n_estimators': 300, 'n_jobs': -1, 'max_depth': 8}
		clf = OneVsRestClassifier(RandomForestClassifier(**clf_params))
		clf.fit(X, Y)
		pickle.dump(clf, open('clf.p', 'wb'), pickle.HIGHEST_PROTOCOL)
		pickle.dump(classes, open('classes.p', 'wb'), pickle.HIGHEST_PROTOCOL)
	else:
		clf = pickle.load(open('clf.p','rb'))
		classes = pickle.load(open('classes.p','rb'))

	Xtest = []
	Ytest = []",train_and_test_multi_v3.py,jimbotonic/df_nlp,1
"from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

from skml.problem_transformation import BinaryRelevance
from skml.datasets import load_dataset

X, y = load_dataset('yeast')
clf = BinaryRelevance(RandomForestClassifier())
clf.fit(X, np.array(y))
y_pred = clf.predict(X)
y_pred_proba = clf.predict_proba(X)

print(""hamming loss: "")
print(hamming_loss(y, y_pred))

print(""accuracy:"")
print(accuracy_score(y, y_pred))",examples/example_br.py,ChristianSch/skml,1
"
    numberOfSamples = trainingSamples + testingSamples

    rf_selectedFeatures = ""all""
    svm_selectedFeatures = [20, 21, 22, 23, 24]

    
    rf_features, rf_labels = sc.Data_Preparation(inputfile, rf_selectedFeatures)
    svm_features, svm_labels = sc.Data_Preparation(inputfile, svm_selectedFeatures)

    Scikit_RandomForest_Model = ensemble.RandomForestClassifier(n_estimators=510, criterion='gini', max_depth=7,
                                                                 min_samples_split=2, min_samples_leaf=1, max_features='sqrt',
                                                                 bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, verbose=0,
                                                                 min_density=None, compute_importances=None)

    Scikit_SVM_Model = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)

    
    accuracy, testing_Labels, predict_Labels =  sc.Classification_Blending(Scikit_RandomForest_Model, rf_features, rf_labels, Scikit_SVM_Model, svm_features, svm_labels, trainingSamples, testingSamples)
    ",Blending.py,DistributedSystemsGroup/YELP-DS,1
"
  print('- Unique titles: %s' % df.Title.unique())
  for title in df.Title.unique():
    counts = len(df[df.Title == title])
    print('title %d: %d times, rate: %f' % (title, counts, len(df[(df.Title == title) & (df.Survived == 1)]) / counts))

  print('... done printing statistics!')
  print(SEPARATOR)

def run_prediction(train, test):
  forest = RandomForestClassifier(n_estimators=100)
  forest = forest.fit(train[0::,1::], train[0::,0] )

  return forest.predict(test).astype(int), forest

def write_predictions(ids, predictions):
  with open('prediction.csv', 'wt') as predictions_file:
    open_file_object = csv.writer(predictions_file)
    open_file_object.writerow(['PassengerId','Survived'])
    open_file_object.writerows(zip(ids, predictions))",titanic/submissions/6/randomforest.py,furgerf/kaggle-projects,1
"	return s if len(s) <= 80 else s[:77] + ""...""


names = [""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression""]


classifiers = [
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4)]

def main():

	#set the timer
	start = time()

	#ignore all warnings",UMKL/textData/20_dl_MKM.py,akhilpm/Masters-Project,1
"        self.max_features = kwargs.get('max_features', ""auto"")
        self.max_leaf_nodes = kwargs.get('max_leaf_nodes', None)
        self.bootstrap = kwargs.get('bootstrap', True)
        self.oob_score = kwargs.get('oob_score', True)
        self.n_jobs = kwargs.get('n_jobs', 1)
        self.random_state = kwargs.get('random_state', None)
        self.verbose = kwargs.get('verbose', 0)
        self.warm_start = kwargs.get('warm_start', False)
        self.class_weight = kwargs.get('class_weight', None)

        self.rfc = RandomForestClassifier(
            self.n_estimators,
            self.criterion,
            self.max_depth,
            self.min_samples_split,
            self.min_samples_leaf,
            self.min_weight_fraction_leaf,
            self.max_features,
            self.max_leaf_nodes,
            self.bootstrap,",DSTK/BoostedFeatureSelectors/boselector.py,jotterbach/dstk,1
"    #...

    plt.savefig(modeler.version + ""_confusion_comparisons.png"")






    modeler.models = {    ""KNeighbors_default"": sklearn.neighbors.KNeighborsClassifier()
                      , ""RandomForest"": sklearn.ensemble.RandomForestClassifier()
                      , ""LogisticRegression"": sklearn.linear_model.LogisticRegression(penalty='l1', C=0.1)
                      , ""GaussianNB"": GaussianNB()
                      , ""SVC_rbf"": SVC(kernel = 'rbf', probability = True, random_state = 0)
                      , ""SVC_linear"": SVC(kernel = 'linear', probability = True,  random_state = 0)
                      , ""SVC_poly"": SVC(kernel = 'poly', degree = 3, probability = True,  random_state = 0)
                      }

    #Different method for KNeighbors allows us to compare multiple k's
    for i in range(3,13):",presentation/code_snippets.py,georgetown-analytics/housing-risk,1
"                compstring2 = '%s %s' % (c[1]['first_name'], c[1]['city'])
                if self.jaccard_sim(self.shingle(compstring1.lower(), 2), self.shingle(compstring2.lower(), 2)) >= self.initial_sim:
                    c1, c2 = c[0], c[1]
                    featurevector = str(self.create_featurevector(c1, c2))
                    match = Match(c1, c2, featurevector)
                    to_create.append(match)
        pickle.dump(to_create, open(self.output_file, 'w'))

    def train(self):
        print ""Training classifier""
        c = RandomForestClassifier(n_estimators=10, random_state=0)
        training_matches = self.load_training_matches()
        c = c.fit([eval(t.features) for t in training_matches], [int(t.matchpct) for t in training_matches])
        return c

    def load_training_matches(self):
        print ""Loading training matches""
        match_file = open(self.output_file, 'rb')
        tm = pickle.load(match_file)
        match_file.close()",campfin/trainer.py,huffpostdata/campfin-linker,1
"
train_data = data[:TRAIN_SIZE]
train_labels = requester_pizza_status[:TRAIN_SIZE]
test_data = data[TRAIN_SIZE:]
test_labels = requester_pizza_status[TRAIN_SIZE:] 

#Initializing Classifiers

svm = SVC(kernel='rbf')
mnb = MultinomialNB()
rf = RandomForestClassifier(n_estimators=51)
ada = AdaBoostClassifier(n_estimators=100)

classifiers = [svm,mnb,rf,ada]
classifier_names = [""SVM"",""Multinomial NB"",""Random Forest"",""AdaBoost""]

for classifier,classifier_name in zip(classifiers,classifier_names):
    classifier.fit(train_data,train_labels)
    predicted_labels = classifier.predict(test_data)",PizzaPOSModel.py,rupakc/Kaggle-Random-Acts-of-Pizza,1
"    return dataframe.values

if __name__ == '__main__':
    train_df = pd.read_csv('train.csv', header=0)
    test_df = pd.read_csv('test.csv', header=0)
    ids = test_df['PassengerId'].values

    train_data = extract_feature(train_df)
    test_data = extract_feature(test_df)

    forest = RandomForestClassifier(n_estimators=50)
    forest.fit(train_data[:, 1:], train_data[:, 0])
    result = forest.predict(test_data).astype(int)

    with open(""result.csv"", 'w') as predictions_file:
        open_file_object = csv.writer(predictions_file)
        open_file_object.writerow([""PassengerId"", ""Survived""])
        open_file_object.writerows(zip(ids, result))",Kaggle/Titanic/randomforest.py,xueweuchen/pythonDemo,1
"if __name__ == '__main__':
    X, y = make_moons(n_samples = 200, random_state = 100)
    
    ns = np.logspace(0, 7, 8, endpoint = True, base = 2.0, dtype = np.int32)
        
    fig, axes = plt.subplots(2, ns.size / 2)
    fig.suptitle('Decision boundaries for Random Forests with different number of trees', fontsize = 'large')
    axes = np.reshape(axes, ns.size)
    
    for n, ax in zip(ns, axes):
        ensemble_clf = RandomForestClassifier(n_estimators = n)
        ensemble_clf.fit(X, y)
        
        ax.set_title('n_estimators = {}'.format(n))
        plot_surface(ax, ensemble_clf, X, y)
    
    plt.tight_layout()",ensembles.py,lidalei/DataMining,1
"  
  # Drop the index columns
  for column in index_columns:
    final_vectorized_features = final_vectorized_features.drop(column)
  
  # Inspect the finalized features
  final_vectorized_features.show()
  
  # Instantiate and fit random forest classifier on all the data
  from pyspark.ml.classification import RandomForestClassifier
  rfc = RandomForestClassifier(
    featuresCol=""Features_vec"",
    labelCol=""ArrDelayBucket"",
    predictionCol=""Prediction"",
    maxBins=4657,
    maxMemoryInMB=1024
  )
  model = rfc.fit(final_vectorized_features)
  
  # Save the new model over the old one",ch08/train_spark_mllib_model.py,rjurney/Agile_Data_Code_2,1
"def EsembleLearning():
    ClfList = []
    train_num = 2000
    test_num = 100

    X_train, y_train = RandomSelectData(data, target, train_num)
    X_test, y_test = RandomSelectData(data, target, test_num)
    y_predict = []

    for index in range(len(k_mers)):
        ClfList.append(RandomForestClassifier(n_estimators=94, max_depth=25,
                                              min_samples_split=2, random_state=0))
        X = GetMatrix(X_train, k_mers[index])
        y = y_train
        ClfList[index].fit(X, y)

    for data_index in range(len(X_test)):
        value = 0
        for clf_index in range(len(k_mers)):
            X = GetMatrix(X_test, k_mers[clf_index])",Experiment/Sklearn/K-Mer-Essemble.py,mushroom-x/piRNA,1
"		clean_train_data.append(' '.join(KaggleWord2VecUtility.review_to_wordlist(train_data['review'][i],True) ))
	
	# Creating bag of words
	vectorizer = CountVectorizer(analyzer='word', tokenizer=None, preprocessor=None, stop_words=None, max_features=5000)
	
	# word vs freauency
	train_data_features = vectorizer.fit_transform(clean_train_data)
	train_data_features = train_data_features.toarray()
	
	# Training Random Forest Classifier	
	clf = RandomForestClassifier()
	clf = clf.fit(train_data_features, train_data['sentiment'])

	'''
	# calculating accuracy over test_data
	clean_test_data = []
	for i in range(0, len(test_data['review']) ):
		clean_test_data.append(' '.join(KaggleWord2VecUtility.review_to_wordlist(test_data['review'][i],True) ))
 	test_data_features = vectorizer.transform(clean_test_data).toarray()
 	prediction = clf.predict(test_data_features)",Movie Review Analyser/review_analyser.py,iamharshit/ML_works,1
"        
        # Here we specify the number of features that we will use for the analysis
        nFeatures = np.array([N_FEATURES, 200, 100, 50, 20, 10])  
        
        # In this section we specify the classification models.
        
        clfs = [
         BernoulliNB(),
         GaussianNB(),
         DecisionTreeClassifier(),
         RandomForestClassifier(n_estimators=10),
         OneVsRestClassifier(LinearSVC(random_state=0)),
         OneVsRestClassifier(LogisticRegression()),
         OneVsRestClassifier(SGDClassifier()),
         OneVsRestClassifier(RidgeClassifier()),
        ]
        
        # We get the names of each model.
        clfnames = map(lambda x: type(x).__name__
          if type(x).__name__ != 'OneVsRestClassifier'",core/__init__.py,Rickyfox/MLMA2,1
"
    # Scale features and labels
    # NOTE: the scaler is fit only to the training features
    stdsc = StandardScaler()
    X_train_std = stdsc.fit_transform(X_train)
    X_test_std = stdsc.transform(X_test)

    print('events = ' + str(y_train.shape[0]))

    for depth in range(1, 10):
        forest = RandomForestClassifier(n_estimators=100, max_depth=depth, n_jobs=-1)
        # Train forest on training data
        forest.fit(X_train_std, y_train)
        train_proba = forest.predict_proba(X_train_std)
        test_proba = forest.predict_proba(X_test_std)
        # print(train_proba)
        # print(train_proba[:,0])
        # print(train_proba[:,1])
        print(stats.ks_2samp(train_proba[:,0], test_proba[:,0]))
    # name = forest.__class__.__name__",analysis/ks-test_max-depth.py,jrbourbeau/composition,1
"  eluted = pd.read_html(get_url(base_url, ELUTED, allele), infer_types=False, header=0)[0]
  binding = pd.read_html(get_url(base_url, BINDING, allele), infer_types=False, header=0)[0]
  nonbinding = pd.read_html(get_url(base_url, BINDING, allele), infer_types=False, header=0)[0]

  return eluted, binding, nonbinding

if __name__ == '__main__':

  E, B, N = get_data()

  model = ensemble.RandomForestClassifier(n_estimators = 50)

  EB = pd.concat([E, B])
  print EB.count()
  print N.count()

  X, Y = data.make_ngram_dataset(EB.Peptide, N.Peptide, max_ngram=2, normalize_row=True, rebalance=True)

  print cross_val_score(model, X, Y, scoring='roc_auc')",eluted_peptide_prediction.py,hammerlab/immuno_research,1
"#### initial visualization
plt.xlim(0.0, 1.0)
plt.ylim(0.0, 1.0)
plt.scatter(bumpy_fast, grade_fast, color = ""b"", label=""fast"")
plt.scatter(grade_slow, bumpy_slow, color = ""r"", label=""slow"")
plt.legend()
plt.xlabel(""bumpiness"")
plt.ylabel(""grade"")
plt.show()

clf = RandomForestClassifier()
clf.fit(features_train, labels_train)
pred = clf.predict(features_test)
acc = accuracy_score(pred, labels_test)
print ""Decision Tree accuracy: %r"" % acc

try:
    prettyPicture(clf, features_test, labels_test, f_name=""random_forest.png"")
except NameError:
    pass",004 - AdaBoost + kNN +  Random Forrest/random_forest.py,mdegis/machine-learning,1
"      if test:
        test_scores['graphconv'] = model_graphconv.evaluate(
            test_dataset, [classification_metric], transformers)

  if model == 'rf':
    # Loading hyper parameters
    n_estimators = hyper_parameters['n_estimators']

    # Building scikit random forest model
    def model_builder(model_dir_rf):
      sklearn_model = RandomForestClassifier(
          class_weight=""balanced"", n_estimators=n_estimators, n_jobs=-1)
      return dc.models.sklearn_models.SklearnModel(sklearn_model, model_dir_rf)

    model_rf = dc.models.multitask.SingletaskToMultitask(tasks, model_builder)

    print('-------------------------------------')
    print('Start fitting by random forest')
    model_rf.fit(train_dataset)
",examples/benchmark.py,joegomes/deepchem,1
"
    return m



#-------------------------------------------------------------------------------
def get_clfmethod (clfmethod, n_feats, n_subjs):

    #classifiers
    classifiers = { 'cart'   : tree.DecisionTreeClassifier(random_state = 0),
                    'rf'     : RandomForestClassifier(max_depth=None, min_samples_split=1, random_state=None),
                    'gmm'    : GMM(init_params='wc', n_iter=20, random_state=0),
                    'rbfsvm' : SVC (probability=True, max_iter=50000, class_weight='auto'),
                    'polysvm': SVC (probability=True, max_iter=50000, class_weight='auto'),
                    'linsvm' : LinearSVC (class_weight='auto'),
                    'sgd'    : SGDClassifier (fit_intercept=True, class_weight='auto', shuffle=True, n_iter = np.ceil(10**6 / 416), loss='modified_huber'),
                    'percep' : Perceptron (class_weight='auto'),
    }

    #Classifiers parameter values for grid search",aizkolari_classification.py,alexsavio/aizkolari,1
"# train[""family_size""] = train[""SibSp""].values + train[""Parch""].values + 1

feature_names = [""Pclass"", ""Sex"", ""Age"", ""Fare"", ""SibSp"", ""Parch"", ""Embarked""]

target = train[""Survived""].values
features_one = train[feature_names].values

#Fitting the selected features in a decision tree
max_depth = 10
min_samples_split = 2
my_forest = RandomForestClassifier(max_depth = max_depth, min_samples_split = min_samples_split, n_estimators = 100, random_state = 1)
my_forest = my_forest.fit(features_one, target)

# -------------------------------------------------------------------------------
# ------------------------------- Test cases ------------------------------------
# -------------------------------------------------------------------------------
# Impute the missing value with the median
test.Fare[152] = test.Fare.median()

test[""Sex""][test[""Sex""] != ""male""] = 1 # all females and empty entries will be 1",titanic.py,xordux/titanic,1
"        k_neighboors = KNeighborsClassifier()
        n_neighbors = [3, 5, 11, 21, 31]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(k_neighboors, dict(n_neighbors=n_neighbors), X, y)
        f = open('output/age.knn.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates Random Forest classifier
        random_forest = RandomForestClassifier()
        n_estimators = [2, 3, 5, 10, 20, 40, 60, 120]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(random_forest, dict(n_estimators=n_estimators), X, y)
        f = open('output/age.randomforest.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates MLP classifier",tests/test_age.py,fberanizo/author-profiling,1
"
features, class_outputs = get_training('../joined_matrix_split.txt', 'LARCENY/THEFT') 
# 0:27 category  28:38= supervisor district, 39 = count 311, 
# 40:76 counts of 911 categories, 77=count of 911 aggregate
#class_outputs = preprocessing.binarize(class_outputs)
#features_10k, class_10k = features[:10000, 0:40], class_outputs[:10000]
#test_features_10k, test_class_10k = features[10000:20000, 0:40], class_outputs[10000:20000]

features_10k, class_10k, test_features_10k, test_class_10k = get_equal_training_test(features, class_outputs)

#call_models = [svm.LinearSVC(), svm.SVC(), tree.DecisionTreeClassifier(), ensemble.RandomForestClassifier(), ensemble.AdaBoostClassifier(), neighbors.KNeighborsClassifier()] 
#all_models = [tree.DecisionTreeClassifier(), ensemble.RandomForestClassifier(), ensemble.AdaBoostClassifier(), neighbors.KNeighborsClassifier()] 
all_models = [svm.SVR()]
opt_models = dict()
for i,model in enumerate(all_models):
    param_grid = None
    opt_model = train(features_10k, class_10k, test_features_10k, test_class_10k, model, params_grid = param_grid)
    (accuracy, precision, recall) = get_classification_metrics(test_features_10k, test_class_10k, opt_model)
    opt_models[opt_model] = (accuracy, precision, recall) 
",classificationSpecific/train_classification_specific.py,JamesWo/cs194-16-data_manatees,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_absolute_error

print('starting')
X = np.asarray(data, 'float32')
y = np.asarray(labels, 'float32')
N = len(y)
kf = cross_validation.KFold(N, n_folds=5)
fold = 1 ; rmse = []; mae = [];
#model = RandomForestRegressor(n_estimators=100, n_jobs=4) # n_jobs=4
model = RandomForestClassifier(n_estimators=300, n_jobs=4) # n_jobs=4    
for train_index, test_index in kf:
    X_train = X[train_index]
    y_train = y[train_index]

    X_test = X[test_index]
    y_test = y[test_index]

    print(""FOLD:"",fold,""TRAIN:"", len(X_train), ""TEST:"", len(y_test)); fold+=1
",RBM vs RandomForestRegressor.py,jordansilva/lorien,1
"Y = iris.target
# print(iris.DESCR)

dialects = [""db2"", ""hive"", ""mssql"", ""mysql"", ""oracle"", ""postgresql"", ""sqlite""];

for lNbEstimatorsInEnsembles in [16 , 32, 64, 128, 256, 512]:
    models = [AdaBoostClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960),
              BaggingClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960),
              ExtraTreesClassifier(n_estimators=lNbEstimatorsInEnsembles, max_depth=3, min_samples_leaf=15, random_state = 1960),
              GradientBoostingClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960),
              RandomForestClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960)]
    
    for clf in models:
        print(""TRAINING_MODEL"" , clf.__class__.__name__, lNbEstimatorsInEnsembles)
        clf.fit(X, Y)
        pickle_data = pickle.dumps(clf)
        for dialect in dialects:
            print(""GENERATING_SQL_FOR"" , clf.__class__.__name__ , lNbEstimatorsInEnsembles, dialect)
            lSQL = test_ws_sql_gen(pickle_data, dialect)
            print(lSQL)",tests/classification/test_client_iris_large_models.py,antoinecarme/sklearn2sql_heroku,1
"	train = tokenize(dataset)
	scaler = preprocessing.MinMaxScaler()
	train = scaler.fit_transform(train)
	target = [x[4] for x in dataset]

	dataset = genfromtxt(open('test_data.csv','r'), delimiter=',', dtype=None)
	test = tokenize(dataset)
	test = scaler.transform(test)

	clf = tree.DecisionTreeClassifier()
	# clf = RandomForestClassifier(n_estimators=100)
	# clf = svm.SVC()
	clf.fit(train, target)
	result = clf.predict(test)

	dataset = dataset.astype(object)
	dataset = [dataset[i] + (result[i],) for i in range(len(dataset))]
	savetxt('result.csv', dataset, delimiter = ',', fmt=""%s"", header='language, owner_org, following_owner,no_of_stars,class', comments = '' )

if __name__ == '__main__':",GithubStarringDataset/classifier.py,NagabhushanS/DataMining,1
"
pcba_tasks, pcba_datasets, transformers = load_pcba(
    base_dir, reload=reload)
(train_dataset, valid_dataset) = pcba_datasets

classification_metric = Metric(metrics.roc_auc_score, np.mean,
                               verbosity=verbosity,
                               mode=""classification"")

def model_builder(model_dir):
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=500)
  return SklearnModel(sklearn_model, model_dir)
model = SingletaskToMultitask(muv_tasks, model_builder, model_dir)


# Fit trained model
model.fit(train_dataset)
model.save()
",examples/pcba/pcba_sklearn.py,bowenliu16/deepchem,1
"    salary_result = {}
    trainFeature, testFeature, trainIdList, testIdList, trainTarList = merge_feature()
    # tmp = [t<32 for t in trainTarList]
    # tmp = np.array(tmp)
    # trainFeature = trainFeature[tmp]
    target = np.array(trainTarList)
    # target = target[tmp]
    trainIdList = np.array(trainIdList)
    # trainIdList = trainIdList[tmp]
    Cfeature = trainFeature.columns[:]
    clf = RandomForestClassifier(n_estimators=200, min_samples_split=13)
    clf.fit(trainFeature[Cfeature], target)
    preds = clf.predict(testFeature)
    right = 0
    for i in range(len(preds)):
        salary_result[testIdList[i]] = preds[i]
    return salary_result


if __name__ == '__main__':",position_predict/predict_code/salary_predict.py,yinzhao0312/Position-predict,1
"        from sklearn.dummy import DummyClassifier
        return DummyClassifier()
    if m == ""LR"":
        from sklearn.linear_model import LogisticRegression
        if not args.param:
            args.param = 1
        return LogisticRegression(C=float(args.param))
    if m == ""RF"":
        from sklearn.ensemble import RandomForestClassifier
        n = int(args.param) if args.param else 10
        return RandomForestClassifier(
            n_estimators=n, criterion='entropy')
    if m == ""RF_gini"":
        from sklearn.ensemble import RandomForestClassifier
        n = int(args.param) if args.param else 10
        return RandomForestClassifier(
            n_estimators=n, criterion='gini')
    if m == ""NB"":
        from sklearn.naive_bayes import MultinomialNB
        return MultinomialNB()",kagura/getmodel.py,nishio/kagura,1
"        y[i] = category_dict[label]

    # ranges for cross validation parameters (I know this is only 1
    # combination, checking more would require more than my 8GB RAM :/
    n_estimators_range = [i for i in range(20,22,40)]
    max_features_range = [i for i in range(3,5,2)]

    # does CV and fits the best model
    param_grid = {'n_estimators': n_estimators_range, 'max_features':
                  max_features_range}
    rfc = RandomForestClassifier(random_state = 2, n_jobs = -1)
    clf = GridSearchCV(rfc, param_grid = param_grid, scoring = make_scorer(log_loss, greater_is_better = False, needs_proba = True),
                       refit = True, cv = 4)
    trained_clf = clf.fit(X, y)
    # you can pickle the best CV estimator if you want
    #pickle.dump(trained_clf.best_estimator_, open(""trainedclassifier.p"", ""wb""))
    
    # plot a CV log loss plot
    scores = [-1*x[1] for x in trained_clf.grid_scores_]
    scores = np.array(scores).reshape(len(max_features_range), ",main.py,NoahZinsmeister/sf_crime_classification_kaggle,1
"from sklearn.pipeline import Pipeline

names = [""KNN (3)"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""FDA"",
         ""QDA"", ""Linear BPM"", ""Quadratic BPM"", ""Cubic BPM"", ""Nystron BPM""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis(),
    BayesPointMachine(),
    Pipeline([('poly2', PolynomialFeatures(degree=2)), ('bpm', BayesPointMachine())]),
    Pipeline([('poly3', PolynomialFeatures(degree=3)), ('bpm', BayesPointMachine())]),
    Pipeline([('nys', Nystroem(gamma=.2, random_state=1)), ('bpm', BayesPointMachine())])
]",plot_classifier_comparison.py,IRC-SPHERE/sklearn-Infer.NET-wrapper,1
"    data = list(csv.reader(f))
    
train_data = np.array(data[1:])
labels = train_data[:, 0].astype('float')
train_data = train_data[:, 1:].astype('float') / 255.0

with open('test.csv', 'rt') as f:
    data = list(csv.reader(f))
test_data = np.array(data[1:]).astype('float') / 255.0

clf = RandomForestClassifier(n_estimators=50, n_jobs=2)
clf = clf.fit(train_data, labels)
preds = clf.predict(test_data)

with open('submission.csv', 'wt') as f:
    fieldnames = ['ImageId', 'Label']
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    i = 1
    for elem in preds:",problems/Kaggle/DigitRecognizer/RF.py,nesterione/problem-solving-and-algorithms,1
"from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np


from skml.problem_transformation import ClassifierChain
from skml.datasets import load_dataset

X, y = load_dataset('yeast')
cc = ClassifierChain(RandomForestClassifier(),
                     threshold=.5)
cc.fit(X, np.array(y))
y_pred = cc.predict(X)

print ""real: "", y.shape
print ""y_pred: "", y_pred.shape
y_pred_proba = cc.predict_proba(X)
print ""pred_proba: "", y_pred_proba
",doc/auto_examples/example_cc.py,ChristianSch/skml,1
"        DummyClassifier(strategy='most_frequent', random_state=0),
        DummyClassifier(strategy='stratified', random_state=0),
        DummyClassifier(strategy='uniform', random_state=0),
        DummyClassifier(strategy='constant', random_state=0, constant=True),
        LogisticRegression(C=100),
        SVC(C=1.0, kernel='rbf', probability=True),
        SVC(C=1.0, kernel='linear', probability=True),
        KNeighborsClassifier(n_neighbors=10),
        tree.DecisionTreeClassifier(),
        NuSVC(probability=True),
        RandomForestClassifier(n_estimators=100)
    ]

    document_levels = ['review', 'sentence', 1]

    num_cyles = len(my_resamplers) * len(my_classifiers) * len(document_levels)
    index = 1

    results_list = []
",source/python/evaluation/classifier_evaluator.py,melqkiades/yelp,1
"                                   bootstrap=False)
    xgb_bagged = SklearnClassifier(xgb_bagged)
    xgb_big_stacker = copy.deepcopy(xgb_bagged)
    xgb_bagged = CacheClassifier(name='xgb_bagged1', clf=xgb_bagged)

    xgb_single = XGBoostClassifier(n_estimators=350, eta=0.1, max_depth=4, nthreads=3)
    xgb_single = FoldingClassifier(base_estimator=xgb_single, stratified=True,
                                   n_folds=10, parallel_profile='threads-2')
    xgb_single = CacheClassifier(name='xgb_singled1', clf=xgb_single)

    rdf_clf = SklearnClassifier(RandomForestClassifier(n_estimators=300, n_jobs=3))
    rdf_folded = FoldingClassifier(base_estimator=rdf_clf, stratified=True,
                                   parallel_profile='threads-2')
    rdf_bagged = BaggingClassifier(base_estimator=rdf_folded, n_estimators=n_base_clf,
                                   bootstrap=False)
    rdf_bagged = SklearnClassifier(rdf_bagged)
    rdf_bagged = CacheClassifier(name='rdf_bagged1', clf=rdf_bagged)

    gb_clf = SklearnClassifier(GradientBoostingClassifier(n_estimators=50))
    gb_folded = FoldingClassifier(base_estimator=gb_clf, stratified=True,",raredecay/analysis/physical_analysis.py,mayou36/raredecay,1
"    final_df = df[selected_features]
    return final_df

def perform_random_forest(df,labels,N):
    # Store feature names
    column_names=list(df.columns.values)

    # Obtain important features - indices
    X = df.as_matrix()
    y = labels
    forest = RandomForestClassifier(n_estimators = 100)
    forest = forest.fit(X, y)
    important_features = forest.feature_importances_

    # Sort feature indices from best to worst
    ind = [i for i in range(len(important_features))]
    all_pairs = []
    for a, b in zip(important_features, ind):
        all_pairs.append((a, b))
    sorted_pairs=sorted(all_pairs,key=lambda p:p[0],reverse=True)",src/main/feature_select.py,ab93/Depression-Identification,1
"from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

# get some data
digits = load_digits()
X, y = digits.data, digits.target

# build a classifier
clf = RandomForestClassifier(n_estimators=20)


# Utility function to report best scores
def report(grid_scores, n_top=3):
    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]
    for i, score in enumerate(top_scores):
        print(""Model with rank: {0}"".format(i + 1))
        print(""Mean validation score: {0:.3f} (std: {1:.3f})"".format(
            score.mean_validation_score,",projects/scikit-learn-master/examples/model_selection/randomized_search.py,DailyActie/Surrogate-Model,1
"	svm = SGDClassifier(n_iter=100, alpha=.0001, class_weight=""auto"", l1_ratio=0, fit_intercept=True, n_jobs=-1)
	svm.fit(training_data, training_labels)
	print ""Done fitting both SVM. Self score: {0:.2f}%"".format(svm.score(training_data, training_labels)*100)
	model_params['SVM'] = svm

	filters = svm.coef_
	# filters = [hog2image(c, [height,width], orientations=5, pixels_per_cell=model_params['hog_size'], cells_per_block=[3,3]) for c in svm.coef_]
	model_params['filters'] = filters
	# filters = [f*(f>0) for f in filters]

	# rf = RandomForestClassifier(n_estimators=50)
	# rf.fit(training_both_kernel, labels)
	# print ""Done fitting forest. Self score: {0:.2f}%"".format(rf.score(training_both_kernel, labels)*100)
	# model_params['rf'] = rf

	# Grid search for paramater estimation
	if 0:
		from sklearn.grid_search import GridSearchCV
		from sklearn.cross_validation import cross_val_score
		params = {'alpha': [.0001],",pyKinectTools/scripts/Pose_tests.py,colincsl/pyKinectTools,1
"model.fit(X_train, y_train)
print('[INFO] kNN score:')
get_score(model)

# try random forest
params = {
          'max_depth': [20, 30, 40, 50, 60],
          'n_estimators': [30, 40]
         }
print('[INFO] RandomForest gridsearching', params)
clf = RandomForestClassifier(random_state=42, n_jobs=-1)
model = GridSearchCV(clf, params, cv=3, refit=False)
model.fit(data, labels)
print('[INFO] best hyperparameters: {}'.format(model.best_params_))
model = RandomForestClassifier(max_depth=50, n_estimators=30, random_state=42, n_jobs=-1) # best parameters
model.fit(X_train, y_train)
print('[INFO] RandomForest score:')
get_score(model)

# try using the 13-dim haralick features instead of PCA",process_ims/machine_learning.py,nateGeorge/IDmyDog,1
"
    counter = 0
    for review in clean_test_reviews:
        test_centroids[counter] = create_bag_of_centroids(review, \
            word_centroid_map)
        counter += 1


    # ****** Fit a random forest and extract predictions
    #
    forest = RandomForestClassifier(n_estimators = 100)

    # Fitting the forest may take a few minutes
    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit(train_centroids,train[""sentiment""])
    result = forest.predict(test_centroids)

    # Write the test results
    output = pd.DataFrame(data={""id"":test[""id""], ""sentiment"":result})
    output.to_csv(""BagOfCentroids.csv"", index=False, quoting=3)",backup/Word2Vec_BagOfCentroids.py,ddboline/kaggle_imdb_sentiment_model,1
"
def train_classifier_tree2(dataset, feature_index, stats):
    # print(dataset, feature_index)
    test_set, train_set = split_sets(dataset, 0.1)
    x_train, y_train = split_train_result_set(train_set, feature_index)
    x_train_test, y_train_test = split_train_result_set(test_set, feature_index)

    n_estimators=[100, 180]
    min_samples_split=[2, 10]

    clf = RandomForestClassifier()
    # clf = tree.DecisionTreeClassifier()
    # clf = clf.fit(x_train, y_train)

    nFolds = 5
    param_grid = dict(n_estimators=n_estimators, min_samples_split=min_samples_split)

    # param_grid = dict(max_depth=[10, 11, 12, 15],)

    cv = cross_validation.StratifiedKFold(y_train, nFolds)",src/mod_suggest/tree_trainer.py,Drob-AI/The-Observer,1
"
def make_model(model_name, parameters):

    if model_name == 'RandomForest':
        defaults = {""n_estimators"": 10,
                    ""depth"": None,
                    ""max_features"": ""auto"",
                    ""criterion"": ""gini""}
        parameters = {name: parameters.get(name, defaults.get(name)) for name in defaults.keys()}
        if parameters[""depth""] == ""None"": # None gets converted to string in yaml file
            return ensemble.RandomForestClassifier(n_estimators=parameters['n_estimators'],
                                               max_features=parameters['max_features'],
                                               criterion=parameters['criterion'])

        return ensemble.RandomForestClassifier(n_estimators=parameters['n_estimators'],
                                               max_depth=parameters['depth'],
                                               max_features=parameters['max_features'],
                                               criterion=parameters['criterion'])

    elif model_name == 'SVM':",blight_risk_prediction/model.py,dssg/cincinnati2015-public,1
"

dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

ab=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.1, n_estimators=10, random_state=1)
dt=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5, max_features=10, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=1, splitter='best')
gb=GradientBoostingClassifier(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=5, max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=25, presort='auto', random_state=1, subsample=1.0, verbose=0, warm_start=False)
rf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=5, max_features=10, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=75, n_jobs=1, oob_score=False, random_state=1, verbose=0, warm_start=False)
svcl=LinearSVC(C=0.9, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l2', random_state=1, tol=0.0001, verbose=0)
lr=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=1, solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
",scripts/histograms/normalised-ml.py,jmrozanec/white-bkg-classification,1
"                y_train=self.y_train,
                X_test=self.X_test,
                use_saved_model=use_saved_model)

        if self.modeltype == 'classification' and method == 'rf':

            # TODO: think about moving this to model_eval mtry function
            if not mtry:
                mtry = math.floor(math.sqrt(len(self.X_train.columns.values)))

            algorithm = RandomForestClassifier(n_estimators=trees,
                                               max_features=mtry,
                                               n_jobs=cores,
                                               verbose=(
                                                   2 if debug is True else 0)
                                               )

            self.y_pred = model_eval.clfreport(
                modeltype=self.modeltype,
                debug=debug,",healthcareai/deploy_supervised_model.py,HealthCatalyst/healthcareai-py,1
"	i=-1
	for line in f:
		if i>=0:
			line=line.split(',')
			Y[i]=float(line[1])
		i+=1
	return Y


def validator(path_to_features,path_to_labels):
	clf=RandomForestClassifier(n_estimators=n_est, max_features=n_feat,max_depth=9, n_jobs=-1)
	labels=load_labels(path_to_labels)
	X=load_features(path_to_features)
	print len(X),len(labels)
	nclasses=len(set(labels))
	Conf=np.zeros((nclasses,nclasses))
	for i in range(len(X)-1):
		train=np.array(list(range(i,i+1)))
		test=np.array(list(range(0,i))+list(range(i+1,len(X))))
		XX = X[train]",code/validator.py,danielmiorandi/riskPredictorBDC2015,1
"        continue




y = np.array(y)

X = np.array(X)

clf = KNeighborsClassifier(n_neighbors=11)
# clf = RandomForestClassifier(n_estimators=20)
# clf = SVC(gamma=0.001, kernel='rbf', C=100)

skf = StratifiedKFold(y, n_folds=2)
for train_index, test_index in skf:
    print(""Detailed classification report:"")
    print()
    print(""The model is trained on the full development set."")
    print(""The scores are computed on the full evaluation set."")
    print()",tests/ensemble_gradient.py,schae234/gingivere,1
"    if result[0] == 2:
        print ""Predicted straight""
    if result[0] == 3:
        print ""Predicted right""

    return result

def init_turns_module(values, trees, data, labels):
    # Fit regression model
    global turns_regr
    turns_regr = RandomForestClassifier(n_estimators=trees)
    turns_regr.fit(data[:, [0,1]], labels)
    print ""init_turns, importances: "", turns_regr.feature_importances_
    return

def predicted(data):
    return turns_regr.predict(data)",turns.py,dkdbProjects/server-result-sender,1
"iris = datasets.load_iris()
X= iris.data[:,[2,3]]
y = iris.target

#split the dataset into separate training and test datasets
from sklearn.cross_validation import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 0)

#random forest
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion='entropy',n_estimators = 10,random_state = 1,n_jobs = 2)
forest.fit(X_train,y_train)

X_combined = np.vstack((X_train,X_test))
y_combined = np.hstack((y_train,y_test))

import DecisionBoundary
DecisionBoundary.plot_decision_regions(X_combined,y_combined,classifier=forest,test_idx = range(105,150))
plt.xlabel('petal length [cm]')
plt.ylabel('petal width [cm]')",1_supervised_classification/17-Random_Forest/random_forest/random_forest.py,PhenixI/machine-learning,1
"    pred_df=pd.DataFrame()
    pred_df['CASE_ENQUIRY_ID']=Xvalidate['CASE_ENQUIRY_ID']
    col_map={'score':0}
    for col in DataConstants.train_columns:
        if col!=""CASE_ENQUIRY_ID"":
            col_train = Xtrain[col]
            col_validate = Xvalidate[col] 
            vect = TfidfVectorizer(stop_words='english',sublinear_tf=True)
            
            tdm = vect.fit_transform(col_train, ytrain)
            clf  = RandomForestClassifier()
            
            clf.fit(tdm,ytrain)
            
            pipeline = Pipeline([
            ('vect', vect),
            ('clf', clf)
            ])
          
            parameters={",src/train/ColTraining.py,awgreene/311InYourNeighborhood,1
"import numpy as np
from sklearn.base import TransformerMixin
from sklearn.ensemble import RandomForestClassifier


class RFSelection(TransformerMixin):

    def __init__(self, n_features=None, n_estimators=100, random_state=0):
        self.rf = RandomForestClassifier(
            n_estimators=n_estimators, random_state=0)
        self.n_features = None

        if n_features is not None:
            self.n_features = n_features

    def fit(self, X, y=None):
        X_local = np.array(X)
        self.rf.fit(X_local, y)",trans4mers/feature_selection/rf_selection.py,rloliveirajr/sklearn_transformers,1
"            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,
                                                            random_state=88)
        else :
            X_train=X
            y_train=y
            X_test_df=data_test.loc[:,list(X_cols)]
            X_test_df=denan(X_test_df,axis='both',condi='all any')
            X_test=X_test_df.as_matrix()
            y_test=None

        model = RandomForestClassifier(random_state =88)
        param_grid = {""n_estimators"": [1000],
                      ""max_features"": ['sqrt'],#[None,'sqrt','log2'],
                      ""min_samples_leaf"":[1],#[1,25,50,100],
                      ""criterion"": ['entropy'],#[""gini"", ""entropy""]
                     }

        grid_search = GridSearchCV(model, param_grid=param_grid,cv=10)
        grid_search.fit(X_train,y_train)
",dms2dfe/lib/io_ml.py,kc-lab/dms2dfe,1
"
### import all learning data from .xlsx file
Locatation = r'C:\Users\UserTrader\Documents\IQ14_testmatris_2stdav_5classifiersROC_ALL.xlsx'
data = pd.read_excel(Locatation)
X = np.array(data[FEATURES].values) # making a np array from the pd dataset

### START - Using part from targets who calculate how Big the future move was, Rate of Change (ROC)
######################
y = data['_end1'].values
from sklearn.ensemble import RandomForestClassifier
RFclf = RandomForestClassifier(n_estimators=198,
                               max_leaf_nodes=365,
                               min_samples_leaf=365,
                               min_samples_split=76,
                               verbose=20,
                               max_depth=52,
                               n_jobs=2)
RFclf.fit(X,y)
from sklearn.externals import joblib  ### Needed if you want to SAVE your learned dataset to .pkl
joblib.dump(RFclf, 'C:\Users\UserTrader\Documents\_PKLfiles_02\IQ14_5cat1dROC.pkl')",IQ16_pklProduction.py,MikaelFuresjo/ImundboQuant,1
"    
    normalizer = StandardScaler()
    normalizer.fit(X_train)
    X_train_norm = normalizer.transform(X_train)
    X_test_norm = normalizer.transform(X_test)


    classifiers = [
            KNeighborsClassifier(),
            SVC(),
            RandomForestClassifier(),
            GaussianNB(),
            LDA(),
            ]

    names = model_names 
           

    params = [{""n_neighbors"": 3. ** np.arange(5)},
              {""kernel"": [""rbf""], ""gamma"": 3. ** np.arange(-5, 5), ",cell_classifier.py,cyanut/cell-classifier,1
"    predict = np.zeros(test.shape[0])
    #dtest = xgb.DMatrix(test)
    skf = StratifiedKFold(target, n_folds=n_folds, random_state=random_state)
    for train_index, cv_index in skf:
        # train
        x_train, x_cv = train[train_index], train[cv_index]
        y_train, y_cv = target[train_index], target[cv_index]
        #dtrain = xgb.DMatrix(x_train, label=y_train)
        #dvalid = xgb.DMatrix(x_cv, label=y_cv)
        #watchlist = [(dtrain, 'train'), (dvalid, 'eval')]
        clf = RandomForestClassifier(**params).fit(x_train, y_train)
        #bst = xgb.train(params, dtrain, num_round, watchlist, early_stopping_rounds=early_stopping_rounds, maximize=True)
            # test / score
        predict_cv = clf.predict_proba(x_cv)[:,1]
        avg_score += -log_loss(y_cv, predict_cv)
        predict += clf.predict_proba(test)[:,1]#bst.predict(dtest, ntree_limit=bst.best_iteration)
    predict /= n_folds
    avg_score /= n_folds 
    # store
    new_row = pd.DataFrame([np.append([avg_score], list(params.values()))],",optim_hyperopt_RF.py,AurelienGalicher/DStoolkit,1
"    print 'Done...\n\n'

    print 'Testing BBall Random Forest Classifier...' 
    test = CrossValidation(rf, 4, test_set)
    test.run_test()
    print 'Done...'

    domain = t.get_domain(ATS)

    print 'Testing Random Forest Classifier...' 
    rf = RandomForestClassifier(CLASS_ID, ATS, train, 15, 7, 5, domain=domain)

    test = CrossValidation(rf, 4, test_set)
    test.run_test()
    print 'Done...\n\n'

    print 'Testing decision tree... '
    tree = DecisionTreeClassifier(CLASS_ID, ATS, train, domain=domain)
    test = CrossValidation(tree, 4, test_set)
    test.run_test()",test.py,bheinzelman/NBAShotPredictor,1
"

v_len = 38


EXTClf = ExtraTreesClassifier(n_estimators=100, max_depth=None,
    min_samples_split=7, random_state=0)

DTClf = DecisionTreeClassifier(max_depth=None, min_samples_split=100, random_state=0)
        
RFClf = RandomForestClassifier(n_estimators=20, max_depth=None,min_samples_split=1, random_state=0)
        

# 38 is best 
'''
for v_len in range(35,41):
    print(""v_len: %d""%v_len)
'''
data = pirna_data['data_h']
target = pirna_data['target_h']",Experiment/Sklearn/CrossSpeciesRF.py,mushroom-x/piRNA,1
"
			clfAll = linear_model.LogisticRegression(C=1e5, penalty='l2', tol=1e-6)
			clfAll.fit(Ytr, lab_tr)
			predicted = clfAll.predict_proba(Ytst)
			if i<len(CV_list)-1:
				predLRall[cv_tst,:] = predicted
			else:
				predLRall_ts = predicted
			forest = ExtraTreesClassifier(n_estimators=500,
										  random_state=0, criterion=""entropy"", bootstrap=False)
			#forest = RandomForestClassifier(n_estimators=500,
			#							 random_state=0, criterion=""entropy"")
			forest.fit(Ytr, lab_tr)
			pred = forest.predict_proba(Ytst)
			#pdb.set_trace()
			if i<len(CV_list)-1:
				predRF[cv_tst,:] = pred#[:,1]
			else:
				predRF_ts = pred#[:,1]
			importances = forest.feature_importances_",py/core/cyclone.py,PMBio/cyclone,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/max_aggregate_only.py,diogo149/CauseEffectPairsPaper,1
"print ""Creating average feature vecs for test reviews""
clean_test_reviews = []
for review in test[""review""]:
    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))

testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)


# Fit a random forest to the training data, using 100 trees
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier( n_estimators = 100 )

print ""Fitting a random forest to labeled training data...""
forest = forest.fit( trainDataVecs, train[""sentiment""] )

# Test & extract results 
result = forest.predict( testDataVecs )

# Write the test results 
output = pd.DataFrame( data={""id"":test[""id""], ""sentiment"":result} )",Word2Vec.py,ScoffM/ITESO-Word2Vec,1
"	X_test = LD.data['X_test']
	
	
	Xtv = np.copy(X_valid)
	Xts = np.copy(X_test)
	

	import xgboost as xgb
	if LD.info['name']== 'alexis':

		model = ensemble.RandomForestClassifier(max_depth=140, n_estimators=1800, n_jobs=-1, random_state=0, verbose=0, warm_start=True)
		model2 = ensemble.RandomForestClassifier(max_depth=140, n_estimators=1800, n_jobs=-1, random_state=1, verbose=0, warm_start=True)
		model.fit(X_train, Y_train)	
		model2.fit(X_train, Y_train)
		
		preds_valid0 = model.predict_proba(X_valid)
		preds_test0 = model.predict_proba(X_test)
		
		preds_valid2 = model2.predict_proba(X_valid)
		preds_test2 = model2.predict_proba(X_test)",lib/automl.py,djajetic/AutoML3Final,1
"
    rewards = calcRewards(data)
    print len(rewards), 'rewards calculated'
    print json.dumps(rewards[-10:-5], indent=4)

    # split sets
    X_train, X_test, y_train, y_test = getSplit(features, rewards)
    print 'sets splitted'

    # fitting regressor
    # rfc = RandomForestClassifier(n_estimators=30)
    rfc = ExtraTreesClassifier(
        n_estimators=30,
        # max_features='sqrt',
    )
    rfc.fit(X_train, y_train)
    print 'fitted classifier'

    # saving
    sk.externals.joblib.dump(rfc, 'models/' + currency + '.pkl', compress=9)",04_oanda/classifier-rf.py,Tjorriemorrie/trading,1
"        'random_state':None, 
        'class_weight':'balanced', 
        'n_jobs':1 
    }
        
    def __init__(
        self,data_block, predictors=[],cv_folds=10,
        scoring_metric='accuracy',additional_display_metrics=[]):

        base_classification.__init__(
            self, alg=RandomForestClassifier(), data_block=data_block, 
            predictors=predictors,cv_folds=cv_folds,
            scoring_metric=scoring_metric, 
            additional_display_metrics=additional_display_metrics
            )

        self.model_output = pd.Series(self.default_parameters)
        self.model_output['Feature_Importance'] = ""-""
        self.model_output['OOB_Score'] = ""-""
",easyML/models_classification.py,aarshayj/easyML,1
"import math

train_final = '../data/features/extend/train.csv'
class Train:
    def select_dataset(self, path):
        self.ddir = path
    def select_model(self):
        print 'select model'
        #return LogisticRegression()#penalty = 'l2', C=0.3)
        #return GradientBoostingClassifier(n_estimators=21,max_depth=5,loss='deviance')
        self.clf = RandomForestClassifier(n_estimators=35,max_depth = 13, max_features='auto',n_jobs=2)

    def train(self, clf_path):
        X, y, ids = mylibs.myio.load_train(self.ddir)
        print 'train'
        self.clf.fit(X, y)
        self.evaluation(X, y)
        mylibs.models.save(clf_path, self.clf)
    
    def evaluation(self, X, y):",ftrldata/TCReBuild/codes/revision.py,CharLLCH/jianchi_alimobileR,1
"               interpolation='nearest', cmap=plt.cm.binary)
    plt.colorbar()
    plt.xlabel(""true label"")
    plt.ylabel(""predicted label"")
    plt.title(""SVC: kernel = {0}"".format(kernel))
    
# random forest results
from sklearn.ensemble import RandomForestClassifier

for max_depth in [3, 5, 10]:
    clf = RandomForestClassifier(max_depth=max_depth).fit(Xtrain, ytrain)
    ypred = clf.predict(Xtest)
    print(""RF: max_depth = {0}"".format(max_depth))
    print(metrics.f1_score(ytest, ypred))
    plt.figure()
    plt.imshow(metrics.confusion_matrix(ypred, ytest),
               interpolation='nearest', cmap=plt.cm.binary)
    plt.colorbar()
    plt.xlabel(""true label"")
    plt.ylabel(""predicted label"")",notebooks/solutions/04_svm_rf.py,brianjwoo/sklearn_pycon2014,1
"
from sklearn.ensemble import RandomForestClassifier

sys.path.append('src')

from data import get_featues, get_label


class RandomForestModel(object):
    def __init__(self):
        self.clf = RandomForestClassifier(n_estimators=50, max_depth=700)
        self.name = 'RandomForest'

    def get_params(self):
        return self.clf.get_params()

    def train(self, dframe):
        X = get_featues(dframe)
        y = get_label(dframe)
        self.clf.fit(X, y)",src/models/random_forest.py,artofai/overcome-the-chaos,1
"
# Constructing the k-fold cross validation iterator (k=10)
cv = KFold(n=features.shape[0],  # total number of samples
           n_folds=10,           # number of folds the dataset is divided into
           shuffle=True,
           random_state=123)

t0 = time()

parameters = {'min_samples_split':[10, 50, 100, 1000]}
clf = grid_search.GridSearchCV(RandomForestClassifier(), parameters)
clf.fit(features, labels)

print ""escape time : "", round(time()-t0, 3), ""s""

print ""best estimator is %s"" % clf.best_estimator_
print ""best score is %s"" % clf.best_score_
print ""best parameter is %s"" % clf.best_params_
print clf.grid_scores_
",nytimes/step4_randomforest.py,dikien/Machine-Learning-Newspaper,1
"# Licensed under GNU GPL v3

import pickle
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree._tree import DTYPE
#from multiprocessing import pool

class DT(object):
	def __init__(self, verb=False):
		self.algo = RandomForestClassifier(n_jobs=-1)
		self.verb = verb
#		self.pool = pool.Pool()

	def store(self, dst): pickle.dump(self.algo, dst)
	def load(self, src): self.algo = pickle.load(src)

	def fit(self, X, Y):
		protos = set(Y)
		self.algo.fit(X, Y)",npkts/DT.py,iitis/mutrics,1
"    trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )

    print ""Creating average feature vecs for test reviews""

    testDataVecs = getAvgFeatureVecs( getCleanReviews(test), model, num_features )


    # ****** Fit a random forest to the training set, then make predictions
    #
    # Fit a random forest to the training data, using 100 trees
    forest = RandomForestClassifier( n_estimators = 100 )

    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit( trainDataVecs, train[""sentiment""] )

    # Test & extract results
    result = forest.predict( testDataVecs )

    # Write the test results
    output = pd.DataFrame( data={""id"":test[""id""], ""sentiment"":result} )",pycode/Word2Vec_AverageVectors.py,CharLLCH/Word2Vec,1
"        rdg = RidgeCV(alphas=np.logspace(-3, 3, 7))
        rdg.fit(x_train, y_train)
        x_train_stacked.append(rdg.predict(x_train))
        x_test_stacked.append(rdg.predict(x_test))
        coeffs.append(rdg.coef_)

    x_train_ = np.asarray(x_train_stacked).T
    x_test_ = np.asarray(x_test_stacked).T


    rfc = RandomForestClassifier()
    rfc.fit(x_train_, y_train)
    scores = rfc.score(x_test_, y_test)
    
    print 'score rfc: %.2f' % (scores)    
    B = Bunch(score=scores
              #proba=probas,
              #coeff=coeffs,
              #coeff_lgr=coeff_lgr,
              #proba_kbest=probas_kbest,",rf_fmri.py,mrahim/adni_petmr_analysis,1
"    print ""Reading data...""
    X, Y = utils.read_data(""../files/train_10.csv"")
    print ""Preprocessing...""
    X = preprocess(X)
    print ""Extracting Features...""
    X = extractFeatures(X)
    Y = [int(x) for x in Y]
    X, Y = np.array(X), np.array(Y)
    classMap = sorted(list(set(Y)))
    accs = []
    rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, compute_importances=True, oob_score=True)
    rf.fit(X, Y)
    importantFeatures = []
    for x,i in enumerate(rf.feature_importances_):
        print len(rf.feature_importances_)
        print x, i
        if i>np.average(rf.feature_importances_):
            importantFeatures.append(str(x))
    print 'Most important features:', ', '.join(importantFeatures)
    print rf.oob_score_",Eye Movement Tracking/src/feature_extraction.py,shaileshahuja/MineRush,1
"
		#get list of labels for the trips in traintrips
		g = open(""driver_stats/""+str(self.name)+""_trainingLabels.csv"")
		trainLabels = np.genfromtxt(g, delimiter=',')
		g.close()

		res = []
		print ""starting predictions:""
		for i in range(0, len(traintrips)):
			print ""trial "" + str(i)
			clf = RandomForestClassifier(random_state=1, n_estimators=500, n_jobs=1, min_samples_leaf=3) 
			#print target
			clf.fit(traintrips[i], trainLabels)
			predLabels = clf.predict (testtrips[i])
			print predLabels
			r = self.calculateResults(predLabels, testLabels[i])
			print r
			res.append(r)

		scoring = np.array(res)",Driver_val.py,mkery/CS349-roads,1
"        from sklearn.ensemble import RandomForestClassifier

        digits = load_digits()  # 1797 by 64
        X = digits.data
        y = digits.target

        # simple splitting for validation testing
        X_train, X_test = X[:1200], X[1200:]
        y_train, y_test = y[:1200], y[1200:]

        rfc = RandomForestClassifier()
        rfc.fit(X_train, y_train)

        from estimators.models import Evaluator

        ev = Evaluator(X_test=X_test, y_test=y_test, estimator=rfc)
        assert ev.X_test is X_test
        assert ev.y_test is y_test
        assert ev.estimator is rfc
        assert ev.y_predicted is None",estimators/tests/test_evaluations.py,fridiculous/django-estimators,1
"def better_inv_dist(dist):
    c=1
    return 1. /(c+dist)

features, class_outputs = get_training('../joined_matrix_split.txt') 
# 0:27 category  28:38= supervisor district, 39 = count 40= output
features_10k, class_10k = features, class_outputs
test_features_10k, test_class_10k = features[10000:20000], class_outputs[10000:20000]

#all_models = [tree.DecisionTreeRegressor(), ensemble.RandomForestRegressor(), ensemble.AdaBoostRegressor(), neighbors.KNeighborsRegressor()] 
#all_models = [tree.DecisionTreeClassifier(), ensemble.RandomForestClassifier(), ensemble.AdaBoostClassifier(), neighbors.KNeighborsClassifier()] 
all_models = [ensemble.RandomForestRegressor()]
opt_models = dict()
for i,model in enumerate(all_models):
    param_grid = None
    opt_model = train(features_10k, class_10k, test_features_10k, test_class_10k, model, params_grid = param_grid)
    r2 = opt_model.best_score_
    opt_models[opt_model.best_estimator_] = r2

print(""\n"")",regressionSpecific/train_regression.py,dolphyin/cs194-16-data_manatees,1
"# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division

import sklearn.ensemble

import submissions
from data import *

rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=2)
rf.fit(train, target)
pred = rf.predict_proba(test)

submissions.save_csv(pred, ""random_forest.csv"")",walmart_recruiting_trip_type_classification/random_forest.py,wjfwzzc/Kaggle_Script,1
"      if test:
        test_scores['graphconv'] = model_graphconv.evaluate(
            test_dataset, [classification_metric], transformers)

  if model == 'rf':
    # Loading hyper parameters
    n_estimators = hyper_parameters['n_estimators']

    # Building scikit random forest model
    def model_builder(model_dir_rf):
      sklearn_model = RandomForestClassifier(
          class_weight=""balanced"", n_estimators=n_estimators, n_jobs=-1)
      return dc.models.sklearn_models.SklearnModel(sklearn_model, model_dir_rf)

    model_rf = dc.models.multitask.SingletaskToMultitask(tasks, model_builder)

    print('-------------------------------------')
    print('Start fitting by random forest')
    model_rf.fit(train_dataset)
",examples/benchmark.py,ktaneishi/deepchem,1
"
from sklearn.ensemble import RandomForestClassifier


Builder.load_file('view/blocks/randomforestblock.kv')

class RandomForestBlock(Block):
    out_1 = ObjectProperty()

    def function(self):
        self.out_1.val = RandomForestClassifier()",persimmon/view/blocks/randomforestblock.py,AlvarBer/Persimmon,1
"    print np.sum(y_pred == y_test)/len(y_pred)
    return y_pred

def getSVMClassifier():
    return svm.SVC(kernel='rbf', C=10, gamma=10, probability=True)

def getSVMLinearClassifier():
    return svm.SVC(kernel='linear', C=10, probability=True)

def getSimpleRDFClassifier():
    rdf = RandomForestClassifier(max_features = 'auto', max_depth=10)
    return rdf

def getCustomRDFClassifier():
    class RandomForestClassifierWithCoef(RandomForestClassifier):
        def fit(self, *args, **kwargs):
            super(RandomForestClassifierWithCoef, self).fit(*args, **kwargs)
            self.coef_ = self.feature_importances_

    rdf = RandomForestClassifierWithCoef(max_features = 'auto', max_depth=6)",src/classifiers.py,javierfdr/credit-scoring-analysis,1
"targets.extend([1] * len(getYPercGrand(boySamplesPickle)))
targets = np.array(targets)
#from sklearn.naive_bayes import GaussianNB
#gnb = GaussianNB()
#from sklearn.qda import QDA
#gnb = QDA()
from sklearn.neighbors import KNeighborsClassifier
gnb = KNeighborsClassifier(5)
#from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
#gnb= AdaBoostClassifier()
#gnb= RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)
#from sklearn.tree import DecisionTreeClassifier
#gnb = DecisionTreeClassifier()
gnb.fit(training, targets)
y_pred = gnb.predict(training)
print >> sys.stderr, ""Testing classifier on trainingset.\nNumber of mislabeled points : %d"" % (targets != y_pred).sum()

## Start analyzing the test samples and create plots

f, ((ax1,ax2),(ax3,ax4)) = subplots(nrows=2, ncols=2)",defrag.py,aukluobo/wisecondor,1
"                preparer_pipeline=[
                    NoiseInjector(stddev=1e-4),
                ],
                label_encoder=OrdinalLabelEncoder()
            ))
        ],
        classifiers=[
            (""ab"", AdaBoostClassifier(n_estimators=100)),
            (""gb"", GradientBoostingClassifier(n_estimators=100)),
            (""ba"", BaggingClassifier(n_estimators=100)),
            (""rf"", RandomForestClassifier(n_estimators=100, max_features=""log2"")),
            (""et"", ExtraTreesClassifier(n_estimators=100, max_features=""log2""))
        ],
        n_folds=5,
        n_runs=10,
        normalize=normalize
    )


def get_results_file_name(dataset, normalize, subset_size):",emetrics/evaluation/all_experiments.py,etamponi/emetrics,1
"from modules.LanguageUnderstanding.utils.utils import *


class DialogueActTypePredictor(object):

    def __init__(self, file_name='model.pkl'):
        try:
            file_path = os.path.join(os.path.dirname(__file__), file_name)
            self.estimator = joblib.load(file_path)
        except FileNotFoundError:
            self.estimator = RandomForestClassifier()

    def train(self, train_x, train_y, file_name='model.pkl'):
        self.estimator.fit(train_x, train_y)
        joblib.dump(self.estimator, file_name)

    def predict(self, X):
        return self.estimator.predict(X)[0]

    def evaluate(self, test_x, test_y):",modules/LanguageUnderstanding/DialogueActType/predictor.py,Hironsan/HotPepperGourmetDialogue,1
"	>>> postags = [[(""z_POS"",token[1]) for token in instance] for instance in test if len(instance)>0]
	>>> instances = [[token[0] for token in instance] for instance in test if len(instance)>0]
	
	And finally we classify the test instances
	>>> result = extractor.extract(instances, postags)

	As deafult, a CRF model is used. However, when initialising the `citation_extractor` you can 
	pass on to it any scikit classifier, e.g. RandomForest:

	>>> from sklearn.ensemble import RandomForestClassifier
	>>> extractor = citation_extractor(base_settings,RandomForestClassifier())

	""""""

	def __init__(self,options,classifier=None,labelled_feature_sets=None):
		self.classifier=None
		self.fe = FeatureExtractor()
		if(options.DATA_FILE != """"):
			allinone_iob_file = options.DATA_FILE
		elif(options.DATA_DIRS != """"):",citation_extractor/core.py,mfilippo/CitationExtractor,1
"        'ada__n_estimators': np.linspace(1, 100, 10, dtype=np.dtype(np.int16)),
        # 'svc__gamma': np.linspace(0, 50, 20),
    }
    gs = grid_search.GridSearchCV(clf, parameters, verbose=1, refit=False, cv=kFolds)
    gs.fit(X_norm, y)
    return gs.best_params_['ada__n_estimators'], gs.best_score_

lr_l1 = LogisticRegression(C=100, penalty='l1', tol=0.01)
lr_l2 = LogisticRegression(C=100, penalty='l2', tol=0.01)

rf = RandomForestClassifier(max_depth=5, min_samples_split=2)
et = ExtraTreesClassifier(max_depth=20, min_samples_split=2)

dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)
dt_9_1 = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)
dt_20_1 = DecisionTreeClassifier(max_depth=20, min_samples_leaf=1)

clf_default = AdaBoostClassifier(n_estimators=100)
clf_stump = AdaBoostClassifier(base_estimator=dt_stump, n_estimators=100)
clf_dt = AdaBoostClassifier(base_estimator=dt_9_1, n_estimators=100)",finance/EnsembleTest.py,Ernestyj/PyStudy,1
"    # (""PCA_2"", PCA(2)),
    # (""PCA_10"", PCA(2)),
    # (""PCA_100"", PCA(2)),
    # (""PCA_400"", PCA(2)),
    #
    # (""TruncatedSVD_2"", TruncatedSVD(2)),
    # (""TruncatedSVD_10"", TruncatedSVD(2)),
    # (""TruncatedSVD_100"", TruncatedSVD(2)),
    # (""TruncatedSVD_400"", TruncatedSVD(2)),
    # (""LogisticRegression"", SelectFromModel(LogisticRegression(penalty=""l1""))),
    # (""RandomForestClassifier_20"", SelectFromModel(RandomForestClassifier(n_estimators=20, max_depth=3))),
    # (""RandomForestClassifier_100"", SelectFromModel(RandomForestClassifier(n_estimators=100, max_depth=None))),
]

estimators = [
    (""SVC_linear"", SVC(kernel='linear')),
    # (""LinearSVC"", LinearSVC(penalty='l1', dual=False)),
    # (""SVC_rbf"", SVC(kernel='rbf')),
    # (""RandomForestClassifier_20"", RandomForestClassifier(n_estimators=20, max_depth=3)),
    # (""RandomForestClassifier_100"", RandomForestClassifier(n_estimators=100, max_depth=5)),",scripts/quick_feature_selection_and_pipeline_evaluation.py,juanmirocks/LocText,1
"CudaTree is an implementation of Leo Breiman's Random Forests adapted to run on the GPU. A random forest is an ensemble of randomized decision trees which vote together to predict new labels. CudaTree parallelizes the construction of each individual tree in the ensemble and thus is able to train faster than the latest version of scikits-learn.

Usage
-------------

::

  import numpy as np
  from cudatree import load_data, RandomForestClassifier
  x_train, y_train = load_data(""digits"")
  forest = RandomForestClassifier(n_estimators = 50, max_features = 6)
  forest.fit(x_train, y_train)
  forest.predict(x_train)

Dependencies
--------------

CudaTree is writen for Python 2.7 and depends on:

* scikit-learn",setup.py,EasonLiao/CudaTree,1
"from sklearn.metrics import roc_curve, auc, confusion_matrix
from Python.RUSRandomForest import RUSRandomForestClassifier
import pickle

mci_df = pd.read_csv('../../Classification_Table.csv', delimiter=',')
mci_df = mci_df.drop('ID', axis=1)
Y = mci_df.Conversion.values
mci_df = mci_df.drop('Conversion', axis=1)
X = mci_df.as_matrix()

RUSRFC = RUSRandomForestClassifier.RUSRandomForestClassifier()
predClasses, classProb, featureImp, featureImpSD = RUSRFC.CVJungle(X, Y, k=2, shuffle=True, print_v=True)

pickle.dump(RUSRFC, open('pickleTest.pkl', 'wb'))

UNpickledRUS = pickle.load(open('pickleTest.pkl', 'rb'))


cm = confusion_matrix(Y, predClasses)
",Python/RUSRandomForest/runRUSRFC.py,sulantha2006/Conversion,1
"except:
    print ""\tLoading traing data...""
    train_xs, train_y = load_training_rasters(
        response_raster, explanatory_rasters, selected)

    print ""\tTraining classifier...""
    import time
    start = time.time()

    # Instansiate the classifier
    rf = RandomForestClassifier(n_estimators=10, n_jobs=1)
    # fit the classifier to the training data
    rf.fit(train_xs, train_y)

    print ""\ttime:"", time.time() - start, ""seconds""
    joblib.dump(rf, pfile)

###############################################################################
# Assess predictive accuracy
print ""Cross validation""",agzones/aez_predict.py,Ecotrust/climate-prediction,1
"from ..tools import transform_board,inverse_transform
from sklearn import clone
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.grid_search import ParameterGrid
from random import shuffle

class LocalClassifier(Classifier):
    ''' Predict each cell 1 at a time. '''

    def __init__(self,window_size=1,off_board_value=0,clf=RandomForestClassifier(), test_use_transforms = False, test_threshold = 0.5):
        '''
        LocalClassifier constructor.
        window_size is number of cells (in all 4 directions) to include in the features for predicting the center cell.
        So window_size=1 uses 3x3 chunks, =2 uses 5x5 chunks, and so on. off_board_value is the value to represent
        features that are outside the board, defaults to 0 (ie DEAD). Copies of clf will be made and separate
        classifiers used for each unique delta.
        test_use_transforms - if true, uses all 8 board transforms and averages results to predict, uses test_treshold as proba threshold for ALIVE predictions
        '''
        self.window_size = window_size",reverse_game_of_life/classifier/local_classifier.py,valisc/reverse-game-of-life,1
"#clf = SGDClassifier(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'linearsvm accuracy ', accuracy_score(test_gt,pred)

#clf = LogisticRegression(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'logistic accuracy ', accuracy_score(test_gt,pred)

#clf = RandomForestClassifier(min_samples_leaf=20,n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'rfc accuracy ', accuracy_score(test_gt,pred)
#pred = clf.predict(raw_orig)
#with open('rfc.txt','w') as otf:
#    for p in pred:
#        otf.write(str(int(p)) + '\n')
from keras.utils.np_utils import to_categorical
",learning/cosine-basic.py,leonidk/centest,1
"from sklearn.svm import *
from sklearn.multiclass import OneVsRestClassifier
from sklearn.pipeline import Pipeline
import sklearn.cross_validation as skcv
import sklearn.preprocessing as skpp

normalise = True
select = False

def random_forest(X, Y):
	trainer = RandomForestClassifier(n_jobs=-1, n_estimators=300, max_features=None)
	return build_classifier(X, Y, trainer)
	
def extra_random_trees(X, Y):
	trainer = ExtraTreesClassifier(n_jobs=-1, n_estimators=300,  max_features=None)
	return build_classifier(X, Y, trainer)

def forest_one_v_rest(X, Y):
	trainer = OneVsRestClassifier(ExtraTreesClassifier(n_jobs=-1, n_estimators=300,  max_features=None))
	return build_classifier(X, Y, trainer)",project2/forest.py,ethz-nus/lis-2015,1
"
#print(train.columns)
scl=preprocessing.RobustScaler()
X_train=scl.fit_transform(X_train)
X_val=scl.transform(X_val)
X_test=scl.transform(X_test)


from sklearn import linear_model,ensemble
#
clf=ensemble.RandomForestClassifier(n_estimators=400,random_state=100,max_depth=10,max_features=None,n_jobs=-1)
#clf =linear_model.SGDClassifier(loss='hinge', penalty='l2', alpha=0.00001, l1_ratio=0.15, fit_intercept=True, n_iter=200,
#                                shuffle=True, verbose=0, epsilon=0.1, n_jobs=-1, random_state=17, learning_rate='invscaling',eta0=1.0, power_t=0.5, class_weight=None, warm_start=False, average=False)
clf.fit(X_train,y_train)
predictions=clf.predict_proba(X_test)[:,1]
from sklearn.metrics import accuracy_score
y_pred=clf.predict(X_val)
score=accuracy_score(y_val, y_pred)

model='RF'",cervical-cancer-screening/models/src/RFonfeatures-new-NotGrouped.py,paulperry/kaggle,1
"
    print 'Get test data...'
    data = read_csv('./test.csv')

    return data

# final function
def go():
    data = get_train_data()

    model_rfc = RandomForestClassifier(n_estimators = 1024, criterion = 'entropy', n_jobs = -1)

    print 'Go!!!'

    print 'RFC...'
    test = get_test_data()
    target = data.label
    train = data.drop(['label'], axis = 1)

    print ""...""",digit_recognizer.py,zhzhussupovkz/digit-recognizer,1
"# Train a Support Vector Machine classifier with Radial Basis Function kernel with
# chosen C and gamma arguments
def svmTrain(C, gamma, X, y):
    svm = SVC(C = C, gamma = gamma, kernel = 'rbf')
    svm.fit(X, y)
    return svm

# Train a Random Forest classifier with chosen n_estimators and max_features arguments
def randomForestTrain(n_estimators, max_features, X, y):
    if (max_features is None):
        rf = RandomForestClassifier(n_estimators = n_estimators, max_features = None)
    else:
        rf = RandomForestClassifier(n_estimators = n_estimators, max_features = max_features)
    rf.fit(X, y)
    return rf

# Train a K-Neighbors classifier with selected neighbors, weight and
# algorithm
def knnTrain(n_neighbors, weights, algorithm, X, y):
    knn = KNeighborsClassifier(n_neighbors = n_neighbors, weights = weights, algorithm = algorithm)",processing/classification.py,marcusangeloni/smc2016,1
"raw_train_data, y_train = read_train_file()
raw_test_data = read_test_file()

raw_data = np.append(raw_train_data, raw_test_data, axis=0)

data = process_raw_data(raw_data)

X_train = data[:891]
X_test = data[891:]

clf = RandomForestClassifier(n_estimators=100)

def perform_cross_validation(X, y, clf):
	sss = StratifiedShuffleSplit(y, 3, test_size=0.1, random_state=42)

	for train_index, test_index in sss:
		X_train, X_test = X[train_index], X[test_index]
		y_train, y_test = y[train_index], y[test_index]

		clf.fit(X_train, y_train)",titanic.py,reetawwsum/Titanic,1
"    ""Min. Temp (C)"", ""Median Temp (C)"", ""Max. Temp (C)""]
bands = [""TreeCover"", ""Soil-Cover"", ""Veg-Cover"", ""Impervious-Cover"", ""Temp-Min"", 
    ""Temp-Median"", ""Temp-Max""]
output_files = []
for band in bands:
    output_files.append(""{base}CR-{band}-plot.png"".format(base = base, band = band))
cols = aei.color.color_blind(len(bands))

# set the models to apply
models = [""DecisionTree"", ""SVM"", ""RandomForest"", ""GradientBoosting""]
mods = [tree.DecisionTreeClassifier(), svm.SVC(), ensemble.RandomForestClassifier(),
    ensemble.AdaBoostClassifier()]
output_models = []
for model in models:
    output_models.append(""{base}CR-southern-prediction-{model}.tif"".format(base = base, model = model))

# grab the indices for the aedes aegyptii and field plot data
fs_cb_band = fs_cb_ref.ReadAsArray()
fs_cb = np.where(fs_cb_band == 1)
fs_cb_band = None",bin/mosquito-plot-histograms.py,christobal54/aei-grad-school,1
"    X = df_col_to_matrix(df['hidden_repr'])
    Y = df['shape']

    # prepare shuffle split cross-validation, set random_state to a arbitrary constant for recomputability
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)

    n_estimators = [10, 30, 50, 80, 100]
    max_depth = [2, 5, 10, 15, 20, 25, 30, 35]

    rf = sklearn.ensemble.RandomForestClassifier()

    classifier = GridSearchCV(estimator=rf, cv=cv, param_grid=dict(n_estimators=n_estimators, max_depth=max_depth))
    # run fit with all hyperparameter values
    classifier.fit(X_train, y_train)
    rf = rf.set_params(n_estimators=classifier.best_estimator_.n_estimators,
                       max_depth=classifier.best_estimator_.max_depth)


    title = 'Learning Curves (Logistic Regression, n_trees=%i, depth=%i, %i shuffle splits, %i train samples)' %  (",data_postp/similarity_computations.py,jonasrothfuss/DeepEpisodicMemory,1
"    
    not_click = data[data.click_bool==0]
    not_click_train = not_click.sample(frac = percent_for_train)
    not_click_test = not_click.drop(list(not_click_train.axes[0]))
    
    train = click_train.sample(n=not_click_train.shape[0], replace=True).append(not_click_train, ignore_index=True)
    test = click_test.append(not_click_test, ignore_index=True)
    return [train, test]

Ada = AdaBoostClassifier()
Random = RandomForestClassifier()
Decision = sklearn.tree.DecisionTreeClassifier()

sampling = random_sampling(train, 0.6)

x_train = sampling[0].drop(['click_bool', 'gross_bookings_usd', 'booking_bool'], axis=1)
y_train = sampling[0].booking_bool
x_test = sampling[1].drop(['click_bool', 'gross_bookings_usd', 'booking_bool'], axis=1)
y_test = sampling[1].booking_bool
keys = list(x_train.keys())",Classification_S3.py,BigDataAnalytics2017/ProjectOTA,1
"X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, classifier=tree, test_idx=test_idx, xlabel='petal length', ylabel='petal width')

from sklearn.tree import export_graphviz
export_graphviz(tree, out_file='tree.dot', feature_names=['petal length', 'petal width'])

# random forest
from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(criterion='entropy',
                                n_estimators=10, 
                                random_state=1,
                                n_jobs=2)
forest.fit(X_train, y_train)

plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=test_idx, xlabel='petal length', ylabel='petal width')",code/ch03/iris.py,xdnian/pyml,1
"    def set_params_list( self, learner_params, i):
        
        RF_size = self.config.configuration[""n_trees""]
        n_jobs = self.config.configuration[""n_jobs""]

        m_feat = learner_params[0]
        m_dep = learner_params[1]
        m_sam_leaf  = learner_params[2]
        
        if self.method == 'classification':
            self.learner = RandomForestClassifier(n_estimators = RF_size, 
                                                          oob_score = True, n_jobs= n_jobs,
                                                          max_depth = m_dep, max_features = m_feat,
                                                          min_samples_leaf = m_sam_leaf, random_state= i)

        elif self.method == 'regression':
            self.learner = RandomForestRegressor(n_estimators = RF_size, 
                                                         oob_score = True, n_jobs= n_jobs,
                                                         max_depth = m_dep, max_features = m_feat,
                                                         min_samples_leaf = m_sam_leaf, random_state= i)",BCI_Framework/RandomForest_BCI.py,lol/BCI-BO-old,1
"h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""Linear Discriminant Analysis"",
         ""Quadratic Discriminant Analysis""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",Python/testscikit.py,Crobisaur/HyperSpec,1
"
def createKNN():
    clf = KNeighborsClassifier(n_neighbors=13,algorithm='kd_tree',weights='uniform',p=1)
    return clf

def createDecisionTree():
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createRandomForest():
    clf = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createExtraTree():
    clf = ExtraTreesClassifier(n_estimators=500, max_depth=None, min_samples_split=1, random_state=0)
    return clf

def createAdaBoost():
    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    clf = AdaBoostClassifier(dt, n_estimators=300)",kaggle-data-science-london-scikitlearn/src/classify.py,KellyChan/Kaggle,1
"        print(params)

        # cross validation here
        scores = []
        for train_ix, test_ix in makeKFold(5, self.y, 1):
            X_train, y_train = self.X[train_ix, :], self.y[train_ix]
            X_test, y_test = self.X[test_ix, :], self.y[test_ix]
            weight = y_train.shape[0] / (2 * np.bincount(y_train))
            sample_weight = np.array([weight[i] for i in y_train])

            clf = RandomForestClassifier(**params)
            cclf = CalibratedClassifierCV(base_estimator=clf,
                                          method='isotonic',
                                          cv=makeKFold(3, y_train, 1))
            cclf.fit(X_train, y_train, sample_weight)
            pred = cclf.predict(X_test)
            scores.append(f1_score(y_true=y_test, y_pred=pred))

        print(scores)
        score = np.mean(scores)",model/level3_model_rf.py,jingxiang-li/kaggle-yelp,1
"
    self.parameters = {
      'clf__gamma': (0.01, 0.03, 0.1, 0.3, 1),
      'clf__C': (0.1, 0.3, 1, 2, 10, 30)
    }
    self.grid_search = GridSearchCV(self.pipeline, self.parameters, n_jobs=2, verbose=1,
                              scoring='accuracy')

  def create_rf_model_framework(self):
    self.pipeline = Pipeline([
      ('clf', RandomForestClassifier(criterion='entropy'))
    ])

    self.parameters = {
      'clf__n_estimators': (5, 10, 20, 50),
      'clf__max_depth': (50, 150, 250),
      'clf__min_samples_split': (1, 2, 3),
      'clf__min_samples_leaf': (1, 2, 3)
    }
    self.grid_search = GridSearchCV(self.pipeline, self.parameters, n_jobs=2, verbose=1,",create_subtopic_mapper.py,rupendrab/py_unstr_parse,1
"    ""LQA""
]

CLASSIFIERS = [
    KNeighborsClassifier(17), # ~49% acc
    SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1, eta0=0.0,    # ~0.41 acc, 3 sec
                  fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='log',
                  n_iter=5, n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
                  shuffle=True, verbose=VERBOSE, warm_start=False),
    DecisionTreeClassifier(max_depth=None),     # 0.41 acc, 20 sec
    RandomForestClassifier(n_estimators=10, criterion='gini', max_features='auto',      # ~0.49 acc, 12 sec
                           min_samples_split=2, verbose=VERBOSE),
    MLPClassifier(alpha=1, verbose=VERBOSE),        # 0.48 acc, 29 sec
    LinearDiscriminantAnalysis()        # 0.46 acc, 2 sec
]

def generate_tuple_lists(cla, tags):
    assert len(cla) == len(tags)
    l = list()
",src/ml/testing_graphs.py,seokjunbing/cs75,1
"
test_df = pd.read_csv('/home/namukhtar/Datasets/kaggle/titanic/test.csv', header=0)
# print test_df.head(10)
test_data = pre_process(test_df)

# Import the random forest package
from sklearn.ensemble import RandomForestClassifier 

# Create the random forest object which will include all the parameters
# for the fit
forest = RandomForestClassifier(n_estimators=100)

# Fit the training data to the Survived labels and create the decision trees
forest = forest.fit(train_data[0::, 2::], train_data[0::, 1])

# Take the same decision trees and run it on the test data
output = forest.predict(test_data[0::, 1::])

out_df = pd.DataFrame({'PassengerId' : test_data[0::, 0], 'Survived' : output})
out_df[""PassengerId""] = out_df[""PassengerId""].astype(""int"")",src/main/python/titanic.py,Chaparqanatoos/kaggle-knowledge,1
"                    ),
                    preparer_pipeline=[
                        BootstrapSampler(sampling_percent=100),
                        NoiseInjector(stddev=1e-6),
                    ],
                    label_encoder=OrdinalLabelEncoder()
                ))
            ],
            classifiers=[
                (""dt"", DecisionTreeClassifier()),
                (""rf"", RandomForestClassifier())
            ],
            n_folds=10,
            n_runs=10
        )
        results = experiment.run()
        if results is None:
            continue
        for classifier in results[""errors""]:
            corr, _ = pearsonr(results[""scores""][""wilks""], results[""errors""][classifier])",emetrics/evaluation/manual_experiment.py,etamponi/emetrics,1
"
                fout.write(""%s;%s;%s;%s;%s;%s;%s\n""%(part, dataset_name, method_name, 0, hmm_n_states, ""encode_time_train"", encode_time_train))
                fout.write(""%s;%s;%s;%s;%s;%s;%s\n""%(part, dataset_name, method_name, 0, hmm_n_states, ""encode_time_test"", encode_time_test))
                
                
                #### FIT CLASSIFIER ####
                for rf_max_features in rf_max_featuress:
                    print(""Training RF with max features = %s...""%rf_max_features)
                    sys.stdout.flush()
                    
                    cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=rf_max_features, random_state=random_state)
                    start = time()
                    cls.fit(dt_train, train_y)
                    cls_fit_time = time() - start
                        
                    #### PREDICT ####
                    start = time()
                    if len(cls.classes_) == 1:
                        hardcoded_prediction = 1 if cls.classes_[0] == pos_label else 0
                        preds = [hardcoded_prediction] * len(relevant_test_case_names)",experiments_param_optim_cv/run_single_optim_cv.py,irhete/predictive-monitoring-benchmark,1
"import talib
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
import numpy as np
import pandas

def initialize(context):
    set_symbol_lookup_date('2010-01-01')
    
    # Parameters to be changed
    
    context.model1 = RandomForestClassifier(n_estimators=300, 
                                     max_depth=6, max_features=None)
    context.model2 = RandomForestClassifier(n_estimators=300, 
                                     max_depth=6, max_features=None)
    context.lookback = 14
    context.history_range = 1000
    context.beta_coefficient = 0.0
    context.percentage_change = 0.034
    context.maximum_leverage = 2.0
    context.number_of_stocks = 150",two-classifiers.py,KhaledSharif/quantopian-ensemble-methods,1
"        sacrifice = pos[:n_sacrifice]
        y_train_pu[sacrifice] = -1.
        
        print ""PU transformation applied. We now have:""
        print len(np.where(y_train_pu == -1.)[0]),"" are bening""
        print len(np.where(y_train_pu == +1.)[0]),"" are malignant""
        print
        
        #Get f1 score with pu_learning
        print ""PU learning in progress...""
        estimator = RandomForestClassifier(n_estimators=100,
                                           criterion='gini', 
                                           bootstrap=True,
                                           n_jobs=1)
        pu_estimator = PUAdapter(estimator)
        pu_estimator.fit(X_train,y_train_pu)
        y_pred = pu_estimator.predict(X_test)
        precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)
        pu_f1_scores.append(f1_score[1])
        print ""F1 score: "", f1_score[1]",src/tests/breastCancer.py,jayfans3/pu-learning,1
"def CSupSvc(X_train, X_test, y_train, y_test):
    for i in range(len(X_train)):
        csvm = svm.SVC()
        csvm.fit(X_train[i], y_train[i])
        X_train[i] = csvm.score(X_train[i], y_train[i])
        X_test[i] = csvm.score(X_test[i], y_test[i])
    return X_train, X_test
    
def RandomForest(X_train, X_test, y_train, y_test):
    for i in range(len(X_train)):
        rf = ensemble.RandomForestClassifier()
        rf.fit(X_train[i], y_train[i])
        X_train[i] = rf.score(X_train[i], y_train[i])
        X_test[i] = rf.score(X_test[i], y_test[i])
    return X_train, X_test

def ExtraTrees(X_train, X_test, y_train, y_test):
    for i in range(len(X_train)):
        rf = ensemble.ExtraTreesClassifier()
        rf.fit(X_train[i], y_train[i])",mltools.py,jrabenoit/skellify,1
"            if info['is_sparse']==True:
                self.name = ""BaggingRidgeRegressor""
                self.model = BaggingRegressor(base_estimator=Ridge(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            else:
                self.name = ""GradientBoostingRegressor""
                self.model = GradientBoostingRegressor(n_estimators=1, verbose=verbose, warm_start = True)
            self.predict_method = self.model.predict # Always predict probabilities
        else:
            if info['has_categorical']: # Out of lazziness, we do not convert categorical variables...
                self.name = ""RandomForestClassifier""
                self.model = RandomForestClassifier(n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            elif info['is_sparse']:                
                self.name = ""BaggingNBClassifier""
                self.model = BaggingClassifier(base_estimator=BernoulliNB(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...                          
            else:
                self.name = ""GradientBoostingClassifier""
                self.model = eval(self.name + ""(n_estimators=1, verbose="" + str(verbose) + "", min_samples_split=10, random_state=1, warm_start = True)"")
            if info['task']=='multilabel.classification':
                self.model = MultiLabelEnsemble(self.model)
            self.predict_method = self.model.predict_proba  ",autokit/models.py,tadejs/autokit,1
"        'model': MLPClassifier(max_iter=1000),
        'params': [
            {
                'hidden_layer_sizes': [(20, 10)],
                'random_state': [77]
            }
        ]
    },
    {
        'name': 'Random Forest',
        'model': RandomForestClassifier(),
        'params': [
            {
                'max_depth': [8],
                'n_estimators': [20],
                'min_samples_split': [5],
                'criterion': ['gini'],
                'max_features': ['auto'],
                'class_weight': ['balanced'],
                'random_state': [77]",scripts/train_classifier.py,yohanesgultom/id-openie,1
"def dimensionality_reduction():
    vec_length = get_feature_vector_length(DATA_FILE)
    feature_names = get_feature_names(FEATURE_NAMES_FILE)
    # print(vec_length)
    # print(len(feature_names))
    x_train = pd.read_csv(DATA_FILE, sep='|', usecols=range(1, vec_length), header=None)  # , names=feature_names)
    y_train = pd.read_csv(DATA_FILE, sep='|', usecols=[0], header=None)  # , names=feature_names)
    x_test = pd.read_csv(DATA_FILE, sep='|', usecols=(1, vec_length), header=None)

    # shuffled_data = x_train.reindex(np.random.permutation(x_train.index))
    # random_forest = RandomForestClassifier()
    # random_forest.fit()
    # pca = PCA(n_components=20)
    # x_train_reduced = pca.fit(x_train).transform(x_train)
    # print(x_train_reduced)


dimensionality_reduction()",src/models/random_forest.py,seokjunbing/cs75,1
"# ... and labels
print 'create labels'
from numpy import asarray  # The labels must be a numpy array
labels = asarray(['chair']*num_chairs + ['table']*num_tables)

# create random forest classifier
print 'random forest'
trees = 100

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=trees)
#rfc.fit(data, labels)

# cross-validation
if 0:
    print 'cross-validation'

    n_cv = 10
    from sklearn.cross_validation import cross_val_score
    scores = cross_val_score(rfc, data, labels, cv=n_cv)",random_forest.py,krenzlin/random-forest,1
"        if options.svm == SVM_LRL2:
            svm = LogisticRegression(penalty=""l2"", C=options.svm_c)
        if options.svm == ET:
            svm = ExtraTreesClassifier(n_estimators=1000,
                                       max_features=""sqrt"",
                                       #max_features=1000,
                                       min_samples_split=2,
                                       n_jobs=options.pyxit_n_jobs,
                                       verbose=options.verbose)
        if options.svm == RF:
            svm = RandomForestClassifier(n_estimators=1000,
                                         #max_features=1000,
                                         max_features=""sqrt"",
                                         min_samples_split=2,
                                         n_jobs=options.pyxit_n_jobs,
                                         verbose=options.verbose)

        if options.svm == NN:
            svm = neighbors.KNeighborsClassifier(10) 
",cytomine-datamining/algorithms/pyxit/pyxitstandalone.py,cytomine/Cytomine-python-datamining,1
"	clfr.fit(x_train, y_train)
	#print 'Accuracy in training set: %f' % clfr.score(x_train, y_train)
	#print 'Accuracy in cv set: %f' % clfr.score(x_cv, y_cv)
	return clfr

def RandomForest(x_train, y_train, x_cv, y_cv,sw=None):
	""""""
	Random Forest
	""""""
	#print ""Classifier: Random Forest""
	clfr =  RandomForestClassifier(n_estimators = 400,n_jobs=4)
	if sw != None:
		clfr.fit(x_train, y_train,sample_weight=sw)
	else:
		clfr.fit(x_train, y_train)
	#print 'Accuracy in training set: %f' % clfr.score(x_train, y_train)
	#if y_cv != None:
	#	print 'Accuracy in cv set: %f' % clfr.score(x_cv, y_cv)
	
	return clfr",source/Classify.py,tbs1980/Kaggle_DecMeg2014,1
"        
# Delcaration des pre-processing / classifier
ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3), min_df=0)
transformer = TfidfTransformer()

# Chaque paramtres reprsente la meilleur performance de l'ensemble
classifiers = [
    SGDClassifier(alpha = 1e-4),
    DecisionTreeClassifier(max_depth=None),
    SVC(gamma=2, C=1),
    RandomForestClassifier(n_estimators=60),
    AdaBoostClassifier()]
    

Result = numpy.empty((0,3), float)

# Boucle sur les classifiers
for clf in classifiers:

    Scores = numpy.array([])",BIM_Classifier_TextOnly.py,HugoMartin78/bim-classifier,1
"
        if self.modeltype == 'GB':
            model = GradientBoostingClassifier(n_estimators=C)
        elif self.modeltype == 'LOG':
            model = LogisticRegression(C=C)
        elif self.modeltype == 'NN':
            model = MLPClassifier(hidden_layer_sizes=(125,),
                                  activation='relu', alpha=C,
                                  max_iter=200, solver='lbfgs')
        elif self.modeltype == 'RF':
            model = RandomForestClassifier(n_estimators=C)
        elif self.modeltype == 'SVM':
            model = SVC(C=C, decision_function_shape='ovr',
                        kernel='poly')

        self.model = model


if __name__ == '__main__':
",support.py,npaulson/digit_classification,1
"                        Algorithm(
                            SVC(random_state=self.random_state),
                            [{'kernel': ['rbf'],
                              'C': [1, 10, 100, 1000],
                              'gamma': [1e-3, 1e-2, 1e-1, 1.0]}],
                            'Support Vector Machine (RBF Kernel)',
                            ('http://scikit-learn.org/stable/modules/'
                             'generated/sklearn.svm.SVC.html')))
                    algorithms.append(
                        Algorithm(
                            RandomForestClassifier(
                                random_state=self.random_state,
                                n_jobs=self.n_jobs),
                            [{'n_estimators': [10, 100, 500],
                              'max_features': [0.3, 0.6, 0.9],
                              'max_depth': [3, 7, None]}],
                            'Random Forest',
                            ('http://scikit-learn.org/stable/modules/'
                             'generated/'
                             'sklearn.ensemble.RandomForestClassifier.html')))",malss/malss.py,canard0328/malss,1
"from sklearn.ensemble import RandomForestClassifier

config = {
    'models': [RandomForestClassifier()],
    'models_parameters': [{
        'n_estimators': [100],
        'random_state': [1490702865],
        'max_features': [1.0],
        'oob_score': [True]
    }]
}",chatbot/datasets/config.py,nelfurion/jackson,1
"training = pd.read_csv(""protoAlpha_training.csv"")
testing = pd.read_csv(""protoAlpha_testing.csv"")

X = training.iloc[:,1:-1]
y = training['country_destination']

x_train,x_valid,y_train,y_valid = train_test_split(X,y,test_size=0.3,random_state=9372)

# Train classifier
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=10,n_jobs=1,verbose=10)
clf.fit(x_train,y_train)

# Run Predictions
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
y_preds = clf.predict(x_valid)
print( confusion_matrix(y_valid,y_preds) );
print( ""Accuracy: %f"" % (accuracy_score(y_valid,y_preds)) );
print( ""Precision: %f"" % (precision_score(y_valid,y_preds)) );
print( ""Recall: %f"" % (recall_score(y_valid,y_preds)) );",prototype_beta/randomForest_take3.py,valexandersaulys/airbnb_kaggle_contest,1
"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

from utils import binarize
from classification_metrics import CLASSIFICATION_METRICS

CLASSIFICATION_MACHINES = (
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
)


def classification_features(clf, binarize_x=False):
    def inner_func(x, y):
        if binarize_x:",code/classification_machines.py,diogo149/CauseEffectPairsChallenge,1
"
import pandas as pd

df = pd.read_csv('dataset/winequality-white.csv', header=0, sep=';')

X = df[list(df.columns)[:-1]]
y = df['quality']

X_train, X_test, y_train, y_test = train_test_split(X, y)

forest = RandomForestClassifier(n_estimators = 150)
forest.fit(X_train, y_train)
y_predict = forest.predict(X_test)
forest.score(X_test, y_test)

print 'Score:', forest.score(X_test, y_test)
print 'RMSE:', mean_squared_error(y_predict, y_test) ** 0.5",forest.py,behrtam/wine-quality-prediction,1
"test_data[""SP""] = test_data[""SibSp""]+test_data[""Parch""]

print('Defining predictors...')
predictors = [""Pclass"", ""Sex"", ""Age"", ""PSA"", ""Fare"", ""Embarked"", ""SP""]

print('Finding best n_estimators for RandomForestClassifier...')
max_score = 0
best_n = 0
for n in range(1,300):
    rfc_scr = 0.
    rfc = RandomForestClassifier(n_estimators=n)
    for train, test in KFold(len(train_data), n_folds=10, shuffle=True):
        rfc.fit(train_data[predictors].T[train].T, train_data[""Survived""].T[train].T)
        rfc_scr += rfc.score(train_data[predictors].T[test].T, train_data[""Survived""].T[test].T)/10
    if rfc_scr > max_score:
        max_score = rfc_scr
        best_n = n
print(best_n, max_score)

print('Finding best max_depth for RandomForestClassifier...')",random_forest.py,AbnerZheng/Titanic_Kaggle,1
"

dataset = loaddataset(train_file)
testset = loaddataset(test_file)

ab=AdaBoostClassifier(random_state=1)
bgm=BayesianGaussianMixture(random_state=1)
dt=DecisionTreeClassifier(random_state=1)
gb=GradientBoostingClassifier(random_state=1)
lr=LogisticRegression(random_state=1)
rf=RandomForestClassifier(random_state=1)
svcl=LinearSVC(random_state=1)

clfs = [
	('ab', ab, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('dt', dt,  {'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('gb', gb, {'n_estimators':[10,25,50,75,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],'max_depth':[5,10,25,50,75,100]}),
	('rf', rf, {'n_estimators':[10,25,50,75,100],'max_depth':[5,10,25,50,75,100],'max_features':[10,25,50,75]}),
	('svcl', svcl, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}),
	('lr', lr, {'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]})",scripts/05-histogram-normalised-techniques-search.py,jmrozanec/white-bkg-classification,1
"    def set_params(self, **params):
        return self


def test_rfe_features_importance():
    generator = check_random_state(0)
    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target

    clf = RandomForestClassifier(n_estimators=20,
                                 random_state=generator, max_depth=2)
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    rfe.fit(X, y)
    assert_equal(len(rfe.ranking_), X.shape[1])

    clf_svc = SVC(kernel=""linear"")
    rfe_svc = RFE(estimator=clf_svc, n_features_to_select=4, step=0.1)
    rfe_svc.fit(X, y)
",sklearn/feature_selection/tests/test_rfe.py,CforED/Machine-Learning,1
"            return KerasClassifier(nn=model,**self.params)


PARAMS_V3 = {
             'n_estimators':500, 'criterion':'gini', 'n_jobs':8, 'verbose':0,
             'random_state':407, 'oob_score':True,
             }

class ModelV3(BaseModel):
        def build_model(self):
            return RandomForestClassifier(**self.params)

PARAMS_V4 = {
             'n_estimators':550, 'criterion':'gini', 'n_jobs':8, 'verbose':0,
             'random_state':407,
             }

class ModelV4(BaseModel):
        def build_model(self):
            return ExtraTreesClassifier(**self.params)",examples/binary_class/scripts/binary.py,ikki407/stacking,1
"
test_data = data[334650:534650, 1:58]
test_label = data[334650:534650, 58]
#print train_data

sample_leaf_options = list(range(1, 500, 3))
n_estimators_options = list(range(1, 1000, 5))
results = []
for leaf_size in sample_leaf_options:
    for n_estimators_size in n_estimators_options:
        alg = RandomForestClassifier(min_samples_leaf=leaf_size, n_estimators=n_estimators_size, random_state=50)
        alg.fit(train_data, train_label)
        predict = alg.predict(test_data)
        right_num = 0.0
        for i in range(len(predict)):
            if test_label[i] == predict[i]:
                right_num += 1

        right_rate = right_num / len(predict)
        print right_rate",ML/RandomForestClassifier.py,jiajie999/pric-graphviz,1
"#model = 'gb' #GradientBoosting
# model = 'xgb' #eXtremeGradient Boosting
#model = 'xgbt'
model = 'svm'

if model == 'rf':
    params = {'n_estimators': 100,
              'n_jobs': -1,
              'random_state': random_state}
    method = 'rf_{n_estimators}_nfolds_{n_folds}_calibration_{calibration_method}'.format(n_folds=n_folds, n_estimators=params['n_estimators'], calibration_method=calibration_method)
    clf = RandomForestClassifier(**params)
elif model == 'gb':
    params = {'n_estimators': 1000,
              'random_state': random_state}
    method = 'gb_{n_estimators}_nfolds_{n_folds}_calibration_{calibration_method}'.format(n_folds=n_folds, n_estimators=params['n_estimators'], calibration_method=calibration_method)
    clf = GradientBoostingClassifier(**params)
elif model == 'xgb':
    params = {'max_depth': 10,
                    'n_estimators': 100}
",src/sklearn_callibratedCV.py,ternaus/kaggle_otto,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/mode_aggregate_only.py,diogo149/CauseEffectPairsPaper,1
"    adf['TRANCADO_P'] = adf.TRANCADO / adf.TOTAL_CADEIRAS

    # Adiciono a coluna MEDIA com mdia do aluno.
    adf['MEDIA'] = adf['MATRICULA'].map(lambda x: np.mean(df[df.MATRICULA == x][['MEDIA']]).values[0])

    # Retorno o array de matriculas e o dataframe sem a coluna matricula
    return adf['MATRICULA'].values, adf.drop(['MATRICULA'], axis=1)

def predict(train_ids, train_data, test_ids, test_data, filename):
    print 'Training...'
    forest = RandomForestClassifier(n_estimators=100)
    forest = forest.fit( train_data[0::,1::], train_data[0::,0] )

    print 'Predicting...'
    output = forest.predict(test_data).astype(int)

    predictions_file = open(filename, ""wb"")
    open_file_object = csv.writer(predictions_file)
    open_file_object.writerow([""MATRICULA"",""COD_EVASAO""])
    open_file_object.writerows(zip(test_ids, output))",problem6.py,UFCGProjects/analise-dados-2-p5,1
"                                     maxiter=maxiter, maxfun=maxfun,
                                     **kwds)

def rfFit(self):
    # column names
    self.colNames = list(self.traningDf.columns.values)
    self.colNames.remove(""asite"")
    self.X = np.array(pd.get_dummies(self.traningDf[self.colNames]))
    self.y = np.array(self.traningDf[""asite""])
    ## feature selection
    self.clf = RandomForestClassifier(max_features=None, n_jobs=-1)
    self.clf = self.clf.fit(self.X, self.y)
    self.importances = self.clf.feature_importances_
    self.selector = RFECV(self.clf, step=1, cv=5)
    self.selector = self.selector.fit(self.X, self.y)
    self.sltX = self.selector.transform(self.X)
    print(""[status]\tOptimal number of features based on recursive selection: %d"" % self.selector.n_features_)
    ## define a new classifier for reduced features
    self.reducedClf = RandomForestClassifier(max_features=None, n_jobs=-1)
    self.reducedClf = self.reducedClf.fit(self.sltX, self.y)",scripts/backupcode.py,hanfang/scikit-ribo,1
"  # print(y)

  le = preprocessing.LabelEncoder()
  sorted_y = sorted(set(y))
  le.fit(sorted_y)
  y_train = le.transform(y)


  # le.inverse_transform(0)
  pipeline = Pipeline([
      ('clf', RandomForestClassifier(criterion='entropy'))
  ])
  parameters = {
      'clf__n_estimators': (5, 10, 20, 50),
      'clf__max_depth': (50, 150, 250),
      'clf__min_samples_split': (1, 2, 3),
      'clf__min_samples_leaf': (1, 2, 3)
  }
  grid_search = GridSearchCV(pipeline, parameters, n_jobs=2, verbose=1,
                            scoring='precision')",create_toc_mapper_rf.py,rupendrab/py_unstr_parse,1
"
  print('- Unique family size: %s' % df.FamilySize.unique())
  for family in df.FamilySize.unique():
    counts = len(df[df.FamilySize == family])
    print('%d familylings: %d times, rate: %f' % (family, counts, len(df[(df.FamilySize == family) & (df.Survived == 1)]) / counts))

  print('... done printing statistics!')
  print(SEPARATOR)

def run_prediction(train, test):
  forest = RandomForestClassifier(n_estimators=100)
  forest = forest.fit(train[0::,1::], train[0::,0] )

  return forest.predict(test).astype(int)

def write_predictions(ids, predictions):
  with open('prediction.csv', 'wt') as predictions_file:
    open_file_object = csv.writer(predictions_file)
    open_file_object.writerow(['PassengerId','Survived'])
    open_file_object.writerows(zip(ids, predictions))",titanic/submissions/1/randomforest.py,furgerf/kaggle-projects,1
"    global speed_regr
    data = np.array(data).reshape(1, 2)
    predicted_test = speed_regr.predict(data)
    acceleration = data.item((0, 0))
    print ""Predicted %d"" % predicted_test[0]
    return predicted_test[0]

def init_speed_module(values, trees, data, labels):
    # Fit regression model
    global speed_regr
    speed_regr = RandomForestClassifier(n_estimators=trees)
    speed_regr.fit(data[:, [0,1]], labels)
    print ""init_speed_module: "", speed_regr.feature_importances_
    return

def predicted(data):
   return speed_regr.predict(data)

def calculate_speed(data, predicted_data, times, speed):
    result = np.empty([len(data)+1,1])",speed.py,dkdbProjects/server-result-sender,1
"            #
            ##############################################################
            import seaborn as sns
            import pandas as pd
            import numpy as np
            import matplotlib.pyplot as plt
            from sklearn.ensemble import RandomForestClassifier
            from pandas_ml import ConfusionMatrix # https://github.com/pandas-ml/pandas-ml/

            lg(""Building RandomForest Classifier"", 1)
            model                   = RandomForestClassifier(n_jobs=num_jobs)

            # Now train it:
            model.fit(req[""FeaturesTrain""], req[""TargetTrain""])

            # Predict against test data
            if debug:
                self.lg("""", 6)
            lg(""Predicting Values"", 6)
            predictions         = model.predict(req[""FeaturesTest""])",src/pycore.py,jay-johnson/datanode,1
"for x in range(TRAIN_SIZE):
	data_T = np.reshape(train_data[x], [-1, 1])
	fake_train_data[x] = lda.transform(data_T)
for x in range(EVAL_SIZE):
	data_T = np.reshape(eval_data[x], [-1, 1])
	fake_eval_data[x] = lda.transform(data_T)

train_data = fake_train_data
eval_data = fake_eval_data

clf = RandomForestClassifier(n_estimators=200)
clf.fit(train_data, train_label)

print (train_data.shape)
print (eval_data.shape)
print (eval_label)

train_predict = clf.predict(train_data)
eval_predict = clf.predict(eval_data)
eval_result = np.sum(eval_predict == eval_label) / float(eval_label.shape[0])",work/ML/tensorflow/separa/lda_rf.py,ElvisLouis/code,1
"from sklearn.lda import LDA
from sklearn.qda import QDA

from ut.util.log import printProgress

default_classifiers = [
        KNeighborsClassifier(3),
        SVC(kernel=""linear"", C=0.025),
        SVC(gamma=2, C=1),
        DecisionTreeClassifier(max_depth=5),
        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
        AdaBoostClassifier(),
        GaussianNB(),
        LDA(),
        QDA()]


def try_out_multiple_classifiers(datasets, classifiers=None, print_progress=True, **kwargs):
    h = .02  # step size in the mesh
",stats/classification/explore.py,thorwhalen/ut,1
"del f
print('==> Data sizes:',X_train.shape, y_train.shape, X_test.shape, y_test.shape)

y_train = [i[0] for i in y_train]
y_test = [i[0] for i in y_test]

testList = [20, 40, 60, 80, 100, 150, 200, 250]

for numTrees in testList:
    print(""Number of Trees:"",numTrees)
    clf = RandomForestClassifier(n_estimators=numTrees, max_depth=None, min_samples_split=2, random_state=0)
    startTime = time.time()
    clf = clf.fit(X_train, y_train)
    endTime = time.time()
    print(""Time:"",endTime-startTime)
    print(""Score:"",clf.score(X_test,y_test))
    print(""======================================"")",exp1/exp1d_randomforest.py,Haunter17/MIR_SU17,1
"
def runLogitAndNB(Xtrainsparse, Xtestsparse):
	for i in range(len(ytrainraw[0])):
		print ""Output type %i"" % i
		logit1 = linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', C=1)
		logit2 = linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', C=100)
		logit3 = linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10000)
		nb1 = naive_bayes.MultinomialNB(alpha=0.01, fit_prior=True, class_prior=None)
		nb2 = naive_bayes.MultinomialNB(alpha=0.1, fit_prior=True, class_prior=None)
		nb3 = naive_bayes.MultinomialNB(alpha=1, fit_prior=True, class_prior=None)
		RF1 = RandomForestClassifier(1, ""entropy"", None)
		RF2 = RandomForestClassifier(10, ""entropy"", None)
		RF3 = RandomForestClassifier(20, ""entropy"", None)
		ytrain = numpy.hstack((ytrainraw[:, i], ydevraw[:, i]))
		ytest = ytestraw[:, i]
		RF1.fit(Xtrainsparse, ytrain)
		RF2.fit(Xtrainsparse, ytrain)
		RF3.fit(Xtrainsparse, ytrain)
		scores = [RF1.score(Xtestsparse, ytest), RF2.score(Xtestsparse, ytest), RF3.score(Xtestsparse, ytest)]
		print ""R-FOREST: Best score %.2f%%, min of %.2f%%"" % (max(scores) * 100, min(scores) * 100)",bach_code/logit.py,glasperfan/thesis,1
"def train(df, features, test_training=True, max_fpr=.05, nfolds = 10):
    for feature, feature_func in features.items():
        df[feature] = feature_func(df)

    df = df.dropna()
    X = df.as_matrix(features.keys())
    y = np.array(df['class'].tolist())
    # Make 0-1
    y = [x=='dga' for x in y]
    try:
        clf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_depth=5, n_jobs=-1)
        validation_data = cross_validate(X, y, clf, nfolds)
        clf.fit(X, y)
        thr = validation_data['thr'][np.max(np.where(validation_data['fpr'] < .05))]
    except Exception as e:
        import pdb; pdb.set_trace()
        raise e
    return clf, thr, validation_data

def predict(clf, df, features, threshold):",dga_classifier.py,endgameinc/SANS_THIR16,1
"X[""class""] = X[""class""].astype(""category"").cat.codes
X[""user""] = X[""user""].astype(""category"").cat.codes
X = X.dropna()

y = X[""class""]
y = pd.DataFrame(y)
X = X.drop(""user"",1)
X = X.drop(""class"",1)


model = RandomForestClassifier(n_estimators=30,max_depth=10,oob_score=True,random_state=0)

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=7)
y_test = y_test[""class""]
y_train = y_train[""class""]


print ""Fitting...""
s = time.time()
model.fit(X_train,y_train)",MLandDS/MachineLearning/Random-Forests.py,adewynter/Tools,1
"                    'med':[int(i[3]) for i in cln_data[subj_id]],
                    'difficulty':[int(i[4]) for i in cln_data[subj_id]]})
df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75
df.head()

from sklearn.metrics import classification_report

train, test = df[df['is_train']==True], df[df['is_train']==False]
features = df.columns[[0,2]]

clf = RandomForestClassifier(n_jobs=2)
clf.fit(train[features], train['difficulty'])

preds = clf.predict(test[features])

print classification_report(test['difficulty'], preds)
pd.crosstab(test['difficulty'], preds, rownames=['actual'], colnames=['preds'])",cleanup/random_forest.py,sagarjauhari/BCIpy,1
"data_X, data_y = data['data'], data['target']


def sampler(rng):
    return {'max_depth': rng.randint(1, 100), 'n_estimators': rng.randint(1, 300)}


def feval(d):
    max_depth = d['max_depth']
    n_estimators = d['n_estimators']
    clf = RandomForestClassifier(n_jobs=-1, max_depth=max_depth, n_estimators=n_estimators)
    scores = cross_val_score(clf, data_X, data_y, cv=5, scoring='accuracy')
    return np.mean(scores) - np.std(scores)

opt = Bandit(sampler=sampler, score=ucb_maximize,
             model=Wrapper(RandomForestRegressorWithUncertainty()))
n_iter = 100
for i in range(n_iter):
    print('iter {}...'.format(i))
    x = opt.suggest()",examples/plot_hyperopt.py,mehdidc/fluentopt,1
"	clean_test_reviews.append( "" "".join( KaggleWord2VecUtility.review_to_wordlist( review, True )))

# Get a bag of words for the test set, and convert to a numpy array
test_data_features = vectorizer.transform( clean_test_reviews )
test_data_features = test_data_features.toarray()

###

print ""Training the random forest (this may take a while)...""

forest = RandomForestClassifier( n_estimators = 100, n_jobs = -1, verbose = 1 )
forest = forest.fit( train_data_features, train[""sentiment""] )

print ""Predicting test labels...\n""
rf_p = forest.predict_proba( test_data_features )

auc = AUC( test['sentiment'].values, rf_p[:,1] )
print ""random forest AUC:"", auc

# a random score from a _random_ forest",bow_validate.py,nkhuyu/classifying-text,1
"  for k, _ in leaves(tree):  # for k, _ in leaves(tree):
    for j in k.val:
      tmp = (j.cells)
      tmp.append('Class_' + str(id(k) % 1000))
      j.__dict__.update({'cells': tmp})
      Rows.append(j.cells)
  return pd.DataFrame(Rows, columns = get_headers(data) + ['klass'])

def haupt():
  train, test = explore('./Data')
  clf = RandomForestClassifier(n_estimators = 100, n_jobs = 2)
  """"""
  Cluster using FASTMAP
  """"""
  # Training data
  train_DF = createDF(train[1])

  # Testing data
  test_df = createDF(test[1])
  # set_trace()",old/SOURCE/rf_test.py,ai-se/Transfer-Learning,1
"trainData = eegTrainData
testData = eegTestData

# Add template scores to the data
trainData[trainTemplateScores.columns] = trainTemplateScores
testData[testTemplateScores.columns] = testTemplateScores

features = trainData.columns

# Random Forest Classifier (very original, I know)
myClassifier = skensemble.RandomForestClassifier(n_estimators=100,max_depth=5,max_features=1.0)

# Fit training data
myClassifier.fit(trainData[features],trainLabel)

# Predict test data
testPrediction = myClassifier.predict_proba(testData[features])

print 'Creating submission file'
mySubmission = pd.read_csv('SampleSubmission.csv')",bci/run_classification.py,sidh0/kaggle,1
"        test, train = [], []
        for label in labels:
            index = 0
            for kgroup in kmap[label]:
                if index == j: test += kgroup 
                else: train += kgroup
                index += 1
        x_test, y_test = unzip(test)
        x_train, y_train = unzip(train)

        if RF: MNB = RandomForestClassifier()
        else: MNB = MultinomialNB()
        MNB.fit(x_train, y_train)
        results.append(score_by_label(y_test, MNB.predict(x_test), n_labels))

    rlen, wsum, c0sum, c1sum = float(len(results)), 0.0, 0.0, 0.0
    for result in results:
        wght, cmap = result
        wsum += wght
        c0sum += cmap[0]",classifier_tools.py,rs93/nytml,1
"#clf = SGDClassifier(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'linearsvm accuracy ', accuracy_score(test_gt,pred)

#clf = LogisticRegression(n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'logistic accuracy ', accuracy_score(test_gt,pred)

#clf = RandomForestClassifier(min_samples_leaf=20,n_jobs=-1)
#clf.fit(train_raw,train_gt)
#pred = clf.predict(test_raw)
#print 'rfc accuracy ', accuracy_score(test_gt,pred)
#pred = clf.predict(raw_orig)
#with open('rfc.txt','w') as otf:
#    for p in pred:
#        otf.write(str(int(p)) + '\n')
from keras.utils.np_utils import to_categorical
",learning/1dcnn-basic.py,leonidk/centest,1
"
    #classify across each fold
    i = 0
    for train_index, test_index in kf.split(X):
        i += 1
        
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        #random forest classifier
        rf = RandomForestClassifier(100)
        rf.fit(X_train,y_train)
        acc = rf.score(X_test,y_test)
        print ""kfold %i of %i accuracy: %.4f"" % (i,splits,acc*100)
        score.append(acc)

    #average test accuracy
    print ""average within-subject accuracy: %.4f"" % (np.mean(score)*100)
    
#check if between-subject data exists",Advanced_ML/Human_Activity_Recognition/RF/rf.py,iamshang1/Projects,1
"    y_pred2 = rfe.fit(X, y).predict(X)
    assert_array_equal(y_pred, y_pred2)


def test_rfe_features_importance():
    generator = check_random_state(0)
    iris = load_iris()
    X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
    y = iris.target

    clf = RandomForestClassifier(n_estimators=10, n_jobs=1)
    rfe = RFE(estimator=clf, n_features_to_select=4, step=0.1)
    rfe.fit(X, y)
    assert_equal(len(rfe.ranking_), X.shape[1])

    clf_svc = SVC(kernel=""linear"")
    rfe_svc = RFE(estimator=clf_svc, n_features_to_select=4, step=0.1)
    rfe_svc.fit(X, y)

    # Check if the supports are equal",sklearn/feature_selection/tests/test_rfe.py,smartscheduling/scikit-learn-categorical-tree,1
"    #learners = [GaussianNB(), DecisionTreeClassifier(class_weight='auto', criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, random_state=42, splitter='best'), SVC(C=C, gamma=gamma), LogisticRegression()]
    
    learners = [DecisionTreeClassifier(class_weight='auto', criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, random_state=42, splitter='best')]

    for i in xrange(len(learners)):

        ############################################################################     

        #learner = GaussianNB()
        #learner = DecisionTreeClassifier()
        #learner = RandomForestClassifier()
        #learner = SVC()
        #learner = LogisticRegression()

        learners[i] = learners[i].fit(X_train,y_train)

        y_pred = learners[i].predict(X_test)

        ############################################################################
",python/py/quant/avg_clf_train.py,austinjalexander/sandbox,1
"

# In[33]:

data.fillna(0, inplace=True)
count_nonzero(pandas.isnull(data.ix[train_idx,feats1]))


# In[34]:

rf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1)
rf1.fit(data.ix[train_idx,feats1], data['tipped'].ix[train_idx])


# In[35]:

preds1 = rf1.predict_proba(data.ix[test_idx,feats1])


# In[36]:",examples/realWorldMachineLearning/Chapter+6+-+NYC+Taxi+Full+Example (1).py,remigius42/code_camp_2017_machine_learning,1
"
def train_data():
    train = pd.read_csv(""train.csv"")
    features = train.columns[1:]
    X = train[features]
    y = train['label']
    return model_selection.train_test_split(X / 255., y, test_size=0.25, random_state=0)

def train_evaluate_model():
    X_train, X_test, y_train, y_test = train_data()
    model = RandomForestClassifier(n_estimators=50, n_jobs=-1)
    model.fit(X_train, y_train)

    y_pred_rf = model.predict(X_test)
    acc_rf = accuracy_score(y_test, y_pred_rf)
    print(""accuracy : "", acc_rf)

def train_final_model():
    X_train, X_test, y_train, y_test = train_data()
    model = RandomForestClassifier(n_estimators=50, n_jobs=-1)",models.py,remirobert/MNIST-machine-learning,1
"    _ = joblib.dump(model, path, compress=9)

def load_model(path):
    """"""
    Load trained model from file
    :param path:
    :return:
    """"""
    return joblib.load(path)

CLASSIFIERS = [DummyClassifier(), GaussianNB(), LinearSVC(), SVC(), DecisionTreeClassifier(), RandomForestClassifier()]
PARAMETERS = [{}, {}, {},{'kernel':('linear', 'rbf', 'poly', 'sigmoid')},{}, {}]",solution/application/modules/ml_utils.py,ssip16teamb/ssip16teamb.github.io,1
"        'Dog_5',
        'Patient_1',
        'Patient_2',
    ]
    pipelines = [
        # NOTE: you can enable multiple pipelines to run them all and compare results
        Pipeline(gen_preictal=True,  pipeline=[FFTWithTimeFreqCorrelation(50, 2500, 400, 18, 'usf')]), # winning submission
    ]
    classifiers = [
        # NOTE: you can enable multiple classifiers to run them all and compare results
#         (RandomForestClassifier(n_estimators=300, min_samples_split=1, max_features=0.5, bootstrap=False, n_jobs=-1, random_state=0), 'rf300mss1mf05Bfrs0'),

#         (ExtraTreesClassifier(n_estimators=3000, min_samples_split=1, max_features=0.15, bootstrap=False, n_jobs=-1, random_state=0), 'ET3000mss1mf015Bfrs0'),
#         
#         (GradientBoostingClassifier(n_estimators=3000, min_samples_split=1, max_features=0.15, learning_rate=0.02, subsample = 0.5, random_state=0), 'GBRT3000mms1mf015Lr002Ss05rs0'),

        (SVC(C=1e6, kernel='rbf', gamma=0.01, coef0=0.0, shrinking=True, probability=True, tol=1e-5, cache_size=2000, class_weight='auto', max_iter=-1, random_state=0), 'svcce6rbfg001co0stte-5cwautors0'),
    ]
    cv_ratio = 0.5
",seizure_detection.py,jlnh/SeizurePrediction,1
"    ## get dataset
    X, y, attribute_names, target_attribute_names = get_dataset(44)
    
    ns = np.logspace(11, 0, num = 12, endpoint = True, base = 2.0, dtype = np.int32)
    
    fig, ax = plt.subplots(1, 1)
    fig.suptitle('OOB error versus cross validation error', fontsize = 'x-large')
    ## OOB scores
    oob_err_rates = []
    for n in ns:
        rnd_forest_clf = RandomForestClassifier(n_estimators = n, bootstrap = True, oob_score = True)
        rnd_forest_clf.fit(X, y)
        oob_err_rates.append(1.0 - rnd_forest_clf.oob_score_)
        # plot_surface(ax, rnd_forest_clf, X, y)
    ax.plot(ns, oob_err_rates, '-o', label = 'OOB error')
    
    ## cross validation scores
    cv_err_rates = []
    for n in ns:
        rnd_forest_clf = RandomForestClassifier(n_estimators = n, bootstrap = True, oob_score = False)",random_forests.py,lidalei/DataMining,1
"    print ""%d over %d experiments"" % (n+1, CV_N)

    for i, (idx_tr, idx_ts) in enumerate(idx):

        print ""_"" * 80
        print ""-- %d over %d folds"" % (i+1, CV_K)

        x_tr, x_ts = x[idx_tr], x[idx_ts]
        y_tr, y_ts = ys[n][idx_tr], ys[n][idx_ts]

        forest = RandomForestClassifier(n_estimators=500, criterion='gini', random_state=n, n_jobs=2)
        forest.fit(x_tr, y_tr)
        
        if RANK_METHOD == 'random':
            ranking_tmp = np.arange(len(var_names))
            np.random.seed((n*CV_K)+i)
            np.random.shuffle(ranking_tmp)
        elif RANK_METHOD == 'ReliefF':
            relief = ReliefF(relief_k, seed=n)
            relief.learn(x_tr, y_tr)",scripts/sklearn_rf_training_fixrank.py,AleZandona/INF,1
"    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_pca1_none.py,diogo149/CauseEffectPairsPaper,1
"    # 80% of 138 = 110
    # train_set = a[0:117]
    # test_set = a[118:]

    labels = []
    for i in range(len(fv)):
        labels.append(fv[i][-1])
    labels_train_set = labels[0:530]
    labels_test_set = labels[531:]

    clf = RandomForestClassifier(n_estimators=1000)
    clf = clf.fit(train_set, labels_train_set)

    test_set_results = []
    count = 0
    for ind, each in enumerate(test_set):
        predicted_label = clf.predict(each)
        
        if predicted_label == labels_test_set[ind]:
            count += 1",scripts/train.py,PragyaJaiswal/Projection-Profile-Analysis-CXR,1
"
    print('################# classifyUsingSVM() finished ##################')
    print(""--- %s seconds ---"" % (time.time() - start_time))
    

def classifyUsingRandomForest(trainX, trainY, testX, testY):
    
    print('################# classifyUsingRandomForest() started ##################')
    start_time = time.time()
    
    clf = RandomForestClassifier(n_estimators=100,verbose=1 )
    print('classifyUsingRandomForest Initialized')
    
    clf.fit(trainX, trainY)
    print('classifyUsingRandomForest Trained')
    
    predictedY = clf.predict(testX)
    print('classifyUsingRandomForest prediction completed')
 
    accuracy = accuracy_score(testY, predictedY)",src/Classifier.py,md-k-sarker/ML-Analyze-Morality,1
"        self.p = len(distargs['inputs']['stattypes'])
        # Sufficient statistics.
        self.N = 0
        self.data = Data(x=OrderedDict(), Y=OrderedDict())
        self.counts = [0] * self.k
        # Outlier and random forest parameters.
        if params is None: params = {}
        self.alpha = params.get('alpha', .1)
        self.regressor = params.get('forest', None)
        if self.regressor is None:
            self.regressor = RandomForestClassifier(random_state=self.rng)

    def incorporate(self, rowid, query, evidence=None):
        assert rowid not in self.data.x
        assert rowid not in self.data.Y
        x, y = self.preprocess(query, evidence)
        self.N += 1
        self.counts[x] += 1
        self.data.x[rowid] = x
        self.data.Y[rowid] = y",src/regressions/forest.py,probcomp/cgpm,1
"        trainingSet['features'].append(features[where1,:])
        trainingSet['labels'].append(numpy.zeros(len(where0)))
        trainingSet['labels'].append(numpy.ones(len(where1)))

features = numpy.concatenate(trainingSet['features'], axis=0)
labels = numpy.concatenate(trainingSet['labels'], axis=0)

#############################################################
# Train the random forest (RF):
# ===============================
rf = RandomForestClassifier(n_estimators=200, oob_score=True)
rf.fit(features, labels)
print(""OOB SCORE"",rf.oob_score_)



#############################################################'
# Predict Edge Probabilities & Optimize Multicut Objective:
# ===========================================================
#",src/python/examples/multicut/plot_isbi_2012_multicut_2D_simple.py,DerThorsten/nifty,1
"#from sklearn import svm   
#clf = svm.SVC(C=10,gamma=0.1)#0.8238
#clf=GridSearchCV(svm.SVC(), param_grid={""C"":np.logspace(-2, 10, 13),""gamma"":np.logspace(-9, 3, 13)})
#output = clf.fit( train_data[0::,1::], train_data[0::,0] ).predict(test_data).astype(int)
#print(""The best parameters are %s with a score of %0.2f""% (knnClf.best_params_, knnClf.best_score_))

#from sklearn.tree import DecisionTreeClassifier
#clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1,random_state=1)#0.778

#from sklearn.ensemble import RandomForestClassifier
#clf = RandomForestClassifier(min_samples_split = 16, n_estimators = 300)#0.8350

#from sklearn.ensemble import GradientBoostingClassifier
#clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.05,max_depth=2)#0.8395

#from sklearn.ensemble import ExtraTreesClassifier
#clf=ExtraTreesClassifier(n_estimators=100, max_depth=None,min_samples_split=1, random_state=0)#0.7968

#from sklearn.ensemble import AdaBoostClassifier
#clf = AdaBoostClassifier(n_estimators=1000)#0.8115",my_2_featrue_pred.py,zlykan/my_test,1
"			print 'iteration', i + 1 , 'out of ', n_iterations
		train, test = train_test_split(pos_lst, test_size=test_split)
		train_data = balanced_dataset(train, neg_lst, training_size)
		test_data = balanced_dataset(test, neg_lst, testing_size)

		X_train = train_data[:,:vec_size]
		y_train = train_data[:,vec_size:].ravel() # better shape for the classifier
		X_test = test_data[:,:vec_size]
		y_test = test_data[:,vec_size:]

		rfc = RandomForestClassifier(n_estimators=n_estimators)
		rfc.fit(X_train, y_train)
		y_predict = rfc.predict(X_test)
		scores[i] = accuracy_score(y_test, y_predict)
	print 'classifier estimated accuracy (average over n_iterations):'
	print np.average(scores)

	# Now try to retreive high relevant articles
	classifier = RandomForestClassifier(n_estimators=n_estimators)
	dataset = balanced_dataset(pos_lst, neg_lst, training_size + testing_size)",classifier.py,lmartinet/ISTEX_MentalRotation,1
"    """"""
    # get data
    if replacement == 'unmodified':
        x, y = make_original_forest(condition, config)
    else:
        if ltd:
            x, y = make_ltd_forest(replacement, condition, config)
        else:
            x, y = make_forest(replacement, condition, config)
    # random forest classification
    clf = RandomForestClassifier(n_estimators=2000, oob_score = True)
    clf = clf.fit(x, y)
    importances = clf.feature_importances_
    for importance in importances:
        importance = abs(importance)
    # std = np.std([tree.feature_importances_ for tree in clf.estimators_],
    #          axis=0)
    indices = np.argsort(importances)[::-1]
    print (clf.oob_score_)
    print (clf.oob_decision_function_)",openSMILE_analysis/trees.py,shnizzedy/SM_openSMILE,1
"Data = np.loadtxt(""./datasets/learndata.dat"", unpack=""False"")		# Training dataset
Test = np.loadtxt(""./datasets/testdata.dat"", unpack=""False"")    	# Testing dataset

Test = np.transpose(Test)       # Transpose data into rows of data sets
Data = Data.transpose()         # Same idea

test = Test[:, :-1]             # Data is everything but the last entry of the row
X = Data[:,:-1]                 # Data is everything but the last entry of the row
y = Data[:, -1]                 # The last entry of each row is the class

model = RandomForestClassifier(n_estimators=25)     # Construct Neighbor model
pca = decomposition.PCA()
pipe = Pipeline(steps=[('pca', pca), ('model', model)])
pca.fit(X)

# <<< Plot the PCA spectrum >>>
# import matplotlib.pyplot as plt
# plt.plot(pca.explained_variance_)
# plt.axis('tight')
# plt.show()",src/learn.py,mnaka/HandsHear,1
"           ]

# classifiers
classifiers = [
               dict(name=""nb"", clf=MultinomialNB()),
               dict(name=""bnb"", clf=BernoulliNB(binarize=0.5)),
               dict(name=""svm"",
                    clf=LinearSVC(loss='l2', penalty=""l2"",
                                  dual=False, tol=1e-3)),
#               dict(name=""forest30"",
#                    clf=RandomForestClassifier(n_estimators=30,
#                                               random_state=123,
#                                               verbose=3))]
              ]

# combinations to repeat
repeat = dict() #data=""2-unbalanced"", feat=""tfidf_ng3"")

if os.path.exists(file_name):
    scores = pickle.load(open(file_name))",python/experiments_acl2013.py,mohamedadaly/labr,1
"    print(""\r--- Completed {:,} out of {:,}"".format(completed, num_tasks), end='')
    sys.stdout.flush()
    time.sleep(1)
    if (completed == num_tasks): break
p.close()
p.join()
df_full = pd.DataFrame(list(results))
print()

print('--- Training random forest')
clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)
train = df_full[df_full.sponsored.notnull()].fillna(0)
test = df_full[df_full.sponsored.isnull() & df_full.file.isin(test_files)].fillna(0)
clf.fit(train.drop(['file', 'sponsored'], 1), train.sponsored)

print('--- Create predictions and submission')
submission = test[['file']].reset_index(drop=True)
submission['sponsored'] = clf.predict_proba(test.drop(['file', 'sponsored'], 1))[:, 1]
submission.to_csv('native_btb_basic_submission.csv', index=False)",btb_native_basic.py,carlsonp/kaggle-TrulyNative,1
"
#scale = preprocessing.MinMaxScaler()
#XtrainPos = scale.fit_transform(XtrainPos)
#XtestPos = scale.fit_transform(XtestPos)
#
#scale = preprocessing.Normalizer().fit(XtrainPos)
#XtrainPos = scale.fit_transform(XtrainPos)
#XtestPos = scale.fit_transform(XtestPos)

#classification
clf = RandomForestClassifier(n_estimators=10)
clf = clf.fit(XtrainPos, YtrainPos)
print(metrics.classification_report(YtestPos, clf.predict(XtestPos)))

## Crossvalidation 5 times using different split
#scores = cross_validation.cross_val_score(clf_svm, posfeat, label, cv=5, scoring='f1')
#print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))

# Visualization
#plt.hist(XtrainPos[:,0])",scikit_algo/RandomForest.py,sankar-mukherjee/CoFee,1
"        training_label = [arr[idx_imb] for idx_arr, arr in enumerate(label_bal)
                         if idx_arr != idx_lopo_cv]
        # Concatenate the data
        training_data = np.vstack(training_data)
        training_label = label_binarize(np.hstack(training_label).astype(int),
                                        [0, 255])
        print 'Create the training set ...'

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_bal.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/balancing/pipeline_classifier_adc.py,I2Cvb/mp-mri-prostate,1
"
    # Automatically identify categorical features, and index them.
    # Set maxCategories so features with > 4 distinct values are treated as continuous.
    featureIndexer =\
        VectorIndexer(inputCol=""features"", outputCol=""indexedFeatures"", maxCategories=4).fit(data)

    # Split the data into training and test sets (30% held out for testing)
    (trainingData, testData) = data.randomSplit([0.7, 0.3])

    # Train a RandomForest model.
    rf = RandomForestClassifier(labelCol=""indexedLabel"", featuresCol=""indexedFeatures"", numTrees=10)

    # Convert indexed labels back to original labels.
    labelConverter = IndexToString(inputCol=""prediction"", outputCol=""predictedLabel"",
                                   labels=labelIndexer.labels)

    # Chain indexers and forest in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])

    # Train model.  This also runs the indexers.",examples/src/main/python/ml/random_forest_classifier_example.py,kevinyu98/spark,1
"    # Ridge(),
    # LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_pca1_rf_only.py,diogo149/CauseEffectPairsPaper,1
"lls = list()
for i in range(n_folds):
    # split the dataset 
    valid_ = np.row_stack((train_pos[ratio_pos*i:ratio_pos*(i+1),:], 
                           train_neg[ratio_neg*i:ratio_neg*(i+1),:]))
    train_ = np.row_stack((train_pos[:ratio_pos*i,:], train_pos[ratio_pos*(i+1):,:],
                           train_neg[:ratio_neg*i,:], train_neg[ratio_neg*(i+1):,:]))

    ############### Here to define model ################
    #cls = LogisticRegression(n_jobs=-1)
    cls = RandomForestClassifier(n_estimators=2000,max_depth=10,verbose=1)
    model = cls.fit(train_[:,1:], train_[:,0])
    result = model.predict_proba(valid_[:,1:])
    ll = logloss(valid_[:,0], result[:,1])
    print ('fold ' + str(i) + ': logloss = ' + str(ll))
    lls.append(ll)

lls = np.array(lls)
print ('complete. mean logloss = {:.6f}, std = {:.6f}'.format(np.mean(lls), np.std(lls)))",cv.py,NCLAB2016/DF_STEALL_ELECTRIC,1
"
            usedDsName = [n[1] for n in inputDsList]
            if withOther:
                X += d.getSample(len(X))
                y += len(y) * ['other']
                usedDsName += ['other']

            if len(usedDsName) == 1:
                continue

            for clfModel in [RandomForestClassifier(n_estimators=11, max_depth=7), KNeighborsClassifier()]:
                for feature in [cpf.identity, cpf.polar, cpf.angular]:
                    for post in [pf.noPost, pf.postNormalize, pf.postAbs]:
                        expStr = '_'.join(['-'.join(usedDsName), strictStr, str(clfModel.__class__.__name__), str(feature.__name__), str(post.__name__)])
                        savePath = '../data/learnedModel/domain/' + expStr + '.dill'
                        print '&&&&&&&&&&'
                        print expStr

                        clf = cpf.ConceptClf(clfModel, post(feature))
                        sku.printClassificationReport(clf, X, y)",toolbox/experiment/trainAll_domainClf.py,pelodelfuego/word2vec-toolbox,1
"
       ### Random Forest
       algorithm.append('RF')
       list_features.discard('id')
       in_modelF = list_features
       
       X = df[list(in_modelF)]
       y = df[output_var]
       start_time = time.time() #start time to calculate speed
       #modelRF= RandomForestClassifier(n_estimators=1000, max_depth=60, class_weight = {0:0.1, 1:0.9} )
       modelRF= RandomForestClassifier(n_estimators=1000, max_depth=60 )
       resultRF = modelRF.fit(X, y)
       elapsed_timeRF = time.time() - start_time  # end time for Algorithm
       pred_RF = resultRF.predict(X)
       pred_RFprob = resultRF.predict_proba(X)
       timeAlg.append(elapsed_timeRF)

       gini_score_RF = 2*roc_auc_score(df[output_var], pred_RF)-1
       giniAlg.append(gini_score_RF)
       print (""\nRandom Forest Elapsed time= "",elapsed_timeRF) ",ex3groupf/FA_GroupF_Final_A1A41A6_H1H5.py,LJohnnes/iembdfa,1
"            Empty list if parameter free.

        """"""
        self.randParameter('num_estimators', args, sample=numpy.random.randint(30)+1)
        return super(RandomForestBatchModel, self).randomize_parameters(**args)

    def _genregressor(self, isReward=False, isTermination=False):
        return RandomForestRegressor(n_jobs=self.params['num_jobs'], n_estimators=self.params['num_estimators'])

    def _genclassifier(self):
        return RandomForestClassifier(n_jobs=self.params['num_jobs'], n_estimators=self.params['num_estimators'])


class SVMBatchModel(BatchModel):

    def __init__(self, **kwargs):
        BatchModel.__init__(self, **kwargs)
        self.params.setdefault('C', 1.0)
        self.params.setdefault('epsilon', 0.00001)
        self.params.setdefault('tolerance', 1.0e-6)",pyrl/agents/models/batch_model.py,kastnerkyle/python-rl,1
"test_dataset = fold_datasets[-1]

# Get supports on test-set
support_generator = dc.data.SupportGenerator(
    test_dataset, n_pos, n_neg, n_trials)

# Compute accuracies
task_scores = {task: [] for task in range(len(test_dataset.get_task_names()))}
for (task, support) in support_generator:
  # Train model on support
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=100)
  model = dc.models.SklearnModel(sklearn_model)
  model.fit(support)

  # Test model
  task_dataset = dc.data.get_task_dataset_minus_support(
      test_dataset, support, task)
  y_pred = model.predict_proba(task_dataset)
  score = metric.compute_metric(",examples/low_data/sider_rf_one_fold.py,rbharath/deepchem,1
"    c=1
    return 1. / (c+dist)


features, class_outputs = get_training('../joined_matrix_2.txt') 
# 0:27 category  28:38= supervisor district, 39 = count 40= output
class_outputs = preprocessing.binarize(class_outputs)
features_10k, class_10k = features[:10000, 28:39], class_outputs[:10000]
test_features_10k, test_class_10k = features[10000:20000, 28:39], class_outputs[10000:20000]

#all_models = [svm.LinearSVC(), svm.SVC(), tree.DecisionTreeClassifier(), ensemble.RandomForestClassifier(), ensemble.AdaBoostClassifier(), neighbors.KNeighborsClassifier()] 
#all_models = [tree.DecisionTreeClassifier(), ensemble.RandomForestClassifier(), ensemble.AdaBoostClassifier(), neighbors.KNeighborsClassifier()] 
all_models = [ensemble.AdaBoostClassifier()]
""""""
param_grids = [[{'min_samples_leaf':[500], 'splitter':['best']}],
		[{'min_samples_leaf':[5], 'n_estimators':[300]}],
		[{'learning_rate':[0.7], 'n_estimators':[100]}],
		[{'weights':[better_inv_dist], 'n_neighbors':[500]}]]
""""""
opt_models = dict()",classificationAggregate/train_classification.py,JamesWo/cs194-16-data_manatees,1
"    trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )

    print ""Creating average feature vecs for test paper content""

    testDataVecs = getAvgFeatureVecs( getCleanReviews(test), model, num_features )


    # ****** Fit a random forest to the training set, then make predictions
    #
    # Fit a random forest to the training data, using 100 trees
    forest = RandomForestClassifier( n_estimators = 100 )

    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit( trainDataVecs, train[""Marks_obtained""] )

    # Test & extract results
    result = forest.predict( testDataVecs )

    # Write the test results
    output = pd.DataFrame( data={""Paper_id"":test[""Paper_id""], ""Given_marks"":test[""Marks_obtained""], ""Marks_obtained"":result } )",Word2Vec_AverageVectors.py,rkc007/Automated-Content-Grading-Using-Machine-Learning,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/len(X1) )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X1,target, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 100, n_jobs = 12, random_state = 4)

result_rf = cross_val_predict(rf,X1,target, cv = 10)

CM = confusion_matrix(target,result_rf) 
print(""Confusion Matrix : "")
print(CM)
for i in range(0,len(X1)):
	if(target[i]== result_rf[i]):
		score_rf+=1",sandbox/petsc/solvers/scripts/ScikitClassifiersRS1CV.py,LighthouseHPC/lighthouse,1
"X,y = settings.flattened_train_paths(settings.classes)
X_test = settings.image_fnames['test']

X = np.array(X)
y = np.array(y)

detector = lambda image: neukrill_net.image_features.get_FAST_keypoints(image, n=max_num_kp)

describer = neukrill_net.image_features.get_ORB_descriptions

kprf_base = sklearn.ensemble.RandomForestClassifier(n_estimators=750, max_depth=15, min_samples_leaf=25, n_jobs=12, random_state=42)

hlf = neukrill_net.highlevelfeatures.KeypointEnsembleClassifier(detector, describer, kprf_base,
                                                                     return_num_kp=True, n_jobs=16, verbosity=1, summary_method='vote')

# Partition the data

print ""Partitioning the training data""

# Remove the data which is going to be held out",generate_local_cache_FAST.py,Neuroglycerin/neukrill-net-work,1
"                    DecisionTreeClassifier(max_depth=100),
                'DTC(max_depth=500)':
                    DecisionTreeClassifier(max_depth=500),
                'DTC(max_depth=1000)':
                    DecisionTreeClassifier(max_depth=1000),
                'DTC(max_depth=2000)':
                    DecisionTreeClassifier(max_depth=2000),
                'DTC(max_depth=2500)':
                    DecisionTreeClassifier(max_depth=2500),
                'RFC(n_estimators=10, max_features=""sqrt"")':
                    RandomForestClassifier(n_estimators=10, max_features=""sqrt""),
                'RFC(n_estimators=20, max_features=""log2"")':
                    RandomForestClassifier(n_estimators=20, max_features=""log2""),
                'RFC(n_estimators=25, max_features=""sqrt"")':
                    RandomForestClassifier(n_estimators=25, max_features=""sqrt""),
                'RFC(n_estimators=50, max_features=""log2"")':
                    RandomForestClassifier(n_estimators=50, max_features=""log2""),
                'RFC(n_estimators=100, max_features=""sqrt"")':
                    RandomForestClassifier(n_estimators=100, max_features=""sqrt""),
                'RFC(n_estimators=500, max_features=""log2"")':",predict_with_dt_and_rf.py,rafaelvalle/MDI,1
"    """"""
    Random Forest classifier module
    Args:
       train_vectors: training feature vectors
       trainMacros: label to integer map
       test_vectors: testing feature vectors
       reverseMap: integer to label map
    Returns:
        rfresultData: Random Forest prediction results
    """"""
    rfClf = OneVsRestClassifier(RandomForestClassifier())
    rfClf.fit(train_vectors,trainMacros)
    rfresult = rfClf.predict(test_vectors)
    rfresultList = rfresult.tolist()
    rfresultData=[]
    for item in rfresultList:
        rfresultData.append(reverseMap[item])
    return rfresultData
    
def main(categoryDataPath,stopWordsDataPath):",src/acd_acs_ml.py,pnisarg/ABSA,1
"    y_test_b = np.hstack((y_test_b, cyy_test, cey_test))
    weights = 1
    X_train_bw = X_train_b * weights
    paramgrid = {'n_estimators': [200],
                 'max_features': ['auto'],
                 'criterion': ['entropy'],
                 'min_samples_split': [10],
                 'min_samples_leaf': [8],
                 'max_depth': [30],
                 'bootstrap': [True]}
    model = RandomForestClassifier(n_jobs=-1)
    model, gs = gridsearch(paramgrid, model, X_train_bw, y_train_b)
    print(""\nthis is the model performance on the training data\n"")
    view_classification_report(model, X_train_bw, y_train_b)
    view_classification_report(model, X_test_b*weights, y_test_b)
    print(confusion_matrix(y_train_b, model.predict(X_train_bw)))
    print(""this is the model performance on the test data\n"")
    print(confusion_matrix(y_test_b, model.predict(X_test_b*weights)))
    print(""this is the model performance on different split ratios\n"")
    print(""\nthese are the model feature importances\n"")",src/lightweight_classifier.py,brityboy/BotBoosted,1
"
    # get our train/test
    X_train, X_test, y_train, y_test = train_test_split(X, iris.target, train_size=0.75, random_state=42)
    custom_cv = KFold(n=y_train.shape[0], n_folds=5, shuffle=True, random_state=42)


def test_pipeline_basic():
    pipe = Pipeline([
        ('selector', FeatureRetainer(cols=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)'])),
        ('scaler',   SelectiveScaler()),
        ('model',    RandomForestClassifier())
    ])

    pipe.fit(X, iris.target)


def test_pipeline_complex():
    pipe = Pipeline([
        ('selector',  FeatureRetainer(cols=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)'])),
        ('scaler',    SelectiveScaler()),",skutil/tests/test_pipe.py,tgsmith61591/skutil,1
"# split training and test data
def split_data(features, labels):
  train_features, test_features, train_labels, test_labels = sklearn.model_selection.train_test_split(features, labels, test_size = 0.2)
  return train_features, test_features, train_labels, test_labels

def get_accuracy(predictions, labels):
  return float(np.sum(predictions == labels))/len(labels)

# build RF model
def predict_rf(train_features, test_features, train_labels, test_labels):
  model = RandomForestClassifier(n_estimators=1000)
  model.fit(train_features, train_labels)
  predictions = model.predict(train_features)
  # print get_accuracy(predictions, train_labels)
  predictions = model.predict(test_features)
  # print get_accuracy(predictions, test_labels)
  return predictions

# build LR model
def predict_lr(train_features, test_features, train_labels, test_labels):",app/webtool/views.py,BIDS-collaborative/EDAM,1
"    return file_name, point_file_name


def get_classifier_model(classifier_name):
    model = None
    if classifier_name=='Logistic Regression':
        classifier_name='LogisticRegression'
        model = LogisticRegression()
    elif classifier_name=='Random Forest':
        classifier_name='RandomForestClassifier'
        model = RandomForestClassifier()
    elif classifier_name=='KNN':
        classifier_name='KNeighborsClassifier'
        model = KNeighborsClassifier()
    elif classifier_name=='Decision Tree':
        classifier_name='DecisionTreeClassifier'
        model = DecisionTreeClassifier()
    elif classifier_name == 'Ada Boost':
        classifier_name = 'AdaBoostClassifier'
        model = AdaBoostClassifier()",analytic/trajectory_manager.py,amilcarsj/analytic,1
"    d2[""who""][d2[""who""] == ""woman""] = 1
    d2[""who""][d2[""who""] == ""man""] = 2

    d2[""sex""][d2[""sex""] == ""female""] = 0
    d2[""sex""][d2[""sex""] == ""male""] = 1

    return d2

def plot_feature_selection(data, features, figsize=(10,6)):

    clf = RandomForestClassifier(n_estimators=100)
    clf.fit(data[features[:-1]].values, data[""survived""])

    n_features = len(features[:-1])

    fig, ax = plt.subplots(figsize=figsize)

    index = np.arange(n_features)
    bar_width = 0.5
",ekstra_funktioner.py,digitaldingo/task_force,1
"  Split = .91
  Mode = ""log""


  PUFs = [
#  [""crpuf"",5,5,
#    sim.DelayGenerator(""crpuf"",5,5), 
#    sim.ChallengeGenerator(""crpuf"",5,10000),
#    [ 
#      #[svm.SVC(),  ""SVM""],
#      [ensemble.RandomForestClassifier(n_estimators=128,),""RF"",],
#      #[linear_model.LogisticRegression(),""LR"",], 
#    ],
#    [""arrxor""],
#  ],
#  [""crpuf"",5,5,
#    sim.DelayGenerator(""crpuf"",5,5), 
#    sim.ChallengeGenerator(""crpuf"",5,10000),
#    [ 
#      #[svm.SVC(),  ""SVM""],",PUFMLAttack.py,rodrigosurita/freepuf,1
"from sklearn.naive_bayes import GaussianNB

features = np.genfromtxt('features.csv', delimiter=',')[1:]
labels = np.genfromtxt('labels.csv', delimiter=',')[1:]


x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)

algos = {
    'DecisionTree': tree.DecisionTreeClassifier(),
    'RandomForest': ensem.RandomForestClassifier(),
    'GradientBoosting': ensem.AdaBoostClassifier(),
    'AdaBoost': ensem.AdaBoostClassifier(),
    'GNB': GaussianNB()
}
results={}
for algo in algos:
    clf = algos[algo]
    clf.fit(x_train,y_train)
    score = clf.score(x_test,y_test)",template.py,iamharshit/ML_works,1
"#print(test)
ids = test['PassengerId'].values
test = test.drop(['PassengerId'], axis=1)

## finish munging ##

test_data = test.values

from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_estimators = 100)

forest = forest.fit(train_data[:, 1:], train_data[:, 0])

output = forest.predict(test_data)

print(output)

prediction_file = open('randomforests.csv', 'wt')
p = csv.writer(prediction_file)",titanic/python/visualization.py,lmarchesoti/kaggle,1
"        words = p.get_words(filter=lambda x: x[""num""], flatten=True)
        
        return X, words


    def model(self, C=1.0):
        # clf = Pipeline([
        #                 ('feature_selection', RandomizedLogisticRegression()),
        #                 ('classification', SVC(probability=True))
        #                ])
        clf = RandomForestClassifier()
        #SVC(C=C, kernel='linear')
        # clf = LogisticRegression(C=C, penalty=""l1"")
        
        
        return clf



",bilearn.py,ijmarshall/cochrane-nlp,1
"from sklearn.exceptions import NotFittedError
from sklearn.model_selection import cross_val_score

iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target


def test_StackingClassifier():
    np.random.seed(123)
    meta = LogisticRegression()
    clf1 = RandomForestClassifier()
    clf2 = GaussianNB()
    sclf = StackingCVClassifier(classifiers=[clf1, clf2],
                                meta_classifier=meta,
                                shuffle=False)

    scores = cross_val_score(sclf,
                             X,
                             y,
                             cv=5,",mlxtend/classifier/tests/test_stacking_cv_classifier.py,rasbt/mlxtend,1
"            ('inv_vec1', InverseDictVectorizer(vect1, vt)),
            ('vect2', vect2),
            ('skb', skb),
            ('inv_vec2', InverseDictVectorizer(vect2, skb)),
            ('pathway_scoring', PathwayFvaScaler()),
            ('vect3', DictVectorizer(sparse=False)),
            ('pca', PCA()),
            # ('clf', SVC(C=1e-6, kernel='rbf', random_state=0))
            # ('clf', KNeighborsClassifier(n_neighbors=31))
            # ('clf', DecisionTreeClassifier())
            # ('clf', RandomForestClassifier(n_estimators=10000, n_jobs=-1))
            # ('clf', LinearSVC(C=0.1e-5, random_state=43))
            ('clf', LogisticRegression(C=0.3e-6, random_state=43))
            # ('clf', MLPClassifier(activation=""logistic"",
            #                       random_state=43,
            #                       hidden_layer_sizes=(300, 100),
            #                       verbose=True,
            #                       #   alpha=1e-2,
            #                       max_iter=1000))
        ])",src/classifiers/fva_disease_classifier.py,MuhammedHasan/metabolitics,1
"        np.random.seed(0)
        data = np.random.normal(size=(1000,))
        fig = plot_simple_histogram(data, verbose=False)
        self.add_fig_to_report(fig, 'plot_simple_histogram')

    def test_plot_prec_recall(self):
        M, labels = uft.generate_correlated_test_matrix(1000)
        M_train, M_test, labels_train, labels_test = train_test_split(
                M, 
                labels)
        clf = RandomForestClassifier(random_state=0)
        clf.fit(M_train, labels_train)
        score = clf.predict_proba(M_test)[:,-1]
        fig = dsp.plot_prec_recall(labels_test, score, verbose=False)
        self.add_fig_to_report(fig, 'plot_prec_recall')

    def test_plot_roc(self):
        M, labels = uft.generate_correlated_test_matrix(1000)
        M_train, M_test, labels_train, labels_test = train_test_split(
                M, ",tests/test_display.py,jamestwhedbee/diogenes,1
"y = data.iloc[:,4].values

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)

# standard Scaling the data
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Implemeting Logistic Reg
clf = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)
clf.fit(X_train, y_train)

# predict
pred_y = clf.predict(X_test)

# confusion prediction
cm = confusion_matrix(y_test, pred_y)
print(""Confusion Matrix:\n"",cm)
",ML_A2Z/14_RandomForest_Tree_Classifier.py,atulsingh0/MachineLearning,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/min_aggregate_only.py,diogo149/CauseEffectPairsPaper,1
"import numpy as np
from cudatree import load_data, RandomForestClassifier, timer
from cudatree import util

x, y = load_data(""diabetes"")

def test_diabetes_memorize():
  with timer(""Cuda treelearn""):
    forest = RandomForestClassifier(bootstrap = False)
    forest.fit(x, y)
  with timer(""Predict""):
    diff, total = util.test_diff(forest.predict(x), y)  
    print ""%s(Wrong)/%s(Total). The error rate is %f."" % (diff, total, diff/float(total))
  assert diff == 0, ""Didn't perfectly memorize, got %d wrong"" % diff

from helpers import compare_accuracy, compare_hybrid_accuracy
def test_diabetes_accuracy():
  compare_accuracy(x,y)",test/test_diabetes_classify.py,EasonLiao/CudaTree,1
"def test_PCA():
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,
                                                        random_state=1111)
    p = PCA(100, solver='eigen')

    # fit PCA with training data, not the entire dataset
    p.fit(X_train)
    X_train_reduced = p.transform(X_train)
    X_test_reduced = p.transform(X_test)

    model = RandomForestClassifier(n_estimators=10, max_depth=4)
    model.fit(X_train_reduced, y_train)
    predictions = model.predict(X_test_reduced)[:, 1]
    print(roc_auc_score(y_test, predictions))
    assert roc_auc_score(y_test, predictions) >= 0.70",mla/tests/test_reduction.py,rushter/MLAlgorithms,1
"        f1         : f1 score
        parameters : previous parameters in the order previously specified
    """"""
    if max_depth==-1:
        max_depth = None

    labels = np.unique(Y_train)

    ## # Run rf
    # Define classifier
    rf = RandomForestClassifier(n_estimators = n_estimators,
                                criterion    = criterion,
                                max_features = max_features,
                                max_depth    = max_depth,
                                n_jobs       = n_jobs)
    # Fit
    rf.fit(X_train, Y_train)

    # Predict
    Y_hat   = rf.predict(X_test)",lobpredictrst/rf.py,doutib/lobpredict,1
"            criterion = 'gini'
        elif params['clf12:criterion'] == '1' or params['clf12:criterion'] == 'entropy':
            criterion = 'entropy'
        max_features = int(float(params['clf12:max_features']))
        min_samples_split = int(float(params['clf12:min_samples_split']))
        min_samples_leaf = int(float(params['clf12:min_samples_leaf']))
        if params['clf12:bootstrap'] == '0' or params['clf12:bootstrap'] == 'True':
            bootstrap = True
        elif params['clf12:bootstrap'] == '1' or params['clf12:bootstrap'] == 'False':
            bootstrap = False
        clf = RandomForestClassifier(n_estimators=100,
                                     criterion=criterion,
                                     max_features=max_features,
                                     min_samples_split=min_samples_split,
                                     min_samples_leaf=min_samples_leaf,
                                     bootstrap=bootstrap,
                                     class_weight=params['class_weight'])

    elif params['classifier'] == str(d_clf['SGDClassifier']) or params['classifier'] == 'SGDClassifier':
        l1_ratio = 0.15",benchmarks/sklearn/ml_framework.py,yuyuz/FLASH,1
"# TODO - parameter tuning
model1 = xgb.XGBClassifier(max_depth=3, n_estimators=700, learning_rate=0.05)
data1, test1 = dataPreprocessing(data.copy(deep=True), test.copy(deep=True))
solution_best1 = modeling(data1, test1, model1) # 

test1['Disbursed'] = solution_best1
test1.to_csv('Solution_xgb.csv', columns=['Disbursed'],index=True)

# TODO ensembling
## approach 2
#model2 = RandomForestClassifier(n_estimators=700)
#data2, test2 = dataPreprocessing2(data.copy(deep=True), test.copy(deep=True))
#solution_best2 = modeling(data2, test2, model2)
#
#test2['Disbursed'] = solution_best2
#test2.to_csv('Solution2.csv', columns=['Disbursed'],index=True)

## approach 3
#model3 = GradientBoostingClassifier(n_estimators=700)
#data3, test3 = dataPreprocessing1(data.copy(deep=True), test.copy(deep=True))",AV_Hackathon3x.py,Bolaka/AV-Hackathon-3.x,1
"from numpy import genfromtxt
features_test = genfromtxt('d:/CODE/ml-crops/preproc/dataset/features_train.csv', delimiter=',')
classes_test = genfromtxt('d:/CODE/ml-crops/preproc/dataset/classes_train.csv', delimiter=',')

features_train = genfromtxt('d:/CODE/ml-crops/preproc/dataset/features_test.csv', delimiter=',')
classes_train = genfromtxt('d:/CODE/ml-crops/preproc/dataset/classes_test.csv', delimiter=',')

###############################################
# perform Random Forest classification
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
fit_start_time = time()
clf.fit(features_train, classes_train)
fit_end_time = time()
print ""\nTraining time : "", round(fit_end_time - fit_start_time, 3), ""s""

###############################################
# predict
predict_start_time = time()
classes_predicted = clf.predict(features_test)",RandomForest/dt_ml_RandomForest.py,stefan-contiu/ml-crops,1
"    train = np.loadtxt(traindata)
    test = np.loadtxt(testdata)
    X = train[0:4628,0:27]
    y = train[0:4628,27]
    test_x = test[0:1437,0:27]
    test_y = test[0:1437,27]

    model1 = LinearSVC()
    model2 = LogisticRegression()
    model3 = GaussianNB()
    model4 = RandomForestClassifier()
    model5 = KNeighborsClassifier()
    model1.fit(X,y)
    model2.fit(X,y)
    model3.fit(X,y)
    model4.fit(X,y)
    model5.fit(X,y)
    predicted1 = model1.predict(test_x)
    predicted2 = model2.predict(test_x)
    predicted3 = model3.predict(test_x)",/newcla.py,vimilimiv/weibo-popularity_judge-and-content_optimization,1
"    print ""After reduction: "", df_input.shape


    # splitting the data into training and testing sets
    from sklearn.cross_validation import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(df_input_data, df_input_target.tolist())

    # Import the random forest package
    from sklearn.ensemble import RandomForestClassifier
    # Create the random forest object which will include all the parameters for the fit
    forest = RandomForestClassifier(n_estimators=500)
    # Fit the training data to the Survived labels and create the decision trees
    forest = forest.fit(X_train, numpy.ravel(y_train[:]))
    # Take the same decision trees and run it on the test data
    predicted = forest.predict(X_test)

    # Prediction Performance Measurement
    matches = (predicted == [item for sublist in y_test for item in sublist])
    print ""Accuracy : "", (matches.sum() / float(len(matches)))
",Code/Machine_Learning_Algos/10k_Tests/ml_classification_RandomForest.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"    use_joblib : bool, optional
        Whether or not joblib will be used to save the classifier.

    Returns
    -------
    ext : string
        File extension

    Examples
    --------
    >>> cl = RandomForestClassifier()
    >>> default_classifier_extension(cl)
    '.classifier.joblib'
    >>> default_classifier_extension(cl, False)
    '.classifier'
    """"""
    if isinstance(cl, VigraRandomForest):
        return "".classifier.h5""
    elif use_joblib:
        return "".classifier.joblib""",gala/classify.py,janelia-flyem/gala,1
"import numpy as np
import datetime

np.seterr(divide='ignore',invalid='ignore')

trainArticles= open('data/singleShort.txt','r').readlines()#=importArticles.getData('train')
testArticles = open('data/singleShortTest.txt','r').readlines()#= importArticles.getData('test')
print len(trainArticles)
print len(testArticles)
listOfYears = []
reg = ensemble.RandomForestClassifier() #linear_model.LinearRegression()
probs = []
titles = []
#A
def getArticle(article):
    singleSets = []
    try:
        chunks = gc.getChunks(article[1])
        tags =  tag.getTags(article[1],chunks)
        #if tags == []:",randomForest.py,JFriel/honours_project,1
"y = np.array(y)

detector_list = [lambda image: neukrill_net.image_features.get_ORB_keypoints(image, n=max_num_kp_orb, patchSize=9),
                 lambda image: neukrill_net.image_features.get_FAST_keypoints(image, n=max_num_kp_fast),
                 lambda image: neukrill_net.image_features.get_MSER_keypoints(image, n=max_num_kp_mser)]

describer_list = [neukrill_net.image_features.get_ORB_descriptions,
                  neukrill_net.image_features.get_ORB_descriptions,
                  neukrill_net.image_features.get_ORB_descriptions]

kprf_base = sklearn.ensemble.RandomForestClassifier(n_estimators=1000, max_depth=15, min_samples_leaf=20, n_jobs=16, random_state=42)

hlf_list = []
for index,detector in enumerate(detector_list):
    hlf_list += [neukrill_net.highlevelfeatures.KeypointEnsembleClassifier(detector, describer_list[index], kprf_base,
                                                                     return_num_kp=True, n_jobs=0, verbosity=1, summary_method='vote')]

hlf = neukrill_net.highlevelfeatures.MultiHighLevelFeature(hlf_list)

# Partition the data",generate_local_cache.py,Neuroglycerin/neukrill-net-work,1
"
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.svm import SVR

from revenue import RevenueCompetition, RevenueTransform

if __name__ == '__main__':
        
    train_size = 0.75
    cls = RandomForestClassifier()
    reg = RandomForestRegressor(n_estimators=20, max_features=5, max_depth=None,
                                 min_samples_split=2, min_samples_leaf=1,
                                 max_leaf_nodes=None, bootstrap=True,
                                 oob_score=False, n_jobs=-1)
    reg = SVR(C=10., gamma=0.1)
    train_df_orig = RevenueCompetition.load_data()
    y = train_df_orig['revenue'].values
    del train_df_orig['revenue']
",revenue/linear.py,ldamewood/kaggle,1
"testing = pd.read_csv(""protoAlpha_testing.csv"")

X = training.iloc[:,1:-1]
y = training['country_destination']

x_train,x_valid,y_train,y_valid = train_test_split(X,y,test_size=0.3,random_state=None)

# Train classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsOneClassifier
clf = OneVsOneClassifier(RandomForestClassifier(n_estimators=50,n_jobs=5))
clf.fit(x_train,y_train)
print( clf.feature_importances_ );

# Run Predictions
from sklearn.metrics import confusion_matrix, accuracy_score
y_preds = clf.predict(x_valid)
print( confusion_matrix(y_valid,y_preds) );
print( ""Accuracy: %f"" % (accuracy_score(y_valid,y_preds)) );
f = open('randomForest_take1.txt', 'w')",prototype_alpha/randomForest_take1.py,valexandersaulys/airbnb_kaggle_contest,1
"names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",
         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",
         ""Naive Bayes"", ""QDA""]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",Classifrer/classifer.py,hitlonewind/PR-experiment,1
"
    # try something with no __iter__ attr
    assert_fails(log, ValueError, 'A')
    assert_fails(exp, ValueError, 'A')


def test_grid_search_fix():
    df = load_iris_df(shuffle=True, tgt_name='targ')
    y = df.pop(""targ"")

    pipe = Pipeline([('rf', RandomForestClassifier())])
    pipe2 = Pipeline([('pca', SelectivePCA())])

    hyp = {'rf__n_estimators':  [10, 15]}
    hyp2 = {'pca__n_components': [1,  2]}

    with warnings.catch_warnings():
        warnings.simplefilter(""ignore"")

        for iid in [True, False]:",skutil/utils/tests/test_util.py,tgsmith61591/skutil,1
"    count_enrollment = df_sub['3COURSEID'].value_counts()
    #print ""Number of %s enrollment: %s""%(subject,count_enrollment)

    A = df_sub.as_matrix()
    X = A[:,6:117]
    X = X.astype(np.int64, copy=False)
    y = A[:,2]
    y = y.astype(np.int64, copy=False)

    #Training data
    forest = RandomForestClassifier(n_estimators=10, max_depth=None, 
            min_samples_split=1, random_state=None, max_features=None)
    clf = forest.fit(X, y)
    scores = cross_val_score(clf, X, y, cv=5)
    print scores
    print ""Random Forest Cross Validation of %s: %s""%(subject,scores.mean())
    precision_rf[subject] = scores.mean()
    df_precision.loc[subject]=precision_rf[subject]
    
    sheet1.write(count, 0, subject)",pae/final_code/src/feature_eachSub.py,wasit7/book_pae,1
"data = np.hstack((data_ref,data))
target = np.hstack((target_ref,target))
cut = int(float(len(data)) * PROP)
data_train, data_test, target_train, target_test = data[:cut], data[cut:], target[:cut], target[cut:]

data_train_size_mb = size_mb(data_train)
data_test_size_mb = size_mb(data_test)
print('data loaded - train: {0}Mb test: {1}Mb'.format(data_train_size_mb, data_test_size_mb))


for clf, clf_name in [ (RandomForestClassifier(), ""Random Forest""),
                      (RidgeClassifier(tol=1e-2, solver=""lsqr""), ""Ridge Classifier""),
                      (Perceptron(n_iter=50), ""Perceptron""),
                      (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
                      (KNeighborsClassifier(n_neighbors=10), ""kNN""),
                      (BernoulliNB(alpha=.01), 'BNB'),
                      (LinearSVC(penalty=""l1"", dual=False, tol=1e-3), 'LVC1'),
                      (LinearSVC(),'LVC2') ]:
    
    print ""CHECKING "",clf_name",feature_selection.py,linucks/textclass,1
"        clf_descr = str(clf).split('(')[0]
        return clf_descr, score, train_time, test_time

    results = []

    for clf, name in (
            (RidgeClassifier(tol=1e-2, solver=""lsqr""), ""Ridge Classifier""),
            (Perceptron(n_iter=50), ""Perceptron""),
            (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
            (KNeighborsClassifier(n_neighbors=3), ""kNN""),
            (RandomForestClassifier(n_estimators=100), ""Random forest"")
            ):
        print('=' * 80)
        print(name)
        results.append(benchmark(clf))

    for penalty in [""l2"", ""l1""]:
        print('=' * 80)
        print(""%s penalty"" % penalty.upper())
        # Train Liblinear model",OLD/Master Classifier/TextClassifierFNF.py,tpsatish95/Youtube-Comedy-Comparison,1
"        params = log_params
        model = SGDClassifier()
    elif choice=='huber':
        params = huber_params
        model = SGDClassifier()
    elif choice=='svm':
        params = SVM_params
        model = svm.SVC(C=1)
    elif choice=='rf':
        params = RF_params
        model = RandomForestClassifier(n_estimators=1000, bootstrap=False)
        #clf = RandomForestClassifier(n_estimators=1000, bootstrap=False)
    
    # Set up Grid Search
    print ""Grid search...""
    clf = GridSearchCV(model, params, n_jobs=2, scoring='f1')
    clf.fit(X[:255], y[:255]) # Grid search only on part of the data
    clf = clf.best_estimator_
    print clf
",baseline.py,momiah/cvariants_theano,1
"        args.label = np.arange(Y_train.shape[1])
    
    logging.info(""Fitting classifiers"")
    clfs = []
    for i in args.label:
        logging.info(""Label %d"" % i)
        clfsi = []
        grid = itertools.product((0.2, 0.4, 0.6), (2, 4, 8, 16))
        for mf, msl in grid:
            logging.info(""mf = %s, msl = %s"" % (mf, msl))
            clf = RandomForestClassifier(
                n_estimators=32, criterion='entropy',
                min_samples_split=1, max_depth=None, max_leaf_nodes=None,
                max_features=mf, min_samples_leaf=msl,
                n_jobs=-1, random_state=42, verbose=2)
            try:
                clf.fit(X_train, Y_train[:,i])
                p = clf.predict_proba(X_test)
                s = score_predictions(Y_test[:,i], p[:,1])
                logging.info(""...score = %f"" % s)",scripts/predictors/random_forest.meta.py,timpalpant/KaggleTSTextClassification,1
"def classify(features_train, labels_train):   
    ### import the sklearn module for GaussianNB
    ### create classifier
    ### fit the classifier on the training features and labels
    ### return the fit classifier
    
    
    ### your code goes here!
    from sklearn import ensemble
    clf=ensemble.RandomForestClassifier()
    clf.fit(features_train, labels_train)
    return clf",random forest/ClassifyNB.py,AhmetHamzaEmra/Machine-Learning-,1
"    return parser

if __name__ == ""__main__"":
    args = opts().parse_args()
    
    print ""Loading and preparing data""
    X = prepare_features(args.train)
    Y = args.labels['labels']
    
    print ""Training classifier""
    clf = ensemble.RandomForestClassifier(n_estimators=16, n_jobs=-1,
        random_state=42, verbose=2)
    clf.fit(X, Y)
    del X, Y
    
    print ""Predicting""
    X = prepare_features(args.test)
    p = np.vstack([v[:,0] for v in clf.predict_proba(X)])
    Y = 1 - p.T
    ",scripts/practice/predict.4.py,timpalpant/KaggleTSTextClassification,1
"
        '''print
        adaClf = AdaBoostClassifier(n_estimators=100)
        print ""Classifier:""
        print adaClf
        print ""Training"", subject
        adaClf.fit(xx, yy)
        classifiers.append(adaClf)

        print
        rfClf = RandomForestClassifier(n_estimators=1000, max_depth=None, max_features=375, n_jobs=-1)
        print ""Classifier:""
        print rfClf
        print ""Training"", subject
        rfClf.fit(xx, yy)
        classifiers.append(rfClf)

        print
        sgdClf = SGDClassifier(loss=""hinge"", penalty=""l2"")
        print ""Classifier:""",src/split_multiclassification_stacked_gen.py,LooseTerrifyingSpaceMonkey/DecMeg2014,1
"	
	# SVM & RF
	rbf = svm.SVC(kernel='rbf', C=10000, gamma=0.1)	
	#rbf = svm.SVC(kernel='rbf', C=10, gamma=10)	
	#rbf = svm.LinearSVC()
	#rbf = linear_model.SGDClassifier()
	#rbf = linear_model.Perceptron(n_iter=4, shuffle=True)
	rbf.fit(X, y)  
	
	# whole vectors:
	#rf = RandomForestClassifier(n_estimators=rf_n_estimators)
	#
	# rf = GaussianNB()
	# rf = MultinomialNB()
	rf = AdaBoostClassifier(DecisionTreeClassifier(criterion='gini', max_depth=conf['classification']['dtree_depth'], max_features=None, min_density=None, min_samples_leaf=1, min_samples_split=2), algorithm=""SAMME"", n_estimators=conf['classification']['ada_n_estimators'])
	rf.fit(X, y)  
	
	print 'testing SVM & RF - training data'

	total = sample_size * 2 ",train_and_test_binary.py,jimbotonic/df_nlp,1
"        # Perhaps we will just exclude the weird class from our ML training
        not_weird = all_domains[all_domains['class'] != 'weird']
        X = not_weird.as_matrix(['length', 'entropy', 'alexa_grams', 'word_grams'])

        # Labels (scikit learn uses 'y' for classification labels)
        y = np.array(not_weird['class'].tolist())


        # Random Forest is a popular ensemble machine learning classifier.
        # http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestClassifier.html
        clf = sklearn.ensemble.RandomForestClassifier(n_estimators=20, compute_importances=True) # Trees in the forest


        # Train on a 80/20 split
        from sklearn.cross_validation import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)

        # Now plot the results of the holdout set in a confusion matrix",dga_detection/dga_model_gen.py,trogdorsey/data_hacking,1
"cols = ['points_per_game','now_cost','selected_by', 'interaction_term1','interaction_term2', 'ea_index']


X = regular_forwards_df[cols]

y = regular_forwards_df.event_total

lm = LinearRegression()

# Also tried RandomForestClassifier but it didn't produce better model as compared to Linear Regression
#rf = RandomForestClassifier(n_estimators=100) 
scores = cross_val_score(lm, X, y, cv=5, scoring='mean_squared_error')

#Calculating Root Mean Squared Error
np.sqrt(-scores)
np.mean(np.sqrt(-scores)) #RMSE for Nov Model 2.0596


# Creating the Same Model for Dec 1 Data Set
",fpl_api.py,prathmesh/Fantasy-Premier-League-Points-Predictor,1
"            train.loc[train_series.isnull(), train_name] = -999
        #and Test
        tmp_len = len(test[test_series.isnull()])
        if tmp_len>0:
            test.loc[test_series.isnull(), test_name] = -999


kf = KFold(train.shape[0], n_folds=3, random_state=1)
alg = GaussianNB()
 
#RandomForestClassifier(n_estimators=100,criterion= 'entropy')

predictions = []
for trainkf, test in kf:
	print trainkf
	# The predictors we're using the train the algorithm.  Note how we only take the rows in the train folds.
	train_predictors = (train.iloc[trainkf,:])
	# The target we're using to train the algorithm.
	train_target = target.iloc[trainkf]
	print train_predictors.shape[0],train_predictors.shape[1]",other methods with not so great accuracy/bayesian.py,souravsarangi/BNPParibasKaggle,1
"
# setting up the parameter search space

param_grid_linear_svc = {'C':[0.001, 0.01, 0.1, 1.0, 2.0]}
param_grid_log_reg = {'C': [0.001, 0.01, 0.1, 1, 1.0, 2.0]}
param_grid_rf = {'n_estimators': [50, 100, 200]}

# setting up the function search space
model_dict = {'linear_svm':{'clf':LinearSVC(dual = False, penalty = ""l1""), 'param_grid' : param_grid_linear_svc},
              ""logistic"" :{'clf': LogisticRegression(penalty = ""l1""), 'param_grid' : param_grid_log_reg},
                ""RF"": {'clf': RandomForestClassifier(), 'param_grid': param_grid_rf}
                
                }
     
# a function to run gridsearches with multiple base model           
def test_models(model_dict, nfolds, X, y, score):
    results = {}
    for model_key in model_dict.keys():
        print model_key 
        base_model = model_dict[model_key]['clf']",Code/deeplearning.py,TimKreienkamp/TextMiningProject,1
"        
            if options.debug: print(""Fitting..."")
        
            if options.method==""SVM"":
                clf = svm.SVC()
            elif options.method==""nuSVM"":
                clf = svm.NuSVC()
            elif options.method=='NN':
                clf = neighbors.KNeighborsClassifier(options.n)
            elif options.method=='RanForest':
                clf = ensemble.RandomForestClassifier(n_estimators=options.n,random_state=options.random)
            elif options.method=='AdaBoost':
                clf = ensemble.AdaBoostClassifier(n_estimators=options.n,random_state=options.random)
            elif options.method=='tree':
                clf = tree.DecisionTreeClassifier(random_state=options.random)
            else:
                clf = svm.LinearSVC()
        
            clf.fit(training_X,training_Y)
        ",examples/image_classify.py,BIC-MNI/pyezminc,1
"scores = -np.log10(selector.pvalues_)

# Plot the scores.  See how ""Pclass"", ""Sex"", ""Title"", and ""Fare"" are the best?
plt.bar(range(len(predictors)), scores)
plt.xticks(range(len(predictors)), predictors, rotation='vertical')
plt.show()

# Pick only the four best features.
predictors = [""Pclass"", ""Sex"", ""Fare"", ""Title""]

alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=8, min_samples_leaf=4)
scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[""Survived""], cv=3)

# Take the mean of the scores (because we have one for each fold)
print(scores.mean())

# The algorithms we want to ensemble.
# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.
algorithms = [
    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [""Pclass"", ""Sex"", ""Age"", ""Fare"", ""Embarked"", ""FamilySize"", ""Title"", ""FamilyId""]],",titanic/titanic2.py,seymour1/Kaggle,1
"        training_data = np.vstack(training_data)
        training_label = np.ravel(label_binarize(
            np.hstack(training_label).astype(int), [0, 255]))
        print 'Create the training set ...'

        # Perform the classification for the current cv and the
        # given configuration
        sel = SelectPercentile(f_classif, p)
        training_data = sel.fit_transform(training_data, training_label)
        testing_data = sel.transform(testing_data)
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data, training_label).predict_proba(
            testing_data)

        results_cv.append([pred_prob, crf.classes_])
        feat_imp_cv.append(sel.get_support(indices=True))

    results_p.append(results_cv)
    feat_imp_p.append(feat_imp_cv)
",pipeline/feature-classification/exp-3/selection-extraction/anova/pipeline_classifier_aggregation.py,I2Cvb/mp-mri-prostate,1
"    """"""Test to ensure that we don't return spurious repeating thresholds.

    Duplicated thresholds can arise due to machine precision issues.
    """"""
    dataset = datasets.load_digits()
    X = dataset['data']
    y = dataset['target']

    # This random forest classifier can only return probabilities
    # significant to two decimal places
    clf = ensemble.RandomForestClassifier(n_estimators=100, random_state=0)

    # How well can the classifier predict whether a digit is less than 5?
    # This task contributes floating point roundoff errors to the probabilities
    train, test = slice(None, None, 2), slice(1, None, 2)
    probas_pred = clf.fit(X[train], y[train]).predict_proba(X[test])
    y_score = probas_pred[:, :5].sum(axis=1)  # roundoff errors begin here
    y_true = [yy < 5 for yy in y[test]]

    # Check for repeating values in the thresholds",venv/lib/python2.7/site-packages/sklearn/metrics/tests/test_metrics.py,chaluemwut/fbserver,1
"            np.hstack(training_label).astype(int), [0, 255]))
        print 'Create the training set ...'

        # Learn the PCA projection
        pca = SparsePCA(n_components=sp)
        training_data = pca.fit_transform(training_data)
        testing_data = pca.transform(testing_data)

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_sp.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/selection-extraction/sparse-pca/pipeline_classifier_mrsi.py,I2Cvb/mp-mri-prostate,1
"        self.param_dist_random = {'max_depth': sp_randint(1, 100),
                                      'min_samples_leaf': sp_randint(1, 150),
                                      'criterion': ['entropy', 'gini']}
        self.clf = DecisionTreeClassifier(**args)

class Forest(GIClassifier):
    def __init__(self, **args):
        self.param_dist_random = {'max_depth': sp_randint(1, 100),
                                      'min_samples_leaf': sp_randint(1, 100),
                                      'criterion': ['entropy', 'gini']}
        self.clf = RandomForestClassifier(**args)


class KNeighbors(GIClassifier):
    def __init__(self, **args):
        self.param_dist_random = {'leaf_size':sp_randint(20, 50),
                                    'n_neighbors': sp_randint(4, 30)}
        self.clf = KNeighborsClassifier(**args)

",app/classifier.py,WGierke/git_better,1
"# RandomForestFilename = ""randomForest500Model""
#
# train_dataset = pd.read_csv(""../../Data/act_train_features_reduced.csv"")
# test_dataset = pd.read_csv(""../../Data/act_test_features_reduced.csv"")
# train_output = pd.read_csv(""../../Data/act_train_output.csv"")
#
# train_dataset = pd.merge(train_dataset, train_output, on=""activity_id"", how='inner')
# print(""--- %s seconds ---"" % (time.time() - start_time))
#
# randomForestModel = Utility.loadModel(""randomForestModel_OHE"")
# # randomForestModel = RandomForestClassifier(n_estimators=500)
# #
# # randomForestModel.fit(train_dataset[columns], train_dataset[[""outcome""]].values.ravel())
#
# prob_train = randomForestModel.predict_proba(train_dataset[columns])
# prob_test = randomForestModel.predict_proba(test_dataset[columns])
# # Utility.saveModel(randomForestModel, ""randomForestModel_OHE"")
#
# train_dataset[""Random_Forest_1""] = prob_train[:,1]
#",Initial_Classification_Models/Ensemble/RandomForest500Logistic.py,BhavyaLight/kaggle-predicting-Red-Hat-Business-Value,1
"        Number of folds used to estimate the drift
        
    stratify : bool, defaut = True
        Whether the cv is stratified (same number of train and test samples within each fold)

    random_state : int, defaut = 1
        Random state for cv 

    """"""
    
    def __init__(self, estimator = RandomForestClassifier(n_estimators = 50, n_jobs=-1, max_features=1., min_samples_leaf = 5, max_depth = 5), n_folds = 2, stratify = True, random_state = 1):


        self.estimator = estimator
        self.n_folds = n_folds
        self.stratify = stratify
        self.random_state = random_state
        self.__cv = None
        self.__pred = None
        self.__cible = None",python-package/mlbox/preprocessing/drift/drift_estimator.py,AxeldeRomblay/MLBox,1
"    assert_array_equal(X_transformed_sparse.toarray(), X_transformed.toarray())


def test_parallel_train():
    rng = check_random_state(12321)
    n_samples, n_features = 80, 30
    X_train = rng.randn(n_samples, n_features)
    y_train = rng.randint(0, 2, n_samples)

    clfs = [
        RandomForestClassifier(n_estimators=20, n_jobs=n_jobs,
                               random_state=12345).fit(X_train, y_train)
        for n_jobs in [1, 2, 3, 8, 16, 32]
    ]

    X_test = rng.randn(n_samples, n_features)
    probas = [clf.predict_proba(X_test) for clf in clfs]
    for proba1, proba2 in zip(probas, probas[1:]):
        assert_array_almost_equal(proba1, proba2)
",uplift/ensemble/tests/test_forest.py,psarka/uplift,1
"from sklearn.ensemble import RandomForestClassifier

from ..Classifier import Classifier
from ...language.Java import Java


class RandomForestClassifierJavaTest(Java, Classifier, TestCase):

    def setUp(self):
        super(RandomForestClassifierJavaTest, self).setUp()
        self.mdl = RandomForestClassifier(n_estimators=100, random_state=0)

    def tearDown(self):
        super(RandomForestClassifierJavaTest, self).tearDown()",tests/classifier/RandomForestClassifier/RandomForestClassifierJavaTest.py,nok/sklearn-porter,1
"from sklearn.qda import QDA

h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
        ""Random Forest"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    GaussianNB(),
    LDA(),
    QDA()
    ]

X, y = make_classification(n_features=2, n_redundant=0,
        n_informative=2, random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",python/sklearn/examples/plot_classifier_comparison.py,seckcoder/lang-learn,1
"
  print('- Unique titles: %s' % df.Title.unique())
  for title in df.Title.unique():
    counts = len(df[df.Title == title])
    print('title %d: %d times, rate: %f' % (title, counts, len(df[(df.Title == title) & (df.Survived == 1)]) / counts))

  print('... done printing statistics!')
  print(SEPARATOR)

def run_prediction(train, test):
  forest = RandomForestClassifier(n_estimators=100)
  forest = forest.fit(train[0::,1::], train[0::,0] )

  return forest.predict(test).astype(int)

def write_predictions(ids, predictions):
  with open('prediction.csv', 'wt') as predictions_file:
    open_file_object = csv.writer(predictions_file)
    open_file_object.writerow(['PassengerId','Survived'])
    open_file_object.writerows(zip(ids, predictions))",titanic/submissions/4/randomforest.py,furgerf/kaggle-projects,1
"
Train_Data = pd.read_csv(""../PrepareData/ProducedData/PCA(25)_prepared_Train_Data.csv"",header=None)
Test_Data = pd.read_csv(""../PrepareData/ProducedData/PCA(25)_prepared_Test_Data.csv"",header=None)

Labels_index = len(Train_Data.iloc[0][:])-1
Train_Data_labels = Train_Data.iloc[:][Labels_index]
del Train_Data[Labels_index]
Test_Data_labels = Test_Data.iloc[:][Labels_index]
del Test_Data[Labels_index]

Model = RandomForestClassifier(random_state=30).fit(Train_Data,Train_Data_labels)

print(""Result on PCA with 25 components :"")
Predicted_Train = Model.predict(Train_Data)
print(""Accuracy on Training Data :"",metrics.accuracy_score(Train_Data_labels,Predicted_Train))

Predicted_Train = Model.predict(Test_Data)
print(""Accuracy on Testing Data :"",metrics.accuracy_score(Test_Data_labels,Predicted_Train))
print(""\nconfusion_matrix : \n"",metrics.confusion_matrix(Test_Data_labels,Predicted_Train,labels=[0,1]))
print(""\nclassification report : \n\n"",metrics.classification_report(Test_Data_labels,Predicted_Train,labels=[0,1]))",IDS/Random_Forest.py,danialjahed/IDS-KDDcup,1
"    trainDataVecs = getAvgFeatureVecs(getCleanReviews(train), model, num_features)

    print ""Creating average feature vecs for test reviews""

    testDataVecs = getAvgFeatureVecs(getCleanReviews(test), model, num_features)


    # ****** Fit a random forest to the training set, then make predictions
    #
    # Fit a random forest to the training data, using 100 trees
    forest = RandomForestClassifier(n_estimators=100)

    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit(trainDataVecs, train[""sentiment""])

    # Test & extract results
    result = forest.predict(testDataVecs)

    # Write the test results
    output = pd.DataFrame(data={""id"":test[""id""], ""sentiment"":result})",src/main/python/Word2Vec_AverageVectors.py,Chaparqanatoos/kaggle-knowledge,1
"
""""""
**model** module
-------------------

""""""

__all__ = ['cls_rfc', 'cls_xgb']

def cls_rfc(X, y, n_estimators=10):
	model = RandomForestClassifier(n_estimators=n_estimators)

	model.fit(X, y)

	return model

def cls_xgb(X, y, objective='binary:logistic'):
	params = {'max_depth':5, 'eta':0.3, 'silent':0, 'objective':objective }

	dtrain = xgb.DMatrix( X, label=y)",src/func/model.py,megemini/DataCastle2017,1
"        pcb = PCA(n_components = 15)
        fabsPCA = pca.fit(vectorize(training[""FFT""]))
        fphiPCA = pcb.fit(vectorize(training[""Phase""]))

        #Adding ffts to feature set
        train_feats_final = np.hstack( ( train_feats, fphiPCA.transform(vectorize(training[""Phase""])), fabsPCA.transform(vectorize(training[""FFT""])) ) )
        testImg_feats_final = np.hstack( ( testImg_feats, fphiPCA.transform(vectorize(testImg[""Phase""])), fabsPCA.transform(vectorize(testImg[""FFT""])) ) )

        train_feats_final = normalize_columns(train_feats_final)
        testImg_feats_final = normalize_columns(testImg_feats_final)
        clf = RandomForestClassifier(n_estimators = 9)
        print(len(train_feats_final), len(Y_training))
        clf.fit(np.nan_to_num(train_feats_final), np.nan_to_num(Y_training) )
        Y_predict = clf.predict(np.nan_to_num(testImg_feats_final))
        for number, cellclass in enumerate(Y_predict):
            self.table.setItem(number , 1, QtGui.QTableWidgetItem(str(cellclass)))

        self.coloring_types(Y_predict)

",dev python code/main.py,sanchestm/mitotic-index-calc,1
"X_data_scaled = scaler.fit_transform(X_data)


# split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_data_scaled, y_data, test_size=0.4, random_state=42)

### Begin ML ###

# create the classifier
clf = RandomForestClassifier()

# fit the algorithm
clf.fit(X_train, y_train)

# plotting learning curves
plt.title('Learning Curves')
plt.xlabel('Training examples')
plt.ylabel('Score')
train_sizes, train_scores, test_scores = learning_curve(",main.py,VincentHPD/VincentAI,1
"
RandomForest = False
NaiveBayes = False
NeuralNetwork = False
DecisionTree = False
Adaboost = True


if RandomForest:
    BaseEstimator(
        RandomForestClassifier(),
        ""Random Forest""
    ).run()

if NaiveBayes:
    BaseEstimator(
        GaussianNB(),
        ""Naive Bayes""
    ).run()
",main.py,timothymiko/CarAuctionEvaluator,1
"print ""knn done""'''

for i in range(1,11):
    audio_clf = Pipeline([('clf',  KNeighborsClassifier(n_neighbors=i))])
    audio_clf.fit(pitches_tf[:train_size], data_terms[:train_size])
    prediction =  audio_clf.predict(pitches_tf[train_size:])
    print precision_recall_fscore_support(data_terms[train_size:], prediction, average=""macro"")
print ""knn done""

print ""Running random forest""
'''clf = RandomForestClassifier(n_estimators=10)
clf.fit(pitches_tf[:train_size], data_terms[:train_size])
prediction =  clf.predict(pitches_tf[train_size:])
precision, recall, fscore, _ =  precision_recall_fscore_support(data_terms[train_size:], prediction, average=""micro"")
print ""precision: "", precision
print ""recall: "", recall
print ""fscore: "", fscore
print ""random forest done""'''

for i in range(1,11):",msd_bof_knn_rf.py,mayanks43/auto-tag,1
"aucs = []
accuracies = []
base_fpr = np.linspace(0, 1, 101)
plt.figure(figsize=(5, 5))

kf = KFold(n_splits=10)
i =0

for train, test in kf.split(train_features):
    print('KFold Iteration: ', i)
    clf = BaggingClassifier(RandomForestClassifier(n_estimators=100, n_jobs=-1),max_samples=0.5, max_features=0.5, random_state=0)
    clf.fit(train_features[feature_list].loc[train], np.array(train_labels.loc[train]).ravel())
    preds = clf.predict_proba(train_features[feature_list].loc[test])
    accuracy = clf.score(train_features[feature_list].loc[test], train_labels.loc[test])

    auc = roc_auc_score(train_labels.loc[test], preds[:, 1])
    fpr, tpr, _ = roc_curve(train_labels.loc[test], preds[:, 1])

    plt.plot(fpr, tpr, 'b', alpha=0.05)
    tpr = np.interp(base_fpr, fpr, tpr)",examples/remigius-thoemel/titanic-iteration-6.py,remigius42/code_camp_2017_machine_learning,1
"		outfile.write(""Test time : {}\n"".format(test_time))
		outfile.write(""Accuracy : {}\n"".format(acc))

	with open(""Temp\\result_labels.csv"",""wb"") as outfile:
		np.savetxt(outfile,pred)


def randomforest_predict(training_samples,training_labels,test_samples,test_lables,n_estimators =100,criterion = 'gini',min_samples_split=2):
	from sklearn.ensemble import RandomForestClassifier

	clf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, min_samples_split=min_samples_split)

	t0 = time()
	clf.fit(training_samples,training_labels)
	training_time = round(time()-t0, 3)

	t0 = time()
	pred = clf.predict(test_samples)
	test_time = round(time()-t0, 3)
",src/test.py,amogh3892/Audio-classification-using-Bag-of-Frames-approach,1
"
    def learn(self, clf=None):
        trainFeatures   = []
        trainClasses    = []
        for cl in self.Features.keys():
            for ev in self.Features[cl]:
                trainFeatures += [tel[1:] for tel in ev]
                trainClasses  += [cl]*len(ev)

        if clf is None:
            clf = RandomForestClassifier(
                n_estimators=40, max_depth=None,
                min_samples_split=2, random_state=0)
        clf.fit(trainFeatures, trainClasses)
        self.clf = clf

    def save(self, path):
        from sklearn.externals import joblib
        joblib.dump(self.clf, path)
",datapipe/classifiers/EventClassifier.py,jdhp-sap/sap-cta-data-pipeline,1
"
    # Print feature importances
    # importances = clf.feature_importances_
    # print importances
    # importances = importances.reshape(data.shape)
    # pl.matshow(importances, cmap=pl.cm.hot)
    # pl.title(""Pixel importances with forests of trees"")
    # pl.show()

    # Using random forests
    # clf = RandomForestClassifier(n_estimators=100)
    # score = xval.cross_val_score(estimator=clf, X=data, y=clas)
    # print score
    # print ""Done!""

    # Using decision trees
    # clf = tree.DecisionTreeClassifier()
    # score = xval.cross_val_score(estimator=clf, X=data, y=clas)
    # print score
    # print ""Done!""",tools/train_svm_from_csv.py,fmgvalente/agent,1
"    # Add each algorithm and its name to the model array
    models = []
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('NB', GaussianNB()))
    #models.append(('SVM-sigmoid', SVC(kernel='sigmoid')))
    models.append(('SVM-rbf', SVC(kernel='rbf'))) #all kernels are providing results identical to rbf so keeping that only
    #models.append(('SVM-linear', SVC(kernel='linear')))
    #models.append(('SVM-poly', SVC(kernel='poly')))
    #models.append(('SVM-precomputed', SVC(kernel='precomputed')))
    models.append((RANDOM_FORESTS_CLASSIFIER, RandomForestClassifier(n_estimators=1000)))
    
    # Evaluate each model, add results to a results array,
    # Print the accuracy results (remember these are averages and std
    results = []
    names = []
    msg =''
    for name, model in models:
        kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)
        cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)",analyze/classification.py,aarora79/sb_study,1
"
  print('- Unique family size: %s' % df.FamilySize.unique())
  for family in df.FamilySize.unique():
    counts = len(df[df.FamilySize == family])
    print('%d familylings: %d times, rate: %f' % (family, counts, len(df[(df.FamilySize == family) & (df.Survived == 1)]) / counts))

  print('... done printing statistics!')
  print(SEPARATOR)

def run_prediction(train, test):
  forest = RandomForestClassifier(n_estimators=100)
  forest = forest.fit(train[0::,1::], train[0::,0] )

  return forest.predict(test).astype(int)

def write_predictions(ids, predictions):
  with open('prediction.csv', 'wt') as predictions_file:
    open_file_object = csv.writer(predictions_file)
    open_file_object.writerow(['PassengerId','Survived'])
    open_file_object.writerows(zip(ids, predictions))",titanic/submissions/3/randomforest.py,furgerf/kaggle-projects,1
"

iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target
y2 = np.c_[y, y]


def test_StackingClassifier():
    np.random.seed(123)
    meta = LogisticRegression()
    clf1 = RandomForestClassifier()
    clf2 = GaussianNB()
    sclf = StackingClassifier(classifiers=[clf1, clf2],
                              meta_classifier=meta)

    scores = cross_val_score(sclf,
                             X,
                             y,
                             cv=5,
                             scoring='accuracy')",mlxtend/classifier/tests/test_stacking_classifier.py,rasbt/mlxtend,1
"import scipy.sparse as sp

from skml.ensemble import EnsembleClassifierChain
from skml.datasets import load_dataset

X, y = load_dataset('yeast')


class TestECC(Chai):
    def test_ecc_fit_predict(self):
        ensemble = EnsembleClassifierChain(RandomForestClassifier(),
                                           threshold=.6,
                                           max_features=1.0)
        ensemble.fit(X, y)
        y_pred = ensemble.predict(X)
        hamming_loss(y, y_pred)

    def test_ecc_fit_predict_proba(self):
        ensemble = EnsembleClassifierChain(RandomForestClassifier(),
                                           threshold=.6,",test/test_ecc.py,ChristianSch/skml,1
"    bow_rdd = sm.RDD.map(lambda (key, (bow, meta)): (key, bow))
    bow_rdd = sm.RDD.join(sm.target).map(lambda (key, (bow, label)): (label, bow))

    remover = StopWordsRemover(inputCol=""raw"", outputCol=""words"")
    hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=""word_counts"",
                numFeatures=10000)
    tfidf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=""features"",
                minDocFreq=20)
    indexer = StringIndexer(inputCol=""string_label"", outputCol=""label"")

    for model in [GBTClassifier(), RandomForestClassifier(), MultiLayerPerceptronClassifier()]:

        if type(model) == MultiLayerPerceptronClassifier:
            layers = [10000, 100, 2]
            model = MultiLayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128)

        pipeline = Pipeline(stages=[remover, hashingTF, tfidf, # scaler,
                                    indexer, model])
        scores = cross_val_score(pipeline, bow_rdd)
        print type(model), scores, scores.mean()",code/grid_search.py,Nathx/subs_check,1
"training_set = [ [clean[i][0]>=0.995,  clean[i][0]<=0.005] + clean[i]    for i in range(len(clean))]

#and finally. make it into a numpy array
training_set = np.array(training_set)





#instantiate random forest classfiers
RFC_SA = RandomForestClassifier(max_depth=RFC_depth, n_estimators=RFC_estimators)
RFC_LB = RandomForestClassifier(max_depth=RFC_depth, n_estimators=RFC_estimators)

#train one RFC on SA
RFC_SA.fit(training_set[0::,3:9], training_set[0::,0])

#train another RFC on LB
RFC_LB.fit(training_set[0::,3:9], training_set[0::,1])

",SWMv2_1_RandomForest_SCRIPT.py,buckinha/gravity,1
"  trainingSet = newFeatures[r, :]; trainLabels = labels[r]
  testingSet = newFeatures[r2, :]; testLabels = labels[r2]
      
  if not equalClassSize:
    testingSet, testLabels = balanceClasses(testingSet, testLabels)
    clf = LogisticRegression(C=C, class_weight='auto', intercept_scaling=B, penalty='l2')
#     clf = svm.SVC(C=C, kernel='rbf', class_weight='auto', probability=returnProb)
  else:
    clf = LogisticRegression(C=C, intercept_scaling=B, penalty='l2')
    # clf = svm.SVC(C=C, kernel='rbf', class_weight='auto', probability=returnProb)
    # clf = RandomForestClassifier(n_estimators=100, max_features=""sqrt"", n_jobs=-1)
    # clf = KNeighborsClassifier(n_neighbors=15)
#   print np.arange(20)[clf2.get_support()]
#     clf = AdaBoostClassifier()
#   clf = GradientBoostingClassifier(init=LogisticRegression)
    # clf = GaussianNB()
    # clf = DecisionTreeClassifier()
  
  if featureSelect:
#     rfecv = RFE(estimator=clf, step=1,  n_features_to_select=8)",classify.py,jasonzliang/kick_classifier,1
"
# Prepare for cross validation fittings
num_folds=5
num_instances = len(train_data)
seed = 7
kfold = StratifiedKFold(y, shuffle=True, n_folds=num_folds, random_state=seed)

# Random forest fit
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators=100)
prediction = cross_validation.cross_val_predict(clf,train_data.loc[:,features], y, cv=kfold)

examine_prediction(y, prediction, train_data, features, show_misidentified=False)


# The next few algorithms require the features to be scaled (and I save this scaling so I can apply it to the test data):

# In[60]:
",detect_rap.py,drsaunders/RapDetector,1
"
def classification():
    # Generate a random binary classification problem.
    X, y = make_classification(n_samples=500, n_features=10, n_informative=10,
                               random_state=1111, n_classes=2,
                               class_sep=2.5, n_redundant=0)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15,
                                                        random_state=1111)

    model = RandomForestClassifier(n_estimators=10, max_depth=4)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)[:, 1]
    # print(predictions)
    print('classification, roc auc score: %s'
          % roc_auc_score(y_test, predictions))


def regression():
    # Generate a random regression problem",examples/random_forest.py,zhuhuifeng/PyML,1
"        for filename in [f for f in filenames if f.endswith("".cpp"")]:
            authors = np.append(authors, os.path.basename(dirpath))
            filenames_list = np.append(filenames_list, os.path.join(dirpath, filename))
    return filenames_list, authors


def get_oob_rate(X, y):
    # calculate this on whole original set
    ensemble_clfs = [
        (""RandomForestClassifier, max_features='sqrt'"",
         RandomForestClassifier(warm_start=True, oob_score=True,
                                max_features=""sqrt"")),
        (""RandomForestClassifier, max_features='log2'"",
         RandomForestClassifier(warm_start=True, max_features='log2',
                                oob_score=True)),
        (""RandomForestClassifier, max_features=None"",
         RandomForestClassifier(warm_start=True, max_features=None,
                                oob_score=True))
    ]
",core/whose_cpp_code.py,MarinaMeyta/WhoseCppCode,1
"## Stats for numeric domains
stats = map(base_stats, data)
#print stats
##
names = [""Nearest Neighbors"",  ""Linear SVM"", ""RBF SVM"", 
         ""Decision Tree"", ""Random Forest"", ""AdaBoost"", 
         ""Naive Bayes"", ""LDA""] #, ""QDA""]         
anova_filter = SelectFwe(chi2) #SelectKBest(chi2, k=4)
classifiers = [ KNeighborsClassifier(3), SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(), GaussianNB(), LDA()] #, QDA()]
##
X = zip(*data[:len(data)-1]); Y = data[len(data)-1]
X_trg = min_max_scaler.fit_transform(X[:TRG_SIZE]); Y_trg = Y[:TRG_SIZE]
X_tst = min_max_scaler.fit_transform(X[TRG_SIZE:]); Y_tst = Y[TRG_SIZE:]
for name, classifier in zip(names, classifiers):
    classifier.fit(X_trg, Y_trg)
    print '%s: %f' %(name, classifier.score(X_tst, Y_tst))
    clf = Pipeline([('anova', anova_filter), (name, classifier)])",knn/scripts/exp1.py,xulesc/algos,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_noop.py,diogo149/CauseEffectPairsPaper,1
"from sklearn.preprocessing import FunctionTransformer

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)

exported_pipeline = make_pipeline(
    make_union(VotingClassifier([(""est"", BernoulliNB(alpha=60.0, binarize=0.26, fit_prior=True))]), FunctionTransformer(lambda X: X)),
    RandomForestClassifier(n_estimators=500)
)

exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)",LA_Team/FinalPipeline.py,seg/2016-ml-contest,1
"                              tree.tree_.children_right[1:]]
        new_children = (start_ind - 1 + orig_children).astype(np.int32)
        new_children[orig_children < 0] = -1
        children[start_ind : start_ind + tree_size[i]-1, :] = new_children
    return dict(num_trees=num_trees,
                features=features,
                threshold=threshold,
                children=children,
                classes=classifier.classes_)

rf = RandomForestClassifier(n_estimators=512, max_depth=8, n_jobs=-1)
rf.fit(features, patch_labels)

cPickle.dump(convert_forest(rf), open('classifier.pkl', 'wb'))",train_rf.py,ringw/MetaOMR,1
"    Examples
    --------
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.models import CostSensitiveRandomForestClassifier
    >>> from costcla.metrics import savings_score
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> y_pred_test_rf = RandomForestClassifier(random_state=0).fit(X_train, y_train).predict(X_test)
    >>> f = CostSensitiveRandomForestClassifier()
    >>> y_pred_test_csdt = f.fit(X_train, y_train, cost_mat_train).predict(X_test)
    >>> # Savings using only RandomForest
    >>> print(savings_score(y_test, y_pred_test_rf, cost_mat_test))
    0.12454256594
    >>> # Savings using CostSensitiveRandomForestClassifier
    >>> print(savings_score(y_test, y_pred_test_csdt, cost_mat_test))
    0.499390945808
    """"""",costcla/models/cost_ensemble.py,albahnsen/CostSensitiveClassification,1
"
    # Load the relevant scikit learn data
    raw_data = sklearn_ds

    # Create the train-test datasets
    X_train, X_test, y_train, y_test = train_test_split(
        raw_data.data, raw_data.target, train_size=train_split_propn,
        random_state=random_state_split)

    # Just fit a simple random forest classifier with 2 decision trees
    rf = RandomForestClassifier(
        n_estimators=n_estimators, random_state=random_state_classifier)

    # fit the classifier
    if feature_weight is None:
        rf.fit(X=X_train, y=y_train)
    else:
        rf.fit(X=X_train, y=y_train, feature_weight=feature_weight)

    return X_train, X_test, y_train, y_test, rf",jupyter/utils/irf_jupyter_utils.py,Yu-Group/scikit-learn-sandbox,1
"        self.__bestParams = {}
        self.__clfs = {}
        self.bestClf = ()

    def initModels(self):

        self.__clfs = {
            'knn': neighbors.KNeighborsClassifier(),
            'lm': linear_model.LogisticRegression(),
            'tm': tree.DecisionTreeClassifier(),
            'rf': ensemble.RandomForestClassifier()
        }

    def defaultParams(self):

        print ""Clasifiers with default parameters:\n""

        for clfName, clf in self.__clfs.items():

            clfName = ' '.join(",src/assets/models/util/classifiers.py,kug3lblitz/Heat-Replay,1
"    pair_scores[features] = (dtc.score(training_feature_vals, training_class_vals), list(features))

best_pairs = []
for pair in sorted(pair_scores, key=pair_scores.get, reverse=True)[:3870]:
    best_pairs.extend(list(pair))
best_pairs = sorted(list(set(best_pairs)))

result2 = result1[sorted(list(set(best_pairs + ['class'])))]

# Perform classification with a random forest classifier
rfc3 = RandomForestClassifier(n_estimators=1, max_features=min(64, len(result2.columns) - 1))
rfc3.fit(result2.loc[training_indices].drop('class', axis=1).values, result2.loc[training_indices, 'class'].values)
result3 = result2
result3['rfc3-classification'] = rfc3.predict(result3.drop('class', axis=1).values)

# Perform classification with a decision tree classifier
dtc4 = DecisionTreeClassifier(max_features=min(40, len(result3.columns) - 1), max_depth=7)
dtc4.fit(result3.loc[training_indices].drop('class', axis=1).values, result3.loc[training_indices, 'class'].values)
result4 = result3
result4['dtc4-classification'] = dtc4.predict(result4.drop('class', axis=1).values)",tutorials/tpot_mnist_pipeline.py,pronojitsaha/tpot,1
"from keras.models import Sequential
from keras.layers import Dense, LSTM
from keras.layers import Dropout
from keras.optimizers import SGD
from sklearn.svm import SVC
import numpy as np

np.random.seed(7)

def build_random_forest():
    return RandomForestClassifier(n_estimators=100)

def build_ann(input_dim, layers=4, unit_factor=10, with_dropout=False):
    dropout_rate = 0.2
    units = int( (input_dim * unit_factor) / 2 + 2 ) * 2
    classifier = Sequential()
    for i in range(1, layers + 1):
        classifier.add(Dense(units=units, kernel_initializer='random_uniform', use_bias=True, bias_initializer='random_uniform', activation='relu', input_dim=input_dim))
        if with_dropout:
            classifier.add(Dropout(rate=dropout_rate))",titanic/models.py,davidvartanian/machine-learning-tests,1
"
# To tidy-up a bit, we loop through the poolresult to create a final list of
# the feature extraction results for all images.
combined_result = []
for single_proc_result in poolresult:
    for single_image_result in single_proc_result:
        combined_result.append(single_image_result)

# DATA contains all the data we wanna train. Now is the training part.

#clf = RandomForestClassifier(n_estimators=50, n_jobs=-1, \
#                             compute_importances=True)
#clf.fit(X,Y)
#scores = cross_v.cross_val_score(clf, X, Y, cv = cross_v.KFold(len(Y), 5))
#save the classifier

try:
    DATA = pickle.load(open('data.p', 'rb'))    
except EOFError:
    DATA = list()",hw4/hw4_training.py,yigong/AY250,1
"                          DeprecationWarning)
            # Define the classifier to use
            if self.estimator == 'knn':
                self.estimator_ = KNeighborsClassifier(**self.kwargs)
            elif self.estimator == 'decision-tree':
                from sklearn.tree import DecisionTreeClassifier
                self.estimator_ = DecisionTreeClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'random-forest':
                from sklearn.ensemble import RandomForestClassifier
                self.estimator_ = RandomForestClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'adaboost':
                from sklearn.ensemble import AdaBoostClassifier
                self.estimator_ = AdaBoostClassifier(
                    random_state=self.random_state, **self.kwargs)
            elif self.estimator == 'gradient-boosting':
                from sklearn.ensemble import GradientBoostingClassifier
                self.estimator_ = GradientBoostingClassifier(
                    random_state=self.random_state, **self.kwargs)",imblearn/ensemble/balance_cascade.py,scikit-learn-contrib/imbalanced-learn,1
"    return np.asarray(scores)

if __name__ == ""__main__"":
    args = opts().parse_args()
    
    print ""Loading and preparing data""
    X = prepare_features(args.train)
    Y = args.labels['labels']
    
    print ""Cross-validating classifier""
    clf = ensemble.RandomForestClassifier(
        n_estimators=48, min_samples_split=1, max_depth=None,
        criterion='entropy',
        n_jobs=-1, random_state=42, verbose=2)
    scores = cross_validate_classifier(clf, X, Y)
    print ""Classifier performance = %f (+/-) %f"" \
        % (scores.mean(), scores.std())
        
    print ""Training classifier on full dataset""
    clf.fit(X, Y)",scripts/practice/predict.1.py,timpalpant/KaggleTSTextClassification,1
"

#Features
X = train_img

#Labels
y = train_labels

X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,y,test_size=0.1)

clf = RandomForestClassifier(n_estimators=100, n_jobs=10,)
clf.fit(X_train,y_train)

with open('MNIST_RFC.pickle','wb') as f:
	pickle.dump(clf, f)

pickle_in = open('MNIST_RFC.pickle','rb')
clf = pickle.load(pickle_in)

acc = clf.score(X_test,y_test)",3. Random Forest Classifier/RFC.py,anujdutt9/Handwritten-Digit-Recognition-using-Deep-Learning,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/len(X1) )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X1,target, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 100, n_jobs = 12, random_state = 4)

result_rf = cross_val_predict(rf,X1,target, cv = 10)

CM = confusion_matrix(target,result_rf) 
print(""Confusion Matrix : "")
print(CM)
for i in range(0,len(X1)):
	if(target[i]== result_rf[i]):
		score_rf+=1",sandbox/petsc/solvers/scripts/ScikitClassifiersRS1CVSolverList.py,LighthouseHPC/lighthouse,1
"estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                      axis=0, verbose=2)))
estimators.append(('scaler', StandardScaler()))
estimators.append(('mlp', KerasClassifier(build_fn=create_baseline,
                                          nb_epoch=175,
                                          batch_size=1024,
                                          verbose=2)))
clf_nn = Pipeline(estimators)


clf_rf = RandomForestClassifier(n_estimators=1400,
                                max_depth=16,
                                max_features=5,
                                min_samples_split=16,
                                min_samples_leaf=2,
                                class_weight={0: 1, 1: 28},
                                verbose=1, random_state=1, n_jobs=4)


# Create model for LR",ml4vs/learning_curves.py,ipashchenko/ml4vs,1
"X,y = settings.flattened_train_paths(settings.classes)
X_test = settings.image_fnames['test']

X = np.array(X)
y = np.array(y)

detector = lambda image: neukrill_net.image_features.get_MSER_keypoints(image, n=max_num_kp)

describer = neukrill_net.image_features.get_ORB_descriptions

kprf_base = sklearn.ensemble.RandomForestClassifier(n_estimators=750, max_depth=15, min_samples_leaf=20, n_jobs=12, random_state=42)

hlf = neukrill_net.highlevelfeatures.KeypointEnsembleClassifier(detector, describer, kprf_base,
                                                                     return_num_kp=True, n_jobs=16, verbosity=1, summary_method='vote')

# Partition the data

print ""Partitioning the training data""

# Remove the data which is going to be held out",generate_local_cache_MSER.py,Neuroglycerin/neukrill-net-work,1
"DDL2 = colors.ListedColormap(ddl2)

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"",
         ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",diagnostics/pycon/code/classifiercompare.py,rebeccabilbro/viz,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn.externals.six import StringIO
from db_operation import db_exec
from decisiontree_extracting import tree_to_code, tree_to_code_db

def DSTree(X, y, feature_names, class_names):
	# Undefined input variable
	package_name = 'EPP-14-225'
	
	print 'Generating Random Forest Classifier...'
	clf = RandomForestClassifier(n_estimators = 4)
	clf = clf.fit(X, y)
	
	# print 'Different importances of each features: %s' % clf.feature_importances_
	print 'Removing the old package result...'
	sql = 'delete from dsresult where package=\'{}\''.format(package_name)
	db_exec('catl', sql)
	print 'Generating Tree plot...'
	for i in xrange(len(clf.estimators_)):
		dot_data = StringIO()",decisiontree.py,Kyle-Crypton/CATL_Project,1
"    print '{}: writing heldout to disk'.format(time.time()-t0)
    
    joblib.dump( (p_avg, yy_test), out_fname + '_heldout.pkl', )


n_trees = 500
max_depth = 10
n_jobs = 16

settings = neukrill_net.utils.Settings('settings.json')
clf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_trees, max_depth=max_depth, min_samples_leaf=3, n_jobs=n_jobs, random_state=42)

test_cache_paths = ['/disk/data1/s1145806/cached_kpecORB_quick_train_data_raw.pkl']
train_cache_paths = ['/disk/data1/s1145806/cached_kpecORB_quick_test_data_raw.pkl']
out_fname = 'cached_kpecORB_' + ""{}trees_{}deep"".format(n_trees,max_depth) + '_predictions.csv'

predict(pathpair, out_fname, clf, settings, is_subfitted=False, generate_predictions=True)
",test_hlf_cache_with_local.py,Neuroglycerin/neukrill-net-work,1
"
    # Automatically identify categorical features, and index them.
    # Set maxCategories so features with > 4 distinct values are treated as continuous.
    featureIndexer =\
        VectorIndexer(inputCol=""features"", outputCol=""indexedFeatures"", maxCategories=4).fit(data)

    # Split the data into training and test sets (30% held out for testing)
    (trainingData, testData) = data.randomSplit([0.7, 0.3])

    # Train a RandomForest model.
    rf = RandomForestClassifier(labelCol=""indexedLabel"", featuresCol=""indexedFeatures"", numTrees=10)

    # Convert indexed labels back to original labels.
    labelConverter = IndexToString(inputCol=""prediction"", outputCol=""predictedLabel"",
                                   labels=labelIndexer.labels)

    # Chain indexers and forest in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])

    # Train model.  This also runs the indexers.",spark-2.x/src/main/python/ml/random_forest_classifier_example.py,lhfei/spark-in-action,1
"    #Create and train classifiers
    clf_svm = svm.SVC(kernel='linear')
    clf_svm.fit(X_train, y_train)
    
    clf_mNB=MultinomialNB()
    clf_mNB.fit(X_train, y_train)
    
    clf_knn = KNeighborsClassifier()
    clf_knn.fit(X_train, y_train)
    
    clf_ada=RandomForestClassifier(n_estimators=25)
    clf_ada.fit(X_train, y_train)
    #Evaluate the models and output accuracies
    print ""Linear SVM Classifier Accuracy:""+str(clf_svm.score(X_test, y_test))
    print ""Multinomial Naive Bayes Accuracy:""+str(clf_mNB.score(X_test, y_test))
    print ""5 Nearest Neighbors Accuracy:""+str(clf_knn.score(X_test, y_test))
    print ""Random Forest (25 learners) Accuracy:""+str(clf_ada.score(X_test, y_test))
    #Obtain the predicted labels for the test subsets
    predicted_svm = clf_svm.predict(X_test)
    predicted_mNB = clf_mNB.predict(X_test)",code_python27/tfidfANDclasification/classByTags.py,rcln/tag.suggestion,1
"

def get_accuracy(true_values, predicted_values, print_it=True):
    accuracy = accuracy_score(true_values, predicted_values)
    if print_it:
        print accuracy
    return accuracy


def sklearn_prediction(train_set, test_set):
    forest = RandomForestClassifier(n_estimators = 100)
    forest = forest.fit(train_set.iloc[:,1:], train_set.iloc[:,0])
    prediction_list = forest.predict(test_data).tolist()
    return prediction_list


def main():
    # set up environments
    client_id = CLIENT_ID
    client_secret = CLIENT_SECRET",hello_prediction.py,fyears/covertype-gp,1
"
seed = random.seed(1990)

X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.70, random_state=seed)

features = 10

num_trees = int(X_train.shape[1]/features)


clf = RandomForestClassifier(n_estimators=num_trees, max_features=features)

start_time = time.time()

clf = clf.fit(X_train, y_train)
      
y_pred = clf.predict(X_test)

print('error rate %.5f' %(np.mean(1 - np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1)))))
print(""--- %s seconds ---"" % (time.time() - start_time))",Otto/DT.py,Knight13/Exploring-Deep-Neural-Decision-Trees,1
"
h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]
""""""
pdb.set_trace()
#TODO replace this
X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)",test.py,malimome/game-auth,1
"              'deferred_income',
              'long_term_incentive',
              'from_poi_to_this_person',
              'ttl_pay_stock']

# Selecting the best features using GridSearchCV
data = featureFormat(data_dict, feat_list, sort_keys = True)
labels, features = targetFeatureSplit(data)

pipe = Pipeline([('KBest', SelectKBest()),
                ('clf', RandomForestClassifier())])
param_grid = [{'KBest__k': [1,2,3,4,5],
                'clf__n_estimators': [5,10,15,20,25],
                'clf__criterion': ['entropy','gini'],
                'clf__random_state': [20],
                'clf__min_samples_split': [2,4,8,16,32],
                'clf__min_samples_leaf': [1,2,4,8,16]}]

gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='f1')
gs.fit(features, labels)",random_forest_clf.py,rjegankumar/enron_email_fraud_identification,1
"
#clf = AdaBoostClassifier(n_estimators=3000)
clf.fit(train_data, train_label)

#clf = BaggingClassifier(n_estimators = 2000)
#clf.fit(train_data, train_label)


#eclf2 = VotingClassifier(estimators=[('lr', clf1), ('gb', clf2), ('gnb', clf3)], voting='soft')

#clf  = RandomForestClassifier(n_estimators=6000, max_depth = 4, verbose=1).fit(train_data, train_label)
#knn = neighbors.KNeighborsClassifier()
#logistic = linear_model.LogisticRegression()
#clf = svm.SVC(probability = True)
#clf = tree.DecisionTreeClassifier()


#print('KNN score: %f' % knn.fit(train_data, train_label).score(valid_data, valid_label))
#result = knn.fit(train_data, train_label).predict_proba(test_data)
#train_data = train_data[0:5000,:]",liao/final.py,NCLAB2016/DF_STEALL_ELECTRIC,1
"        n_folds = 5
        
        (X,y) = joblib.load('trainTest.bin')
        sum_accuracy,sum2_accuracy = 0.,0.
        for train_idx,test_idx in KFold(len(y), n_folds=n_folds, shuffle=True):
            pca = PCA()
            X_train,y_train = X[train_idx],y[train_idx]
            X_test,y_test = X[test_idx],y[test_idx]
            X_train = pca.fit_transform(X_train)
            X_test = pca.transform(X_test)
            clf = RandomForestClassifier(n_estimators=500)
            #clf = LinearSVC(C=0.05)
            clf = clf.fit(X_train, y_train)
            
            y_pred = clf.predict(X_test)    
            accuracy = np.mean(y_pred==y_test)
            sum_accuracy += accuracy
            sum2_accuracy += accuracy**2
        accuracy = sum_accuracy / float(n_folds)
        sum2_accuracy /= float(n_folds)",train.py,gabrielhuang/OhSnap,1
"    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.linear_model import LogisticRegression
    import numpy as np
    y_binary = np.zeros(len(y))
    y_binary[y.values > 0] = 1
    y_binary[y.values < 0] = -1
    x_train, x_test, y_train, y_test = train_test_split(X, y_binary,
                                                        random_state=42)

    model = RandomForestClassifier(n_estimators=1000, n_jobs=-1)
    model.fit(x_train, y_train)
    print symbol, 'random forest classifier score:', model.score(x_test, y_test)

    model = LogisticRegression()
    model.fit(x_train, y_train)
    print symbol, 'logit score:', model.score(x_test, y_test)

    x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42)
    model = RandomForestRegressor(n_estimators=1000, n_jobs=-1)",features.py,cbyn/bitmicro,1
"        self.y_scale = 1.0
        self.z_scale = 1.0
        self.divisionProbabilityFeatureName = 'divProb'
        self.detectionProbabilityFeatureName = 'detProb'

        self.TraxelsPerFrame = {}
        ''' this public variable contains all traxels if we're not using pgmlink '''
    
    def _loadClassifiers(self):
        if self._options.objectCountClassifierPath != None and self._options.objectCountClassifierFilename != None:
            self._countClassifier = RandomForestClassifier(self._options.objectCountClassifierPath,
                                                           self._options.objectCountClassifierFilename, self._options)
        if self._options.divisionClassifierPath != None and self._options.divisionClassifierFilename != None:
            self._divisionClassifier = RandomForestClassifier(self._options.divisionClassifierPath,
                                                              self._options.divisionClassifierFilename, self._options)
        if self._options.transitionClassifierPath != None and self._options.transitionClassifierFilename != None:
            self._transitionClassifier = RandomForestClassifier(self._options.transitionClassifierPath,
                                                                self._options.transitionClassifierFilename, self._options)
    
    def __getstate__(self):",hytra/core/probabilitygenerator.py,chaubold/hytra,1
"  elif mode == ""classification"":
    roc_auc_metric = Metric(metrics.roc_auc_score, verbosity=verbosity)
    accuracy_metric = Metric(metrics.accuracy_score, verbosity=verbosity)
    mcc_metric = Metric(metrics.matthews_corrcoef, verbosity=verbosity)
    # Note sensitivity = recall
    recall_metric = Metric(metrics.recall_score, verbosity=verbosity)
    model_class = RandomForestClassifier
    all_metrics = [accuracy_metric, mcc_metric, recall_metric, roc_auc_metric]
    metric = roc_auc_metric 
    def rf_model_builder(model_params, model_dir):
      sklearn_model = RandomForestClassifier(**model_params)
      return SklearnModel(sklearn_model, model_dir)
  else:
    raise ValueError(""Invalid mode %s"" % mode)

  params_dict = {
      ""n_estimators"": [10, 100],
      ""max_features"": [""auto"", ""sqrt"", ""log2"", None],
      }
",examples/bace/bace_rf.py,bowenliu16/deepchem,1
"domains = x_train_pos.columns[39:]
domain_train_pos = x_train_pos[domains]
domain_train_neg = x_train_neg[domains]
full_train_pos = pd.merge(mini_train_pos, domain_train_pos, left_index=True, right_index=True)
full_train_neg = pd.merge(mini_train_neg, domain_train_neg, left_index=True, right_index=True)
X_train = np.concatenate((full_train_pos.values, full_train_neg.values), axis=0)
y_train = np.squeeze(np.concatenate((np.ones((len(full_train_pos),1)), np.zeros((len(full_train_neg),1)))))

print '[ fitting classifiers ] {}'.format(datetime.now().strftime(""%I:%M%p %B %d, %Y""))
gbc = ensemble.GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=20).fit(X_train, y_train)
rfc = ensemble.RandomForestClassifier(n_estimators=100).fit(X_train, y_train)

print '[ serializing fitted models ] {}'.format(datetime.now().strftime(""%I:%M%p %B %d, %Y""))
with open('trained_model_gbc.pk','wb') as fh_gbc:
    cPickle.dump(gbc, fh_gbc)
with open('trained_model_rfc.pk','wb') as fh_rfc:
    cPickle.dump(rfc, fh_rfc)

print '------------------------------'",learn/train_model.py,RabadanLab/Pegasus,1
"        lr_filename = filename % ('RandomForestClassifier', subject, name_addendum)
        try:
            lr_clf = joblib.load(lr_filename)
        except IOError:
            print ""No job is available for %s "" % lr_filename

            print
            t0 = time()
            if best:
                param_grid = {'max_features': ['auto', None], 'n_estimators': [200, 500, 750, 1000]}
                lr_clf = GridSearchCV(RandomForestClassifier(max_depth=None, n_jobs=-1, random_state=0), param_grid)
            else:
                lr_clf = RandomForestClassifier(random_state=0, n_jobs=-1)
            print ""Classifier:""
            print lr_clf
            print ""Training"", subject
            lr_clf.fit(xx, yy)
            # dump the classifier
            joblib.dump(lr_clf, lr_filename, compress=9)
            print ""Done in:"", (time() - t0)",src/best_classifier_save_split_subject.py,LooseTerrifyingSpaceMonkey/DecMeg2014,1
"        # lin_clf = svm.SVC()
        # lin_clf = svm.SVC(class_weight='auto')
        lin_clf = svm.SVC(decision_function_shape='ovo')
        # lin_clf = sklearn.neighbors.nearest_centroid.NearestCentroid()
        # lin_clf = sklearn.linear_model.Lasso(alpha = 0.1)
        # lin_clf = sklearn.linear_model.SGDClassifier(loss=""hinge"", penalty=""l2"")
        # lin_clf = sklearn.linear_model.SGDClassifier(loss=""hinge"", penalty=""l2"", class_weight='auto')
        # lin_clf = sklearn.naive_bayes.MultinomialNB()
        # lin_clf = sklearn.tree.DecisionTreeClassifier()
        # lin_clf = sklearn.tree.DecisionTreeClassifier(class_weight='auto')
        # lin_clf = sklearn.ensemble.RandomForestClassifier(n_estimators=10)
        # lin_clf = sklearn.ensemble.RandomForestClassifier(n_estimators=10, class_weight='auto')
        # lin_clf = sklearn.ensemble.AdaBoostClassifier(n_estimators=100)
        # lin_clf = sklearn.ensemble.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)
        lin_clf.fit(X_train, y_train)
        s = lin_clf.score(X_ct, y_ct)
        if s > score:
            score = s
            clf = lin_clf
",main_old.py,joergsimon/gesture-analysis,1
"                        scale_pos_weight=6,
                        max_delta_step=7)
estimators = list()
estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                      axis=0, verbose=2)))
estimators.append(('clf', clf))
pipeline_xgb = Pipeline(estimators)


# Create model for RF
clf = RandomForestClassifier(n_estimators=1200,
                             max_depth=17,
                             max_features=3,
                             min_samples_split=2,
                             min_samples_leaf=3,
                             class_weight='balanced_subsample',
                             verbose=1, random_state=1, n_jobs=4)
estimators = list()
estimators.append(('imputer', Imputer(missing_values='NaN', strategy='median',
                                      axis=0, verbose=2)))",ml4vs/plot_auprc_for_all.py,ipashchenko/ml4vs,1
"
    def classify(self, uuid, args):
        classifier = args['classifier']
        classifier_type = classifier['type']
        classifier_params = classifier['params']
        classifier_labels = classifier['labels']
        classifier_features = classifier['features']

        b = None
        if classifier_type == 'forest':
            b = RandomForestClassifier(n_jobs=-1, random_state=0)
            #n_estimators = 100
            #max_features = None
            #b = RandomForestClassifier(n_estimators=n_estimators,
            #            max_features=max_features, n_jobs=-1, random_state=0)
        elif classifier_type == 'logreg':
            b = SGDClassifier(loss='log', class_weight='auto', random_state=0)
        elif classifier_type == 'svm':
            b = SGDClassifier(loss='squared_hinge', class_weight='auto', random_state=0)
",vizdom/executor.py,feiyanzhandui/tware,1
"    outputCol='features')
data = (assembler.transform(df).select(""features"", df.Response.astype('double')))

(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=451)

data.printSchema()


# In[3]:

cls = RandomForestClassifier(numTrees=60, seed=1111, maxDepth=15, labelCol=""Response"", featuresCol=""features"")

pipeline = Pipeline(stages=[cls])
evaluator = MulticlassClassificationEvaluator(
    labelCol=""Response"", predictionCol=""prediction"", metricName=""accuracy"")
trainingData=trainingData.na.drop()
trainingData.printSchema()


# In[4]:",scripts/spark.py,vibhutiM/Production-Failures,1
"        print(sum(Y))
        print(Xte.shape)
        print(sum(Yte))

    with open(""feat_ind.pkl"", ""rb"") as f_feat:
        feat2ind = pkl.load(f_feat)
    ind2feat = {v: k for k, v in feat2ind.items()}
    ind2feat[len(ind2feat)] = ""Age""

    clf_search = GridSearchCV(
        RandomForestClassifier(n_estimators=500, n_jobs=-1),
        param_grid=TUNED_PARAMS,
        scoring='roc_auc',
        n_jobs=1, 
        verbose=4,
        cv=3,
    )
                      
    clf_search.fit(X, Y)
    ",model/model_rf.py,stegben/OperationRiskPrediction,1
"X = imputer.fit_transform(X)
imputer = Imputer(-np.inf, strategy='median')
X = imputer.fit_transform(X)

# load labels for is_stat_map classification
labels = pd.read_csv('labels.csv')[['image_id', 'map_type', 'is_stat_map', 'map_type_v2']]
y = labels['is_stat_map'].values

# cross_validation and prediction
cv = StratifiedShuffleSplit(y, n_iter=50, random_state=10)
clf = RandomForestClassifier(n_estimators=20, max_depth=20, n_jobs=-1)

Y_pred = []
Y_true = []

for train, test in cv:
    y_pred, y_true = clf.fit(X[train], y[train]).predict(X[test]), y[test]

    Y_pred.append(y_pred)
    Y_true.append(y_true)",outlier_detection.py,schwarty/neurovault_retreat2015,1
"import pmbec 
from log_linear_regression import LogLinearRegression
from two_pass_regressor import TwoPassRegressor
from selective_regressor import SelectiveRegressor 
from generate_training_data import generate_training_data
from amino_acids import AMINO_ACID_LETTERS, AMINO_ACID_PAIRS, AMINO_ACID_PAIR_POSITIONS

class TreeEnsemble(object):

    def __init__(self, n_estimators = 100):
        self.rf_binary_med = sklearn.ensemble.RandomForestClassifier(
            n_estimators = n_estimators)
        self.rf_binary_high = sklearn.ensemble.RandomForestClassifier(
            n_estimators = n_estimators)
        self.rf4 = sklearn.ensemble.RandomForestClassifier(
            n_estimators = n_estimators)
        #self.et_binary_med = sklearn.ensemble.ExtraTreesClassifier(
        #    n_estimators = n_estimators)
        #self.et_binary_high = sklearn.ensemble.ExtraTreesClassifier(
        #    n_estimators = n_estimators)",old/train_aa_features.py,iskandr/mhcpred,1
"
    counter = 0
    for review in clean_test_reviews:
        test_centroids[counter] = create_bag_of_centroids( review, \
            word_centroid_map )
        counter += 1


    # ****** Fit a random forest and extract predictions
    #
    forest = RandomForestClassifier(n_estimators = 100)

    # Fitting the forest may take a few minutes
    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit(train_centroids,train[""Marks_obtained""])
    result = forest.predict(test_centroids)

    # Write the test results
    output = pd.DataFrame( data={""Paper_id"":test[""Paper_id""], ""Given_marks"":test[""Marks_obtained""], ""Marks_obtained"":result } )
    output.to_csv(""BagOfCentroids.csv"", index=False, quoting=3)",Word2Vec_BagOfCentroids.py,rkc007/Automated-Content-Grading-Using-Machine-Learning,1
"
classifiers = [(""svc[ovo]"",svm.SVC(decision_function_shape='ovo'), needsScaling),
               (""svc"",svm.SVC(), needsScaling),
               (""lin svc"",svm.LinearSVC(), needsScaling),
               (""lr"",lm.LogisticRegression(), needsScaling),
               (""nn"",nc.NearestCentroid(), needsScaling),
               (""lr(l1)"",lm.LogisticRegression(penalty='l1'), needsScaling),
               (""sgd[hinge]"",lm.SGDClassifier(loss=""hinge"", penalty=""l2""), needsScaling),
               (""navie bayse"",nb.MultinomialNB(), needsScaling),
               (""decision tree"",tree.DecisionTreeClassifier(), needsNoScaling),
               (""random forrest"",em.RandomForestClassifier(n_estimators=10), needsNoScaling),
               (""ada boost"",em.AdaBoostClassifier(n_estimators=100), needsScaling),
               (""gradient boost"",em.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0), needsScaling)]

def fit_classifier(values, labels):
    clf = None
    clf_n = None
    needs_scaling = True
    kf = ms.KFold(n_splits=5)
    score = 0",analysis/Classification.py,joergsimon/gesture-analysis,1
"
#create a bag of words and convert to a array and then print the shape
bag_of_words = vectorizer.fit(words_list)
bag_of_words = vectorizer.transform(words_list)#.toarray()

tf_transformer = TfidfTransformer(use_idf=True)
bag_of_words = tf_transformer.fit_transform(bag_of_words).toarray()

print(bag_of_words.shape)

forest = RandomForestClassifier(n_estimators = 40)
#forest = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)
forest = forest.fit( bag_of_words, train[""cuisine""] )

#Now read the test json file in
test = pd.read_json('../data/test.json')
test.head()

#Do the same thing we did with the training set and create a array using the count vectorizer.
test_ingredients = test['ingredients']",src/bow_model.py,Cospel/kaggle-cooking,1
"    params_forest={""n_estimators"": st.randint(10,20), ""criterion"":['gini','entropy']}
    seed=np.random.seed= 10
    time1 = datetime.datetime.now()
    cvt = CountVectorizer(min_df=1e-2)
    X1 = cvt.fit_transform(text1)
    pickle.dump(cvt.vocabulary_, open(""dictio_NEW"", 'w'))



    clf = RandomizedSearchCV(LogisticRegression(), params_logit, n_iter=10, cv=5, n_jobs=-1)
    #clf = RandomizedSearchCV(RandomForestClassifier(), params_forest)
    pred_res=clf.fit(X1[:25000].toarray(), Y1[:25000]).predict(X1[25000:])
    #pred=clf.fit(X1.toarray(), Y1).predict(X2)
    pred_proba=clf.fit(X1[:25000].toarray(), Y1[:25000]).predict_proba(X1[25000:])

    print ""Resultats:\n best_estimator = {}\n best_score = {}"".format(clf.best_params_, clf.best_score_)
    print ""Accuracy of training is: {%0.2f}\n Accuracy of test is: {%0.2f}""%(clf.score(X1[:25000].toarray(),Y1[:25000]), clf.score(X1[25000:].toarray(),Y1[25000:]))
    print ""Grid_score is: {}\n  "".format(clf.grid_scores_)
    print ""*******************************************************************""
",sentimentTwitter.py,zfkl/sentiment-analysis,1
"        # Pipeline(gen_ictal=False, pipeline=[FreqCorrelation(1, 48, 'us', with_corr=True, with_eigen=True)]),
        # Pipeline(gen_ictal=False, pipeline=[FreqCorrelation(1, 48, 'us', with_corr=True, with_eigen=False)]),
        # Pipeline(gen_ictal=False, pipeline=[FreqCorrelation(1, 48, 'us', with_corr=False, with_eigen=True)]),
        # Pipeline(gen_ictal=False, pipeline=[FreqCorrelation(1, 48, 'none', with_corr=True, with_eigen=True)]),
        # Pipeline(gen_ictal=False, pipeline=[TimeFreqCorrelation(1, 48, 400, 'us')]),
        # Pipeline(gen_ictal=False, pipeline=[TimeFreqCorrelation(1, 48, 400, 'usf')]),
        # Pipeline(gen_ictal=False, pipeline=[TimeFreqCorrelation(1, 48, 400, 'none')]),
    ]
    classifiers = [
        # NOTE(mike): you can enable multiple classifiers to run them all and compare results
        # (RandomForestClassifier(n_estimators=50, min_samples_split=1, bootstrap=False, n_jobs=4, random_state=0), 'rf50mss1Bfrs0'),
        # (RandomForestClassifier(n_estimators=150, min_samples_split=1, bootstrap=False, n_jobs=4, random_state=0), 'rf150mss1Bfrs0'),
        # (RandomForestClassifier(n_estimators=300, min_samples_split=1, bootstrap=False, n_jobs=4, random_state=0), 'rf300mss1Bfrs0'),
        # (RandomForestClassifier(n_estimators=3000, min_samples_split=1, bootstrap=False, n_jobs=4, random_state=0), 'rf3000mss1Bfrs0'),
        # (RandomForestClassifier(n_estimators=3000, min_samples_split=1, max_depth=10, bootstrap=True, n_jobs=-1, random_state=0), 'rf3000mss1md10Bt'),
        # (RandomForestClassifier(n_estimators=1000, min_samples_split=1, max_depth=10, bootstrap=False, n_jobs=-1, random_state=0), 'rf3000mss1md10Bf'),
        (RandomForestClassifier(n_estimators=3000, min_samples_split=1, max_depth=10, bootstrap=False, n_jobs=-1, random_state=0), 'rf3000mss1md10Bf'),
        # (RandomForestClassifier(n_estimators=10000, min_samples_split=1, max_depth=10, bootstrap=False, n_jobs=-1, random_state=0), 'rf10000mss1md10Bf'),
        # (RandomForestClassifier(n_estimators=3000, min_samples_split=1, max_depth=3, bootstrap=False, n_jobs=-1, random_state=0), 'rf3000mss1md3Bf'),
        # (RandomForestClassifier(n_estimators=3000, min_samples_split=1, max_depth=30, bootstrap=False, n_jobs=-1, random_state=0), 'rf3000mss1md30Bf'),",seizure_detection.py,udibr/seizure-prediction,1
"
    accu_p = np.zeros(shape=(2,))
    accu_r = np.zeros(shape=(2,))
    accu_f = np.zeros(shape=(2,))
    accu_a = 0.0
    folds = 10
    for train_idx, test_idx in StratifiedKFold(y=y, n_folds=folds, shuffle=True):
        train_x, train_y = x[train_idx], y[train_idx]
        test_x, test_y = x[test_idx], y[test_idx]

        cls = RandomForestClassifier()

        # train
        train_x = vectorizer.fit_transform(train_x).toarray()

        cls.fit(train_x, train_y)

        # test
        test_x = vectorizer.transform(test_x).toarray()
",yelp-sentiment/experiments/sentiment_rf.py,canast02/csci544_fall2016_project,1
"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.preprocessing import Imputer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator
 
class Classifier(BaseEstimator):
    def __init__(self):
        self.clf = Pipeline([
            ('imputer', Imputer(strategy='most_frequent')),
            ('rf', AdaBoostClassifier(
                base_estimator=RandomForestClassifier(max_depth=5, n_estimators=10),
                n_estimators=10)
            )
        ])
 
    def fit(self, X, y):
        self.clf.fit(X, y)
 
    def predict(self, X):
        return self.clf.predict(X)",classifier.py,ntung/ramp,1
"    trainDataVecs = getAvgFeatureVecs(getCleanReviews(train), model, num_features)

    print ""Creating average feature vecs for test reviews""

    testDataVecs = getAvgFeatureVecs(getCleanReviews(test), model, num_features)


    # ****** Fit a random forest to the training set, then make predictions
    #
    # Fit a random forest to the training data, using 100 trees
    forest = RandomForestClassifier(n_estimators = 100)

    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit(trainDataVecs, train[""sentiment""])

    # Test & extract results
    result = forest.predict(testDataVecs)

    # Write the test results
    output = pd.DataFrame(data={""id"":test[""id""], ""sentiment"":result})",backup/Word2Vec_AverageVectors.py,ddboline/kaggle_imdb_sentiment_model,1
"classification_metric = Metric(metrics.roc_auc_score, np.mean,
                               verbosity=verbosity,
                               mode=""classification"")
params_dict = {
    ""batch_size"": None,
    ""data_shape"": tox_train_dataset.get_data_shape(),
}

def model_builder(tasks, task_types, model_params, model_dir, verbosity=None):
  return SklearnModel(tasks, task_types, model_params, model_dir,
                      model_instance=RandomForestClassifier(
                          class_weight=""balanced"",
                          n_estimators=500,
                          n_jobs=-1),
                      verbosity=verbosity)
tox_model = SingletaskToMultitask(tox_tasks, tox_task_types, params_dict, tox_model_dir,
                              model_builder, verbosity=verbosity)
tox_model.reload()

""""""",examples/sweetlead/sweet.py,rbharath/deepchem,1
"    for line in u:
        ages = json.loads(line)
        mean = np.mean(ages)
        std = np.std(ages)
        data_x_1.append([mean, std])
data_x = [data_x[i]+data_x_1[i] for i in range(len(data_x))]



lr = LogisticRegression(random_state=0)
rf = RandomForestClassifier(random_state=0)

results = []

kf = KFold(n_splits=4, shuffle=True, random_state=0)
for train_index, test_index in kf.split(data_x):
    train_data = np.array(data_x)[train_index]
    train_target = np.array(data_y)[train_index]
    test_data = np.array(data_x)[test_index]
    test_target = np.array(data_y)[test_index]",docs/3.3 Jcard_Similarity_Modified.py,tapilab/is-prefixlt,1
"
# Test out Gestalt.

from sklearn.model_selection import KFold
from gestalt.stackers.stacking import GeneralisedStacking
from gestalt.estimator_wrappers.wrap_xgb import XGBClassifier
from gestalt.estimator_wrappers.wrap_r_ranger import RangerClassifier
from sklearn.ensemble import RandomForestClassifier

skf = KFold(n_splits=3, random_state=42, shuffle=True)
estimators = {RandomForestClassifier(n_estimators=100, n_jobs=8, random_state=42): 'RFC1',
              XGBClassifier(num_round=50,
                            verbose_eval=False,
                            params={'objective': 'multi:softprob',
                                    'num_class': 3,
                                    'silent': 1}):
                  'XGB1',
              RangerClassifier(num_trees=50, num_threads=8, seed=42): 'Ranger1'}
print(""\nPandas Test"")
for stype in ['t', 'cv', 'st', 's']:",examples/iris_multiclass.py,mpearmain/gestalt,1
"

names = [""Nearest Neighbors"", ""Linear SVM"", ""Decision Tree"", ""Random Forest"",
		""AdaBoost Classifier"",""Logistic Regression"", ""Naive Bayes""]


classifiers = [
	KNeighborsClassifier(25),
	SVC(kernel=""linear"", C=3.4),
	DecisionTreeClassifier(),
	RandomForestClassifier(n_estimators=300, n_jobs=-1),
	AdaBoostClassifier(n_estimators=70),
	LogisticRegression(random_state=1, C=0.4),
	GaussianNB()]


def main():

	#set the timer
	start = time.time()",kpcaWithTreeFS/mnistBackImage/classifiers.py,akhilpm/Masters-Project,1
"df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75
df['species'] = pd.Categorical(iris.target, iris.target_names)

train, test = df[df['is_train'] == True], df[df['is_train'] == False]

features = df.columns[:4]

print test[features].head()

clf = RandomForestClassifier(n_jobs=2)
y, _ = pd.factorize(train['species'])
clf.fit(train[features], y)

preds = iris.target_names[clf.predict(test[features])]
print pd.crosstab(test['species'],
                  preds, rownames=['actual'], colnames=['preds'])",node_modules/random-forest-classifier/tests/basic.py,sean-laing/signal_gather,1
"            | P.first
            )
        | as_key('test_X', lambda d:
            (d['test_X'].copy(),)
            | transform(d['std_scaler'])
            | P.first
            )
        | del_key('std_scaler')

        | as_key('RFClf', lambda d:
            (RandomForestClassifier(random_state=1,
                                    n_estimators=nest, n_jobs=njobs,
                                    verbose=1,
                                    max_features=0.1, min_samples_leaf=1.0,
                                    max_depth=50),)
            | fit((d['train_X'],), (d['train_y'],))
            | P.first
            )
        | as_key('y_hat', lambda d:
            (d['test_X'],)",src/RFClf.py,WojciechMigda/KAGGLE-prudential-life-insurance-assessment,1
"    Examples
    --------
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.models import CostSensitiveRandomForestClassifier
    >>> from costcla.metrics import savings_score
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> y_pred_test_rf = RandomForestClassifier(random_state=0).fit(X_train, y_train).predict(X_test)
    >>> f = CostSensitiveRandomForestClassifier()
    >>> y_pred_test_csdt = f.fit(X_train, y_train, cost_mat_train).predict(X_test)
    >>> # Savings using only RandomForest
    >>> print savings_score(y_test, y_pred_test_rf, cost_mat_test)
    0.12454256594
    >>> # Savings using CostSensitiveRandomForestClassifier
    >>> print savings_score(y_test, y_pred_test_csdt, cost_mat_test)
    0.499390945808
    """"""",costcla/models/cost_ensemble.py,madjelan/CostSensitiveClassification,1
"import scipy.sparse as sp

from skml.problem_transformation import BinaryRelevance
from skml.datasets import load_dataset

X, y = load_dataset('yeast')


class TestBR(Chai):
    def test_br_fit_predict(self):
        clf = BinaryRelevance(RandomForestClassifier())
        clf.fit(X, y)
        y_pred = clf.predict(X)
        hamming_loss(y, y_pred)

    def test_br_pipeline(self):
        pl = Pipeline([(""br"", BinaryRelevance(RandomForestClassifier()))])
        pl.fit(X, y)

    def test_br_fit_predict_proba(self):",test/test_br.py,ChristianSch/skml,1
"    # Ridge(),
    LinearRegression(),
    # DecisionTreeRegressor(random_state=0),
    # RandomForestRegressor(random_state=0),
    # GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    # KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_pca1_lr_only.py,diogo149/CauseEffectPairsPaper,1
"
    X, y = dataset.load_train(depth=1)

    raw_scaler = StandardScaler()
    raw_scaler.fit(np.r_[X, dataset.load_test()])
    X_scaled = raw_scaler.transform(X)
    del X
    import gc
    gc.collect()

    rf = RandomForestClassifier(n_estimators=12000, oob_score=True, n_jobs=-1,
                                class_weight='auto')
    rf.fit(X_scaled, y)

    logger.debug('RandomForestClassifier fitted')

    logger.debug('E_val(oob): %f', rf.oob_score_)
    logger.debug('E_in(full): %f', Util.auc_score(rf, X_scaled, y))

    X, y = dataset.load_train()",modeling.py,Divergent914/yakddcup2015,1
"from sklearn import ensemble
import pandas as pd
from os import system

df = pd.read_csv('train.csv')

y = df['target']
df = df.drop('target', 1)
df = df.drop('id', 1)

clf = ensemble.RandomForestClassifier()
t = clf.fit(df, y)

df2 = pd.read_csv('test.csv')

df2 = df2.drop('id', 1)
probs = clf.predict_proba(df2)

with open('submission.csv', 'w') as f:
    f.write('id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9')",otto/use_decision_forest.py,aufziehvogel/kaggle,1
"import numpy as np
import pandas as pd
from sklearn import ensemble

train = pd.read_csv(""train.csv"")
test = pd.read_csv(""test.csv"")

rf = ensemble.RandomForestClassifier()
rf.fit(train.drop(""survived"", axis = 1), train[[""survived""]])
",lab6/exploring.py,cycomachead/info290,1
"        self.check_download = ''
        self.decis = {}
        # Images after processing images
        self.out_ndvistats_folder_tab = defaultdict(list)
        
        # Validation shapefiles information
        self.valid_shp = []
        
        # Radom Forest Model
        # Set the parameters of this random forest from the estimator 
        self.rf = RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=None, min_samples_split=2, \
                                        min_samples_leaf=1, max_features='auto', \
                                        bootstrap=True, oob_score=True)
        
    def i_tree_direction(self):
        
        """"""
        Interface function to can extract one level or two levels of the final classification 
        """"""
",Processing.py,SylvioL/PHYMOBAT,1
"for i, y_ in enumerate(np.unique(y)):
	y[y == y_] = i

n_instances = y.size
idx = np.random.permutation(n_instances)

train_idx = idx[:int(n_instances / 3)]
cal_idx = idx[int(n_instances / 3):2 * int(n_instances / 3)]
test_idx = idx[2 * int(n_instances / 3):]

nc = ClassifierNc(ClassifierAdapter(RandomForestClassifier()))
icp = IcpClassifier(nc)

icp.fit(x[train_idx, :], y[train_idx])
icp.calibrate(x[cal_idx, :], y[cal_idx])


print(pd.DataFrame(icp.predict_conf(x[test_idx, :]),",examples/confidence_credibility.py,donlnz/nonconformist,1
"    elif clf_ == 'svm':
        clf=Pipeline([('scaler',StandardScaler()),
                        ('estimator',SVC(C=1.0,kernel=kernel,
                          max_iter=int(1e4),
                          tol=1e-4,
#                          class_weight={1:10,0:1},
                          class_weight={1:weights*np.count_nonzero(Y)/len(Y),0:1-(np.count_nonzero(Y)/len(Y))},
                          probability=True,random_state=12345))])
    elif clf_ == 'RF':
        clf=Pipeline([('scaler',StandardScaler()),
                      ('estimator',RandomForestClassifier(n_estimators=50,
                                                          class_weight={1:weights*np.count_nonzero(Y)/len(Y),0:1-(np.count_nonzero(Y)/len(Y))},))])
    else:
        clf = clf_
    for jj,(train, test) in enumerate(cv.split(X,Y)):
        print('cv %d'%(jj+1))
        clf = clf
        clf.fit(X[train],Y[train])
        fpr,tpr,_ = roc_curve(Y[test],clf.predict_proba(X[test])[:,-1])
        auc_score = auc(fpr,tpr)",eegPipelineFunctions.py,adowaconan/Spindle_by_Graphical_Features,1
"# scale data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# fit model
#model = SVC(kernel=""linear"", C=0.025)
#model = SVC(gamma=2, C=1)
model = KNeighborsClassifier(10)
#model = GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True)
#model = DecisionTreeClassifier(max_depth=5)
#model = RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1)
#model = MLPClassifier(alpha=1)
#model = AdaBoostClassifier()
#model = GaussianNB()
#model = QuadraticDiscriminantAnalysis()

model.fit(X_train, Y_train)

# cals predictions
if hasattr(model, ""decision_function""):",src/model.py,jokruger/python-ml-basics,1
"    fnn = predict_nn(trndata)
    proba_nn = fnn.activateOnDataset(tstdata)

    print ""Finished training NN: "", time_now_str()

    # no validation labels on actual prediction
    if doTrees:
        # random forest
        sl_ccrf = SKSupervisedLearning(CalibratedClassifierCV, X_train, Y_train, X_test, Y_test)
        sl_ccrf.train_params = \
            {'base_estimator': RandomForestClassifier(**{'n_estimators' : 7500, 'max_depth' : 200}), 'cv': 10}
        sl_ccrf.fit_standard_scaler()

        print ""Starting on RF: "", time_now_str()
        ll_ccrf_trn, ll_ccrf_tst = sl_ccrf.fit_and_validate()

        print ""RF score: {0:.4f}"".format(ll_ccrf_tst if not prediction else ll_ccrf_trn)
        sl_ccrf.proba_test.tofile(""/temp/sl_ccrf.prob"")
        sl_svm.proba_test.tofile(""/temp/sl_svm.prob"")
        proba_nn.tofile(""/temp/nn.prob"")",Learning/predict.py,fierval/KaggleMalware,1
"from sklearn import preprocessing
#scl=decomposition.PCA(n_components=30,whiten=True)
scl=preprocessing.RobustScaler()
X_train=scl.fit_transform(X_train)
X_val=scl.transform(X_val)
X_test=scl.transform(X_test)


from sklearn import linear_model,ensemble
#
clf=ensemble.RandomForestClassifier(n_estimators=300,random_state=100,max_depth=15,max_features=None,n_jobs=-1)
#clf =linear_model.SGDClassifier(loss='hinge', penalty='l2', alpha=0.00001, l1_ratio=0.15, fit_intercept=True, n_iter=200,
#                                shuffle=True, verbose=0, epsilon=0.1, n_jobs=-1, random_state=17, learning_rate='invscaling',eta0=1.0, power_t=0.5, class_weight=None, warm_start=False, average=False)
clf.fit(X_train,y_train)
from sklearn import metrics
y_pred=clf.predict_proba(X_val)[:,1]

score=metrics.roc_auc_score(y_val, y_pred)
print('score on extra set:%s' %score)
",cervical-cancer-screening/models/src/RFfull.py,paulperry/kaggle,1
"
def measure_model(datasize=1000, testsize=500):
    data = cu.get_sample_data_frame(datasize)
    test = cu.get_test_data_frame(testsize)
    #data = full.ix[len(full)/4:].reset_index() # last n/4 * 3 records
    #test = full.ix[:(len(full)/4)-1].reset_index() # first n/4 records
    #data = cu.get_dataframe('train-sample.csv')
    #test = cu.get_dataframe('public_leaderboard.csv')
    fea = features.extract_features(data)
    test_features = features.extract_features(test)
    rf = RandomForestClassifier(n_estimators=50, verbose=2,
                                compute_importances=True, n_jobs=5)
    rf.fit(fea, data[""OpenStatus""])
    probs = rf.predict_proba(test_features)
    new_priors = cu.load_priors('train.csv')
    old_priors = cu.compute_priors(data.OpenStatus)
    probs = cu.cap_and_update_priors(old_priors, probs, new_priors, 0.001)
    y_true = compute_y_true(test)
    score = multiclass_log_loss(y_true, probs)
    return score, rf, fea",src/lab.py,coreyabshire/stacko,1
"        train_labels = smartDownSample(train, n_segments + r1, compactness + r2, threshold + r3) 
        test_labels = smartDownSample(test, n_segments + r1, compactness + r2, threshold + r3)

        print ""Processing features...""
        X_train = extract_features(train, train_labels)
        y_train = extract_classes(train_truth, train_labels)
        X_test = extract_features(test, test_labels)


        print ""Initializing random forest...""
        forest = skl.RandomForestClassifier(n_estimators = 200, max_depth = 100)

        print ""Learning random forest...""
        forest.fit(X_train, y_train)

        print ""Making predictions...""
        y_guess = forest.predict(X_test)

        # Move from prediction vector to prediction pictures
        j = 0",pipeline.py,Connectomics-Classes/hackwicket-silverbacks,1
"subset = ""refined""
pdbbind_tasks, pdbbind_datasets, transformers = load_pdbbind_pockets(
    split=split, subset=subset)
train_dataset, valid_dataset, test_dataset = pdbbind_datasets 

metric = dc.metrics.Metric(dc.metrics.roc_auc_score)

current_dir = os.path.dirname(os.path.realpath(__file__))
model_dir = os.path.join(current_dir, ""pocket_%s_%s_RF"" % (split, subset))

sklearn_model = RandomForestClassifier(n_estimators=500)
model = dc.models.SklearnModel(sklearn_model, model_dir=model_dir)

# Fit trained model
print(""Fitting model on train dataset"")
model.fit(train_dataset)
model.save()

print(""Evaluating model"")
train_scores = model.evaluate(train_dataset, [metric], transformers)",examples/binding_pockets/binding_pocket_rf.py,bowenliu16/deepchem,1
"for val in features:
    if(val[0] == 1.0):
        columns.append(val[1])

train_dataset = pd.read_csv(""../UntitledFolder/merged_old_and_new_train.csv"")
test_dataset = pd.read_csv(""../UntitledFolder/merged_old_and_new_test.csv"")
train_output = pd.read_csv(""../Data/act_train_output.csv"")

combined_train = pd.merge(train_dataset, train_output, on = ""activity_id"", how=""left"")

randomForestModel = RandomForestClassifier(n_estimators=500)
X = combined_train[columns]
Y = combined_train[[""outcome""]].values.ravel()

randomForestModel.fit(X, Y)
print(""--- %s seconds ---"" % (time.time() - start_time))

Utility.saveModel(randomForestModel, filename)
print(""--- %s seconds ---"" % (time.time() - start_time))
",Initial_Classification_Models/RandomForest.py,BhavyaLight/kaggle-predicting-Red-Hat-Business-Value,1
"createScript(""Classification"", ""sklearn_c_template"", ""knn"", ""default"", None, {""n_neighbors"": 5, ""weights"": 'uniform', ""leaf_size"": 30, ""p"": 2})
createScript(""Classification"", ""sklearn_c_template"", ""lda"", ""default"", None, {""solver"": ""svd"", ""tol"": 0.0001})
createScript(""Classification"", ""sklearn_c_template"", ""logistic_regression"", ""default"", None, {""penalty"": 'l2', ""dual"": False, ""tol"": 0.0001, ""C"": 1.0, ""class_weight"": None, ""solver"": 'liblinear', ""max_iter"": 100, ""multi_class"": 'ovr'})
createScript(""Classification"", ""sklearn_c_template"", ""multilayer_perceptron"", ""default"", None, {""hidden_layer_sizes"": (100, ), ""activation"": 'relu', ""solver"": 'adam', ""alpha"": 0.0001, ""learning_rate"": 'constant', ""max_iter"": 200, ""tol"": 0.0001, ""early_stopping"": False})
createScript(""Classification"", ""sklearn_c_template"", ""random_forest"", ""default"", None, {""n_estimators"": 50, ""criterion"": 'gini', ""max_depth"": None, ""min_samples_split"": 2, ""min_samples_leaf"": 1, ""min_weight_fraction_leaf"": 0.0, ""max_leaf_nodes"": None, ""min_impurity_split"": 1e-07, ""bootstrap"": True, ""oob_score"": False, ""class_weight"": None})
createScript(""Classification"", ""sklearn_c_template"", ""sgd"", ""default"", None, {""penalty"": 'l2', ""alpha"": 0.0001, ""n_iter"": 5, ""epsilon"": 0.1, ""learning_rate"": 'optimal', ""class_weight"": None})
createScript(""Classification"", ""sklearn_c_template"", ""svm"", ""default"", None, {""C"": 1.0, ""kernel"": 'rbf', ""shrinking"": True, ""tol"": 0.001, ""class_weight"": None})

anova = ""F, score = f_classif(train_X, train_y)""
mutual_info = ""score = 1 - mutual_info_classif(train_X, train_y, n_neighbors={n_neighbors})""
random_forest_rfe = ""selector = RFE(RandomForestClassifier(n_estimators=50, random_state=R_SEED), n_features_to_select=1, step={step})""
random_logistic_regression = ""scorer = RandomizedLogisticRegression(C={C}, scaling={scaling}, sample_fraction={sample_fraction}, n_resampling={n_resampling}, selection_threshold={selection_threshold}, tol={tol}, fit_intercept=True, verbose=False, normalize=True, random_state=R_SEED)""
svm_rfe = ""selector = RFE(SVC(random_state=R_SEED, kernel='linear'), n_features_to_select=1, step={step})""

createScript(""FeatureSelection"", ""sklearn_f_template"", ""anova"", ""default"", ""score"", {})
createScript(""FeatureSelection"", ""sklearn_f_template"", ""mutual_info"", ""default"", ""score"", {""n_neighbors"": 3})
createScript(""FeatureSelection"", ""sklearn_f_template"", ""random_forest_rfe"", ""default"", ""rfe"", {""step"": 0.1})
createScript(""FeatureSelection"", ""sklearn_f_template"", ""random_logistic_regression"", ""default"", ""coef"", {""C"": 1, ""scaling"": 0.5, ""sample_fraction"": 0.75, ""n_resampling"": 200, ""selection_threshold"": 0.25, ""tol"": 0.001})
createScript(""FeatureSelection"", ""sklearn_f_template"", ""svm_rfe"", ""default"", ""rfe"", {""step"": 0.1})
",AlgorithmScripts/Helper/build_sklearn_scripts.py,srp33/ShinyLearner,1
"n_samples = 2000
n_features = 2
random_state_classifier = 2018
np.random.seed(random_state_classifier)
X_train = np.random.uniform(low=0, high=1, size=(n_samples, n_features))
y_train = np.random.choice([0, 1], size=(n_samples,), p=[.5, .5])
X_test = np.random.uniform(low=0, high=1, size=(n_samples, n_features))
y_test = np.random.choice([0, 1], size=(n_samples,), p=[.5, .5])

# fit the classifier
rf = RandomForestClassifier(
    n_estimators=20, random_state=random_state_classifier)
#feature_weight = [.6, .4]
feature_weight = [.1, .9]
rf.fit(X=X_train, y=y_train, feature_weight=feature_weight)

# extract features
""""""for idx, dtree in enumerate(rf.estimators_):
    print(idx)
",test_bug/weighted_RandomForest_bug.py,Yu-Group/scikit-learn-sandbox,1
"print ""knn done""'''

for i in range(1,11):
    audio_clf = Pipeline([('clf',  KNeighborsClassifier(n_neighbors=i))])
    audio_clf.fit(pitches_tf[:train_size], data_terms[:train_size])
    prediction =  audio_clf.predict(pitches_tf[train_size:])
    print precision_recall_fscore_support(data_terms[train_size:], prediction, average=""macro"")
print ""knn done""

print ""Running random forest""
'''clf = RandomForestClassifier(n_estimators=10)
clf.fit(pitches_tf[:train_size], data_terms[:train_size])
prediction =  clf.predict(pitches_tf[train_size:])
precision, recall, fscore, _ =  precision_recall_fscore_support(data_terms[train_size:], prediction, average=""micro"")
print ""precision: "", precision
print ""recall: "", recall
print ""fscore: "", fscore
print ""random forest done""'''

for i in range(1,11):",msd_knn_rf.py,mayanks43/auto-tag,1
"
model3 = MultinomialNB()
model3.fit(features, d_train.sentiment)
pred3 = model3.predict_proba(vectorizer.transform(d_test.review))
performance(d_test.sentiment, pred3)


# In[33]:

from sklearn.ensemble import RandomForestClassifier
model2 = RandomForestClassifier(n_estimators=100)
model2.fit(features, d_train.sentiment)


# In[28]:

pred2 = model2.predict_proba(vectorizer.transform(d_test.review))
performance(d_test.sentiment, pred2)

",examples/realWorldMachineLearning/Chapter+8+-+Movie+Review+Full+Example.py,remigius42/code_camp_2017_machine_learning,1
"print('training set loaded')

del labels

# Parameters for Randomforest
random_state = 5342
n_jobs = 8
verbose = 2
clf1 = ExtraTreesClassifier(criterion='entropy', random_state=random_state, n_jobs=n_jobs, verbose=verbose)
clf2 = ExtraTreesClassifier(criterion='entropy', random_state=random_state, n_jobs=n_jobs, verbose=verbose)
clf3 = RandomForestClassifier(criterion='entropy', random_state=random_state, n_jobs=n_jobs, verbose=verbose)
clf4 = RandomForestClassifier(criterion='entropy', random_state=random_state, n_jobs=n_jobs, verbose=verbose)

# Start training
print('training started')
clf1.fit(train[:, :-1], train[:, -1])
X_new1 = clf1.transform(train[:, :-1])
X_new2 = clf3.fit_transform(train[:, :-1], train[:, -1])
# print('importances', clf1.feature_importances_)
clf2.fit(X_new1, train[:, -1])",solution4.py,canast02/microsoft-malware-classification-challenge,1
"		score_dtree+=1
print('Accuracy Decision Tree : =====> ', round(((score_dtree/no_test_instances )*100),2),'%')
print(""With cross validation : "")
score = cross_val_score(dtree,X,Y, cv = 10, scoring = 'accuracy')
print(score)
print(""Mean"", round((score.mean() * 100),2) , ""%""  )
print('--------------------------------------------------')


#Random Forests
rf = RandomForestClassifier(n_estimators = 20, n_jobs = 8)
rf.fit(X,Y)
result_rf = rf.predict(Z)
#print('X', len(X),len(Y),len(X1[train_size:dataset_size]))
#print('RF prediction : ---> ',result_rf )
#print('actual ans: -->',test_class)
CM = confusion_matrix(test_class,result_rf) 
print(""Confusion Matrix : "")
print(CM)
for i in range(0,no_test_instances):",sandbox/ml/scikit_learn_classifiers.py,LighthouseHPC/lighthouse,1
"outcomes = ramp.shortcuts.cv_factory(
    data=data,
    folds=10,

    target=[AsFactor('class')],

    reporter_factories=reporters,

    # Try out two algorithms
    estimator=[
        sklearn.ensemble.RandomForestClassifier(
            n_estimators=20),
        sklearn.linear_model.LogisticRegression(),
        ],

    # and 4 feature sets
    features=[
        expanded_features,

        # Feature selection",examples/iris.py,kvh/ramp,1
"        # for elt in ['L1', 'ALU', 'SVA']:
        #     for vplot_label in vplot_labels:
        #         sns.plt.clf()
        #         logger.info('plotting %s' % vplot_label)
        #         sns_plot = sns.violinplot(x=""KnownGood"", y=vplot_label, data=data[data.Superfamily==elt], jitter=True)
        #         fig = sns_plot.figure
        #         fig.savefig(args.outbase+elt+'.'+vplot_label+'.png')


        for elt in ['L1', 'ALU', 'SVA']:
            clf = RandomForestClassifier()
            #clf = LinearDiscriminantAnalysis()
            #clf = AdaBoostClassifier()
            #clf = GaussianNB()
            #clf = KNeighborsClassifier()
            #clf = SVC(gamma=2, C=1)

            elt_data = data[data.Superfamily==elt]

            x = elt_data[vplot_labels]",scripts/autofilter.py,adamewing/tebreak,1
"
    return dict(clf_params, **parameters)


def randomForest_pipeline():
    if PRODUCTION:
        return Pipeline([('vect', CountVectorizer(ngram_range=(2,3),
                                      analyzer='word', tokenizer=LemmaTokenizer(),
                                      max_features=1000, binary=True)),
                         ('tfidf', TfidfTransformer(use_idf=True, norm='l1')),
                         ('forest', RandomForestClassifier(max_features='sqrt',
                                    n_estimators=1000))
                        ])

    return Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('forest', RandomForestClassifier())])


def svc_parameters():",src/data_set_builder/bag_of_words.py,piatra/ssl-project,1
"# from sklearn.qda import QDA
# clf = QDA(priors=None, reg_param=0.001).fit(X_cropped, np.ravel(y_cropped[:]))
# y_validation_predicted = clf.predict(X_validation)
# print ""Error rate for QDA: "", ml_aux.get_error_rate(y_validation,y_validation_predicted)



# # Start Random Forest Classification
# print ""Performing Random Classification:""
# from sklearn.ensemble import RandomForestClassifier
# forest = RandomForestClassifier(n_estimators=500)
# forest = forest.fit(X_cropped, np.ravel(y_cropped[:]))
# y_validation_predicted = forest.predict(X_validation)
# print ""Error rate for Random Forest: "", ml_aux.get_error_rate(y_validation,y_validation_predicted)
# ml_aux.plot_confusion_matrix(y_validation, y_validation_predicted, ""CM Random Forest (t1)"")
# plt.show()


# # Start k nearest neighbor Classification
# print ""Performing kNN Classification:""",Code/Machine_Learning_Algos/training_t1.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"
spam_X, spam_y = make_classification(5000)

# split the datainto training and test set
spam_X_train, spam_X_test, spam_y_train, spam_y_test = train_test_split(
                                                       spam_X, spam_y,
                                                       test_size=0.2)

# create RandomForestClassifier
n_trees = 500
spam_RFC = RandomForestClassifier(max_features=5, n_estimators=n_trees,
                                  random_state=42)
spam_RFC.fit(spam_X_train, spam_y_train)
spam_y_hat = spam_RFC.predict_proba(spam_X_test)

# calculate inbag and unbiased variance
spam_inbag = fci.calc_inbag(spam_X_train.shape[0], spam_RFC)
spam_V_IJ_unbiased = fci.random_forest_error(spam_RFC, spam_X_train,
                                             spam_X_test)
",examples/plot_spam.py,arokem/sklearn-forest-ci,1
"
    vectorizer = CountVectorizer(analyzer = 'word', tokenizer = None,  preprocessor = None, stop_words = None, max_features = nfeatures)

    train_review_subset_x = clean_labeledtrain_reviews[::2]
    train_review_subset_y = labeledtrain_data['sentiment'][::2]
    test_review_subset_x = clean_labeledtrain_reviews[1::2]
    test_review_subset_y = labeledtrain_data['sentiment'][1::2]

    train_data_features = vectorizer.fit_transform(train_review_subset_x).toarray()

    forest = RandomForestClassifier(n_estimators = 100)
    forest = forest.fit(train_data_features, train_review_subset_y)

    test_data_features = vectorizer.transform(test_review_subset_x).toarray()

    print forest.score(test_data_features, test_review_subset_y)

    del train_review_subset_x, train_review_subset_y, test_review_subset_x, test_review_subset_y, test_data_features, train_data_features

    if run_test_data:",backup/my_model.py,ddboline/kaggle_imdb_sentiment_model,1
"from sklearn.linear_model import LogisticRegression
lr = LogisticRegression().fit(train_data_finite, train_labels)
print(""logistic regression score: %f"" % lr.score(test_data_finite, test_labels))

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=500, random_state=0).fit(train_data_finite, train_labels)
print(""random forest score: %f"" % rf.score(test_data_finite, test_labels))

features_dummies_sub = pd.get_dummies(features[['pclass', 'sex', 'age', 'sibsp', 'fare']])
data_sub = features_dummies_sub.values

train_data_sub, test_data_sub, train_labels, test_labels = train_test_split(data_sub, labels, random_state=0)

imp = Imputer()
imp.fit(train_data_sub)",scipy-2017-sklearn-master/notebooks/solutions/10_titanic.py,RPGOne/Skynet,1
"
		test_index = test.loc[test.cellID == i].index
		test_label = test.loc[(test.cellID == i),'place_id'].values
		test_curr = test.loc[test.cellID == i].reset_index(drop = True)
		test_X = test_curr[FEATURE_LIST].as_matrix()

		# print len(train_curr)
		# print len(test_curr)

		#increase estimators won't influence too much
		rf = RandomForestClassifier(max_depth = 20, n_estimators = 30)  

		#add weight won't influence too much
		#rf.fit(train_X, train_Y, sample_weight = weight)

		rf.fit(train_X, train_Y)
		#test_predict = rf.predict_proba(test_X)
		test_predict = rf.predict(test_X)
		# test_predict = np.argsort(test_predict, axis=1)[:,::-1][:,:3]
		# test_predict = le.inverse_transform(test_predict)",facebook-kaggle/06_05.py,SonneSun/MYSELF,1
"    # Numpy arrays are easy to work with, so convert the result to an
    # array
    train_data_features = train_data_features.toarray()

    # ******* Train a random forest using the bag of words
    #
    print(""Training the random forest (this may take a while)..."")


    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators = 100)

    # Fit the forest to the training set, using the bag of words as
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit( train_data_features, train[""sentiment""] )


",study/kaggle/DeepLearningMovies/BagOfWords.py,AppleFairy/machinelearning,1
"
h = .02  # step size in the mesh

names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel=""linear"", C=0.025),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    AdaBoostClassifier(),
    GaussianNB(),
    LDA(),
    QDA()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)",scripts/plot_classifier_comparison.py,JohnAshburner/ISBI,1
"iris = datasets.load_iris()
x = iris.data[:, [2,3]]
y = iris.target

x_train, x_test, y_train, y_test = \
    train_test_split(x, y, test_size = 0.3, random_state=0)

x_combined = np.vstack((x_train, x_test))
y_combined = np.hstack((y_train, y_test))

forest = RandomForestClassifier(criterion='entropy', n_estimators=50, random_state=1, n_jobs=2)

forest.fit(x_train, y_train)

plot_decision_regions(x_combined, y_combined, classifier=forest, test_idx=range(105, 150))

plt.xlabel('petal length [cm]')
plt.ylabel('petal width  [cm]')
plt.legend(loc='upper left')
plt.show()",3/p88_randomforest.py,yukke42/machine-learning,1
"default_classifiers = [
    svm.LinearSVC(random_state=0),
    svm.SVC(random_state=0),
    linear_model.LogisticRegression(),
    tree.DecisionTreeClassifier(),
    naive_bayes.BernoulliNB(),
    naive_bayes.MultinomialNB(),
    naive_bayes.GaussianNB(),
    linear_model.SGDClassifier(),
    linear_model.RidgeClassifier(),
    ensemble.RandomForestClassifier(n_estimators=10)
]

plt.style.use('fivethirtyeight')

_PLT_LEGEND_OPTIONS = dict(loc=""upper center"",
                           bbox_to_anchor=(0.5, -0.15),
                           fancybox=True,
                           shadow=True,
                           ncol=3)",ml/testing_multiple_classifiers.py,thorwhalen/ut,1
"    test_id.extend(no_test)
    str_test_id = '(' + ','.join(test_id) + ')'    
    x_test = db.get_feature_by_row(str_test_id)
    y_test = [1] * 25 + [0] * 25
    
    rowIdx.test50_data_x = test_id
    rowIdx.test50_data_y = y_test
    
#     print 'normal x {} y {}'.format(len(x_train), len(y_train))
# Case 1
    m1 = RandomForestClassifier()
    m1.fit(x_train, y_train)
    y_pred = m1.predict(x_test)
    fsc1 = f1_score(y_test, y_pred)
    

# Case 2
    m2_x_train = []
    m2_y_train = []
    str_measure_id = '(' + ','.join(measure_data) + ')'",filter_range.py,chaluemwut/filter01,1
"
        if shape_1X[0] > 1:
            print('Slicing Images...')
            sliced_X, sliced_y = self._window_slicing_img(X, window, shape_1X, y=y, stride=stride)
        else:
            print('Slicing Sequence...')
            sliced_X, sliced_y = self._window_slicing_sequence(X, window, shape_1X, y=y, stride=stride)

        if y is not None:
            n_jobs = getattr(self, 'n_jobs')
            prf = RandomForestClassifier(n_estimators=n_tree, max_features='sqrt',
                                         min_samples_split=min_samples, oob_score=True, n_jobs=n_jobs)
            crf = RandomForestClassifier(n_estimators=n_tree, max_features=None,
                                         min_samples_split=min_samples, oob_score=True, n_jobs=n_jobs)
            print('Training MGS Random Forests...')
            prf.fit(sliced_X, sliced_y)
            crf.fit(sliced_X, sliced_y)
            setattr(self, '_mgsprf_{}'.format(window), prf)
            setattr(self, '_mgscrf_{}'.format(window), crf)
            pred_prob_prf = prf.oob_decision_function_",GCForest.py,pylablanche/gcForest,1
"for root, dirs, files in os.walk('data'):
    for name in files:
        if name.endswith('.csv'):
            print ""Loading "" + root + ""/"" + name
            dataset = load_data.load_data(name, root)

            splits = tts(dataset.data, dataset.target, test_size=0.2)
            X_train, X_test, y_train, y_test = splits

            # Build a forest and compute the feature importances
            forest = RandomForestClassifier(n_estimators=250)
            forest.fit(X_train, y_train)
            importances = forest.feature_importances_
            std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)
            indices = np.argsort(importances)[::-1]

            # Print the feature ranking
            print(""Feature ranking:"")

            for f in range(X_train.shape[1]):",modeling/feature_weighting/rfc.py,nd1/Listener_Classification_Public,1
"        aggregate_axis = 1 - axis
        features_non_null_series = df.count(axis=aggregate_axis)
        # Whenever count() differs from row_length it implies a null value exists in feature column and a False in mask
        mask = row_length == features_non_null_series
        return mask

    @staticmethod
    def estimate_by_mice(df):
        df_estimated_var = df.copy()
        random.seed(129)
        mice = MICE()  # model=RandomForestClassifier(n_estimators=100))
        res = mice.complete(np.asarray(df.values, dtype=float))
        df_estimated_var.loc[:, df.columns] = res[:][:]
        return df_estimated_var

    def feature_scaling(self, df):
        df = df.copy()
        # Standardization (centering and scaling) of dataset that removes mean and scales to unit variance
        standard_scaler = StandardScaler()
        numerical_feature_names_of_non_modified_df = HousePrices._numerical_feature_names",house_prices.py,MizioAnd/HousePrices,1
"

import scipy
import itertools
from sklearn.feature_selection import GenericUnivariateSelect, RFECV, SelectFromModel


def get_feature_selection_model_from_name(type_of_estimator, model_name):
    model_map = {
        'classifier': {
            'SelectFromModel': SelectFromModel(RandomForestClassifier(n_jobs=-1, max_depth=10, n_estimators=15), threshold='20*mean'),
            'RFECV': RFECV(estimator=RandomForestClassifier(n_jobs=-1), step=0.1),
            'GenericUnivariateSelect': GenericUnivariateSelect(),
            'RandomizedSparse': RandomizedLogisticRegression(),
            'KeepAll': 'KeepAll'
        },
        'regressor': {
            'SelectFromModel': SelectFromModel(RandomForestRegressor(n_jobs=-1, max_depth=10, n_estimators=15), threshold='0.7*mean'),
            'RFECV': RFECV(estimator=RandomForestRegressor(n_jobs=-1), step=0.1),
            'GenericUnivariateSelect': GenericUnivariateSelect(),",auto_ml/utils_feature_selection.py,doordash/auto_ml,1
"def training_and_test(token, train_data, test_data, num_classes, result):
    """"""Train and test

    Args:
        token (:obj:`str`): token representing this run
        train_data (:obj:`tuple` of :obj:`numpy.array`): Tuple of training feature and label
        test_data (:obj:`tuple` of :obj:`numpy.array`): Tuple of testing feature and label
        num_classes (:obj:`int`): Number of classes
        result (:obj:`pyActLearn.performance.record.LearningResult`): LearningResult object to hold learning result
    """"""
    model = RandomForestClassifier(n_estimators=20, criterion=""entropy"")
    model.fit(train_data[0], train_data[1].flatten())
    # Test
    predicted_y = model.predict(test_data[0])
    predicted_proba = model.predict_proba(test_data[0])
    # Evaluate the Test and Store Result
    confusion_matrix = get_confusion_matrix(num_classes=num_classes,
                                            label=test_data[1].flatten(), predicted=predicted_y)
    result.add_record(model.get_params(), key=token, confusion_matrix=confusion_matrix)
    # In case any label is missing, populate it",examples/CASAS_Single_Test/b1_randomforest.py,TinghuiWang/pyActLearn,1
"            lab.show()
            answer = raw_input('Has bird? (y/n)')
            if answer.lower() in ['y', 'yes']:
                has_bird.append(1)
            else:
                has_bird.append(0)
            lab.close('all')
            if j > max:
                break
        # do some prediction modeling to figure out evidence needed
    clf = RandomForestClassifier(n_estimators=10)
    # put into correct format
    # some arrays have different size remove?
    size = np.asarray([len(j) for i,j in training])
    max_size = 0
    for i in np.unique(size):
        if sum(size == i) > max_size:
            max_size = sum(size == i)
            out_size = i +0
    # do it in a weird way to save memory",examples/post_process.py,drdangersimon/image_diff,1
"labels2 = df['discharge_disposition_id'].values
cols = list(df)
f1 = [x for x in cols if x not in set(['admission_type_id', 'discharge_disposition_id'])]
f2 = [x for x in cols if x not in set(['discharge_disposition_id'])]
features1 = df[list(f1)].values
features2 = df[list(f2)].values

#clf = KNeighborsClassifier(10, weights='distance')
#clf1 = AdaBoostClassifier(n_estimators=50)
#clf2 = AdaBoostClassifier(n_estimators=50)
clf1 = RandomForestClassifier(n_jobs=-1, n_estimators=200, min_samples_split=12, max_features=None)
clf2 = RandomForestClassifier(n_jobs=-1, n_estimators=200, min_samples_split=12, max_features=None)
#clf1 = GaussianNB()
#clf2 = GaussianNB()
clf1.fit(features1, labels1)
clf2.fit(features2, labels2)

pred1 = clf1.predict(f_test)
f_test = np.insert(f_test,6, pred1,1)
pred2 = clf2.predict(f_test)",data/jerry/submit.py,isabellewei/deephealth,1
"        min_samples_leaf=1,
        min_weight_fraction_leaf=0.0,
        max_features=None,
        random_state=None,
        max_leaf_nodes=None,
        class_weight=None,
        presort=False,
      ),
        param_grid={'min_samples_split': list(range(2, 5))}
      ),
      rfc=dict(model=RandomForestClassifier(),
        param_grid={""min_samples_split"": list(range(2, 5))}
      ),
      etc=dict(model=ExtraTreesClassifier(
        n_estimators=10,
        criterion='gini',
        max_depth=None,
        min_samples_split=2,
        min_samples_leaf=1,
        min_weight_fraction_leaf=0.0,",semclassify/classify.py,arnold-jr/sem-classify,1
"start = time.clock()
clf = SVC(C=1, kernel='rbf', gamma = 0.1)
clf.fit(X_train,y_train)
accuracy = clf.score(X_test, y_test) * 100.0
end = time.clock()
print ""rbf support vector machine accuracy on titanic dataset: %.2f%%"" % accuracy
print ""time to train rbf support vector machine: %.2f seconds\n"" % (end - start)

#random forest on titanic
start = time.clock()
clf = RandomForestClassifier(100)
clf.fit(X_train,y_train)
accuracy = clf.score(X_test, y_test) * 100.0
end = time.clock()
print ""random forest accuracy on titanic dataset: %.2f%%"" % accuracy
print ""time to train random forest: %.2f seconds\n"" % (end - start)

#adaboost on titanic
start = time.clock()
clf = AdaBoostClassifier()",Basic_ML/Classifier_Comparison/classifier_comparison.py,iamshang1/Projects,1
"    >>> from sklearn.cross_validation import train_test_split
    >>> from costcla.datasets import load_creditscoring1
    >>> from costcla.sampling import cost_sampling, undersampling
    >>> from costcla.metrics import savings_score
    >>> data = load_creditscoring1()
    >>> sets = train_test_split(data.data, data.target, data.cost_mat, test_size=0.33, random_state=0)
    >>> X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = sets
    >>> X_cps_o, y_cps_o, cost_mat_cps_o =  cost_sampling(X_train, y_train, cost_mat_train, method='OverSampling')
    >>> X_cps_r, y_cps_r, cost_mat_cps_r =  cost_sampling(X_train, y_train, cost_mat_train, method='RejectionSampling')
    >>> X_u, y_u, cost_mat_u = undersampling(X_train, y_train, cost_mat_train)
    >>> y_pred_test_rf = RandomForestClassifier(random_state=0).fit(X_train, y_train).predict(X_test)
    >>> y_pred_test_rf_cps_o = RandomForestClassifier(random_state=0).fit(X_cps_o, y_cps_o).predict(X_test)
    >>> y_pred_test_rf_cps_r = RandomForestClassifier(random_state=0).fit(X_cps_r, y_cps_r).predict(X_test)
    >>> y_pred_test_rf_u = RandomForestClassifier(random_state=0).fit(X_u, y_u).predict(X_test)
    >>> # Savings using only RandomForest
    >>> print(savings_score(y_test, y_pred_test_rf, cost_mat_test))
    0.12454256594
    >>> # Savings using RandomForest with cost-proportionate over-sampling
    >>> print(savings_score(y_test, y_pred_test_rf_cps_o, cost_mat_test))
    0.192480226286",costcla/sampling/cost_sampling.py,albahnsen/CostSensitiveClassification,1
"        cov = np.diagflat([1] * n_features)
        _X = np.random.multivariate_normal(center, cov, n_samples)
        _y = [i] * n_samples
        Xl.append(_X)
        yl.append(_y)
        center += center_step        
        
    y = np.concatenate(yl)
    X = np.vstack(Xl)
    
    uc = UnSupervisedRandomForestClassifier(n_estimators=1, max_depth=5, max_features=None, bootstrap=False) #, min_samples_leaf=60)
    uc.fit(X, y)
    #yr = uc.predict(X)
    
    t0 = uc.estimators_[0].tree_
    
    print 'features:', t0.feature
    print 'impurity:', t0.impurity
    print 'threshold', t0.threshold
",sklearn/tree/_playground.py,loli/semisupervisedforests,1
"                test_encode_time += time() - start
                del relevant_grouped_test
                del relevant_test_ids
                
            # fit classifier
            print(""Fitting classifier..."")
            sys.stdout.flush()
            start = time()
            
            if classifier == ""rf"":
                cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=best_params[dataset_name][method_name][nr_events]['rf_max_features'], random_state=random_state)
                
            elif classifier == ""gbm"":
                cls = GradientBoostingClassifier(n_estimators=best_params[dataset_name][method_name][nr_events]['gbm_n_estimators'], max_features=best_params[dataset_name][method_name][nr_events]['gbm_max_features'], learning_rate=best_params[dataset_name][method_name][nr_events]['gbm_learning_rate'], random_state=random_state)
                
            else:
                print(""Classifier unknown"")
                break
                
            cls.fit(train_X, train_y)",experiments_final/run_index_lossless.py,irhete/predictive-monitoring-benchmark,1
"from sklearn.cross_validation import train_test_split

from sklearn_evaluation import plot

data = datasets.make_classification(200, 10, 5, class_sep=0.65)
X = data[0]
y = data[1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

est = RandomForestClassifier()
est.fit(X_train, y_train)

y_pred = est.predict(X_test)
y_score = est.predict_proba(X_test)
y_true = y_test
",examples/precision_recall.py,edublancas/sklearn-evaluation,1
"    #create the training & test sets, skipping the header row with [1:]
    dataset = genfromtxt(open('../example_files/train_rf.csv','r'), delimiter=',', dtype='f8')[1:]
    print(dataset)


    target = [x[0] for x in dataset]
    train = [x[1:] for x in dataset]
    test = genfromtxt(open('../example_files/test_rf.csv','r'), delimiter=',', dtype='f8')[1:]

    #create and train the random forest
    #multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)
    rf = RandomForestClassifier(n_estimators=100)
    rf.fit(train, target)

    savetxt('../example_files/submission2.csv', rf.predict(test), delimiter=',', fmt='%f')


#####################
#      CLASSES      #
#####################",termite/prev/random_forest.py,SMV818VMS/termite,1
"rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,
	random_state=0)

rt_lm = LogisticRegression()
pipeline = make_pipeline(rt, rt_lm)
pipeline.fit(X_train, y_train)
y_pred_rt = pipeline.predict_proba(X_test)[:, 1]
fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)

# Supervised transformation based on random forests
rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)
rf_enc = OneHotEncoder()
rf_lm = LogisticRegression()
rf.fit(X_train, y_train)
rf_enc.fit(rf.apply(X_train))
rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)

y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]
fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)
",scikit-learn-0.17.1-1/examples/ensemble/plot_feature_transformation.py,RPGOne/Skynet,1
"import math
import numpy as np

def main():
#    train = csv_io.read_data(""../Data/train.csv"")
    dataset = np.genfromtxt(open('../Data/train.csv','r'), delimiter=',', dtype='f8')[1:]
    target = np.array([x[0] for x in dataset])
    train = np.array([x[1:] for x in dataset])
#    test = csv_io.read_data(""../Data/test.csv"")

    rf = RandomForestClassifier(n_estimators=100)

    #Simple K-Fold cross validation. 5 folds.
    cv = cross_validation.KFold(len(train), k=5, indices=False)

    #iterate through the training and test cross validation segments and
    #run the classifier on each one, aggregating the results into a list
    results = []
    for traincv, testcv in cv:
        rf.fit(train[traincv], target[traincv])",BioResponse/Benchmarks/rf_benchmark.py,francis-liberty/kaggle,1
"# with the resulting classifier
# e.g.
# exec(open(""./train_big_d1.py"").read())


from reverse_game_of_life import *
import gzip
import pickle
from sklearn.ensemble import RandomForestClassifier

lc_rf64_w3_d1_40k = LocalClassifier(window_size=3,off_board_value=-1,clf=RandomForestClassifier(n_estimators=64,bootstrap=False,min_samples_split=256,max_features=20,n_jobs=16))
# train on 40k examples all for delta=1
ex_train = create_examples(40000,deltas=[1])
lc_rf64_w3_d1_40k.train(ex_train,use_transformations=True)",train_big_d1.py,valisc/reverse-game-of-life,1
"    while time_spent <= time_budget * 0.75:
        if quit_next_time:
            break
        # Exponentially scale the amount of data
        n_data = int(np.exp2(cycle + 4))
        if n_data >= D.data['X_train'].shape[0]:
            n_data = D.data['X_train'].shape[0]
            quit_next_time = True
        print('n_data = %d' % n_data)
        # Fit estimator to subset of data
        M = RandomForestClassifier(n_estimators=1000, min_samples_leaf=msl, n_jobs=1)
        M.fit(D.data['X_train'][:n_data, :], D.data['Y_train'][:n_data])
        M.fit(D.data['X_train'], D.data['Y_train'])
        vprint( verbose,  ""[+] Fitting success, time spent so far %5.2f sec"" % (time.time() - start))
        # Make predictions
        if 'X_valid' in D.data:
            Y_valid = M.predict_proba(D.data['X_valid'])[:, 1]
        if 'X_test' in D.data:
            Y_test = M.predict_proba(D.data['X_test'])[:, 1]
        vprint( verbose,  ""[+] Prediction success, time spent so far %5.2f sec"" % (time.time() - start))",automl.py,jamesrobertlloyd/automl-phase-1,1
"X = pd.read_csv('data/train.csv')
X = X.drop(['ROLE_CODE'], axis=1)
y = X['ACTION']
X = X.drop(['ACTION'], axis=1)
X_test = pd.read_csv('data/test.csv', index_col=0)
X_test = X_test.drop(['ROLE_CODE'], axis=1)
X_test['ACTION'] = 0
y_test = X_test['ACTION']
X_test = X_test.drop(['ACTION'], axis=1)

modelRF =RandomForestClassifier(n_estimators=1999, max_features='sqrt', max_depth=None, min_samples_split=9, compute_importances=True, random_state=SEED)#8803
modelXT =ExtraTreesClassifier(n_estimators=1999, max_features='sqrt', max_depth=None, min_samples_split=8, compute_importances=True, random_state=SEED) #8903
modelGB =GradientBoostingClassifier(n_estimators=50, learning_rate=0.20, max_depth=20, min_samples_split=9, random_state=SEED)  #8749
# 599: 20/90/08
#1999: 24/95/06

X_all = pd.concat([X_test,X], ignore_index=True)

# I want to combine role_title as a subset of role_familia and see if same results
X_all['ROLE_TITLE'] = X_all['ROLE_TITLE'] + (1000 * X_all['ROLE_FAMILY'])",BSMan/ensemble.py,wavelets/amazonaccess,1
"	train_features=train.values[:,1:]
	
	#target from training set
	train_target=train.ix[:,0]
	
	#pre-processing train features
	my_pca=PCA(n_components=0.90,whiten=True)
	pca_train_features=pca.fit_transform(train_features)

	#selecting feature using Random Forest
	rfc=RandomForestClassifier()
	rfc.fit(pca_train_features,train_target)
	final_train_features=rfc.transform(pca_train_features)
	
	#training SVM model
	model=SVC(kernel='rbf')

	#Grid search for model evaluation 
	C_power=[decimal.Decimal(x) for x in list(range(-5,17,2))]                                    
	gamma_power=[decimal.Decimal(x) for x in list(range(-15,5,2))]",Digit Recognizer/main/second_attempt.py,tranlyvu/kaggle,1
"counter = CountVectorizer()
counter.fit(rev_train)


#count the number of times each term appears in a document and transform each doc into a count vector
counts_train = counter.transform(rev_train)#transform the training data
counts_test = counter.transform(rev_test)#transform the testing data

#train classifier
#clf = LogisticRegression(fit_intercept=True, class_weight=None,intercept_scaling=1,tol=0.0001, random_state=None)
clf = RandomForestClassifier(n_estimators=2500, n_jobs=15,criterion=""entropy"",max_features='log2',random_state=150,max_depth=600,min_samples_split=163)
#train all classifier on the same datasets
clf.fit(counts_train,labels_train)

#use hard voting to predict (majority voting)
pred=clf.predict(counts_test)

#print accuracy
print (accuracy_score(pred,labels_test))",Machine Learning/NB.py,getmykhan/PythonforDataAnalytics,1
"        errors = []
        for train_index, test_index in kf:
            X_train, X_test = df.ix[train_index], df.ix[test_index]
            y_train, y_test = y[train_index], y[test_index]

            X_test = np.array(X_test)
            y_test = np.array(y_test)
            #X_test.index = range(0, len(X_test))
            #y_test.index = range(0, len(y_test))

            clf = ensemble.RandomForestClassifier(n_estimators=params[0],
                    max_depth=params[1])
            t = clf.fit(X_train, y_train)
            
            probs = clf.predict_proba(X_test)
            errors.append(evaluation.logloss(probs, y_test, t.classes_))

        error = np.average(errors)
        errors_for_params.append((error, params, y_test, probs, t.classes_))
        print(params)",otto/test_decision_forest.py,aufziehvogel/kaggle,1
"    else:
        y=y.copy()
    X,y,windowGen=getData(X,y,window,N)
    X=np.array([x[0] for x in X])
    X= np.array(list(map(lambda x:hog.compute(x).T[0] ,X)))
    return X,y,windowGen

    
X,y,windowGen=getF(X_train,y_train,window,10)
X,y=strap(X,y)
clf=RandomForestClassifier(n_estimators=30)
clf.fit(X,y)
DimDecressor=clf.feature_importances_>0.005*sum(clf.feature_importances_)
X=X[:,DimDecressor]

clf.fit(X,y)

#
joblib.dump(clf,'./model/question2.tree')
joblib.dump(DimDecressor,'./model/DimDecressorForQ2.arr')",PedestrianRecognition/Question2/question2.py,thautwarm/Recognition,1
"X_train = X[:,li_train,:]
X_validate = X[:,li_validate,:]
y_train = y[li_train]
y_validate = y[li_validate]
XX_train = X_train.reshape((X_train.shape[0]*X_train.shape[1],X_train.shape[2]))
XX_validate = X_validate.reshape((X_validate.shape[0]*X_validate.shape[1],X_validate.shape[2]))
yy_train = np.tile(y_train, n_augments)
yy_validate = np.tile(y_validate, n_augments)


clf = sklearn.ensemble.RandomForestClassifier(n_estimators=1000, max_depth=25, min_samples_leaf=3, n_jobs=16, random_state=42)
pcfilter = sklearn.feature_selection.SelectPercentile(sklearn.feature_selection.f_classif, percentile=100)

pipeline = sklearn.pipeline.Pipeline([('filter', pcfilter), ('clf', clf)])

parameters = {'filter__percentile': [75, 80, 85, 90, 95, 100]}

scorer = sklearn.metrics.make_scorer(sklearn.metrics.log_loss, greater_is_better=False, needs_proba=True)

grid_search = sklearn.grid_search.GridSearchCV(pipeline, parameters, scoring=scorer, n_jobs=1, cv=4, verbose=1)",crossval_cache.py,Neuroglycerin/neukrill-net-work,1
"for i in [1,10,20]:
    audio_clf = Pipeline([('clf',  KNeighborsClassifier(n_neighbors=i))])
    audio_clf.fit(tf[:train_size], data_terms[:train_size])
    prediction =  audio_clf.predict(tf[train_size:])
    print precision_recall_fscore_support(data_terms[train_size:], prediction, average=""micro"")
print ""knn done""

print ""Running random forest""

for i in [1,10,20]:
    clf = RandomForestClassifier(n_estimators=i)
    clf.fit(tf[:train_size], data_terms[:train_size])
    prediction =  clf.predict(tf[train_size:])
    print precision_recall_fscore_support(data_terms[train_size:], prediction, average=""micro"")
print ""random forest done""
'''",vlad.py,mayanks43/auto-tag,1
"
    model.fit(xtrain, ytrain)
    ytest_pred = model.predict(xtest)

    output = pd.DataFrame(data={'id': ytest, 'sentiment': ytest_pred})
    output.to_csv('submission.csv', index=False, quoting=3)

if __name__ == '__main__':
    xtrain, ytrain, xtest, ytest = load_data()

    model = RandomForestClassifier(n_estimators=400, n_jobs=-1)

    score_model(model, xtrain, ytrain)
    prepare_submission(model, xtrain, ytrain, xtest, ytest)",my_model.py,ddboline/kaggle_imdb_sentiment_model,1
"    @staticmethod
    def SklearnKNeighbours():
        return NltkClassifierWrapper(SklearnClassifier(KNeighborsClassifier()))

    @staticmethod
    def SklearnNearestCentroid():
        return NltkClassifierWrapper(SklearnClassifier(NearestCentroid()))

    @staticmethod
    def SklearnRandomForest():
        return NltkClassifierWrapper(SklearnClassifier(RandomForestClassifier()))

    @staticmethod
    def SklearnVoting():
        return NltkClassifierWrapper(SklearnClassifier(VotingClassifier(
            estimators=[
                ('Perceptron', Perceptron()),
                ('PassiveAggressiveClassifier', PassiveAggressiveClassifier()),
                ('SGDClassifier', SGDClassifier(loss='log'))
            ])))",analysis/textclassification/NltkClassifierFactory.py,dhermyt/WONS,1
"
print('random_forest')
sent_pos_slice = int(round(len(sent_pos_vals)*0.9))
sent_neg_slice = int(round(len(sent_neg_vals)*0.9))
train = sent_pos_vals[:sent_pos_slice] + sent_neg_vals[:sent_neg_slice]
train_values = []
test = sent_pos_vals[sent_pos_slice:] + sent_neg_vals[sent_neg_slice:]
test_sents = sent_pos_vals[sent_pos_slice:] + sent_neg_vals[sent_neg_slice:]
target = [1] * sent_pos_slice + [0] * sent_neg_slice
test_res = [1]*(len(sent_pos_vals) - sent_pos_slice) + [0]*(len(sent_neg_vals) - sent_neg_slice)
rf = RandomForestClassifier(n_estimators = 100, n_jobs=2)
print('training random forest')
train = pd.DataFrame(train)
train = train.fillna(0)
rf.fit(train, target)
test = pd.DataFrame(test)
test = test.fillna(0)
prediction = rf.predict(test)
score = 0
for i in range(len(prediction)-1):",test.py,denis-gordeev/CNN-aggression-RU,1
"    print()
    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time

results = []
for clf, name in (
        (RidgeClassifier(tol=1e-2, solver=""lsqr""), ""Ridge Classifier""),
        (Perceptron(n_iter=50), ""Perceptron""),
        (PassiveAggressiveClassifier(n_iter=50), ""Passive-Aggressive""),
        (KNeighborsClassifier(n_neighbors=10), ""kNN""),
        (RandomForestClassifier(n_estimators=100), ""Random forest"")):
    print('=' * 80)
    print(name)
    results.append(benchmark(clf))

for penalty in [""l2"", ""l1""]:
    print('=' * 80)
    print(""%s penalty"" % penalty.upper())
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,",sklearn_doc_classification_eg.py,linucks/textclass,1
"
    for _ in range(q):
        line = raw_input().strip().split()
        params.append(map(lambda x: float(x.split(':')[1]), line[1:]))
        names.append(line[0])

    sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
    # params = sel.fit_transform(params)

    # clf = svm.LinearSVC(max_iter = 3000, dual = False)
    clf = RandomForestClassifier(min_samples_split = 4, criterion = ""entropy"")

    params_normalized = preprocessing.normalize(params, axis = 0)

    params_scaled = preprocessing.scale(params_normalized)

    clf.fit(params_scaled[:-q], annotations)

    ans = clf.predict(params_scaled[-q:])
",ai/Machine-Learning/Quora-Answer-Classifier/main.py,m00nlight/hackerrank,1
"    for _ in range(T):
        line = f.readline().split()
        ans.append(line[0])
        X_.append(map(lambda x: float(x.split(':')[1]), line[1:]))
    X_ = np.array(X_)
    return X, y, X_, ans

if __name__ == '__main__':

    from sklearn.ensemble import RandomForestClassifier
    clf = RandomForestClassifier(n_estimators=10, max_depth= 7, random_state=0)
    params = { 'n_estimators': range(8,16), 'max_depth':range(1,11) }
    
    from sklearn import tree
    clf = tree.DecisionTreeClassifier(random_state=0)
    params = { 'max_depth':range(1,11) }
                
    X, y, X_, ans = read(stdin)
    m = ML(clf).params((X, y), X_=X_ , split=False)
",ml.py,nayanshah/python,1
"            Y[i][0] = 1 #TRUSTED
        pass   
      return Y


class classifier:
  def __init__(self):
    self.features = []
    self.classes = []
    #self.models = [GaussianNB(), DecisionTreeClassifier(), DecisionTreeClassifier(class_weight = 'balanced'), RandomForestClassifier(), RandomForestClassifier(class_weight = 'balanced'), LogisticRegression(), LogisticRegression(class_weight = 'balanced')]#, AdaBoostClassifier(), AdaBoostClassifier(DecisionTreeClassifier(class_weight = 'balanced'))]
    #self.modelnames = ['GaussianNB', 'DecisionTreeClassifier', 'DecisionTreeClassifier(balanced)', 'RandomForestClassifier', 'RandomForestClassifier(balanced)', 'LogisticRegression', 'LogisticRegression(balanced)']#, 'AdaBoostClassifier', 'AdaBoostClassifier(balanced)']
    
    
    self.models = [naive(), statistical(), GaussianNB(), DecisionTreeClassifier(class_weight = 'balanced'), RandomForestClassifier(class_weight = 'balanced'), LogisticRegression(class_weight = 'balanced')]#, AdaBoostClassifier(), AdaBoostClassifier(DecisionTreeClassifier(class_weight = 'balanced'))]
    self.modelnames = ['naive', 'statistical', 'GaussianNB', 'DecisionTreeClassifier', 'RandomForestClassifier', 'LogisticRegression(balanced)']#, 'AdaBoostClassifier', 'AdaBoostClassifier(balanced)']
    
    
    self.best_model = naive()
  
  def add(self, num1, num2):",src/python/blackbox_kmer.py,algomaus/PAEC,1
"#Training classifier (first argument to the script) against a train_path (second argument)

from my_fun import *
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.externals import joblib


train_path = sys.argv[2]

clfRandom = RandomForestClassifier(n_estimators=100)

clfRandom = train_obj(train_path, clfRandom)
#9 stands for maximum compression
joblib.dump(clfRandom, sys.argv[1], 9)",prensilia/classifiers_train.py,parloma/Prensilia,1
"	# now we should analyze, what is the minimum test score log loss for 0.2 learning rate
	min_log_loss = min(test_scores[3])
	min_log_loss_iter = test_scores[3].index(min_log_loss) + 1
	answer = str(np.round(min_log_loss, 2)) + ' ' + str(min_log_loss_iter)
	submission_file = open('submissions/min_log_loss.txt', 'w+')
	submission_file.write(answer)
	submission_file.close()
	print(answer)

	# now we should train random forest classifier and compare results
	model = RandomForestClassifier(random_state=241, n_estimators=min_log_loss_iter)
	model.fit(X_train, y_train)
	predictions = model.predict_proba(X_test)
	# predictions = [x[0] for x in predictions.tolist()] # unpack this stupid format
	# predictions = [1 / (1 + math.exp(-x)) for x in predictions]
	score = log_loss(y_test, predictions)
	answer = str(np.round(score, 2))
	submission_file = open('submissions/rfc_score.txt', 'w+')
	submission_file.write(answer)
	submission_file.close()",intro-into-ml/tasks/gbm/gbm.py,universome/courses,1
"print(""Eliminate SPDhits, which makes the agreement check fail"")
features = list(train.columns[1:-5])
print(""Train a UGradientBoostingClassifier"")
loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0)
clf = UGradientBoostingClassifier(loss=loss, n_estimators=50, subsample=0.1, 
                                  max_depth=6, min_samples_leaf=10,
                                  learning_rate=0.1, train_features=features, random_state=11)
clf.fit(train[features + ['mass']], train['signal'])
fb_preds = clf.predict_proba(test[features])[:,1]
print(""Train a Random Forest model"")
rf = RandomForestClassifier(n_estimators=250, n_jobs=-1, criterion=""entropy"", random_state=1)
rf.fit(train[features], train[""signal""])

print(""Train a XGBoost model"")
params = {""objective"": ""binary:logistic"",
          ""eta"": 0.2,
          ""max_depth"": 10,
          ""min_child_weight"": 1,
          ""silent"": 1,
          ""colsample_bytree"": 0.7,",ensemble_model.py,korbonits/kaggle-tau-to-three-muons,1
"# -*- coding: utf-8 -*-

from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn_porter import Porter


iris_data = load_iris()
X, y = iris_data.data, iris_data.target

clf = RandomForestClassifier(n_estimators=15, max_depth=None,
                             min_samples_split=2, random_state=0)
clf.fit(X, y)

output = Porter(clf).port()
# output = Porter(clf, language='java').export()
print(output)

""""""
class Brain {",examples/classifier/RandomForestClassifier/java/basics.py,nok/sklearn-porter,1
"
"""""" train random forest classifier with tf-idf recipe representations """"""
def train_wc(cv_n_fold=8, random_state=None, n_jobs=1, verbose=0):
    # use term frequencey inverse document frequencey feature representation
    wc_components = build_tfidf_wc(verbose=(verbose > 0))
    # prepare training set
    y_train = wc_components['train']['df']['cuisine_code'].as_matrix()
    X_train = wc_components['train']['features_matrix']
    # create random forest supervised classifier
    time_0 = time.time()
    clf = RandomForestClassifier(n_estimators=100, max_depth=None,
        n_jobs=n_jobs, random_state=random_state, verbose=verbose)
    if cv_n_fold > 0:
        # perform cross validation
        print 'cross validating %s ways...' % cv_n_fold
        scores_cv = cross_val_score(clf, X_train, y_train, cv=cv_n_fold, n_jobs=n_jobs)
        print 'accuracy: %0.5f (+/- %0.5f)' % (scores_cv.mean(), scores_cv.std() * 2)
    else:
        # fit random forest model
        print 'training random forest classifier...'",train_bow_rf.py,eifuentes/kaggle_whats_cooking,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.mllib.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = sqlContext.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = sqlContext.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0
    >>> numpy.argmax(result.probability)
    0",python/pyspark/ml/classification.py,pronix/spark,1
"            from sklearn.neighbors import KNeighborsClassifier
            estimator = KNeighborsClassifier(
                **self.kwargs)
        elif self.estimator == 'decision-tree':
            from sklearn.tree import DecisionTreeClassifier
            estimator = DecisionTreeClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'random-forest':
            from sklearn.ensemble import RandomForestClassifier
            estimator = RandomForestClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'adaboost':
            from sklearn.ensemble import AdaBoostClassifier
            estimator = AdaBoostClassifier(
                random_state=self.random_state,
                **self.kwargs)
        elif self.estimator == 'gradient-boosting':
            from sklearn.ensemble import GradientBoostingClassifier",imblearn/under_sampling/instance_hardness_threshold.py,glemaitre/UnbalancedDataset,1
"            The features that the model will be trained on
        version : str
            A version string representing the version of the model
        `**kwargs`
            Passed to :class:`sklearn.ensemble.RandomForestClassifier`
    """"""
    def __init__(self, features, *, version=None, rf=None,
                 **kwargs):

        if rf is None:
            rf = RandomForestClassifier(**kwargs)

        super().__init__(features, classifier_model=rf, version=version)
RFModel = RF
""Alias for backwards compatibility""",revscoring/scorer_models/rf.py,ToAruShiroiNeko/revscoring,1
"def apply_algorithm(paras, data):
    X = data[""X""]
    y = data[""y""]

    if paras['clf'] == 'svm':
        clf = svm.SVC(kernel=paras['svm'][1], C=paras['svm'][0], probability=True)
    elif paras['clf'] == 'knn':
        clf = neighbors.KNeighborsClassifier(paras['knn'][0],\
                                             weights=paras['knn'][1])
    elif paras['clf'] == 'rf':
        clf = RandomForestClassifier(max_depth=paras['rf'][0], \
                                     n_estimators=paras['rf'][1],\
                                     max_features=paras['rf'][2])
    elif paras['clf'] == 'lr':
        clf = linear_model.LogisticRegression(C=0.5)
    else:
        print str(""unknown classifier"") 
        sys.exit(2)
    
",python/change_features/methods.py,Healthcast/RSV,1
"               
        min_tree_depth 
              
        max_tree_depth 
              
        """"""
        self.name = name
        self.trees = trees
        self.min_tree_depth = min_tree_depth
        self.max_tree_depth = max_tree_depth
        self.forest = RandomForestClassifier(n_estimators=trees, max_depth=max_tree_depth)

    def train(self, training_data, training_labels):
        """"""
         
        Parameters
        ----------
        training_data 
              
        training_labels",decision_trees_ensemble.py,mkeyran/EmotionRecognizer,1
"dataPath = ""G:/vimFiles/python/kaggle/201405-Forest/src/outputs/data/""
outPath = ""G:/vimFiles/python/kaggle/201405-Forest/src/outputs/results/""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier

def loadData(datafile):
    return pd.read_csv(datafile)

def classifyRF(test, train, cols, target):
    clf = RandomForestClassifier(n_estimators=500, n_jobs=2)
    clf.fit(train[cols], train[target])

    with open(outPath+""submission-randomforest.csv"", ""wb"") as outfile:
        outfile.write(""Id,Cover_Type\n"")
        for index, value in enumerate(list(clf.predict(test[cols]))):
            outfile.write(""%s,%s\n"" % (test['Id'].loc[index], value))

def main():
",kaggle-forest-cover-type-prediction/src/classify-randomforest.py,KellyChan/Kaggle,1
"
# Sum up the counts of each vocabulary word
dist = np.sum(train_data_features, axis=0)

# For each, print the vocabulary word and the number of times it 
# appears in the training set
#for tag, count in zip(vocab, dist):
	#print count, tag

# Initialize a Random Forest classifier with 100 trees
forest = RandomForestClassifier(n_estimators = 100, n_jobs=4)

# Fit the forest to the training set, using the bag of words as 
# features and the sentiment labels as the response variable
#
# This may take a few minutes to run
forest = forest.fit( train_data_features, trained_ordered[""sponsored""] )


test = pd.read_csv(""./data/sampleSubmission.csv"", header=0, delimiter="","", quoting=3)",process.py,carlsonp/kaggle-TrulyNative,1
"    feature_importances.append(clf.feature_importances_) ;
    return learning_time,predicting_time,result
  

def Rforest_learn_predict(gid, X, Y,weight, labels, k_folds, random_forest_trees ,plot_directory): 
    from sklearn.metrics import classification_report 
    import datetime;
    scaler =  preprocess_data(X); 
    
    #creating the random forest object
    clf = RandomForestClassifier(random_forest_trees, criterion=""entropy"" ,min_samples_leaf=20) ; 
    
    #cutting the set into 10 pieces, then propossing 10 partiion of 9(trainng)+1(test) data
    kf_total = cross_validation.KFold(len(X), n_folds = k_folds, shuffle = True, random_state = 4) ;
    result = pd.DataFrame() ;
    feature_importances = [] ;
    learning_time = 0.0 ;
    predicting_time = 0.0 ;
    
    ",script/loading benchmark/rforest_on_patch_lean.py,Remi-C/LOD_ordering_for_patches_of_points,1
"xx = []
for x in X:

    xx.append(np.histogram(x, density=True, range=r)[0])

    print(xx[-1])
X = np.asarray(xx)
y = np.asarray(y)

# clf = LinearRegression()
clf = RandomForestClassifier(n_estimators=20)
# clf = SVC(gamma=0.001, kernel='rbf', C=100)

skf = StratifiedKFold(y, n_folds=2)
for train_index, test_index in skf:
    print(""Detailed classification report:"")
    print()
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    clf.fit(X_train, y_train)",tests/lr.py,schae234/gingivere,1
"
Q = int(raw_input())
questions = dict(parse_q(raw_input()) for _ in xrange(Q))

## train here

D = int(raw_input())
training = (parse_d(raw_input()) for _ in xrange(D))

X, Y = zip(*((reduce_pair(k1, k2), -1 if score < 1 else 1) for k1,k2,score in training))
base_clf = RandomForestClassifier(n_estimators=7, min_samples_leaf=4) 

clf = Pipeline([
    ('classification', BaggingClassifier(base_estimator=base_clf, random_state=1, n_estimators=16))
]).fit(X, Y)

## predict here

N = int(raw_input())
testing = (parse_t(raw_input()) for _ in xrange(N))",quora_duplicate.py,py-in-the-sky/challenges,1
"target_train = train_y

prob = 1
#Naive Bayes 
nb_estimator = GaussianNB()
nb_estimator.fit(features_train, target_train)
cal_score(""NAIVE BAYES CLASSIFICATION"",nb_estimator, features_test, target_test)
#predictions = nb_estimator.predict(test)
#SVC Ensemble

rf = RandomForestClassifier(n_estimators=100)
rf = rf.fit(features_train, target_train)
cal_score(""RANDOM FOREST CLASSIFIER"",rf, features_test, target_test)
predictions = rf.predict_proba(test)
print predictions

#Gradient Boosting
gb = GradientBoostingClassifier(n_estimators=100, subsample=.8)
params = {
		'learning_rate': [.05, .1,.2,.3,2,3, 5],",opencosmics.py,sidgan/opencosmics,1
"import numpy as np


def main():
    # read in  data, parse into training and target sets
    dataset = np.genfromtxt(open('Data/train.csv','r'), delimiter=',', dtype='f8')[1:]
    target = np.array([x[0] for x in dataset])
    train = np.array([x[1:] for x in dataset])

    # In this case we'll use a random forest, but this could be any classifier
    cfr = RandomForestClassifier(n_estimators=3000)

    # Simple K-Fold cross validation. 5 folds.
    cv = cross_validation.KFold(len(train), 5, False)

    # iterate through the training and test cross validation segments and
    # run the classifier on each one, aggregating the results into a list
    results = []
    for traincv, testcv in cv:
        probas = cfr.fit(train[traincv], target[traincv]).predict_proba(train[testcv])",BioResponse/crossValidate.py,kraktos/Kaggling,1
"    gid_arr, X,Y_tmp = pg_to_np(gid, feature, gt_weight_vect); 
   
    #preprocess : scale
    from sklearn import preprocessing ; 
    scaler = preprocessing.StandardScaler(copy=False,with_std=False); 
    scaler.fit_transform(X) ; 
    
    #take care of labels
    #create rforest
    #Y = np.ceil(Y_tmp).astype(int) 
    clf = RandomForestClassifier(n_estimators, criterion='entropy', min_samples_split=20 , compute_importances=True)
    Y = Y_tmp
    clf =  RandomForestRegressor(n_estimators, criterion='mse',  min_samples_split=2 , random_state=4  )
    
    kf_total =  KFold(len(X), n_folds = k_folds, indices = True, shuffle = True, random_state = 4) ;    
    
    reports = [];
    for i ,(train, test)  in enumerate(kf_total) :
        X_train, X_test, Y_train, Y_test = X[train],X[test], Y[train], Y[test] ;   
        #learning",script/loading benchmark/rforest_regressor.py,Remi-C/LOD_ordering_for_patches_of_points,1
"# ----------setup models----------

svc = svm.SVC(
    cache_size=1000,
    kernel='linear',
    probability=True,
    random_state=random_state,
    C=0.10)
ovr_svc = OneVsRestClassifier(estimator=svc, n_jobs=1)

rfc = RandomForestClassifier(
    n_jobs=-1,
    n_estimators=8000,
    min_samples_split=2,
    random_state=random_state,
    verbose=0)
ovr_rfc = OneVsRestClassifier(estimator=rfc, n_jobs=1)

lrc = LogisticRegressionCV(
    Cs=15,",scripts/train_ml.py,bzshang/yelp-photo-classification,1
"
# ""Whiten"" the data (both test and training)
from sklearn.preprocessing import StandardScaler 
scaler = StandardScaler()
scaler.fit(Xtrain)  # Use the full training set now
XStrain = scaler.transform(Xtrain)
XStest = scaler.transform(Xtest)

# Instantiate the RF classifier
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=10, max_depth=15, min_samples_split=2, n_jobs=-1, random_state=42)
rfc.fit(XStrain, ytrain)

# Determine RF classifications for the test set
ypredRFC = rfc.predict(XStest)


# Instantiate the SVM classifier
from sklearn.svm import SVC
svm = SVC(random_state=42)",SpIESHighzQuasarsS82all.py,gtrichards/QuasarSelection,1
"    dist = np.sum(train_data_features, axis=0)
    
    # For each, print the vocabulary word and the number of times it 
    # appears in the training set
    for tag, count in zip(vocab, dist):
        print count, tag

    print ""Training the random forest...""
    
    # Initialize a Random Forest classifier with 100 trees
    forest = RandomForestClassifier(n_estimators = 100) 
    
    # Fit the forest to the training set, using the bag of words as 
    # features and the sentiment labels as the response variable
    #
    # This may take a few minutes to run
    forest = forest.fit(train_data_features, train[""sentiment""])
    print forest
    
    # Read the test data",mycode/parse.py,prajitdas/DeepLearningMovies,1
"	songDF = sqlContext.createDataFrame(rdd, schema)

	# Register the DataFrame as a table.
	songDF.registerTempTable(""genclass"")
	labelIndexer = StringIndexer().setInputCol(""genre"").setOutputCol(""indexedLabel"").fit(songDF)

	trainingData, testData = songDF.randomSplit([0.8, 0.2])

	labelConverter = IndexToString().setInputCol(""prediction"").setOutputCol(""predictedLabel"").setLabels(labelIndexer.labels)

	rfc = RandomForestClassifier().setMaxDepth(10).setNumTrees(2).setLabelCol(""indexedLabel"").setFeaturesCol(""features"")
	#rfc = SVMModel([.5, 10, 20], 5)
	#rfc = LogisticRegression(maxIter=10, regParam=0.01).setLabelCol(""indexedLabel"").setFeaturesCol(""features"")

	pipeline = Pipeline(stages=[labelIndexer, rfc, labelConverter])
	model = pipeline.fit(trainingData)

	predictions = model.transform(testData)
	predictions.show()
",src/genre_classification.py,Sunhick/music-cognita,1
"        calibrated_log_loss = log_loss(y_test, probas)
        assert_greater_equal(uncalibrated_log_loss, calibrated_log_loss)

    # Test that calibration of a multiclass classifier decreases log-loss
    # for RandomForestClassifier
    X, y = make_blobs(n_samples=100, n_features=2, random_state=42,
                      cluster_std=3.0)
    X_train, y_train = X[::2], y[::2]
    X_test, y_test = X[1::2], y[1::2]

    clf = RandomForestClassifier(n_estimators=10, random_state=42)
    clf.fit(X_train, y_train)
    clf_probs = clf.predict_proba(X_test)
    loss = log_loss(y_test, clf_probs)

    for method in ['isotonic', 'sigmoid']:
        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)
        cal_clf.fit(X_train, y_train)
        cal_clf_probs = cal_clf.predict_proba(X_test)
        cal_loss = log_loss(y_test, cal_clf_probs)",scikit-learn-0.18.1/sklearn/tests/test_calibration.py,RPGOne/Skynet,1
"
def writeSensAndSpec(fpr, tpr, thresh, filename):
    specificity = 1 - fpr
    a = numpy.vstack([specificity, tpr, thresh])
    b = numpy.transpose(a)
    numpy.savetxt(filename, b, fmt='%.5f', delimiter=',')


def doRUSRFC(analysisDict):
    print('{0} Started'.format(analysisDict['analysisName']))
    RUSRFC = RUSRandomForestClassifier.RUSRandomForestClassifier(n_Forests=200, n_TreesInForest=300)
    predClasses, classProb, featureImp, featureImpSD = RUSRFC.CVJungle(analysisDict['X_train'], analysisDict['Y_train'],
                                                                       shuffle=True, print_v=True, k=10)
    cm = confusion_matrix(analysisDict['Y_train'], predClasses)
    print('Analysis - {0}  - Validation - \n{1}'.format(analysisDict['analysisName'], cm))

    #### For test set
    predClasses_test = RUSRFC.predict(analysisDict['X_test'])
    cm_test = confusion_matrix(analysisDict['Y_test'], predClasses_test)
    print('Test set results - \n{0}'.format(cm_test))",Python/RUSRandomForest/runClassificationCSF_Only.py,sulantha2006/Conversion,1
"    BusTrainFeatureIndices = list(xrange(19,22))
    logging.debug(""generic features = %s"" % genericFeatureIndices)
    logging.debug(""advanced features = %s"" % AdvancedFeatureIndices)
    logging.debug(""location features = %s"" % LocationFeatureIndices)
    logging.debug(""time features = %s"" % TimeFeatureIndices)
    logging.debug(""bus train features = %s"" % BusTrainFeatureIndices)
    return genericFeatureIndices + BusTrainFeatureIndices

  def buildModelStep(self):
    from sklearn import ensemble
    forestClf = ensemble.RandomForestClassifier()
    model = forestClf.fit(self.selFeatureMatrix, self.cleanedResultVector)
    return model

  def generateFeatureMatrixAndIDsStep(self, sectionQuery):
    toPredictSections = self.Sections.find(sectionQuery)
    logging.debug(""Predicting values for %d sections"" % toPredictSections.count())
    featureMatrix = np.zeros([toPredictSections.count(), len(self.featureLabels)])
    sectionIds = []
    sectionUserIds = []",emission/analysis/classification/inference/mode.py,yw374cornell/e-mission-server,1
"	data_test = utils.substract_mean(data_test, mean_image)

	#compute the covariance matrix in 2D space
	SA = compute_covariance_matrix(normalized_data)

	#find eigen values & eigen vectors of covariance matrix
	U, s, _ = np.linalg.svd(SA)

	#extract features using 2DPCA
	selected = []
	clf = RandomForestClassifier(n_estimators=300, n_jobs=-1)
	max_acc = 0.0

	for i in xrange(ncol):
		proj_dir = U[:, i].reshape(ncol, 1)
		tempTrainX = extract_feature(data_train, proj_dir)
		tempTestX = extract_feature(data_test, proj_dir)

		clf.fit(tempTrainX, trainY)
		pred = clf.predict(tempTestX)",2DPCA/2DPCA.py,akhilpm/Masters-Project,1
"    return match/len(output)

def _train_and_predict_sk_rf(train_ex, train_lab, pred_ex, verbose):
    """"""Train and predict.
    Package: sklearn
    Algorithm: random forest
    """"""
    if verbose:
        print('Training sklearn random forest on %d examples with %d features'
              % (train_ex.shape))
    model = RandomForestClassifier()
    model.fit(train_ex, train_lab)
    if verbose:
        print('Predicting using sklearn random forest on %d examples with %d features'
              % (pred_ex.shape))
    pred = model.predict(pred_ex)
    return pred

def _train_and_predict_sk_svm(train_ex, train_lab, pred_ex, bandwidth, reg_param, verbose):
    """"""Train and predict.",CrossVal/classifier.py,chengsoonong/didbits,1
"
  print('- Unique family size: %s' % df.FamilySize.unique())
  for family in df.FamilySize.unique():
    counts = len(df[df.FamilySize == family])
    print('%d familylings: %d times, rate: %f' % (family, counts, len(df[(df.FamilySize == family) & (df.Survived == 1)]) / counts))

  print('... done printing statistics!')
  print(SEPARATOR)

def run_prediction(train, test):
  forest = RandomForestClassifier(n_estimators=100)
  forest = forest.fit(train[0::,1::], train[0::,0] )

  return forest.predict(test).astype(int)

def write_predictions(ids, predictions):
  with open('prediction.csv', 'wt') as predictions_file:
    open_file_object = csv.writer(predictions_file)
    open_file_object.writerow(['PassengerId','Survived'])
    open_file_object.writerows(zip(ids, predictions))",titanic/submissions/2/randomforest.py,furgerf/kaggle-projects,1
"        #print ""1 = Bread""
        #print ""2 = Nonbread""

        train = np.vstack((breadtrain,nonbreadtrain))
        labels = np.hstack((labelsbtr[:,0],labelsnbtr[:,0]))
        test = np.vstack((breadtest,nonbreadtest))

        lin_svc = svm.LinearSVC(C=1.0).fit(train, labels)
        predictionsSVM = lin_svc.predict(test)

        cfr = RandomForestClassifier(n_estimators=120)
        cfr.fit(train,labels) # train

        gtruth = np.hstack((labelsbte[:,0],labelsnbte[:,0]))
        predictionsRF = cfr.predict(test) # test

        print dirListbte
        print dirListbtr
        print ""Random Forest Prediction:""
        print predictionsRF[:cantTestB]",tests/testcomparison.py,rbaravalle/imfractal,1
"                Y = np.array(Y_combined)
                W = np.array(W_combined)
                print ""# imm = %d, # non = %d"" % (len(imm), len(non))
                print ""Data shape"", X.shape, ""n_true"", np.sum(Y)
                
                rf = BalancedEnsembleClassifier(n_estimators = 200)
                #aucs = sklearn.cross_validation.cross_val_score(
	        #  rf, X, Y, cv = 10, scoring='roc_auc')
		#print ""CV AUC %0.4f (std %0.4f)"" % (np.mean(aucs), np.std(aucs))
                #d['cv_auc'].append(np.mean(aucs))
                #rf = RandomForestClassifier(n_estimators = 100)
                rf.fit(X, Y, W)
                def predict(peptides):
                    Y_pred = np.zeros(len(peptides), dtype=float)
                    counts = np.zeros(len(peptides), dtype=int)
                    X_test, _, Indices = expand(peptides)
                    X_test = strings_to_array(X_test)
                    #Y_pred_raw = rf.predict(X_test)

                    Y_pred_prob = rf.predict_proba(X_test)[:, 1]",May13_sliding_kmers.py,hammerlab/immuno_research,1
"        k_neighboors = KNeighborsClassifier()
        n_neighbors = [3, 5, 11, 21, 31]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(k_neighboors, dict(n_neighbors=n_neighbors), X, y)
        f = open('output/ti.knn.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates Random Forest classifier
        random_forest = RandomForestClassifier()
        n_estimators = [2, 3, 5, 10, 20, 40, 60]
        (targets, accuracy, precision, recall, f1_score) = evaluation.run(random_forest, dict(n_estimators=n_estimators), X, y)
        f = open('output/ti.randomforest.out', 'a')
        f.write(""target_names,accuracy,precision,recall,f1_score\n"")
        for t, a, p, r, f1 in zip(targets, accuracy, precision, recall, f1_score):
            f.write(""%s,%.2f,%.2f,%.2f,%.2f\n"" % (t, a, p, r, f1))
        f.close()

        # Evaluates MLP classifier",tests/test_ti.py,fberanizo/author-profiling,1
"print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
#Y_train = np_utils.to_categorical(y_train, nb_classes)
#Y_test = np_utils.to_categorical(y_test, nb_classes)
Y_train = [int(y) for y in y_train]
Y_test = [int(y) for y in y_test]

clf = RandomForestClassifier(n_estimators=100, max_depth=3)
clf = clf.fit(X_train, y_train)

yt_predict = clf.predict_proba(X_train)
y_predict = clf.predict_proba(X_test)
print(log_loss(Y_train, yt_predict))
print(log_loss(Y_test, y_predict))
predict=[]
for yy in y_predict: predict.append(np.argmax(yy))
#print(predict)",mnist/sklearn/rf.py,choupi/NDHUDLWorkshop,1
"    print(""\r--- Completed {:,} out of {:,}"".format(completed, num_tasks), end='')
    sys.stdout.flush()
    time.sleep(1)
    if (completed == num_tasks): break
p.close()
p.join()
df_full = pd.DataFrame(list(results))
print()

print('--- Training random forest')
clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)
train = df_full[df_full.sponsored.notnull()].fillna(0)
test = df_full[df_full.sponsored.isnull() & df_full.file.isin(test_files)].fillna(0)
clf.fit(train.drop(['file', 'sponsored'], 1), train.sponsored)

print('--- Create predictions and submission')
submission = test[['file']].reset_index(drop=True)
submission['sponsored'] = clf.predict_proba(test.drop(['file', 'sponsored'], 1))[:, 1]",run.py,owlz84/dato,1
"        data = np.array(['a', 'b', 'a', 'b', 'b', 'b', 'b', 'a', 'c', 'c', 
                         'b', 'c', 'a'], dtype='O')
        fig = plot_simple_histogram(data, verbose=False)
        self.add_fig_to_report(fig, 'plot_simple_histogram_categorical')

    def test_plot_prec_recall(self):
        M, labels = uft.generate_correlated_test_matrix(1000)
        M_train, M_test, labels_train, labels_test = train_test_split(
                M, 
                labels)
        clf = RandomForestClassifier(random_state=0)
        clf.fit(M_train, labels_train)
        score = clf.predict_proba(M_test)[:,-1]
        fig = dsp.plot_prec_recall(labels_test, score, verbose=False)
        self.add_fig_to_report(fig, 'plot_prec_recall')

    def test_plot_roc(self):
        M, labels = uft.generate_correlated_test_matrix(1000)
        M_train, M_test, labels_train, labels_test = train_test_split(
                M, ",tests/test_display.py,dssg/diogenes,1
"        return mode([c.classify(test_X) for c in self._classifiers]).mode[0]



#from . import KNNClassifier, SVMClassifier, RandomForestClassifier, test_classifier

#test_classifier(VoteClassifier(
#    KNNClassifier(3),
#    KNNClassifier(10),
#    SVMClassifier(),",classifier/VoteClassifier.py,kosigz/DiabetesPatientReadmissionClassifier,1
"        target_resample = (int(counts[0]/self.downsample_bl_factor),counts[1]*self.upsample_seizure_factor)
        self.update_label_above2.emit('Resampling [BL S] from '+str(counts)+' to ' + str(list(target_resample)))
        self.res_y, self.res_x = self.clf.resample_training_dataset(self.clf.labels, self.clf.features,
                                                      sizes = target_resample)

    def run(self):

        '''
        # think you should just be using clf train method here
        self.update_progress_label.emit('Training Random Forest...')
        self.clf.rf =  RandomForestClassifier(n_jobs=self.n_cores, n_estimators= self.ntrees, oob_score=True, bootstrap=True)
        self.clf.rf.fit(self.res_x, np.ravel(self.res_y))
        self.update_progress_label.emit('Getting Hidden Markov Model params...')
        self.clf.make_hmm_model() # this has default fold arguement of 3

        print ('********* oob results on resampled data  - not including HMM *******')
        self.oob_preds = np.round(self.clf.rf.oob_decision_function_[:,1])
        print('ROC_AUC score: '+str(metrics.roc_auc_score(np.ravel(self.res_y), self.clf.rf.oob_decision_function_[:,1])))
        print('Recall: '+str(metrics.recall_score(np.ravel(self.res_y), self.oob_preds)))
        print('F1: '+str(metrics.f1_score(np.ravel(self.res_y), self.oob_preds)))",pyecog/visualisation/subwindows.py,jcornford/pyecog,1
"
  # Get supports on test-set
  support_generator = SupportGenerator(
      test_dataset, range(len(test_dataset.get_task_names())), n_pos, n_neg,
      n_trials, replace)

  # Compute accuracies
  task_scores = {task: [] for task in range(len(test_dataset.get_task_names()))}
  for (task, support) in support_generator:
    # Train model on support
    sklearn_model = RandomForestClassifier(
        class_weight=""balanced"", n_estimators=50)
    model = SklearnModel(sklearn_model, model_dir)
    model.fit(support)

    # Test model
    task_dataset = get_task_dataset_minus_support(test_dataset, support, task)
    y_pred = model.predict_proba(task_dataset)
    score = metric.compute_metric(
        task_dataset.y, y_pred, task_dataset.w)",examples/low_data/tox_rf_K_fold.py,bowenliu16/deepchem,1
"

# *********************************
# Choose the model
# *********************************

# Linear Classifier
#clf = SGDClassifier(loss=""modified_huber"")

# Random forest
clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)


# *********************************
#  Evaluate Model
# *********************************

X_train, X_test, y_train, y_test = train_test_split(train_features[feature_list], train_labels, test_size=0.1, random_state=0)

clf.fit(X_train, np.ravel(y_train))",examples/remigius-thoemel/titanic-iteration-1.py,remigius42/code_camp_2017_machine_learning,1
"
	print ""starting random_forest classifier...""

	# TUNING PARAMS
	n_estimators = 1000
	min_samples_leaf = 50
	max_features = ""log2"" # normally auto = sqrt(n_features)

	print ""fitting data...""

	rf = RandomForestClassifier(n_estimators = n_estimators, 
								max_features = max_features, 
								min_samples_leaf = min_samples_leaf)
	rf.fit(x_train, y_train)

	print ""predicting data...""

	pred = rf.predict(x_test)

	pct = 0.0",skynetNLP.py,ahad-s/getting-rich-with-rnn-nlp-stocks,1
"
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

models = ({
    # 'logreg': LogisticRegression(),
    # 'svc': SVC(),
    'random forest': RandomForestClassifier(),
    # 'knn': KNeighborsClassifier(n_neighbors=3),
    # 'GaussinNB': GaussianNB()
})

paramsDef = ({
    # 'logreg': { 'name': 'C', 'data': [1, 3, 10, 30, 100, 300] },
    # 'svc': { 'name': 'C', 'data': [1, 3, 10, 30, 100, 300] },
    'random forest': { 'name': 'n_estimators', 'data': [10, 20, 50, 100, 200] },
    # 'knn': { 'name': 'leaf_size', 'data': [30, 60, 90, 120, 150, 180] },",titanic/my.py,netssfy/kaggle-lab,1
"        self.SetStartDate(2013, 10, 7)  #Set End Date
        self.AddEquity(""SPY"", Resolution.Daily)

        # numpy test
        print ""numpy test >>> print numpy.pi: "" , numpy.pi
        
        # scipy test: 
        print ""scipy test >>> print mean of 1 2 3 4 5:"", scipy.mean(numpy.array([1, 2, 3, 4, 5]))

        #sklearn test
        print ""sklearn test >>> default RandomForestClassifier:"", RandomForestClassifier()
        
        # cvxopt matrix test
        print ""cvxopt >>>"", cvxopt.matrix([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], (2,3))

        # talib test
        print ""talib test >>>"", talib.SMA(numpy.random.random(100))

        # blaze test
        blaze_test()",Algorithm.Python/PythonPackageTestAlgorithm.py,andrewhart098/Lean,1
"            np.hstack(training_label).astype(int), [0, 255]))
        print 'Create the training set ...'

        # Learn the PCA projection
        ica = FastICA(n_components=sp, whiten=True)
        training_data = ica.fit_transform(training_data)
        testing_data = ica.transform(testing_data)

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_sp.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/selection-extraction/ica/pipeline_classifier_dce.py,I2Cvb/mp-mri-prostate,1
"from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.pipeline import Pipeline
from sklearn.grid_search import GridSearchCV, IterGrid
from sklearn.cross_validation import LeavePLabelOut, KFold, LeaveOneOut, cross_val_score, StratifiedKFold
from itertools import islice

models = [('Naive', DummyClassifier(), {}),
          ('GradBoost', GradientBoostingClassifier(), {}),
          ('DecisionTree', DecisionTreeClassifier(), {'model__criterion':['gini', 'entropy']}),

          ('RandomForest', RandomForestClassifier(), {
                                                      'model__criterion':['gini', 'entropy'],
                                                      'model__n_estimators':[10, 20, 50]
                                                      }),
          ('ExtraTrees', ExtraTreesClassifier(), {
                                                  'model__criterion':['gini', 'entropy'],
                                                  'model__n_estimators':[10, 20, 50]
                                                  }),
          ('KNN', KNeighborsClassifier(warn_on_equidistant=False,
                                       weights='distance'), {",NewCannabAnalysis.py,JudoWill/ResearchNotebooks,1
"    test_id.extend(yes_test)
    test_id.extend(no_test)
    str_test_id = '(' + ','.join(test_id) + ')'    
    x_test = db.get_feature_by_row(str_test_id)
    y_test = [1] * 25 + [0] * 25
    
    rowIdx.test50_data_x = test_id
    rowIdx.test50_data_y = y_test
        
# Case 1
    m1 = RandomForestClassifier()
    m1.fit(x_train, y_train)
    y_pred = m1.predict(x_test)
    fsc1 = f1_score(y_test, y_pred)
    
# Case 2
    m2_x_train = []
    m2_y_train = []
    str_measure_id = '(' + ','.join(measure_data) + ')'
    y_training_measure = db.get_label_data(str_measure_id)",filter01.py,chaluemwut/filter01,1
"        seed = 7
        X_train, X_test, Y_train, Y_test = model_selection.train_test_split(
                X_scaled_classified, Y_classified, test_size=test_size,
                random_state=seed)

        # Test options and evaluation metric
        scoring = 'accuracy'

        # Spot Check Algorithms
        models = []
        models.append(('RF10',RandomForestClassifier(n_estimators=10)))
        models.append(('RF64',RandomForestClassifier(n_estimators=64)))
        models.append(('RF80',RandomForestClassifier(n_estimators=80)))
        models.append(('RF100',RandomForestClassifier(n_estimators=100)))
        models.append(('RF120',RandomForestClassifier(n_estimators=120)))
              
        # evaluate each model in turn
        results = []
        names = []
        for name, model in models:",reservoir-id/classifier_train.py,kysolvik/reservoir-id,1
"
iris = datasets.load_iris()


parameters = {
    'n_estimators': [1, 10, 50, 100],
    'criterion': ['gini', 'entropy'],
    'max_features': ['sqrt', 'log2'],
}

est = RandomForestClassifier()
clf = GridSearchCV(est, parameters, cv=5)

#clf.fit(iris.data, iris.target)
data = datasets.make_classification(1000, 10, 5, class_sep=0.7)
clf.fit(data[0], data[1])

grid_scores = clf.grid_scores_

# changing numeric parameter without any restrictions",examples/grid_search.py,edublancas/sklearn-evaluation,1
"    def __init__(self, verbose=False):
        self.Verbose = verbose
        self.__Choices = [] #list to store history of choices
        self.__Scores = {} #dictionary to store results against each cat
        self.__Rounds = 0
        self.__Options = ['rock', 'paper', 'scissors'] #possible options
        self.__RandomCatLimit = 5
        self.__Cats = [('Random Cat'), 
                       ('Logistic Cat', LogisticRegression()), 
                       ('Naive Bayes Cat', BernoulliNB()), 
                       ('Random Forest Cat', RandomForestClassifier()),
                       ('XGBoost Cat', XGBClassifier())
                      ]
        self.__Outcomes = {0:'win', 1:'draw', 2:'lose'}
        for i, c in enumerate(self.__Cats):
            self.__Scores[i] = [0,0,0]
    @staticmethod
    def __evalscore(p1, p2):
        if p1==p2:
            return 1",rps.py,bartekpi/rps,1
"training_label = dataset['Survived']

#Feature Engineering
training_data = training_data.fillna(0)
#training_data['Name'] = LabelEncoder().fit_transform(training_data['Name'])
training_data['Sex'] = LabelEncoder().fit_transform(training_data['Sex'])
#training_data['Ticket'] = LabelEncoder().fit_transform(training_data['Ticket'])
training_data['Cabin'] = LabelEncoder().fit_transform(training_data['Cabin'])
training_data['Embarked'] = LabelEncoder().fit_transform(training_data['Embarked'])

model = RandomForestClassifier()
model.fit(training_data, training_label)

y_pos = np.arange(len(features))
plt.barh(y_pos, model.feature_importances_, align='center', alpha=0.4)
plt.yticks(y_pos, features)
plt.xlabel('features')
plt.title('feature_importances')
#plt.show()
",src/kaggle/titanic/my_second_kaggle.py,del680202/MachineLearning-memo,1
"test_dataset = fold_datasets[-1]

# Get supports on test-set
support_generator = dc.data.SupportGenerator(
    test_dataset, n_pos, n_neg, n_trials)

# Compute accuracies
task_scores = {task: [] for task in range(len(test_dataset.get_task_names()))}
for (task, support) in support_generator:
  # Train model on support
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=100)
  model = dc.models.SklearnModel(sklearn_model)
  model.fit(support)

  # Test model
  task_dataset = dc.data.get_task_dataset_minus_support(
      test_dataset, support, task)
  y_pred = model.predict_proba(task_dataset)
  score = metric.compute_metric(",examples/low_data/tox_rf_one_fold.py,lilleswing/deepchem,1
"rfe = rfe.fit(trainData[0::, 1::], trainData[0::, 0])
# summarize the selection of the attributes
print(rfe.support_)
print(rfe.ranking_)

trainData = SelectKBest(f_classif, k=3).fit_transform(trainData[0::,1::], trainData[0::,0])
print 'F Values: ', f_classif(trainData[0::,1::], trainData[0::,0])[0]
print 'P Values: ', f_classif(trainData[0::,1::], trainData[0::,0])[1]
'''
print 'Training...'
forest_v = RandomForestClassifier(max_depth=16,n_estimators=150, n_jobs=-1)
%time forest = forest_v.fit(trainData[0::,1::], trainData[0::,0])

#trainData_double = np.concatenate((trainData, trainData))
#%time forest = forest_v.fit(trainData_double[0::,1::], trainData_double[0::,0])

#trainData_quad = np.concatenate((trainData_double, trainData_double))
#%time forest = forest_v.fit(trainData_quad[0::,1::], trainData_quad[0::,0])

# Feature importances",cdsw_usecases/06_sfo_crimerate/01_sfo_crimerate_RF.py,ravi9/aaisg-cdsw,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.ml.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = spark.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> model.featureImportances
    SparseVector(1, {0: 1.0})
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0",python/pyspark/ml/classification.py,gioenn/xSpark,1
"    score = svcClf.score(validData, validLabel)
    sleep(1)
    print(""Mean validation accuracy: "" + str(score))
    pred_test = svcClf.predict_proba(testData)

    return pred_test


def rf(traindata, trainlabel, validData, validLabel, testData):
    print(""Start training Random Forest...\n"" + SEPARATOR)
    rfClf = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=-1, random_state=RDM, verbose=20,
                                   criterion='gini')
    rfClf.fit(traindata, trainlabel)

    score = rfClf.score(validData, validLabel)
    sleep(1)
    print(""Mean validation accuracy: "" + str(score))

    pred_test = rfClf.predict_proba(testData)
",Python-scripts/CNN-to-Machine-Learning.py,mmaguero/Intel-mobileodt-cervical-cancer-screening,1
"#    * min_weight_fraction_leaf
#    * max_leaf_nodes

# 1.) Number of Trees

# In[8]:

score=0
scores=[]
for feature in range(50,1001,50):
    clf = RandomForestClassifier(n_estimators=feature, oob_score=True, random_state=7112016)
    clf.fit(titanic_features,titanic_target)
    score_test = clf.oob_score_
    scores.append(score_test)
    if score_test>score:
        n_out=feature
        score_diff=score_test-score
        score=score_test

",Final_setup_Random_Forest.py,aaschroeder/Titanic_example,1
"tags = {1:'sky', 2:	'greenery' , 3:'building'}#, 4:'crowd'}
tc = 3

#LBP Features
radius = 1
n_points = 8
METHOD = 'default'

a = numpy.zeros(shape = (tc*20, 15000))

dt = ensemble.RandomForestClassifier()
#dt = tree.DecisionTreeClassifier()

def trainSys():
	global a
	global tags
	i = 0
	print ""Starting training...""

	directory = ""/home/leroy/Desktop/ATI - Mini/datasets/""",ATI_ML.py,leroyjvargis/ATI-ML,1
"class SkLearnWrapper(base_wrapper.BaseWrapper):
    """"""docstring for SkLearnWrapper""""""
    data = {}
    def __init__(self, config):
        super(SkLearnWrapper, self).__init__()
        self.config = config

    def __create_sk_instance(self, config):
        if(config.implementation == 'scikit' or config.implementation == 'all'):
            if config.mode == 'classification':
                instance = ensemble.RandomForestClassifier(n_estimators=config.trees, 
                                                     criterion=config.splitting_criterion.lower(),
                                                     max_features=config.features,
                                                     min_samples_split=config.min_split,
                                                     n_jobs=config.threads, random_state=config.seed,
                                                     verbose=0, oob_score=True)
            elif  config.mode == 'gpu':
                instance = hybridforest.RandomForestClassifier(n_estimators=config.trees, 
                                                     max_features=config.features,
                                                     bootstrap=True, n_jobs=config.threads)",scikit_wrapper.py,palicand/random-forest-wrapper,1
"    training_label = [arr for idx_arr, arr in enumerate(label_bal)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = np.ravel(label_binarize(
        np.hstack(training_label).astype(int), [0, 255]))
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    crf_cv.append(crf.fit(training_data, training_label))

percentiles = np.array([10, 12.5, 15., 17.5, 20., 22.5, 25.])

results_p = []
feat_imp_p = []
for p in percentiles:
    print 'percentiles; {}'.format(p)
    results_cv = []",pipeline/feature-classification/exp-3/selection-extraction/rf/pipeline_classifier_aggregation.py,I2Cvb/mp-mri-prostate,1
"		j += 1

precision, recall, f1, accuracy, support, fn = 0, 0, 0, 0, 0, 0

loo = LeaveOneOut()

start = timer()
for train, test in loo.split(data):
	X_train, X_test = data[train, 0:-1], data[test, 0:-1]
	y_train, y_test = data[train, -1], data[test, -1]
	clf = RandomForestClassifier(n_estimators=15).fit(X_train, y_train)
	y_pred = clf.predict(X_test)
	precision += precision_score(y_test, y_pred, average = 'micro')
	recall += recall_score(y_test, y_pred, average = 'micro')
	f1 += f1_score(y_test, y_pred, average = 'micro')
	accuracy += accuracy_score(y_test, y_pred)
	y = y_test - y_pred
	fn += sum(y[y > 0]) / len(y_test)
end = timer()
",LeaveOneOut/Random_Forests.py,abhijeet-talaulikar/Automatic-Helmet-Detection,1
"
class RandomForestDecisionMaker(_MultiLabelDecisionMaker):
    def __init__(self, n_estimators=10, bootstrap=True, criterion='gini', max_depth=None):
        super(RandomForestDecisionMaker, self).__init__()
        self.n_estimators = n_estimators
        self.bootstrap = bootstrap
        self.criterion = criterion
        self.max_depth = max_depth

    def _init_model(self):
        return RandomForestClassifier(n_estimators=self.n_estimators, bootstrap=self.bootstrap,
                                      criterion=self.criterion, max_depth=self.max_depth, n_jobs=self.n_jobs)


class KNeighborsDecisionMaker(_MultiLabelDecisionMaker):
    def _init_model(self):
        return KNeighborsClassifier()


class _BinaryRelevanceDecisionMaker(DecisionMaker):",src/toolkit/decision.py,matthiasplappert/motion_classification,1
"    classifiers = {
        ""Naive Bayes""         : GaussianNB(),
        ""Gradient Boost""      : GradientBoostingClassifier(),
        ""Adaboost""            : AdaBoostClassifier(DecisionTreeClassifier(max_depth=1)),
        ""Decision Tree""       : DecisionTreeClassifier(),
        ""Extra Random Trees""  : ExtraTreesClassifier(n_estimators=300),
        ""Logistic Regression"" : LogisticRegression(),
        ""K-Nearest-Neighbors"" : KNeighborsClassifier(),
        ""SGD""                 : SGDClassifier(),
        ""SVM""                 : LinearSVC(),
        ""Random Forest""       : RandomForestClassifier(n_estimators=300)
    }

    for c in classifiers:
      # cross validation
      scores = cross_val_score(classifiers[c], Xt, Yt, cv=cvf)

      # report
      print c,scores.mean()",src/five-fold.py,bravelittlescientist/kdd-particle-physics-ml-fall13,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.ml.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = spark.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> model.featureImportances
    SparseVector(1, {0: 1.0})
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0",python/pyspark/ml/classification.py,patrick-nicholson/spark,1
"print fpr
print tpr
print thresholds
print(metrics.classification_report(y_test, yscore))
print(metrics.confusion_matrix(y_test, yscore))
print(pd.crosstab(y_test, yscore, rownames=['True'], colnames=['Predicted'], margins=True))

print """"
print ""Random Forest""
from sklearn.ensemble import  RandomForestClassifier
clf = RandomForestClassifier(n_estimators=10, max_depth=None,
     min_samples_split=2, random_state=0)
clf.fit(X_train, y_train)
yscore=clf.predict(X_test)
fpr, tpr, thresholds = metrics.roc_curve(y_test, yscore, pos_label=2)
print fpr
print tpr
print thresholds
print(metrics.classification_report(y_test, yscore))
print(metrics.confusion_matrix(y_test, yscore))",model.py,sebadiaz/ModelDesign,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/trial3.py,diogo149/CauseEffectPairsPaper,1
"        },
    'expt_9': { 
        'note': 'scaled default decision tree',
        'name': 'scaled default decision tree',
        'pl': Pipeline([ ('scaling', StandardScaler()), 
                        ('decision-tree', DecisionTreeClassifier()) ]) 
        },
    'expt_10': { 
        'note': 'default RF',
        'name': 'default RF',
        'pl': Pipeline([ ('random-forest', RandomForestClassifier()) ]) 
        },
    'expt_11': { 
        'note': 'scaled default RF',
        'name': 'scaled default RF',
        'pl': Pipeline([ ('scaling', StandardScaler()), 
                        ('random-forest', RandomForestClassifier()) ]) 
        },
    'expt_12': { 
        'note': 'default adaboost',",models.py,jrmontag/classifier-comp-year2,1
"		""""""
		This is the constructor responsible for initializing the classifier
		""""""
		self.outputHeader = ""#rf""
		self.clf = None

	def buildModel(self):
		""""""
		This builds the model of the Random Forest Classifier
		""""""
		self.clf = RandomForestClassifier(n_estimators=5, max_depth=None,
			 random_state=0)

	def trainRF(self,X, Y):
		""""""
		Training the Random Forest Classifier
		""""""
		self.clf.fit(X, Y)

	def validateRF(self,X, Y):",classifiers/randomForestClassifier.py,USCDataScience/NN-fileTypeDetection,1
"best = 0
def objective(args):
    global best
    score = []
    for train, test in StratifiedShuffleSplit(_Y):
        trainX = _X[train]
        trainY = _Y[train]
        testX = _X[test]
        testY = _Y[test]

        m = RandomForestClassifier(
            n_estimators=args[0],
            criterion=args[1],
            max_depth=args[2],
        )
        try:
            m = m.fit(trainX, trainY)
            score.append(m.score(testX, testY))
        except Exception, e:
            print e",kagura/find_best_RF_param.py,nishio/kagura,1
"
# ---------------------------------------------------------------------

## STEP 4: GENERATE FEATURES - write a function to discretize a continuous variable
## and a function that can take a categorical variable and create binary variables.

def find_features(df, features, variable):
	'''Uses random forest algorithm to determine the relative importance of each
	potential feature. Takes dataframe, a numpy array of features, and the dependent
	variable. Outputs dataframe, sorting features by importance'''
	clf = RandomForestClassifier()
	clf.fit(df[features], df[variable])
	importances = clf.feature_importances_
	sort_importances = np.argsort(importances)
	rv = pd.DataFrame(data={'variable':features[sort_importances],
							'importance':importances[sort_importances]})
	return rv

def adjust_outliers(x, cap):
	'''Takes series and creates upperbound cap to adjust for outliers'''",pipeline/pipeline.py,jmausolf/HIV_Status,1
"from sklearn.ensemble import RandomForestClassifier

# Load and store training data:

training_data = np.loadtxt('allinteractions.db', delimiter="","")

X = training_data[:,1:]
y = training_data[:,0]

# build a classifier
clf = RandomForestClassifier(n_estimators = 100)

# Utility function to report best scores
def report(grid_scores, n_top=3):
  top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]
  for i, score in enumerate(top_scores):
    print(""Model with rank: {0}"".format(i + 1))
    print(""Mean validation score: {0:.3f} (std: {1:.3f})"".format(
      score.mean_validation_score,
      np.std(score.cv_validation_scores)))",1_Netflix/Supervised Learning/randomrandomforests.py,PhDP/Articles,1
"from sklearn.qda import QDA
clf = QDA(priors=None, reg_param=0.001).fit(X_cropped, np.ravel(y_cropped[:]))
y_validation_predicted = clf.predict(X_validation)
print ""Error rate for QDA (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)



# Start Random Forest Classification
print ""Performing Random Classification:""
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=500)
forest = forest.fit(X_cropped, np.ravel(y_cropped[:]))
y_validation_predicted = forest.predict(X_validation)
print ""Error rate for Random Forest (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)
# ml_aux.plot_confusion_matrix(y_validation, y_validation_predicted, ""CM Random Forest (t1)"")
# plt.show()


# Start k nearest neighbor Classification
print ""Performing kNN Classification:""",Code/Machine_Learning_Algos/training_t2.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"svm = SVC(kernel='linear', C=1.0, random_state=0)
svm.fit(X_train, y_train)

# decision tree model
from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)
tree.fit(X_train, y_train)

# randomforest model
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2)
forest.fit(X_train, y_train)



# Check the accuracy of training dataset
from sklearn.metrics import accuracy_score

# for logistic regression
y_train_pred = lr.predict(X_train)  ",Kaggle/voting.py,wei-Z/Python-Machine-Learning,1
"        np.random.seed(0)
        self.X, self.y = load_data(return_X_y=True)
        p = np.random.permutation(len(self.X))
        self.X, self.y = self.X[p], self.y[p]

    def tearDown(self):
        plt.close(""all"")

    def test_string_classes(self):
        np.random.seed(0)
        clf = RandomForestClassifier()
        scikitplot.classifier_factory(clf)
        clf.fit(self.X, convert_labels_into_string(self.y))
        ax = clf.plot_feature_importances()

    def test_feature_importances_in_clf(self):
        np.random.seed(0)
        clf = LogisticRegression()
        scikitplot.classifier_factory(clf)
        clf.fit(self.X, self.y)",scikitplot/tests/test_classifiers.py,reiinakano/scikit-plot,1
"X_train, X_test, y_train, y_test = train_test_split(review[""review""], review[""categories""], test_size=0.20, random_state=4212)
'''
Applying TF-IDF to the data
'''
vectorizer = TfidfVectorizer() 
tfidfXtrain = vectorizer.fit_transform(X_train)
tfidfXtest = vectorizer.transform(X_test)
'''
Random Forest
'''
forest = RandomForestClassifier(max_features=500,n_estimators=100, n_jobs=4)
forest = forest.fit( tfidfXtrain, y_train )
result = forest.predict(tfidfXtest)
ooutput = pd.DataFrame( data={""predicted"":result,""actual"":y_test,'review':X_test} )
string = output.iloc[0]['review']
print(output.head())
print (""accuracy_score: "", accuracy_score(y_test.values,result))
for index, row in output.iterrows():
    if row['review'] in string:
        print(row['predicted'])",Task 1/Task 1.py,vipmunot/Yelp-Dataset-Challenge,1
"model9 = RCNN.makeModel(Chans, Length, nbClasses, nbRCL=7, nbFilters=33, earlystopping=True, patience=20, filtersize=3, epochs=200)
model10 = RCNN.makeModel(Chans, Length, nbClasses, nbRCL=7, nbFilters=6, earlystopping=True, patience=20, filtersize=3, epochs=200)
model11 = RCNN.makeModel(Chans, Length, nbClasses, nbRCL=7, nbFilters=128, earlystopping=True, patience=20, filtersize=3, epochs=200)
model12 = RCNN.makeModel(Chans, Length, nbClasses, nbRCL=7, nbFilters=150, earlystopping=True, patience=20, filtersize=3, epochs=200)

model_list = [model1, model2, model3, model4,
              model5, model6, model7, model8,    
              model9, model10, model11, model12,]
                 
#==== Second Level: Blending Models ====================
rf = RandomForestClassifier(n_estimators = 200)
svm0 = svm.SVC(decision_function_shape='ovo', probability=True)
trees = ExtraTreesClassifier(max_depth=3, n_estimators=200, random_state=0)
sgd = SGDClassifier(loss=""modified_huber"", penalty=""l2"")
booster = AdaBoostClassifier(n_estimators=200)

meta_classifiers = [rf, svm0, trees, sgd,  booster]
#======================

X_train, y_train, X_test, y_test = utils.train_test_splitter(X, y, test_size=.25)",Examples/EnsembleDemo.py,jacobzweig/RCNN_Toolbox,1
"import subprocess

# load dataset
dataset_x, dataset_y = datasets.load_iris(return_X_y=True)
feature_names = datasets.load_iris().feature_names

# another random sampling way compares to the one in knn example
random_selector = np.random.randint(0, 2, len(dataset_x))
train_x, test_x, train_y, test_y = dataset_x[random_selector==1], dataset_x[random_selector==0], dataset_y[random_selector==1], dataset_y[random_selector==0]

model = RandomForestClassifier(n_estimators=5)

# train the random forest model and keep track of the time
train_start = time.time()
model.fit(train_x, train_y)
train_end = time.time()
result = model.predict(test_x)
test_end = time.time()

# the predicted result test set",random-forest/random-forest.py,tangsttw/python-machine-learning-practice,1
"training_data = np.array(pd.read_csv('data_train.csv', sep=',', quotechar='""', na_values=""nan"", keep_default_na=False))

for column in range( 1, 19 ) :
  # print features[column]
  print column
  # get only one column of the training data
  feature_array  = training_data[:,[column+4]].astype(float)
  class_array = training_data[:,0].astype(float)

  #using random forest becaues it is best classifier
  clf = RandomForestClassifier(n_estimators=100)

  accuracy =  np.mean(cross_validation.cross_val_score(clf, feature_array, class_array, cv=10))
  sorted_features.append( ( accuracy, features[column] , column) );
  pprint.pprint( accuracy )


sorted_features.sort(key=lambda tup: tup[0])

training_data = np.genfromtxt('traindata.csv', delimiter=',')",grid_search.py,dnr2/fml-twitter,1
"                  X, y_class)


def test_base_estimator():
    """"""Test different base estimators.""""""
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)",sklearn/ensemble/tests/test_weight_boosting.py,B3AU/waveTree,1
"    def _cross_validation(self,clf, k_folds = 5):
        self.scores = cross_validation.cross_val_score(clf, self.iss_features, self.labels, cv=k_folds,n_jobs=5, scoring = 'roc_auc')

    def randomforest_info(self, max_trees = 1000, step = 40, k_folds = 5):
        print('Characterising R_forest. Looping through trees: ')
        self.treedata = np.zeros((max_trees/step, 10))
        for i,n_trees in enumerate(np.arange(0, max_trees,step)):
            if n_trees == 0:
                n_trees = 1

            r_forest = RandomForestClassifier(n_estimators=n_trees, n_jobs=5, max_depth=None, min_samples_split=1, random_state=0, )
            scores = cross_validation.cross_val_score(r_forest, self.iss_features, self.labels, cv=k_folds,n_jobs=5)
            r_forest_full = RandomForestClassifier(n_estimators=n_trees, n_jobs=5, max_depth=None, min_samples_split=1, random_state=0)
            r_forest_full.fit(self.iss_features,self.labels)
            self.treedata[i,0] = n_trees
            self.treedata[i,1] = scores.mean()
            self.treedata[i,2] = scores.std()
            # now add the test dataset - score
            self.treedata[i,3] = r_forest_full.score(self.iss_validation_features, self.validation_labels)
",pyecog/light_code/classifier.py,jcornford/pyecog,1
"    lda = LinearDiscriminantAnalysis()
    qda = QuadraticDiscriminantAnalysis()
    linear_svc = svm.SVC(kernel='linear', class_weight='balanced')
    poly_svc = svm.SVC(kernel='poly', class_weight='balanced')
    rbf_svc = svm.SVC(kernel='rbf', class_weight='balanced')
    # LinearSVC minimizes the squared hinge loss while SVC minimizes the
    # regular hinge loss.
    # LinearSVC uses the One-vs-All (also known as One-vs-Rest) multiclass
    # reduction while
    # SVC uses the One-vs-One multiclass reduction.
    rfc = RandomForestClassifier()
    knn = KNeighborsClassifier(12)
    cls.add_classifier(""knn"", knn)
    cls.add_classifier(""decision-trees"", dtc)
    cls.add_classifier(""random-forest"", rfc)
    cls.add_classifier(""gaussian-naive-bayes"", gnb)
    cls.add_classifier(""linear-discriminant-analysis"", lda)
    cls.add_classifier(""quadratic-discriminant-analysis"", qda)
    cls.add_classifier(""linear-support-vector-machine"", linear_svc)
    cls.add_classifier(""poly-support-vector-machine"", poly_svc)",src/hac.py,bhanupratapjain/human-activity-recognition,1
"from sklearn.cross_validation import train_test_split

from sklearn_evaluation import plot

data = datasets.make_classification(200, 10, 5, class_sep=0.65)
X = data[0]
y = data[1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

est = RandomForestClassifier()
est.fit(X_train, y_train)

y_pred = est.predict(X_test)
y_score = est.predict_proba(X_test)
y_true = y_test
",examples/roc.py,edublancas/sklearn-evaluation,1
"                        pixels=np.asanyarray(im)
                        pixels=rgb2gray(pixels)
                        pixels[pixels<>0]=1
                        plt.imshow(pixels)
                        plt.show()
            #print 'total femur images: ', len(femurs)


gen_train_mask()

clf = RandomForestClassifier(n_estimators=25)

#np.concatenate((a,b),axis=2)
# clf.fit(X_train, y_train)
# clf_probs = clf.predict_proba(X_test)
# ani.circletest()
",fl/random_forest/__init__.py,Dearbigdog/python,1
"    
    scores= {'train': [], 'test': []}
    for i in range(25):
        X_train= np.array([data_dict['data'][inner_cv['X_train'][i][j]]\
                          for j in range(len(inner_cv['X_train'][i]))])
        X_test= np.array([data_dict['data'][inner_cv['X_test'][i][j]]\
                         for j in range(len(inner_cv['X_test'][i]))])
        y_train= inner_cv['y_train'][i]
        y_test= inner_cv['y_test'][i]

        est = ensemble.RandomForestClassifier()
        est.fit(X_train, y_train)
        scores['train'].append(est.score(X_train, y_train))
        scores['test'].append(est.score(X_test, y_test))
    
    with open('pickles/rf_scores.pickle','wb') as f:
        pickle.dump(scores, f, pickle.HIGHEST_PROTOCOL) 

    return
",estimators.py,jrabenoit/marvin,1
"    for attr in data.columns.values:
        
        if not (attr in attr_list):
        
            x1 = data[[attr]].as_matrix()[:n]
#            print x1
            x = np.concatenate((x0, x1), axis=1)
#            print x
#            clf = svm.SVC(kernel='rbf')
#            clf = tree.DecisionTreeClassifier()
            clf = ensemble.RandomForestClassifier(n_estimators=10)
                 
            score1 = cross_validation.cross_val_score(clf, x, train_only[:n, 0] , cv=5)
    #        score2 = cross_validation.cross_val_score(clf, x[:1000], train_only[:1000, 1] , cv=5)
            print attr, np.mean(score1)#, np.mean(score2)
        
add_attr(['click_quality', 'price_usd', 'random_bool'])       
",Assignment-2/Code/step_up.py,Ignotus/DataMining,1
"
# end

##names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
##         ""Random Forest"", ""AdaBoost"", ""Naive Bayes""]
##classifiers = [
##    KNeighborsClassifier(3),
##    SVC(kernel=""linear"", C=0.025),
##    SVC(gamma=2, C=1),
##    DecisionTreeClassifier(max_depth = 5),
##    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
##    AdaBoostClassifier(),
##    NaiveBayesClassifier()]


def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)
    word_features = wordlist.keys()
    return word_features
",ml/mytask3b.py,john136/exercises,1
"def plot_forest(max_depth=1):
    plt.figure()
    ax = plt.gca()
    h = 0.02

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    if max_depth != 0:
        forest = RandomForestClassifier(n_estimators=20, max_depth=max_depth,
                                        random_state=1).fit(X, y)
        Z = forest.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, alpha=.4)
        ax.set_title(""max_depth = %d"" % max_depth)
    else:
        ax.set_title(""data set"")
    ax.scatter(X[:, 0], X[:, 1], c=np.array(['b', 'r'])[y], s=60)
    ax.set_xlim(x_min, x_max)",scikit/Chapter 1/plot_forest.py,obulpathi/datascience,1
"  # set_trace()
  return _Abcd(before=actual, after=preds, show=False)[-1]


def rforest(train, test, tunings=None, smoteit=True, duplicate=True):
  ""RF ""
  # Apply random forest Classifier to predict the number of bugs.
  if smoteit:
    train = SMOTE(train, atleast=50, atmost=101, resample=duplicate)
  if not tunings:
    clf = RandomForestClassifier(n_estimators=100, random_state=1)
  else:
    clf = RandomForestClassifier(n_estimators=int(tunings[0]),
                                 max_features=tunings[1] / 100,
                                 min_samples_leaf=int(tunings[2]),
                                 min_samples_split=int(tunings[3])
                                 )
  train_DF = formatData(train)
  test_DF = formatData(test)
  features = train_DF.columns[:-2]",src/Planners/XTREE/Prediction.py,rahlk/RAAT,1
"    net = buildNetwork(indim, node_num, 1, outclass=SigmoidLayer, bias=True)
    trainer = BackpropTrainer(net, ds, learningrate=learning_rate)
    trainer.trainUntilConvergence(maxEpochs=maxEpochs_for_trainer)
    return net


def create_RF_classifier(genes, positive_dataset, negative_dataset):
    n_estimators, max_features, window_size = map(int, genes)
    train_labels, train_dataset = create_train_labels_and_dataset(positive_dataset, negative_dataset) 
    indim = 21 * (2 * window_size + 1)
    clf = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features)
    clf.fit(train_dataset, train_labels)
    return clf


def create_SVM_classifier(genes, positive_dataset, negative_dataset):
    cost, gamma, window_size = genes
    cost, gamma, window_size = float(cost), float(gamma), int(window_size)
    train_labels, train_dataset = create_train_labels_and_dataset(positive_dataset, negative_dataset) 
    indim = 21 * (2 * window_size + 1)",create_model.py,clclcocoro/MLwithGA,1
"    count_enrollment = df_sub['3COURSEID'].value_counts()
    #print ""Number of %s enrollment: %s""%(subject,count_enrollment)

    A = df_sub.as_matrix()
    X = A[:,6:209]
    X = X.astype(np.int64, copy=False)
    y = A[:,2]
    y = y.astype(np.int64, copy=False)

    #Training data
    forest = RandomForestClassifier(n_estimators=10, max_depth=None, 
            min_samples_split=1, random_state=None, max_features=None)
    clf = forest.fit(X, y)
    scores = cross_val_score(clf, X, y, cv=5)
    print scores
    print ""Random Forest Cross Validation of %s: %s""%(subject,scores.mean())
    precision_rf[subject] = scores.mean()
    df_precision.loc[subject]=precision_rf[subject]
    
    sheet2.write(count, 0, subject)",pae/final_code/src/feature_eachSub_progpa.py,wasit7/book_pae,1
"rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,
                          random_state=0)

rt_lm = LogisticRegression()
pipeline = make_pipeline(rt, rt_lm)
pipeline.fit(X_train, y_train)
y_pred_rt = pipeline.predict_proba(X_test)[:, 1]
fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)

# Supervised transformation based on random forests
rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)
rf_enc = OneHotEncoder()
rf_lm = LogisticRegression()
rf.fit(X_train, y_train)
rf_enc.fit(rf.apply(X_train))
rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)

y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]
fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)
",projects/scikit-learn-master/examples/ensemble/plot_feature_transformation.py,DailyActie/Surrogate-Model,1
"Y = iris.target
# print(iris.DESCR)

lNbEstimatorsInEnsembles = 12

models=[DecisionTreeClassifier(max_depth=5, random_state = 1960) ,
        AdaBoostClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960),
        GradientBoostingClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960),
        SGDClassifier( random_state = 1960),
        LogisticRegression( random_state = 1960),
        RandomForestClassifier(n_estimators=lNbEstimatorsInEnsembles, random_state = 1960),
        GaussianNB(),
        SVC(max_iter=200, probability=True, kernel='linear'),
        SVC(max_iter=400, probability=True, kernel='poly'),
        SVC(max_iter=200, probability=True, kernel='rbf'),
        MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3, 5), random_state=1960),
        RidgeClassifier(random_state = 1960)]


dialects = [""db2"", ""hive"", ""mssql"", ""mysql"", ""oracle"", ""postgresql"", ""sqlite""];",tests/classification/test_client_iris_many_models.py,antoinecarme/sklearn2sql_heroku,1
"num_inactives = len(fps_inact)
print ""inactives read and fingerprints calculated:"", num_inactives

# loop over the repetitions
print ""training""

# training 
train_fps = fps_act + fps_inact
ys_fit = [1]*len(fps_act) + [0]*len(fps_inact)
# train the model
ml = RandomForestClassifier(n_estimators=100, max_depth=100, min_samples_split=2, min_samples_leaf=1, n_jobs=4)
ml.fit(train_fps, ys_fit)
print ""model trained""",evaluation/test_RF_model.py,sriniker/TDT-tutorial-2014,1
"    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Some extension of Receiver operating characteristic to multi-class')
    plt.legend(loc=""lower right"")
    return thresholds_for_bci


model = RandomForestClassifier(n_estimators=10, max_depth=3)
print ""Random Forest""
test_model(model)

model_lda = LinearDiscriminantAnalysis()
print ""LDA""
test_model(model_lda)

use_prediction = False
raw_test_data, test_labels = readDataMultipleFiles([3])",src/test.py,kahvel/MAProject,1
"

iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target


def test_EnsembleClassifier():

    np.random.seed(123)
    clf1 = LogisticRegression()
    clf2 = RandomForestClassifier()
    clf3 = GaussianNB()
    eclf = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='hard')

    scores = cross_validation.cross_val_score(eclf, X, y, cv=5, scoring='accuracy')
    scores_mean = (round(scores.mean(), 2))
    assert(scores_mean == 0.94)


def test_EnsembleClassifier_weights():",tests/tests_classifier/test_ensembleclassifier.py,YoungKwonJo/mlxtend,1
"        t_labels = self.training.ix[:, 0].values.copy()
        self.training_labels = np_utils.to_categorical(t_labels, self.nb_classes)

        t_datas = self.testing.ix[:, 0:].values.copy()
        self.testing_datas = t_datas.reshape(t_datas.shape[0], 1, 28, 28)

        self.testing_labels = None

    def model_tuning(self):
        # RandomForestClassifier
        #self.model = RandomForestClassifier()

        # SupportVectorMachine
        #self.model = OneVsRestClassifier(LinearSVC(random_state=0))

        # Deep learning
        dims = self.training_datas.shape[1]
        print(dims, 'dims')

        self.model = Sequential()",DigitRecognizer/src/digitrecognizer.py,EnsekiTT/kaggle_log,1
"from sklearn.ensemble import RandomForestClassifier
from sklearn.externals import joblib
import numpy
from progressbar import ProgressBar


def train(train_data_input, train_data_output, model_path, nr_trees=1000):
    train_data_input = numerical_to_features(train_data_input)
    random_forest = RandomForestClassifier(n_estimators=nr_trees, min_samples_leaf=1000, n_jobs=-1,
                                           class_weight=""balanced"", verbose=1, criterion='gini')
    random_forest.fit(train_data_input, train_data_output)
    joblib.dump(random_forest, model_path)


def predict(test_data_input, model_path):
    test_data_input = numerical_to_features(test_data_input)
    random_forest = joblib.load(model_path)
    probabilities = random_forest.predict_proba(test_data_input)",smiles-vhts/util/random_forest.py,patrick-winter-knime/deep-learning-on-molecules,1
"            print('Review {}'.format(i+1))    
        sentences += review_to_sentences(review)
    return sentences

# Learning

def train_classifier(algorithm, features, train):
    print('Train classifier ({})...'.format(algorithm))
    estimators = []
    if 'rf' in algorithm:
        estimators.append(('rf', RandomForestClassifier(n_estimators=100)))
    if 'lr' in algorithm:
        estimators.append(('lr', LogisticRegression()))
    if 'mb' in algorithm:
        estimators.append(('mb', MultinomialNB()))
    # Training
    classifier = VotingClassifier(estimators=estimators, voting='soft')
    classifier.fit(features, train['sentiment'])
    return classifier
",util.py,pvigier/sa,1
"Y_train = df_train.label.values
X_test = df_test.filter(regex='pixel').astype(np.float).values

# Augment data
print('Augmenting data')
X_train, Y_train = nudge_dataset(X_train, Y_train)

# Set up pipeline
scaler = MinMaxScaler()
pca = PCA(n_components=0.90)
rf = RandomForestClassifier(n_jobs=-1,
                            n_estimators=1000,
                            random_state=42,
                            criterion='gini',
                            bootstrap=False,)

# Make pipeline
clf = Pipeline(steps=[
    ('scaler', scaler),
    ('rf', rf),",rf.py,get9/kaggle_scripts,1
"        args.label = np.arange(Y_train.shape[1])
    
    logging.info(""Fitting classifiers"")
    clfs = []
    for i in args.label:
        logging.info(""Label %d"" % i)
        clfsi = []
        grid = itertools.product((0.2, 0.4, 0.6), (2, 4, 8, 16))
        for mf, msl in grid:
            logging.info(""mf = %s, msl = %s"" % (mf, msl))
            clf = RandomForestClassifier(
                n_estimators=32, criterion='entropy',
                min_samples_split=1, max_depth=None, max_leaf_nodes=None,
                max_features=mf, min_samples_leaf=msl,
                n_jobs=-1, random_state=42, verbose=2)
            try:
                clf.fit(X_train, Y_train[:,i])
                p = clf.predict_proba(X_test)
                s = score_predictions(Y_test[:,i], p[:,1])
                logging.info(""...score = %f"" % s)",scripts/predictors/random_forest.grid_search.py,timpalpant/KaggleTSTextClassification,1
"    parameters = {'random_state': 12345, 'max_features': None,
                  'oob_score': True}  # , 'class_weight': 'balanced'}
    xaxis = []
    yaxis = []
    mean_score = []
    thresh_cutter = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
    print(title)
    if not isinstance(pvalidator, int):
        pvalidator = cross_val_data(x, y, pvalidator)
    for cutoff in thresh_cutter:
        clf = RandomForestClassifier(
            n_estimators=800, n_jobs=-1, max_leaf_nodes=1000, **parameters)
        validated = cross_val_score(
            clf, x, y, cv=pvalidator, scoring=custom_f1(cutoff, metric))
        xaxis.append(cutoff)
        yaxis.append(validated)
        mean_score.append(np.mean(validated))
    visualize_plot(xaxis, yaxis, 'threshold',
                   'f1_score (%f,%f)' % x.shape, title)
    seuil = np.argmax(mean_score)",scripts/RF/classifierdata.py,UdeM-LBIT/CoreTracker,1
"        modeler = train_models.ManyModels()
        dataset = Bunch('stuff') #need to get this to actually load a bunch of data

        X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.33, random_state=1)
        modeler.X = X_train
        modeler.y = y_train

        #Attach our unfitted model instances to the modeler instance
        kn12 = KNeighborsClassifier(n_neighbors=12)
        kn6 = KNeighborsClassifier(n_neighbors=6)
        rf = RandomForestClassifier()
        modeler.models = {""KNeighborsClassifier_6"": kn6
                          , ""KNeighborsClassifier_12"": kn12
                          , ""RandomForestClassifier"": rf
                         }

        #In another test, make sure all 3 methods work
        modeler.fit(""KNeighborsClassifier_6"") #fit just one model
        modeler.fit(model_list=['KNeighborsClassifier_12', 'RandomForestClassifier']) #fit a list of models
        modeler.fit() #fits all models",code/tests/test_prediction.py,georgetown-analytics/housing-risk,1
"    if extra_data is not None:
        X_extra,y_extra = extra_data
        X_extra,y_extra = np.array(X_extra),np.array(y_extra)
    for train_idx,test_idx in skf:
        if len(target_names) == 2:
            clfs = [
                LinearSVC(C=13),
                # SVC(C=11,kernel=""linear"",probability=True),
                # SGDClassifier(loss=""log""),
                LogisticRegression(),
                RandomForestClassifier(n_estimators=10,n_jobs=-1),

            ]
        else:
            clfs = [
                LinearSVC(C=13),
            ]
        print ""Setting up""
        X_train,y_train = X[train_idx],y[train_idx]
        if extra_data is not None:",Subtask1-Revised/main.py,saatvikshah1994/hline,1
"X = pd.DataFrame()
Y = pd.DataFrame()

X = df.ix[:, 2:len(df.columns) - 1]
Y = df.ix[:, len(df.columns) - 1: len(df.columns)]

X_test = test_df.ix[:, 2:]

print X_test

rf = RandomForestClassifier(n_estimators=100, max_features=7)
rf.fit(X, Y)

Y_Result = rf.predict(X_test)
print Y_Result

",src/predict_gender/gender_prediction.py,mohitreddy1996/Gender-Detection-from-Signature,1
"def normalization(scoreList):
    outputList = []
    total = sum(scoreList)
    for score in scoreList:
        outputList.append(score/total)
    return outputList


def trainInfer(doc_train, label_train, doc_test, label_test, ensemble):
    if ensemble == 'RandomForest':
        model = RandomForestClassifier()
    elif ensemble == 'AdaBoost':
        model = AdaBoostClassifier()
    else:
        print 'Ensemble Model Error!'
        sys.exit()

    model.fit(doc_train, label_train)
    '''
    #labelList = model.classes_",ensembleBaselines.py,renhaocui/ensembleTopic,1
"
    # Used to adjust learning parameters/thresholds
    def run_training_test(self):
        trainer = Trainer(self.test_training_file)
        trainer.generate_training_set(10000)

        match_file = open(self.test_training_file, 'rb')
        training_matches = pickle.load(match_file)
        match_file.close()

        clf = RandomForestClassifier(n_estimators=10, random_state=0)
        clf = clf.fit([eval(t.features) for t in training_matches], [int(t.matchpct) for t in training_matches])

        trainer = Trainer(self.test_training_file)
        trainer.group_by_last_name_and_state()

        CONFIDENCE_KEEP = 0.65
        CONFIDENCE_CHECK = 0.51

        num_pairs = 0",campfin/tester.py,huffpostdata/campfin-linker,1
"dataset_path = '/home/vinicius/Projects/the_magic_kingdom_of_python/dataset/titanic/'
X_train, y_train, X_test = dataset_titanic(dataset_path)
print(""Dataset loaded."")

# standardize data
stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.fit_transform(X_test)

# classififer
forest = RandomForestClassifier(criterion='gini',
                                n_estimators=100,
                                random_state=1,
                                n_jobs=-1)

# # selecting features
# sbs = SBS(knn, k_features=1)
# sbs.fit(X_train_std, y_train)
#
# # plotting performance of feature subsets",kaggle/titanic/main2.py,viniciusguigo/the_magic_kingdom_of_python,1
"
class DTClassifier(Classifier):
    """"""Decision Tree classifier""""""
    def __init__(self):
        self.cl = DecisionTreeClassifier(random_state=0)


class RFClassifier(Classifier):
    """"""Random forest classifier""""""
    def __init__(self):
        self.cl = RandomForestClassifier(n_jobs=2)",src/CipCipPy/classification/__init__.py,giacbrd/CipCipPy,1
"	vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
	                                 stop_words='english')

	global clf2
	X2 = vectorizer.fit_transform(data_text)
	clf2 = SGDClassifier(loss='log', penalty='l2',alpha=1e-5, n_iter=25, random_state=42,shuffle=True)
	clf2.fit(X2, labels)
	vocab = my_dict2 = {y:x for x,y in vectorizer.vocabulary_.iteritems()}


	#clf = RandomForestClassifier(n_estimators=750)
	global allTextInOrders
	global similarities
	allTextInOrders = data_text + data_text + unhelpful_exp_text + helpful_exp_text


	trainingDistributions = []

	# for reviewT in data_text:
	# 	trainingDistributions.append(lda[corpus.dictionary.doc2bow(corpus.proc(reviewT))])",src/dialectic_pipeline/build/lib/dialectic_pipeline/trainmodel.py,cudbg/Dialectic,1
"    def setUp(self):
        iris = datasets.load_iris()
        rng = check_random_state(0)
        perm = rng.permutation(iris.target.size)
        iris.data = iris.data[perm]
        iris.target = iris.target[perm]
        self.iris = iris

    def test_stacked_classfier_extkfold(self):
        bclf = LogisticRegression(random_state=1)
        clfs = [RandomForestClassifier(n_estimators=40, criterion = 'gini', random_state=1),
                RidgeClassifier(random_state=1),
                ]
        sl = StackedClassifier(bclf,
                               clfs,
                               n_folds=3,
                               verbose=0,
                               Kfold=StratifiedKFold(self.iris.target, 3),
                               stack_by_proba=False,
                               oob_score_flag=True,",stacked_generalization/lib/test/test.py,fukatani/stacked_generalization,1
"    test[var] = lb.transform(test[var].astype('str'))
print (test['Dependents'].unique())

features = {'Credibility','Gender','Married','Dependents','Self_Employed','Property_Area','ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History'}
 
x_train = train[list(features)].values
y_train = train['Loan_Status'].values
x_test = test[list(features)].values
 

rf = RandomForestClassifier(n_estimators=100,oob_score = True, max_features = ""auto"",random_state=10,min_samples_split=2, min_samples_leaf=2)

rf.fit(x_train, y_train)
print (""Starting to predict on the dataset"")
rec= rf.predict(x_test)
print (""Prediction Completed"")
test['Loan_Status'] = rec
test.to_csv('/home/muthu/Data-Analytics/Muthu/Loan/output/sub.csv',columns=['Loan_ID','Loan_Status'],index=False)",Loan-RF.py,muthu1086/AV-LoanPrediction,1
"# Random Forest

# must encode categories as integers
sexEncoder = preprocessing.LabelEncoder()
train['Sex'] = sexEncoder.fit_transform(train.Sex)
embEncoder = preprocessing.LabelEncoder()
train['Embarked'] = embEncoder.fit_transform(train.Embarked)

columns = np.array(['Sex', 'Pclass', 'Age', 'Fare', 'SibSp', 'Parch', 'Embarked'])

forest = RandomForestClassifier(n_estimators=1000, max_depth=5)
fit = forest.fit(train[columns], train.Survived)

# analyze variable importance
# (inspired by http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html)
varImportance = pd.Series(fit.feature_importances_, index=columns).sort_values()
print varImportance

varImportance.plot.barh() # color='skyblue'
ax = plt.gca()",python/explore.py,goerlitz/kaggle-titanic,1
"    std_data = np.std(train_data.astype('float32'),axis=0)
    train_data /= std_data
    test_data -= mean_data
    test_data /= std_data

    train_data = train_data.reshape(-1, train_data.shape[2])
    train_labels = train_labels.reshape(-1, 1)[:,0]

    print('RUN random forest')

    clf = RandomForestClassifier()

    clf.fit(train_data, train_labels)

    rain_cases = []
    non_rain_cases = []
    predict_Y = []

    from itertools import groupby
    for i, test_sample in enumerate(test_data):",predict_onto.py,ducminhkhoi/PrecipitationPrediction,1
"        result += ""True "" if p == real else ""False ""
        result += ""positive"" if p == 1 else ""negative""
        result += "" - confidence: %.5f"" % prediction[p]
        return result

if __name__ == ""__main__"":
    # ada = AdaBoostClassifier()
    # ada.n_estimators = 50
    # ada.base_estimator.max_depth = 1

    random_forest = RandomForestClassifier(n_estimators=100)

    category = ""trilobite""
    dataset = ""all""
    datamanager = CaltechManager()
    datamanager.PATHS[""RESULTS""] = os.path.join(datamanager.PATHS[""BASE""], ""results_trilobite_rf_testing"")

    # vcd = VisualConceptDetection(ada, datamanager)
    vcd = VisualConceptDetection(random_forest, datamanager)
",ensemble_visualization.py,peret/visualize-bovw,1
"ytest=testDF.loc[:,'genre'].reset_index(drop=True)
#path para salvar os resultados
resultsPath='E:/Dropbox/ProjectML/Data/ecocSVM_Renan'

#RODANDO ecocSVM
h.ecocSVM(X,y,codeDF,resultsPath)
pred=h.combineEcocSVM(X,y,resultsPath,True)
oobScore=h.classifyRFC(pred)
h.classifyRFC(pred)

_RFC=ensemble.RandomForestClassifier(150,max_leaf_nodes=80,oob_score=True)
_RFC.fit(pred.drop('y',1),pred.y)
_RFC.score(pred.drop('y',1),pred.y)
_RFC.oob_score_
_RFC.feature_importances_
#predTest=h.combineEcocSVM(Xtest,ytest,resultsPath)

testDF=pd.read_csv(_DATA+'/testDF.txt')
Xtest=testDF.loc[:,_VARS]
ytest=testDF.loc[:,'genre']",bin/ecocSVM.py,gabrielusvicente/MC886,1
"# Generate a binary classification dataset.
X, y = make_classification(n_samples=500, n_features=25,
                           n_clusters_per_class=1, n_informative=15,
                           random_state=RANDOM_STATE)

# NOTE: Setting the `warm_start` construction parameter to `True` disables
# support for paralellised ensembles but is necessary for tracking the OOB
# error trajectory during training.
ensemble_clfs = [
    (""RandomForestClassifier, max_features='sqrt'"",
        RandomForestClassifier(warm_start=True, oob_score=True,
                               max_features=""sqrt"",
                               random_state=RANDOM_STATE)),
    (""RandomForestClassifier, max_features='log2'"",
        RandomForestClassifier(warm_start=True, max_features='log2',
                               oob_score=True,
                               random_state=RANDOM_STATE)),
    (""RandomForestClassifier, max_features=None"",
        RandomForestClassifier(warm_start=True, max_features=None,
                               oob_score=True,",Statistiques/Science des donnes/Exploration de donnes/random_forest_oob.py,NicovincX2/Python-3.5,1
"        model = CalibratedClassifierCV(
            svm.LinearSVC(C=1, max_iter=1000), method=""sigmoid"")
        return self.ml_model(model)


class RandomForest(SKLP):
    def prepare_model(self):
        from sklearn.ensemble import RandomForestClassifier

        reg = CalibratedClassifierCV(
            RandomForestClassifier(n_estimators=25, min_samples_split=2), method=""sigmoid"")
        reg.fit(self.dataset.train_data, self.dataset.train_labels)
        sig_clf = CalibratedClassifierCV(reg, method=""sigmoid"", cv=""prefit"")
        sig_clf.fit(self.dataset.validation_data, self.dataset.validation_labels)
        return self.ml_model(sig_clf)

    def prepare_model_k(self):
        from sklearn.ensemble import RandomForestClassifier
        
        model = CalibratedClassifierCV(",src/ml/clf/extended/w_sklearn.py,elaeon/ML,1
"            print ""%s: %.5f"" % (feat_set[i_feat], feat_imps[i_feat])

        if save:
            self.save_classifier(clf, name, feature_context_nm)

        self.eval_performance(clf.predict_proba(v_feats), v_labels,
                              clf.predict_proba(te_feats), te_labels,
                              [name, str(n_estimators), str(feature_context_nm)])

    def create_rfc(self, n_estimators=20):
        rfc = RandomForestClassifier(warm_start=False, oob_score=True,
                                     max_features=""auto"",
                                     # max_depth=4,
                                     n_estimators=n_estimators,
                                     class_weight=""balanced"",
                                     n_jobs=-1)
        return rfc

    def create_ext(self, n_estimators=20):
        ext = ExtraTreesClassifier(warm_start=False, oob_score=True,",syconnfs/representations/skel_based_classifier.py,StructuralNeurobiologyLab/SyConnFS,1
"from sklearn.metrics import log_loss

def prepare_data(df):
    df = features.numerical.delete_nonnumerical(df)
    return df

def train_model(df):
    X, y = data.read_data.X_y_split(df)
    X = prepare_data(X)

    clf = RandomForestClassifier(n_estimators=1000)
    clf.fit(X, y)

    return clf

def test_model(clf, df):
    X, y = data.read_data.X_y_split(df)
    X = prepare_data(X)

    y_pred = clf.predict_proba(X)",two-sigma-rental-listing/src/models/baseline_numerical.py,aufziehvogel/kaggle,1
"count_enrollment = df_sub['3COURSEID'].value_counts()
#print ""Number of %s enrollment: %s""%(subject,count_enrollment)

A = df_sub.as_matrix()
X = A[:,6:209]
X = X.astype(np.int64, copy=False)
y = A[:,2]
y = y.astype(np.int64, copy=False)

#Training data
forest = RandomForestClassifier(n_estimators=10, max_depth=None, 
        min_samples_split=1, random_state=None, max_features=None)
clf = forest.fit(X, y)
scores = cross_val_score(clf, X, y, cv=5)
print scores
print ""Random Forest Cross Validation of %s: %s""%(subject,scores.mean())
precision_rf[subject] = scores.mean()
df_precision.loc[subject]=precision_rf[subject]
print ""-----------------------------------""
",pae/forcast/src/tree_classify.py,wasit7/book_pae,1
"    trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )

    print ""Creating average feature vecs for test reviews""

    testDataVecs = getAvgFeatureVecs( getCleanReviews(test), model, num_features )


    # ****** Fit a random forest to the training set, then make predictions
    #
    # Fit a random forest to the training data, using 100 trees
    forest = RandomForestClassifier( n_estimators = 100 )

    print ""Fitting a random forest to labeled training data...""
    forest = forest.fit( trainDataVecs, train[""sentiment""] )

    # Test & extract results
    result = forest.predict( testDataVecs )

    # Write the test results
    output = pd.DataFrame( data={""id"":test[""id""], ""sentiment"":result} )",study/kaggle/DeepLearningMovies/Word2Vec_AverageVectors.py,AppleFairy/machinelearning,1
"import seaborn as sns

# for jupyter notebooks
#%matplotlib inline

# if you're running this in a jupyter notebook, print out the graphs
NOTEBOOK = 1

def define_clfs_params(grid_size):

    clfs = {'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),
        'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),
        'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=""SAMME"", n_estimators=200),
        'LR': LogisticRegression(penalty='l1', C=1e5),
        'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),
        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),
        'NB': GaussianNB(),
        'DT': DecisionTreeClassifier(),
        'SGD': SGDClassifier(loss=""hinge"", penalty=""l2""),
        'KNN': KNeighborsClassifier(n_neighbors=3) ",Pipeline/magicloops.py,anisfeld/MachineLearning,1
"def main():
    # create the training & test sets,skipping the header row with [1:]
    dataset = genfromtxt(open('data/train.csv', 'r'),
                         delimiter=',', dtype='f8')[1:]
    target = [x[0] for x in dataset]
    train = [x[1:] for x in dataset]
    test = genfromtxt(open('data/test.csv', 'r'),
                      delimiter=',', dtype='f8')[1:]

    # create and train the random forest
    # multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100,
    # n_jobs=2)
    rf = RandomForestClassifier(n_estimators=100)
    rf.fit(train, target)
    predicted_probs = [[index + 1, x[1]]
                       for index, x in enumerate(rf.predict_proba(test))]

    savetxt('data/submission.csv', predicted_probs, delimiter=',',
            fmt='%d,%f', header='MoleculeId,PredictedProbability',
            comments=',')",kaggle/predicting-biological-response/makeSubmission.py,Alexoner/data-solution,1
"    params = {""n_estimators"": [10, 50, 100, 200, 400, 750, 800, 1000, 2000], ""base_estimator__max_depth"": [1, 2, 3, 5], ""base_estimator__random_state"": [0], ""random_state"": [0]}
#    params = {""C"": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}

    datamanager = CaltechManager()
    categories = [c for c in os.listdir(datamanager.PATHS[""CATEGORIES_DIR""]) if c != datamanager.BACKGROUND and os.path.splitext(c)[1] != "".py""]

    #kernels, gammas = build_train_kernels(categories, datamanager)
    #print ""Finished building kernels""

    #grids = (GridSearch(SVC(kernel=""precomputed""), c) for c in categories)
    # grids = (GridSearch(RandomForestClassifier(), c) for c in categories)

    grids = [GridSearch(AdaBoostClassifier(), datamanager, c) for c in categories]

    with warnings.catch_warnings():
        warnings.simplefilter(""ignore"")
        for g in grids:
            g.grid_search(params, weight_samples=False)
        generate_evaluation_summary(grids, ""grid_test.csv"")
",runGridSearch.py,peret/visualize-bovw,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/default_categorical_only.py,diogo149/CauseEffectPairsPaper,1
"
    def evaluate(self):
        """"""
        Model evaluation across multiple classifiers based on accuracy of predictions.
        """"""
        classifiers = [
            xgb.XGBClassifier(**Base.xgb_params),
            KNeighborsClassifier(3),
            SVC(probability=True),
            DecisionTreeClassifier(),
            RandomForestClassifier(),
            AdaBoostClassifier(),
            GradientBoostingClassifier(),
            GaussianNB(),
            LogisticRegression()]

        log_cols = [""Classifier"", ""Accuracy""]
        Base.model_ranking = pd.DataFrame(columns=log_cols)

        sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)",speedml/model.py,Speedml/speedml,1
"    todense = DenseTransformer()
    tfidf = TfidfTransformer()
    X_t = tfidf.fit_transform([[1, 2, 3]])
    assert issparse(X_t)
    X_dense = todense.transform(X_t)
    expect = np.array([[0.26726124, 0.53452248, 0.80178373]])
    assert np.allclose(X_dense, expect)


def test_pipeline():
    rf = RandomForestClassifier()
    param_grid = [{'randomforestclassifier__n_estimators': [1, 5, 10]}]
    pipe = make_pipeline(StandardScaler(), DenseTransformer(), rf)
    grid = GridSearchCV(pipe, param_grid, cv=3, n_jobs=1)
    grid.fit(X, y)",mlxtend/preprocessing/tests/test_dense_transformer.py,rasbt/mlxtend,1
"            self.conditions_categorical
        # The dataset.
        self.dataset = pd.DataFrame()
        # Lookup for categoricals to code.
        self.categories_to_val_map = dict()
        # Training set (regressors and labels)
        self.X_numerical = np.ndarray(0)
        self.X_categorical = np.ndarray(0)
        self.Y = np.ndarray(0)
        # Random Forests.
        self.rf_partial = RandomForestClassifier(n_estimators=100)
        self.rf_full = RandomForestClassifier(n_estimators=100)
        # Preprocess the data.
        self.dataset = utils.extract_sklearn_dataset(self.conditions,
            self.targets, df)
        self.categories_to_val_map = utils.build_categorical_to_value_map(
            self.conditions_categorical, self.dataset)
        self.X_categorical = utils.extract_sklearn_features_categorical(
            self.conditions_categorical, self.categories_to_val_map,
            self.dataset)",src/predictors/random_forest.py,probcomp/bdbcontrib,1
"            if info['is_sparse']==True:
                self.name = ""BaggingRidgeRegressor""
                self.model = BaggingRegressor(base_estimator=Ridge(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            else:
                self.name = ""GradientBoostingRegressor""
                self.model = GradientBoostingRegressor(n_estimators=1, verbose=verbose, warm_start = True)
            self.predict_method = self.model.predict # Always predict probabilities
        else:
            if info['has_categorical']: # Out of lazziness, we do not convert categorical variables...
                self.name = ""RandomForestClassifier""
                self.model = RandomForestClassifier(n_estimators=1, verbose=verbose) # unfortunately, no warm start...
            elif info['is_sparse']:                
                self.name = ""BaggingNBClassifier""
                self.model = BaggingClassifier(base_estimator=BernoulliNB(), n_estimators=1, verbose=verbose) # unfortunately, no warm start...                          
            else:
                self.name = ""GradientBoostingClassifier""
                self.model = eval(self.name + ""(n_estimators=1, verbose="" + str(verbose) + "", min_samples_split=10, random_state=1, warm_start = True)"")
            if info['task']=='multilabel.classification':
                self.model = MultiLabelEnsemble(self.model)
            self.predict_method = self.model.predict_proba  ",lib/models.py,SDminers/AutoMLChallenge,1
"    response_variable_column = df[len(df.columns.values)-1]
    # The last column describes the targets
    explanatory_variable_columns.remove(len(df.columns.values)-1)

    y = [1 if e == 'ad.' else 0 for e in response_variable_column]
    X = df[list(explanatory_variable_columns)]
    X.replace(to_replace=' *\?', value=-1, regex=True, inplace=True)
    X_train, X_test, y_train, y_test = train_test_split(X, y)

    pipeline = Pipeline([
        ('clf', RandomForestClassifier(criterion='entropy'))
    ])
    parameters = {
        'clf__n_estimators': (5, 10, 20, 50),
        'clf__max_depth': (50, 150, 250),
        'clf__min_samples_split': (1, 2, 3),
        'clf__min_samples_leaf': (1, 2, 3)
    }

    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1')",MasteringMLWithScikit-learn/8365OS_05_Codes/ch5.py,moonbury/pythonanywhere,1
"test_df = test_df.drop(['Name', 'Sex', 'Ticket', 'Cabin', 'PassengerId'], axis=1) 


# The data is now ready to go. So lets fit to the train, then predict to the test!
# Convert back to a numpy array
train_data = train_df.values
test_data = test_df.values


print 'Training...'
forest = RandomForestClassifier(n_estimators=100)
forest = forest.fit( train_data[0::,1::], train_data[0::,0] )

print 'Predicting...'
output = forest.predict(test_data).astype(int)


predictions_file = open(""myfirstforest.csv"", ""wb"")
open_file_object = csv.writer(predictions_file)
open_file_object.writerow([""PassengerId"",""Survived""])",data/titanic/myfirstforest.py,mikessoldier/mikessoldier.bigd.py,1
"	xtotal_test_samples = round(xData.shape[0] * 0.2)
	ytotal_test_samples = int( round(0.2 * len(yData_Vec)) )

	xTrain = np.array( xData[:xtotal_training_samples] )
	yTrain = np.array( yData_Vec[:ytotal_training_samples] )

	xTest = np.array( xData[-xtotal_test_samples:] )
	yTest = np.array( yData_Vec[-ytotal_test_samples:] )

	#Build model with 50 trees
	forestModel = RandomForestClassifier(50)

	#Train it
	forestModel.fit(xTrain, yTrain)

	#Check accuracy of prediciton
	print (""The Classifier Predicts With %f Percent Accuracy"" % (forestModel.score(xTest, yTest)*100) )

#################################################################
#Bag of words function with json files and desired element to access",Exercise11/exercise11_1.py,gjwajda/Computational-Tools-For-Big-Data,1
"    
    # Recommender System part
    if(plot_output):
        PR_fig = setup_plots()
    
    # Specify the classifiers
    clfs = [
            BernoulliNB(alpha=0.001),
            LogisticRegression(C=0.02, penalty='l1', tol=0.001),
            svm.SVC(C=1,kernel='rbf',probability=True),
            ensemble.RandomForestClassifier(),
            CollaborativeFilter(categories=False),
            Popularity(),
            RandomClassifier(),
            CollaborativeFilter(categories=case_categories)
            ]
    #plot_ops = ['k:','k--','k-','k-.','r-','b-','g-'] 
    plot_ops = [{'linewidth':8.0, 'linestyle':'-','color':PuBuGn4[1]},
                {'linewidth':8.0, 'linestyle':'-','color':PuBuGn4[2]},
                {'linewidth':8.0, 'linestyle':'-','color':PuBuGn4[3]},",paper_experiments.py,IDEALLab/design_method_recommendation_JMD_2014,1
"
	target_df = pd.read_csv(os.path.join(module_dir,'data.csv'))
	target_df = pd.DataFrame(target_df)
	target_df = target_df.append(train_df)
	target_df = target_df.append(train_df)
	target_df = target_df.drop_duplicates('SeriesName', keep=False)

	x_target = scale(target_df.iloc[:, 5:])
	x_target_labels = target_df.iloc[:, 0]

	clf = RandomForestClassifier()
	clf.fit(x_train,y_train)

	y_target = clf.predict(x_target)

	new_df = pd.DataFrame()
	new_df['seriesName'] = x_target_labels
	new_df['tvdbID'] = target_df.iloc[:, 1]
	new_df['PredictedRating'] = y_target
	new_df['indicator'] = (target_df.iloc[:, 4]/target_df.iloc[:, 3])*new_df['PredictedRating']",tvshow/utils/recommender.py,guptachetan1997/Episodes,1
"X_sel = selectK.transform(X)

features = X.columns[selectK.get_support()]
print (features)

# X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_sel,
#   y, random_state=1301, stratify=y, test_size=0.33)
   
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=600, random_state=1301, max_depth=15,
   criterion='gini', class_weight='balanced')
#Of course the parameter class_weight is important. Especially in our case where the variable to predict, TARGET, 
#is very unbalanced in our sample. I would recommand you to set class_weight=""balanced_subsample"", 
#to have a balanced TARGET for each tree. But if you want weaker trees, class_weight=""balanced"" must be enough.
scores = defaultdict(list)

y = np.array(y.astype(int)).ravel()

# Based on http://blog.datadive.net/selecting-good-features-part-iii-random-forests/",general studies/Random Forest with feature selection.py,Diyago/Machine-Learning-scripts,1
"                for j in range(categories):
                        expected[i, j] = distributions[i, 0] * distributions[j, 1]

        # Calculate kappa
        kappa = 1.0 - (sum(sum(weighted * observed)) / sum(sum(weighted * expected)))
	return kappa

train_images, train_labels, train_files = load_subset('train')
val_images, val_labels, val_files = load_subset('val')
  
rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) 
param_grid = {
    'n_estimators': [500],
    'max_features': ['auto']
}
CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=ShuffleSplit(test_size=0.20, n_iter=1, random_state=0, n=len(train_images)))
CV_rfc = CV_rfc.fit(train_images, train_labels)
val_predictions = CV_rfc.predict(val_images)
print 'Kappa =', kappa(val_labels, val_predictions)
",Code/rf/rf.py,skyfallen/Kaggle-Diabetic-Retinopathy-Detection,1
"
        Returns
        -------
        C : ArrayRDD
            Predicted class label per sample.
        """"""
        check_rdd(X, (sp.spmatrix, np.ndarray))
        return X.map(lambda X: super(SparkRandomForestClassifier, self).predict(X))

    def to_scikit(self):
        new = RandomForestClassifier()
        new.__dict__ = self.__dict__
        return new",splearn/ensemble/__init__.py,lensacom/sparkit-learn,1
"sparse = sparse_mat_train_test.toarray()

# pull out training examples
X = sparse[:classes.shape[0],:]

X_test = sparse[classes.shape[0]:,:]
print X_test.shape

Y = classes_to_Y(classes)

RF = RandomForestClassifier(n_jobs=-1, verbose=42, n_estimators=100)
RF.fit(X, Y)

test_pred = RF.predict(X_test)


print test_pred
test_ids = np.load(""../data/features/test_ids.npy"")
print test_ids
write_predictions(test_pred, test_ids, ""../predictions/rfc_100_big_tfifd.csv"")",models/RF_tfifd.py,sandias42/mlware,1
"        verbose = False
        resampler = Constants.RESAMPLER
        classifier = Constants.DOCUMENT_CLASSIFIER
        random_state = Constants.DOCUMENT_CLASSIFIER_SEED
        classifiers = {
            'logistic_regression': LogisticRegression(C=100),
            'svc': SVC(),
            'kneighbors': KNeighborsClassifier(n_neighbors=10),
            'decision_tree': DecisionTreeClassifier(),
            'nu_svc': NuSVC(),
            'random_forest': RandomForestClassifier(n_estimators=100)
        }
        samplers = {
            'random_over_sampler': RandomOverSampler(
                ratio, random_state=random_state, verbose=verbose),
            'smote_regular': SMOTE(
                ratio, random_state=random_state, verbose=verbose,
                kind='regular'),
            'smote_bl1': SMOTE(
                ratio, random_state=random_state, verbose=verbose,",source/python/etl/reviews_preprocessor.py,melqkiades/yelp,1
"            np.hstack(training_label).astype(int), [0, 255]))
        print 'Create the training set ...'

        # Learn the PCA projection
        ica = FastICA(n_components=sp, whiten=True)
        training_data = ica.fit_transform(training_data)
        testing_data = ica.transform(testing_data)

        # Perform the classification for the current cv and the
        # given configuration
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        pred_prob = crf.fit(training_data,
                            np.ravel(training_label)).predict_proba(
                                testing_data)

        result_cv.append([pred_prob, crf.classes_])

    results_sp.append(result_cv)

# Save the information",pipeline/feature-classification/exp-3/selection-extraction/ica/pipeline_classifier_mrsi.py,I2Cvb/mp-mri-prostate,1
"iteration = 0
for i in train_y:
    if i >  train_rating_average:
        binary_train_y[iteration] = 1.0
    else:
        binary_train_y[iteration] = 0.0
    iteration += 1
##

X_train, X_test, y_train, y_test = train_test_split(train_x, binary_train_y, test_size=0.3, random_state=0)
cls = RandomForestClassifier()
cls.fit(X_train,y_train)
p=cls.predict(X_test)
s = cls.score(X_test,y_test)
print(s)

r = p - y_test
r = np.power(r,2)
print(""wrong guesses: "",np.sum(r))
### on data not used for createing bag of words",analisys/random_forest_classifier.py,chavdim/amazon_comments,1
"        scoring = 'accuracy'

        # Spot Check Algorithms
        models = []
        models.append(('LR', LogisticRegression()))
        models.append(('LDA', LinearDiscriminantAnalysis()))
        models.append(('KNN', KNeighborsClassifier()))
        models.append(('CART', DecisionTreeClassifier()))
        models.append(('NB', GaussianNB()))
        models.append(('SVM', SVC()))
        models.append(('RF',  RandomForestClassifier(n_estimators=100)))

        # evaluate each model in turn
        results = []
        names = []
        for name, model in models:
            kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)
            cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
            results.append(cv_results)
            names.append(name)",core/demo/main.py,god99me/RandomRoughForest,1
"        x[index, :] = np.reshape(img, (1, imageSize))
    return x


def train(xTrain, yTrain):
    """"""
    :param xTrain: training data features
    :param yTrain: training data labels
    :return: model object
    """"""
    RFC = ensemble.RandomForestClassifier()
    RFC.fit(xTrain, yTrain)
    return RFC


def test(xTest, model):
    """"""
    :param xTest: testing data features
    :param model: model object
    :return: predictions list",project/CharRec/tmp1.py,DrigerG/MachineLearning,1
"        else:
            candsplit = title.split('.')
        allmethods = dir(sklearn)
        if len(candsplit) > 1:
            name = candsplit[0]
            #model = sklearn
            return functools.reduce(lambda x, a: getattr(x, a), candsplit[1:], getattr(sklearn, name))(**args)

    def _random_forest(self):
        from sklearn.ensemble import RandomForestClassifier
        return RandomForestClassifier(n_estimators=100)

    def _construct_default_model(self, typetitle):
        """""" This comes from 'type'""""""
        logging.info(""Start to construct deafault model"")
        typetitle = typetitle.lower()
        if typetitle == 'classification':
            return self._random_forest()
        if typetitle == 'regression':
            from sklearn.linear_model import LogisticRegression",scikit_json/scikit_json.py,saromanov/scikit-json,1
"#coding: utf-8
import numpy as np

def trainAndStoreTheModel():
    from scipy.io import loadmat
    from sklearn.ensemble import RandomForestClassifier
    from sklearn import preprocessing
    import pickle
    
    loaded_data = loadmat(""dataToTrainRfModel"", matlab_compatible=True)
    rforee = RandomForestClassifier(n_estimators=2000)
    
    X = loaded_data['X'].squeeze()
    y1 = loaded_data['y1'].squeeze()
    y2 = loaded_data['y2'].squeeze()
    
    scaler = preprocessing.StandardScaler().fit(X)
    X=scaler.transform(X)  
    rfore = rforee.fit(X, y1)
    f = open('projATT_strfore.pckl', 'wb')",src/projATT_functions.py,felipeband/agilent,1
"X[0:, language_pos] = label_language.fit_transform(X[0:, language_pos])
label_country = LabelEncoder()
X[0:, country_pos] = label_country.fit_transform(X[0:, country_pos])
label_content_rating = LabelEncoder()
X[0:, content_rating_pos] = label_content_rating.fit_transform(X[0:, content_rating_pos])

imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
X = imp.fit_transform(X)

#Lets pick the important features
model = RandomForestClassifier()
rfe = RFE(model,15)
rfe = rfe.fit(X,Ystr)

#print(rfe.support_)    #These are the coloumns we're keeping
#print(rfe.ranking_)    #These are ranking the coloumns

#Drop the unimportant features
drop_list = []
for i in range(0,len(rfe.support_)):",DataPreprocessor.py,animesharma/Machine-Learning-Movie-Rating-Prediction,1
"        self.target_feature = target_feature
        self.input_features = input_features
        self.parallel_jobs = 1
        if classifier_type == 'Linear_SVM':
            self.clf = SVC(kernel='linear', C=0.025)
        elif classifier_type == 'Nearest_Neighbours':
            self.clf = KNeighborsClassifier(3)
        elif classifier_type == 'Decision_Tree':
            self.clf = DecisionTreeClassifier(max_depth=5)
        elif classifier_type == 'Random_Forest':
            self.clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, n_jobs=self.parallel_jobs)
        elif classifier_type == 'Naive_Bayes':
            self.clf = GaussianNB()

    def learn(self):
        print ""Input features "", list(self.input_features)
        print ""Target feature "", self.target_feature
        features = self.input_data[self.input_features].values
        labels = self.input_data[self.target_feature].values
        test_data_percentage = 1 - self.train_data_percentage",cognitive/app/ml_models.py,CiscoSystems/cognitive,1
"        mapdf[[catv]]
    except:
        sys.exit('Value given is not a mapping catagory')
    
    #mapping 
    mapdfch=mapdf.copy()
    mapdfen=encode_mapping(mapdf)

    
    if all(isinstance(item, str) for item in mapdfch[catv]) or all(isinstance(item, bool) for item in mapdfch[catv]):
        clfr = RandomForestClassifier(random_state=rng)
        clfr.fit(otulearn.copy().T.as_matrix(), mapdf[catv].values.ravel())
        importance = clfr.feature_importances_
        importance = pd.DataFrame(importance, index=otulearn.index,columns=[""Importance""])
        importance[""Std""] = np.std([tree.feature_importances_ for tree in clfr.estimators_], axis=0)
        importance=importance.sort_values(by=['Importance'],ascending=False)
        return importance
         
    else: 
        clfr = RandomForestRegressor(random_state=rng)",DEICODE/untangle.py,cjm007/DEICODE,1
"X_test = digits.data[border:, :]
y_train = digits.target[:border]
y_test = digits.target[border:]


y_pred = predict(X_train, y_train, X_test)
error_rate = 1 - accuracy_score(y_test, y_pred)
print error_rate
out('5_4.txt', str(error_rate))

rf = RandomForestClassifier(n_estimators=1000)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
error_rate = 1 - accuracy_score(y_test, y_pred)
print error_rate",course2/week5/task5_2.py,astarostin/MachineLearningSpecializationCoursera,1
"from sklearn.ensemble import RandomForestClassifier

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify = tpot_data['class'].values, train_size=0.75, test_size=0.25)


result1 = tpot_data.copy()

# Perform classification with a random forest classifier
rfc1 = RandomForestClassifier(n_estimators=22, max_features=min(52, len(result1.columns) - 1))
rfc1.fit(result1.loc[training_indices].drop('class', axis=1).values, result1.loc[training_indices, 'class'].values)
result1['rfc1-classification'] = rfc1.predict(result1.drop('class', axis=1).values)",tutorials/tpot_titanic_pipeline.py,rlnsanz/delft,1
"        if model_type == 'rf':
            print ""%s training..."" % model_type
            model = RandomForestRegressor(n_estimators=model_param['n_estimators'], n_jobs=-1)
            model.fit( x_train, y_train )
            pred_val = model.predict( x_test )
            return pred_val

        # random forest classification
        if model_type == 'rfC':
            print ""%s training..."" % model_type
            model = RandomForestClassifier(n_estimators=model_param['n_estimators'])
            model.fit( x_train, y_train )
            pred_val = model.predict( x_test )
            return pred_val

        # extra tree regression
        if model_type == 'extratree':
            print ""%s training..."" % model_type
            model = ExtraTreesRegressor(n_estimators=model_param['n_estimators'], max_features=model_param['max_features'], max_depth=model_param['max_depth'], n_jobs=-1, verbose=1, oob_score=True, bootstrap=True)
            model.fit( x_train, y_train )",model_library.py,weaponsjtu/Kaggle_xBle,1
"
    if shuffle:
        idx = np.random.permutation(y.size)
        X = X[idx]
        y = y[idx]
    
    skf = list(StratifiedKFold(y, n_folds))
    

    clfs = [
            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'), 
            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),
            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),
            xgb.XGBClassifier(objective='multi:softprob',silent =False, n_estimators=40)]
    
    dataset_blend_train_list = []
    for j, clf in enumerate(clfs):
        dataset_blend_train_list.append(np.zeros((X.shape[0], num_class-1 )))
    ",code/stack5.py,UKPLab/semeval2017-scienceie,1
"	else:
		#print ""classifier""
		if (model_prefered == ""SVM""):
			clf = svm.SVC()
		elif (model_prefered==""Gaussian""):
			exit() #cannot use gaussian for classification
			clf = GaussianProcess(theta0=1e-2, thetaL=1e-4, thetaU=1e-1)
		elif (model_prefered==""Gradient""):
			clf = GradientBoostingClassifier(n_estimators=100)
		else:
			clf = ensemble.RandomForestClassifier(n_estimators=100)
	clf.fit(training_x, training_y)
	#print ""Done training""
	

	score = clf.score(test_x, test_y)
	if (printscore):
		print ""Score:"", score
	return clf, score
",test.py,Basvanstein/IARI,1
"    return df

def check_model(X,y):
    #use stratified shuffle for minority class
    sss = StratifiedShuffleSplit(y,test_size=0.3,n_iter=2)
    for train_index, test_index in sss:
        X_train, X_cv = X.ix[train_index], X.ix[test_index]
        y_train, y_cv = y.ix[train_index], y.ix[test_index]
    #check max_depth
    for md in (11,12,13,14,15):
        rf = RandomForestClassifier(max_depth=md)
        rf.fit(X_train,y_train)
        ll = log_loss(y_cv,rf.predict_proba(X_cv))
        print 'Max depth: {0}, Log loss: {1}'.format(md,ll)
    #check max_features
    for mf in (1,2,3,4,5,6):
        rf = RandomForestClassifier(max_depth=13,max_features=mf)
        rf.fit(X_train,y_train)
        ll = log_loss(y_cv,rf.predict_proba(X_cv))
        print 'Max features: {0}, Log loss: {1}'.format(mf,ll)",sf_crime_model_1.py,lingcheng99/Crime-in-San-Francisco,1
"                                    dt_train = feature_combiners[cl].fit_transform(dt_train_cluster)
                                    encode_time_train = time() - start
                                else:
                                    dt_train = feature_combiners[cl].transform(dt_train_cluster)

                                start = time()
                                dt_test = feature_combiners[cl].transform(dt_test_cluster)
                                encode_time_test = time() - start

                                #### FIT CLASSIFIER ####
                                cls = RandomForestClassifier(n_estimators=rf_n_estimators, max_features=rf_max_features, random_state=random_state)
                                cls.fit(dt_train, train_y)

                                #### PREDICT ####
                                start = time()
                                if len(train_y.unique()) == 1:
                                    hardcoded_prediction = 1 if train_y[0] == pos_label else 0
                                    current_cluster_preds = [hardcoded_prediction] * len(relevant_test_cases)
                                else:
                                    # make predictions",experiments_param_optim_cv/run_cluster_optim_cv.py,irhete/predictive-monitoring-benchmark,1
"

	# ------------- Random Forest (Extra Trees) ---------------
	# Note, 2 procs is faster than 8
	truthLabels = np.array([int(x['label']) for x in p])
	from sklearn.ensemble import RandomForestClassifier	
	from sklearn.ensemble import ExtraTreesClassifier
	fCount = 5#featuresNorm.shape[1]
	# forest = ExtraTreesClassifier(n_estimators=10, compute_importances=False, n_jobs=4, bootstrap=False, random_state=0, max_features=1)#26)
	forest = ExtraTreesClassifier(n_estimators=100, compute_importances=True, n_jobs=7, bootstrap=True, random_state=0, max_features=fCount)
	# forest = RandomForestClassifier(n_estimators=30, compute_importances=True, n_jobs=4, bootstrap=True, random_state=0, max_features=10)#26)
	t0 = time.time()
	forest.fit(X, truthLabels)
	print ""Time:"", time.time()-t0
	importances = forest.feature_importances_
	forestScore = forest.score(X, truthLabels) # 100%
	predF = forest.predict(X)
	print forestScore
	if 1:
		figure(3)",pyKinectTools/scripts/AMIA_recognitionTests.py,colincsl/pyKinectTools,1
"
#Convert to a numpy array
train_data = train_df.values
test_data = test_df.values




def train(estimators, features):
    #print 'Training!'
    forest = RandomForestClassifier(n_estimators = estimators, max_features = int(math.sqrt(features)))
    forest = forest.fit(train_data[0::,1::], train_data[0::,0])
    return forest

def predict(forest):
    #print 'Predicting!'
    return forest.predict(test_data).astype(int)

def save(output):
    print 'Saving...'",ImprovedRandomForest.py,Lastin/TSC,1
"	    property_list_list.append(property_list)
	dataDescrs_array = np.asarray(property_list_list)
	dataActs_array   = np.array(TL_list)

	for randomseedcounter in range(1,11):
                if self.verbous: 
                    print ""################################""
                    print ""try to calculate seed %d"" % randomseedcounter
                X_train,X_test,y_train,y_test = cross_validation.train_test_split(dataDescrs_array,dataActs_array,test_size=.4,random_state=randomseedcounter)
#            try:
                clf_RF     = RandomForestClassifier(n_estimators=100,random_state=randomseedcounter)
                clf_RF     = clf_RF.fit(X_train,y_train)

                cv_counter = 5

                scores = cross_validation.cross_val_score( clf_RF, X_test,y_test, cv=cv_counter,scoring='accuracy')

                accuracy_CV = round(scores.mean(),3)
                accuracy_std_CV = round(scores.std(),3)
   ",Contrib/pzc/p_con.py,johnmay/rdkit,1
"                                        ('lr2', LogisticRegression())],
                            voting='soft')
    msg = (""This VotingClassifier instance is not fitted yet. Call \'fit\'""
           "" with appropriate arguments before using this method."")
    assert_raise_message(NotFittedError, msg, eclf.predict_proba, X)


def test_majority_label_iris():
    """"""Check classification by majority label on dataset iris.""""""
    clf1 = LogisticRegression(random_state=123)
    clf2 = RandomForestClassifier(random_state=123)
    clf3 = GaussianNB()
    eclf = VotingClassifier(estimators=[
        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
        voting='hard')
    scores = cross_val_score(eclf, X, y, cv=5, scoring='accuracy')
    assert_almost_equal(scores.mean(), 0.95, decimal=2)


def test_tie_situation():",projects/scikit-learn-master/sklearn/ensemble/tests/test_voting_classifier.py,DailyActie/Surrogate-Model,1
"from sklearn.feature_extraction import DictVectorizer
from sklearn.pipeline import Pipeline
from sklearn.externals import joblib
from sklearn.cross_validation import cross_val_score
from sklearn.metrics import confusion_matrix

def fit(X, Y):
  models = {
    'LogisticRegression': LogisticRegression(),
    'GradientBoostingClassifier': GradientBoostingClassifier(n_estimators=150),
    'RandomForestClassifier': RandomForestClassifier(n_estimators=150)
  }

  best_score = 0
  best_model = ''
  for model in models:
    vec = DictVectorizer(sparse=False)
    clf = models[model]
    pl = Pipeline([('vec', vec), ('clf', clf)])
",fit.py,wlattner/mlserver,1
"  dif = mn1 - mn0
  
  I_features = np.argsort( -np.abs(dif) )
  
  m = np.vstack( (mn0,mn1,dif) ).T[I_features,:]
  M = pd.DataFrame( m, columns = [""mean0"",""mean1"",""dif""], index = feature_names[I_features])
  
  log_y_est = np.dot( dif, X.T )
  y_est = 1.0 / (1.0 + np.exp( -log_y_est ) )
  
  rf = RandomForestClassifier( n_estimators = 100 )
  rf.fit( X, y )
  I_rf = np.argsort( -rf.feature_importances_ )
  print feature_names[I_rf][:20]
  print M[:20]
  pp.show()
  


",tcga_encoder/models/analyses.py,tedmeeds/tcga_encoder,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/numerical_mean_ordinal.py,diogo149/CauseEffectPairsPaper,1
"
X = v[:,0:target_index]
y = v[:,target_index]

random = 99 # pick reproducible pseudo-random sequence

n_estimators = int(sys.argv[1])
min_samples_leaf = int(sys.argv[2])

start = time.clock()
clf = RandomForestClassifier(n_estimators=n_estimators, oob_score=True,
                             max_features=""sqrt"", bootstrap=True,
                             min_samples_leaf=min_samples_leaf, criterion=""entropy"",
                             random_state=random)
clf = clf.fit(X, y)
stop = time.clock()
oob_error = 1 - clf.oob_score_
print ""oob %.5f"" % oob_error

print ""Fitting %d estimators %d min leaf size %f seconds\n"" % (n_estimators,min_samples_leaf,stop-start)",python/higgs_timing.py,parrt/AniML,1
"testing_raw = f.read()

testing_data = json.loads(testing_raw)

#from sklearn.cross_validation import cross_val_score
from sklearn.ensemble import RandomForestClassifier

X = training_data[""data""]
Y = training_data[""categories""]

clf = RandomForestClassifier(n_estimators=10000)

#scores = cross_val_score(clf, X, Y)

clf = clf.fit(X, Y)

prediction_results = clf.predict(testing_data[""data""])

true_positive = 0
false_positive = 0",src/analysis/random_forest.py,sqrlab/GitView,1
"    def resize(self, iim, size):
        im_resize = cv2.resize(iim, (size, size), interpolation=cv2.INTER_CUBIC)
        im_resize[im_resize > 0] = 255
        return im_resize


class CharacterClassifier:

    def __init__(self, icc=None, skcl=None, featurefactory=None):
        if skcl is None:
            self.skcl = RandomForestClassifier()
        else:
            self.skcl = None
        if icc is None:
            self.icc = ImageComponentCollection()
        else:
            self.icc = icc
        if featurefactory is None:
            self.featurefactory = FeatureFactory(size=30)
        else:",character_classifier.py,hunzikp/map-classification,1
"    Ridge(),
    LinearRegression(),
    DecisionTreeRegressor(random_state=0),
    RandomForestRegressor(random_state=0),
    GradientBoostingRegressor(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsRegressor(),
]
CLASSIFICATION_ESTIMATORS = [
    LogisticRegression(random_state=0),
    DecisionTreeClassifier(random_state=0),
    RandomForestClassifier(random_state=0),
    GradientBoostingClassifier(subsample=0.5, n_estimators=10, random_state=0),
    KNeighborsClassifier(),
    GaussianNB(),
]

""""""
Functions to provide a value of how good a fit on a variable is
""""""
REGRESSION_METRICS = [",configs/mean_aggregate_only.py,diogo149/CauseEffectPairsPaper,1
"test_dataset = fold_datasets[-1]

# Get supports on test-set
support_generator = dc.data.SupportGenerator(
    test_dataset, n_pos, n_neg, n_trials)

# Compute accuracies
task_scores = {task: [] for task in range(len(test_dataset.get_task_names()))}
for (task, support) in support_generator:
  # Train model on support
  sklearn_model = RandomForestClassifier(
      class_weight=""balanced"", n_estimators=100)
  model = dc.models.SklearnModel(sklearn_model)
  model.fit(support)

  # Test model
  task_dataset = dc.data.get_task_dataset_minus_support(
      test_dataset, support, task)
  y_pred = model.predict_proba(task_dataset)
  score = metric.compute_metric(",examples/low_data/sider_rf_one_fold.py,ktaneishi/deepchem,1
"from nonconformist.evaluation import ClassIcpCvHelper, RegIcpCvHelper
from nonconformist.evaluation import class_avg_c, class_mean_errors
from nonconformist.evaluation import reg_mean_errors, reg_median_size


# -----------------------------------------------------------------------------
# Classification
# -----------------------------------------------------------------------------
data = load_iris()

icp = IcpClassifier(ClassifierNc(ClassifierAdapter(RandomForestClassifier(n_estimators=100)),
                                 MarginErrFunc()))
icp_cv = ClassIcpCvHelper(icp)

scores = cross_val_score(icp_cv,
                         data.data,
                         data.target,
                         iterations=5,
                         folds=5,
                         scoring_funcs=[class_mean_errors, class_avg_c],",examples/cross_validation.py,donlnz/nonconformist,1
"from sklearn.qda import QDA
clf = QDA(priors=None, reg_param=0.001).fit(X_cropped, np.ravel(y_cropped[:]))
y_validation_predicted = clf.predict(X_validation)
print ""Error rate for QDA (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)



# Start Random Forest Classification
print ""Performing Random Classification:""
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=500)
forest = forest.fit(X_cropped, np.ravel(y_cropped[:]))
y_validation_predicted = forest.predict(X_validation)
print ""Error rate for Random Forest (Validation): "", ml_aux.get_error_rate(y_validation,y_validation_predicted)
# ml_aux.plot_confusion_matrix(y_validation, y_validation_predicted, ""CM Random Forest (t1)"")
# plt.show()


# Start k nearest neighbor Classification
print ""Performing kNN Classification:""",Code/Machine_Learning_Algos/training_t3.py,nishantnath/MusicPredictiveAnalysis_EE660_USCFall2015,1
"# Since final-data.csv contains the whole
# shuffled titanic data, we used 2/3 of 
# the data for training and 1/3 for testing
train = data[0:872]
test  = data[872::]


# We place a RandomForestClassifier inside a Grid Search
# in order to find the optimal parameters for an optimized
# prediction
forest = RandomForestClassifier(random_state=321)
grid   = GridSearchCV(forest, search_param, cv=12, verbose=0)
grid.fit(train[attrib], train['Survived'])

print grid.best_score_
print grid.best_params_

# -----------------------------------------------------------------------------------
# GridSearch Sample Output: 
# {n_estimators=65, criterion='gini', max_depth=5, max_features=0.5,random_state=321}",gridsearch-param.py,cadrev/Titanic-Prediction,1
"for filepath in filepaths:
	if os.path.getsize(filepath) == 0:
		filename = os.path.basename(filepath)
		df_full = df_full[df_full.file != filename]
		if filename in test_files:
			test_files.remove(filename)


#https://www.youtube.com/watch?v=0GrciaGYzV0
print('--- Training random forest')
clf = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=0)
#remove data set #5
df_full = df_full[df_full.sponsored.notnull()]

train_data = df_full[df_full.sponsored.notnull()].fillna(0)
test = df_full[df_full.file.isin(test_files)].fillna(0)
clf.fit(train_data.drop(['file', 'sponsored'], 1), train_data.sponsored)

#normalized value between 0 and 1
feature_importances = pd.Series(clf.feature_importances_, index=train_data.drop(['file', 'sponsored'], 1).columns)",scikit_generate_prediction_evaluation.py,carlsonp/kaggle-TrulyNative,1
"

# DL_lh = Likelihood1D(np.linspace(0, 1, 60))
# DL_lh.fit(y_dl[signal], y_dl[background], weights=(weights[signal], weights[background]))
# DLlikelihood = DL_lh.predict(y_dl)



# from sklearn.ensemble import RandomForestClassifier

# rf = RandomForestClassifier(n_jobs=2)

# df = np.zeros((y_dl.shape[0], 3))
# df[:, 0] = y_dl
# df[:, 1] = mass
# df[:, 2] = tau_21

# rf.fit(df[:n_train], y_[:n_train], sample_weight=weights[:n_train])

",training/plotscript.py,ml-slac/deep-jets,1
"# multiprocessing module for limitations. In the future, we might include additional
# parameters to the function, but for now that is not possible.
X,Y = sklearn.datasets.make_classification(1000, 20, random_state=2)		# seed yields a mediocre initial accuracy on my machine
X_train, X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(X,Y, test_size=0.33, random_state=1)

# The function to be minimezed for this example is the mean accuracy of a random
# forest on the test data set. Note: because SMAC minimizes the objective, we return
# the negative accuracy in order to maximize it.
def random_forest(n_estimators,criterion, max_features, max_depth):
	
	predictor = sklearn.ensemble.RandomForestClassifier(n_estimators, criterion, max_features, max_depth)
	predictor.fit(X_train, Y_train)
	
	return -predictor.score(X_test, Y_test)

parameter_definition=dict(\
		max_depth   =(""integer"", [1,10],    4),
		max_features=(""integer"", [1,20],   10),				
		n_estimators=(""integer"", [10,100], 10, 'log'),			
		criterion   =(""categorical"", ['gini', 'entropy'], 'entropy'),",examples/sklearn_example.py,belkhir-nacim/smac_python,1
"    estimators_grid = {'C': [10 ** k for k in np.arange(-3, 1, 0.05)]}
    # estimators_grid = {'C': np.arange(0.01, 2, 0.01)}
    gs = grid_search(predictor, estimators_grid, X, y)
    print 'Grid Search CV for Logistic Regression. Best parameter C = %.4f, best score = %.8f' % \
          (gs.best_params_['C'], gs.best_score_)

    return gs.best_estimator_, gs.best_score_


def test_rf_cv(X, y):
    predictor = RandomForestClassifier(random_state=42)
    estimators_grid = {'criterion': ['entropy', 'gini'], 'n_estimators': [150, 200, 250]}
    gs = grid_search(predictor, estimators_grid, X, y)
    print 'Grid Search CV for RandomForestClassifier. Best parameter n_estimators = %d, ' \
          'criterion = %s, best score = %.8f' % \
          (gs.best_params_['n_estimators'], gs.best_params_['criterion'], gs.best_score_)

    return gs.best_estimator_, gs.best_score_

",validation.py,astarostin/ALTA-2016-Challenge,1
"        # Get the spatial information
        training_data_spa = [arr for idx_arr, arr in enumerate(data[-1])
                             if idx_arr in idx_patient_training]
        # Concatenate the data
        training_data_mod = np.vstack(training_data_mod)
        training_data_spa = np.vstack(training_data_spa)
        # Concatenate spatial information and modality information
        training_data = np.hstack((training_data_mod, training_data_spa))

        # Create the current RF
        crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
        crf.fit(training_data, training_label)
        # Add the classifier
        rf_ensemble.append(crf)

    # Get the labels
    validation_label = [arr for idx_arr, arr in enumerate(label)
                        if idx_arr in idx_patient_validation]
    validation_label = np.ravel(label_binarize(
        np.hstack(validation_label).astype(int), [0, 255]))",pipeline/feature-classification/exp-2/pipeline_classifier_stacking_gradient_boosting.py,I2Cvb/mp-mri-prostate,1
"
model = word2vec.Word2Vec.load(""300features_40minwords_10context"")
train, clean_train_reviews = load(remove_stopwords=True)
test, clean_test_reviews = load(test=True, remove_stopwords=True)

trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, 300)
testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, 300)


# 
forest = RandomForestClassifier(n_estimators=100, verbose=True)
forest = forest.fit(trainDataVecs, train[""sentiment""])

result = forest.predict(testDataVecs)

output = pd.DataFrame(data={""id"": test[""id""], ""sentiment"": result})
output.to_csv(""avg_w2v.csv"", index=False, quoting=3)",BagOfWordsMeetsBagsOfPopcorn/averageVec.py,IgowWang/MyKaggle,1
"
import csvio
X = csvio.load_csv_data(""../../data/train_inputs.csv"")
X_train, X_test = X[:40000], X[40000:]
y = csvio.load_csv_data(""../../data/train_outputs.csv"", data_type=int)
y_train, y_test = y[:40000], y[40000:]



from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
# 0.24728888

from sklearn import linear_model
clf = linear_model.SGDClassifier()
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
# 0.2051",experiment_logs/first_submission_logs.py,deepanjanroy/aml3,1
"from tests.factories import EstimatorFactory
from tests.shared import db


@pytest.mark.usefixtures(""temporary_root_dir"")
class TestEstimator:

    def test_estimator_init(self):
        es = Estimator()

        rfc = RandomForestClassifier()
        es.estimator = rfc

        assert es.estimator is rfc
        assert es.hash == '83d11c9bf77830ad42c4e93abe9cf397'
        assert es.file_name == 'files/estimators/83d11c9bf77830ad42c4e93abe9cf397'

    def test_estimator_init_with_factory(self):
        es = EstimatorFactory()
",tests/test_estimators.py,fridiculous/estimators,1
"        raw_names[i] = bdata['id']
        i += 1
    
    traindata_mask = np.random.uniform(0,1, r_length) <=.20
    train, testing = np.where(traindata_mask == True), np.where(traindata_mask == False)

    traindata = raw_data[train]
    testdata = raw_data[testing]


    clf = RandomForestClassifier(n_jobs=2)
    clf.fit(traindata, raw_labels[train])

    preds = clf.predict(testdata)

    print preds
    print raw_labels[testing]
    correct = preds - raw_labels[testing]
    idx = np.where(correct == 0)
    correct_ratio = len(correct[idx]) / float(len(correct))",Classification/classifysnova.py,tayebzaidi/snova_analysis,1
"train = train.iloc[np.random.permutation(len(train))]

# Assign data for validation
amount = int(0.8*len(train))
validation = train[amount:]
# train = train[:amount]
print(""Assign data for validation: successfully"")

# Classifier
# clf = tree.DecisionTreeClassifier()
clf = RandomForestClassifier(n_estimators = 200, n_jobs = -1)
print(""Classifier: successfully"")

# Traning
clf.fit(train[column_labels], train[""status_group""])
print(""Traning: successfully"")

# Accuracy
accuracy = accuracy_score(clf.predict(validation[column_labels]), validation[""status_group""])
print(""Accuracy = "" + str(accuracy))",ml_v1.py,BhagyeshVikani/Pump-it-Up-Data-Mining-the-Water-Table,1
"
def test_roc_nonrepeating_thresholds():
    # Test to ensure that we don't return spurious repeating thresholds.
    # Duplicated thresholds can arise due to machine precision issues.
    dataset = datasets.load_digits()
    X = dataset['data']
    y = dataset['target']

    # This random forest classifier can only return probabilities
    # significant to two decimal places
    clf = ensemble.RandomForestClassifier(n_estimators=100, random_state=0)

    # How well can the classifier predict whether a digit is less than 5?
    # This task contributes floating point roundoff errors to the probabilities
    train, test = slice(None, None, 2), slice(1, None, 2)
    probas_pred = clf.fit(X[train], y[train]).predict_proba(X[test])
    y_score = probas_pred[:, :5].sum(axis=1)  # roundoff errors begin here
    y_true = [yy < 5 for yy in y[test]]

    # Check for repeating values in the thresholds",projects/scikit-learn-master/sklearn/metrics/tests/test_ranking.py,DailyActie/Surrogate-Model,1
"    h1 = 1
    h2 = 1

    #names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",
    #         ""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
    classifiers = {
        ""nearest_neighbors"": KNeighborsClassifier(5),
        ""linear_svm"": SVC(kernel=""linear"", C=0.025),
        ""poly_svm"": SVC(kernel=""poly"", degree=2),#, gamma=1, C=1),
        ""decision_tree"": DecisionTreeClassifier(max_depth=None),
        ""random_forest"": RandomForestClassifier(max_depth=None, n_estimators=5, max_features=5, criterion=""gini""),
        ""adaboost"": AdaBoostClassifier(),
        ""naive_bayes"": GaussianNB(),
        ""lda"": LDA(),
        ""qda"": QDA()}

    """"""
    figure = pl.figure(figsize=(27, 9))
    i = 1
    #pdb.set_trace()",svm.py,malimome/game-auth,1
"        calibrated_log_loss = log_loss(y_test, probas)
        assert_greater_equal(uncalibrated_log_loss, calibrated_log_loss)

    # Test that calibration of a multiclass classifier decreases log-loss
    # for RandomForestClassifier
    X, y = make_blobs(n_samples=100, n_features=2, random_state=42,
                      cluster_std=3.0)
    X_train, y_train = X[::2], y[::2]
    X_test, y_test = X[1::2], y[1::2]

    clf = RandomForestClassifier(n_estimators=10, random_state=42)
    clf.fit(X_train, y_train)
    clf_probs = clf.predict_proba(X_test)
    loss = log_loss(y_test, clf_probs)

    for method in ['isotonic', 'sigmoid']:
        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)
        cal_clf.fit(X_train, y_train)
        cal_clf_probs = cal_clf.predict_proba(X_test)
        cal_loss = log_loss(y_test, cal_clf_probs)",projects/scikit-learn-master/sklearn/tests/test_calibration.py,DailyActie/Surrogate-Model,1
"        # Extract training and test set for current CV fold
        X_train = X[train_index,:]
        y_train = y[train_index,:]
        X_test = X[test_index,:]
        y_test = y[test_index,:]

        # Fit the different classifiers
        if algorithm == ""svm"":
            clf = svm.SVC()
        elif algorithm == ""random_forest"":
            clf = ensemble.RandomForestClassifier()
        elif algorithm == ""logistic_regression"":
            clf = linear_model.LogisticRegression()
        elif algorithm == ""ada_boost"":
            clf = ensemble.AdaBoostClassifier()

        # Evaluate the different classifiers
        clf.fit(X_train, y_train.ravel())
        y_est = clf.predict(X_test)
        test_accuracy[k] = sklearn.metrics.accuracy_score(y_test, y_est)",infomine/gender_classifier.py,rluch/InfoMine,1
"    assert clf.verbose == True
    assert clf.n_jobs == 3

### Parallelization tests

def test_relieff_pipeline():
    """"""Ensure that ReliefF works in a sklearn pipeline when it is parallelized""""""
    np.random.seed(49082)

    clf = make_pipeline(ReliefF(n_features_to_select=2, n_neighbors=100, n_jobs=-1),
                        RandomForestClassifier(n_estimators=100, n_jobs=-1))

    assert np.mean(cross_val_score(clf, features, labels, cv=3)) > 0.7

def test_relieff_pipeline_parallel():
    """"""Ensure that ReliefF works in a sklearn pipeline where cross_val_score is parallelized""""""
    np.random.seed(49082)

    clf = make_pipeline(ReliefF(n_features_to_select=2, n_neighbors=100, n_jobs=-1),
                        RandomForestClassifier(n_estimators=100, n_jobs=-1))",tests.py,EpistasisLab/scikit-rebate,1
"
    
    

    # train random forest
    print(""\nFinding optimal random forest"")
    rf_param_grid = {""max_features"": ['sqrt'],
                     'n_estimators': [500],
                     ""max_depth"": [None]}
    # do cross validation to determine optimal parameters to the model
    #rf = run_grid_search(X=x_train, y=y_train, model=RandomForestClassifier(random_state=random_seed), param_grid=rf_param_grid, cv=args.cv, n_jobs=args.n_jobs, verbose=args.verbose)
    #print(""Best random forest performance on test set:"", rf.score(x_test, y_test))
    #y_pred = rf.predict(x_test)
    #cnf_matrix = confusion_matrix(y_test, y_pred)
    #print(cnf_matrix)
    #print(classification_report(y_test, y_pred))
    #np.savetxt(fname=""/home/srivbane/shared/caringbridge/data/word_embeddings/rf_confusion.txt"", X=cnf_matrix, fmt='%1.4f')

    #print(""\nRF splitting cancer up first"")
    #rf_cancer = run_grid_search(X=x_train, y=y_train_c, model=RandomForestClassifier(random_state=random_seed), param_grid=rf_param_grid, cv=args.cv, n_jobs=args.n_jobs, verbose=args.verbose)",src/classify_health_condition/word2vec_features.py,robert-giaquinto/text-analysis,1
"import numpy as np
df = pd.read_csv('../save/trainDataFeatures.tsv', sep='\t', index_col=0)
columns = df.columns[3:]
X = np.asarray(df[columns])
y = np.asarray(df.sentiment.transpose())

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(min_samples_leaf=10, min_samples_split=10)

rf.fit(X=X_train, y = y_train)
y_predict = rf.predict(X_test)

from sklearn.metrics import roc_auc_score, confusion_matrix

auc = roc_auc_score(y_test, y_predict)
con = confusion_matrix(y_test, y_predict)
",src/ensemble.py,switchkiller/ml_imdb,1
"    training_label = [arr for idx_arr, arr in enumerate(label)
                     if idx_arr != idx_lopo_cv]
    # Concatenate the data
    training_data = np.vstack(training_data)
    training_label = label_binarize(np.hstack(training_label).astype(int),
                                    [0, 255])
    print 'Create the training set ...'

    # Perform the classification for the current cv and the
    # given configuration
    crf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    pred_prob = crf.fit(training_data, np.ravel(training_label)).predict_proba(
        testing_data)

    result_cv.append([pred_prob, crf.classes_])

# Save the information
path_store = '/data/prostate/results/mp-mri-prostate/exp-2/aggregation'
if not os.path.exists(path_store):
    os.makedirs(path_store)",pipeline/feature-classification/exp-2/pipeline_classifier_aggregation.py,I2Cvb/mp-mri-prostate,1
"    # y_actu = pd.Series(y_test['SeriousDlqin2yrs'].values, name='Actual')
    # y_pred = pd.Series(model.predict(X_test), name='Predicted')
    # df_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)
    # print df_confusion
    #
    # # F-1 score ########################################################
    # print(""F1 = {}"".format(f1_score(y_actu, y_pred, average='binary')))

    ####################################################################################
    # random forest regression ##############################################################
    rf = RandomForestClassifier(n_estimators=80, n_jobs=4)
    scores = cv.cross_val_score(rf, X, numpy.ravel(y)
                                , cv=5, scoring='roc_auc')

    rf.fit(X, numpy.ravel(y))

    # model = rf.fit(X_train, numpy.ravel(y_train))
    # acc =  model.score(X_test, y_test)

    # Confusion matrix table ###########################################################",Kaggle_CreditRisk/Main.py,kraktos/Data_Science_Analytics,1
"
    X_train, X_validation, Y_train, Y_validation = cross_validation.train_test_split(X, Y, test_size=VALIDATION_SIZE, random_state=RAND_SEED)

    return (X_train, X_validation, Y_train, Y_validation)
    
#Feature Selection
def featureSelection(features, X_train, Y_train):
    print(""\n=== Feature Selection ==="")
    
    printFeaturesByRelevance(features, X_train, Y_train, ExtraTreesClassifier())
    printFeaturesByRelevance(features, X_train, Y_train, RandomForestClassifier())
    
    
#Print Features by Relevance
def printFeaturesByRelevance(features, X_train, Y_train, model):
    
    print ""\nFeatures by Relevance (using '%s'):"" % type(model).__name__
    model.fit(X_train, Y_train)

    idx = 0",lib/eda1.py,FabricioMatos/ifes-dropout-machine-learning,1
"
spam_X, spam_y = make_classification(5000)

# split the datainto training and test set
spam_X_train, spam_X_test, spam_y_train, spam_y_test = xval.train_test_split(
                                                       spam_X, spam_y,
                                                       test_size=0.2)

# create RandomForestClassifier
n_trees = 500
spam_RFC = RandomForestClassifier(max_features=5, n_estimators=n_trees,
                                  random_state=42)
spam_RFC.fit(spam_X_train, spam_y_train)
spam_y_hat = spam_RFC.predict_proba(spam_X_test)

# calculate inbag and unbiased variance
spam_inbag = fci.calc_inbag(spam_X_train.shape[0], spam_RFC)
spam_V_IJ_unbiased = fci.random_forest_error(spam_RFC, spam_inbag,
                                             spam_X_train, spam_X_test)
",examples/plot_spam.py,kpolimis/sklearn-forest-ci,1
"                   ylim=(X[:, 1].min(), X[:, 1].max()))


# In[6]:

interact(fit_randomized_tree, random_state=[0, 100]); 


# In[7]:

clf = RandomForestClassifier(n_estimators=100, random_state=0)
visualize_tree(clf, X, y, boundaries=False); 


# ## Regression 

# In[8]:

x = 10 * np.random.rand(100)
",notebooks/02-Exploratory_(Interactive)_Data_Analysis.py,MrChristophRivera/jupyter-tips-and-tricks,1
"
test_df = pd.read_csv(zipfile.ZipFile('/home/namukhtar/Datasets/kaggle/forest-cover-type-prediction/test.csv.zip').open('test.csv'), header=0)
# print test_df.head(10)
test_data = pre_process(test_df)

# Import the random forest package
from sklearn.ensemble import RandomForestClassifier 

# Create the random forest object which will include all the parameters
# for the fit
forest = RandomForestClassifier(n_estimators = 100)

# Fit the training data to the Survived labels and create the decision trees
forest = forest.fit(train_data[0::,:-1],train_data[0::,-1])

# Take the same decision trees and run it on the test data
output = forest.predict(test_data)

out_df = pd.DataFrame({'Id' : test_data[0::, 0], 'Cover_Type' : output})
out_df[""Id""] = out_df[""Id""].astype(""int"")",src/main/python/forestcover.py,Chaparqanatoos/kaggle-knowledge,1
"    for i in range(20000):
        tt.append(i % 20)
    mfeature['is_train'] = tt
    rightAll = 0
    for i in range(20):
        train, test = mfeature[mfeature['is_train'] != i], mfeature[mfeature['is_train'] == i]
        tmp1 = np.array([t != i for t in mfeature['is_train']])
        tmp2 = np.array([t == i for t in mfeature['is_train']])
        trainTar, testTar = target[tmp1], target[tmp2]
        # testId = idList[tmp2]
        clf = RandomForestClassifier(n_estimators=100, min_samples_split=17)  # ,max_features=0.5)
        features = mfeature.columns[:-1]
        clf.fit(train[features], trainTar)
        preds = clf.predict(test[features])
        right = 0
        for i in range(len(preds)):
            if preds[i] == testTar[i]:
                right += 1.0
                rightAll += 1.0
            outFiles.write(str(preds[i]) + '\t' + str(testTar[i]) + '\n')",position_predict/data_deal/salary_merge.py,yinzhao0312/Position-predict,1
"        else:
            self.project_folder = project_folder

    def define_clfs_params(self):
        '''
        Defines all relevant parameters and classes for classfier objects.
        Edit these if you wish to change parameters.
        '''
        # These are the classifiers; add new classifiers here
        self.clfs = {
            'RF': RandomForestClassifier(n_estimators = 50, n_jobs = -1),
            'ET': ExtraTreesClassifier(n_estimators = 10, n_jobs = -1, criterion = 'entropy'),
            'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth = [1, 5, 10, 15]), algorithm = ""SAMME"", n_estimators = 200),
            'LR': LogisticRegression(penalty = 'l1', C = 1e5),
            'SVM': svm.SVC(kernel = 'linear', probability = True, random_state = 0),
            'GB': GradientBoostingClassifier(learning_rate = 0.05, subsample = 0.5, max_depth = 6, n_estimators = 10),
            'NB': GaussianNB(),
            'DT': DecisionTreeClassifier(),
            'SGD': SGDClassifier(loss = 'log', penalty = 'l2'),
            'KNN': KNeighborsClassifier(n_neighbors = 3),",pipeline/model_loop.py,aldengolab/acg-ml-pipeline,1
"
data = []
for i in range(training_data.shape[0]):
    data.append(np.hstack(([training_data.iloc[i]['dim{}'.format(j)] for j in range(20)], training_data.iloc[i].fgHaralick.flatten())))

labels = training_data.breed

X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, stratify=labels, random_state=42)

print('[INFO] fitting model')
model = RandomForestClassifier(max_depth=50, n_estimators=40, random_state=42, n_jobs=-1) # best parameters
model.fit(X_train, y_train)

print('[INFO] baseline score:')
get_score(model, X_test)

# check what adding noise to the training data does
np.random.seed(42)
for pct in [0.1, 0.2, 0.3, 0.4]:
    X_train_noisy = X_train[:] # copy list",process_ims/check_peturb.py,nateGeorge/IDmyDog,1
"#}

clf_grid = {
        'vot__weights': [[1, 2, 2, 3], [1, 1, 1, 1]]
}
#
## New record! 0.88572
#clf_vot = VotingClassifier(
#            [
#                ('extra', ExtraTreesClassifier(n_estimators=512, criterion='entropy', bootstrap=True, max_features=""sqrt"", min_samples_leaf=2)),
#                ('rf', RandomForestClassifier(
#                        n_estimators=512, n_jobs=1, max_features=0.1, criterion='entropy', bootstrap=True, min_samples_leaf=2)
#                ),
#                ('gradboost', GradientBoostingClassifier(n_estimators=512, max_depth=8, learning_rate=0.05 , max_features=""sqrt"", min_samples_leaf=2)),
#                ('xgboost', xgb.XGBClassifier(
#                        n_estimators=512, 
#                        max_depth=8, 
#                        silent=True, 
#                        objective=""binary:logistic"",
#                        learning_rate=0.05,",best-model.py,JoostVisser/ml-assignment2,1
"    report.scatter([(X.columns[0], X.columns[2])], mask=mask).plot()
    report.learning_curve(RocAuc(), mask=mask).plot()
    report.metrics_vs_cut(significance, mask=mask).plot()


def test_own_classification_reports():
    """"""
    testing classifier.test_on
    """"""
    X, y, sample_weight = generate_classification_data()
    clf = SklearnClassifier(RandomForestClassifier())
    clf.fit(X, y, sample_weight=sample_weight)
    report = clf.test_on(X, y, sample_weight=sample_weight)
    roc1 = report.compute_metric(RocAuc())

    lds = LabeledDataStorage(X, y, sample_weight=sample_weight)
    roc2 = clf.test_on_lds(lds=lds).compute_metric(RocAuc())
    assert roc1 == roc2, 'Something wrong with test_on'

",tests/test_factory_clf.py,yandex/rep,1
"# for key, value in df.iteritems():
#     temp = value
#     tempk = key
#     dflist.append(temp)
#     keylist.append(tempk)
# Y = dflist[0]
# X = dflist[2:]


#-----Making classifier
#estimator = RandomForestClassifier(n_estimators=30)

# estimator = tree.DecisionTreeClassifier()
# estimator = estimator.fit(train_samps.values, train_labels.values)


# print anses
# print test_labels.values
# print sum(anses)/len(anses)
# print sum(test_labels.values)/len(test_labels.values)",decisionpath.py,ucsd-progsys/ml2,1
"  # set_trace()
  return _Abcd(before=actual, after=preds, show=False)[-1]


def rforest(train, test, tunings=None, smoteit=True, duplicate=True):
  ""RF ""
  # Apply random forest Classifier to predict the number of bugs.
  if smoteit:
    train = SMOTE(train, atleast=50, atmost=101, resample=duplicate)
  if not tunings:
    clf = RandomForestClassifier(n_estimators=100, random_state=1)
  else:
    clf = RandomForestClassifier(n_estimators=int(tunings[0]),
                                 max_features=tunings[1] / 100,
                                 min_samples_leaf=int(tunings[2]),
                                 min_samples_split=int(tunings[3])
                                 )
  train_DF = formatData(train)
  test_DF = formatData(test)
  features = train_DF.columns[:-2]",src/Planners/XTREE/Prediction.py,ai-se/XTREE,1
"X_meta = [] 
X_test_meta = []

print ""Build meta""

for i in range(y_base.shape[1]) :
    print i
    
    y = y_base[:, i]
    if len(np.unique(y)) == 2 : 
        rf = RandomForestClassifier(n_estimators = 10, n_jobs = 16)
        rf.fit(X_numerical_base, y)
        X_meta.append(rf.predict_proba(X_numerical_meta))
        X_test_meta.append(rf.predict_proba(X_test_numerical))

        svm = LinearSVC()
        svm.fit(X_sparse_base, y)
        X_meta.append(svm.decision_function(X_sparse_meta))
        X_test_meta.append(svm.decision_function(X_test_sparse))
        ",others/morph.py,timpalpant/KaggleTSTextClassification,1
"        print(training_params)
        # Overwrite our stock params with what the user passes in (i.e., if the user wants 10,000 trees, we will let them do it)
        model_params.update(training_params)
        print('After overwriting our defaults with your values, here are the final params that will be used to initialize the model:')
        print(model_params)


    model_map = {
        # Classifiers
        'LogisticRegression': LogisticRegression(),
        'RandomForestClassifier': RandomForestClassifier(),
        'RidgeClassifier': RidgeClassifier(),
        'GradientBoostingClassifier': GradientBoostingClassifier(),
        'ExtraTreesClassifier': ExtraTreesClassifier(),
        'AdaBoostClassifier': AdaBoostClassifier(),


        'SGDClassifier': SGDClassifier(),
        'Perceptron': Perceptron(),
        'PassiveAggressiveClassifier': PassiveAggressiveClassifier(),",auto_ml/utils_models.py,doordash/auto_ml,1
"            The features that the model will be trained on
        version : str
            A version string representing the version of the model
        `**kwargs`
            Passed to :class:`sklearn.ensemble.RandomForestClassifier`
    """"""
    def __init__(self, features, *, version=None, rf=None,
                 **kwargs):

        if rf is None:
            rf = RandomForestClassifier(**kwargs)

        super().__init__(features, classifier_model=rf, version=version)
RFModel = RF
""Alias for backwards compatibility""",revscoring/scorer_models/rf.py,he7d3r/revscoring,1
"            kwargs = {""n_jobs"": 8,
                      ""max_depth"": 4,
                      ""n_estimators"": 50,
                      ""max_features"": ""log2""
                      }

        allrows[""INDELTYPE""] = allrows[""INDELTYPE""].astype(float).round().astype(int)

        self.clf = {}
        for it in self.itypes:
            self.clf[it] = RandomForestClassifier(**kwargs)
            irows = allrows[allrows[""INDELTYPE""] == it]
            self.clf[it].fit(irows[columns].values, irows[""tag""].values)


    def classify(self, instances, columns, *args, **kwargs):
        """""" Classify a set of instances after training

        :param instances: data frame with instances.
        :type instances: pandas.DataFrame",src/python/scoringModelTraining/somatic/lib/strelka_rf_indel.py,Illumina/strelka,1
"    
    clf_svm = svm.SVC(kernel='linear')
    clf_svm.fit(tfidf_train, y_train)
    
    clf_mNB=MultinomialNB()
    clf_mNB.fit(tfidf_train, y_train)
    
    clf_knn = KNeighborsClassifier()
    clf_knn.fit(tfidf_train, y_train)
    
    clf_ada=RandomForestClassifier(n_estimators=25)
    clf_ada.fit(tfidf_train, y_train)
    
    #Evaluate the models and output accuracies
    print ""Linear SVM Classifier Accuracy:""+str(clf_svm.score(tfidf_test, y_test))
    print ""Multinomial Naive Bayes Accuracy:""+str(clf_mNB.score(tfidf_test, y_test))
    print ""5 Nearest Neighbors Accuracy:""+str(clf_knn.score(tfidf_test, y_test))
    print ""Random Forest (25 learners) Accuracy:""+str(clf_ada.score(tfidf_test, y_test))
    #Obtain the predicted labels for the test subsets
    ",code_python27/tfidfANDclasification/k10foldClassify.py,rcln/tag.suggestion,1
"from sklearn.multiclass import OneVsRestClassifier
from sklearn import ensemble
from src.multi_class import input_preproc
from src.multi_class import calculate_metrics


def runClassifier(X_train, X_test, y_train, y_test):
    # print y_train
    cls = ensemble.RandomForestClassifier(n_estimators=100,
                                          criterion=""gini"",
                                          max_features=None,
                                          verbose=0,
                                          n_jobs=-1)

    predictions = cls.fit(X_train, y_train).predict(X_test)

    # Metrics...
    precision, recall, f1, accuracy = calculate_metrics.calculateMetrics(predictions, y_test)",src/multi_class/random_forest.py,cassinius/right-to-forget-data,1
"        1 Find best clf with default param
        2 vary param of best clf and find best param
        3 use best param and best clf to find recall for 100 percent precision
    """"""
    utils.print_success(""Find Recall for best Precision for each tag"")
    train = utils.abs_path_file(train)
    test = utils.abs_path_file(test)
    train_features, train_groundtruths = read_file(train)
    test_features, test_groundtruths = read_file(test)
    classifiers = {
        # ""RandomForest"": RandomForestClassifier(),#n_estimators=5
        ""DecisionTree"":DecisionTreeClassifier()#,#max_depth=10
        # ""SVM"":SVC(kernel=""linear"", C=0.0205),
        # ""ExtraTreesClassifier"":ExtraTreesClassifier(n_estimators=5, criterion=""entropy"", max_features=""log2"", max_depth=9),
        # ""LogisticRegression"":LogisticRegression()
    }
    tags = list(set(test_groundtruths))
    nb_tag = len(tags)
    step = 0.01
    # for index, tag in enumerate([""i""]):",src/classify.py,ybayle/ISMIR2017,1
"print 'imputing with feature summarization (mode)'
summ_func = lambda x: mode(x)[0]
data_mode = imp.summarize(x, summ_func, missing_data_cond)

# replace categorical features with one hot row
print 'imputing with one-hot'
data_onehot = imp.binarize_data(x, cat_cols)

# replace missing data with predictions using random forest
print 'imputing with predicted values from random forest'
clf = RandomForestClassifier(n_estimators=100, criterion='gini')
data_rf = imp.predict(x, cat_cols, missing_data_cond, clf)

# replace missing data with predictions using SVM
print 'imputing with predicted values usng SVM'
clf = clf = SVM(
    penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', 
    fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, 
    random_state=None, max_iter=1000)
data_svm = imp.predict(x, cat_cols, missing_data_cond, clf)",example_votes.py,rafaelvalle/MDI,1
"from sklearn.model_selection._validation import cross_val_score
import timing

class RandomForestClassifier(object):

    if __name__ == ""__main__"":
        Arffhandler = Arffhandler()
        Arffhandler.Load(sys.argv[1])
        Arffhandler.OneHotEncode()
        timing.log(""Dataset loaded and encoded"")
        randomForest = RandomForestClassifier(n_estimators=30, \
                                          criterion=""gini"", \
                                          max_features=""auto"", \
                                          max_depth=3, \
                                          min_samples_split=2, \
                                          min_samples_leaf=1, \
                                          min_weight_fraction_leaf=0, \
                                          max_leaf_nodes=None, \
                                          bootstrap=True, \
                                          oob_score=True, \",src/root/main/RandomForestClassifier.py,bcraenen/KFClassifier,1
"    anova_filter.fit(data_x, data_y)
    print 'selected features in boolean: \n', anova_filter.get_support()
    print 'selected features in name: \n', test_x.columns[anova_filter.get_support()];
    
    #2. Select the top nFeatures features
    selectedCols = data_x.columns[anova_filter.get_support()]
    #3. Run SVM (or any other) again on this selected features
    return selectedCols
    
def selectFeatureSet_RF(data_x, data_y, nFeatures):
    rf_filter = RandomForestClassifier(max_features = 'auto')
    rf_filter.fit(data_x, data_y);
    rankings = rf_filter.feature_importances_;
    selectedBool = np.argsort(rankings)[-nFeatures:]
#    selectedBool = sorted(range(len(rankings)), key = lambda x: rankings[x])[-nFeatures:];
    return data_x.columns[selectedBool]
 
        
def evalThisFS(train_x, train_y, test_x, test_y, classifier, selectedCols):
        ",simpleBatch_v2.py,cocoaaa/ml_gesture,1
"def get_best_features(features, labels, max_to_return=20):
    return [feature for (feature, importance) in feature_selection_trees(features, labels)[:max_to_return]]


def feature_importances(importances, features):
    return pd.DataFrame(sorted(zip(features.columns, importances), key=lambda x: -x[1]),
                        columns=['feature', 'value'])


def get_importances(features, targets):
    fit = RandomForestClassifier(n_estimators=100).fit(features, targets)
    return feature_importances(fit.feature_importances_, features)


def plot_tree(clf, file_name, **kwargs):
    dot_data = StringIO()
    tree.export_graphviz(clf, out_file=dot_data, **kwargs)
    graph = pydot.graph_from_dot_data(dot_data.getvalue())
    graph.write_pdf(file_name)
",bamboo/modeling/tools.py,ghl3/bamboo,1
"
clean_train_reviews = []
for review in train[""review""]:
    clean_train_reviews.append(clean_review(review))

trainDataVecs = getWeightedFeatureVecs( clean_train_reviews, model, num_features, feature_dict )


# Fit a simple classifier such as logreg or RF 
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 100)

print ""Fitting a random forest to labeled training data...""
forest = forest.fit(trainDataVecs,train[""sentiment""])

print ""Creating average feature vecs for test reviews""
clean_test_reviews = []
for review in test[""review""]:
    clean_test_reviews.append(review_to_words(review))
",Word2Vec_tf-idf.py,angelachapman/Kaggle-DeepLearning-Tutorial,1
"                  X, y_class, sample_weight=np.asarray([-1]))


def test_base_estimator():
    # Test different base estimators.
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)",net-p3/lib/python3.5/site-packages/sklearn/ensemble/tests/test_weight_boosting.py,uglyboxer/linear_neuron,1
"	# 	Y = Y[mask]
	trainX, testX, trainY, testY = train_test_split(X, Y, test_size=test_size, random_state = seed)
	return trainX,trainY,testX,testY

n_examples = 100000
trainX,trainY,testX,testY = load('data/data.pkl',n_chars=12,n_examples=n_examples)
print trainX.shape,trainY.shape,testX.shape,testY.shape

t = time()
# model = lm.Ridge()
model = RandomForestClassifier(n_estimators=12,n_jobs=2,verbose=2)
# model = lm.LogisticRegression()
# model = LinearSVC()
model.fit(trainX,trainY)
pred = model.predict(testX)
# pred = np.argmax(pred,axis=1)
# print pred
# print np.argmax(testY,axis=1)
print metrics.accuracy_score(testY,pred),time()-t",model.py,Newmu/text-generation,1
"    ########## STEP 2: FEATURE EXTRACTION ##########
    print 'Extracting features ...'

    vectorizer = CountVectorizer(stop_words='english')
    data = vectorizer.fit_transform(text)

    ########## STEP 3: MODEL BUILDING ##########
    print 'Training ...'

    #model = MultinomialNB()
    model = RandomForestClassifier()
    fit_model = model.fit(data, correct_labels)

    # ########## STEP 4: EVALUATION ##########
    print 'Evaluating ...'

    # Evaluate our model with 10-fold cross-validation
    scores = cross_validation.cross_val_score(model, data, correct_labels, cv=10)
    print ""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2)
",class7_1/bill_classifier.py,datapolitan/lede_algorithms,1
"
X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)

plt.plot(X, y)
plt.show()

clf = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
scores = cross_val_score(clf, X, y)
print ""mdia com decision tree: "", scores.mean()

clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)
scores = cross_val_score(clf, X, y)
print ""mdia com random forest: "", scores.mean()

clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=1, random_state=0)
scores = cross_val_score(clf, X, y)
print ""mdia com extra trees  : "", scores.mean()",forest/ex1_forest.py,felipeband/calvin,1
"# Parameters for trees
random_state = 5342
n_jobs = 8
verbose = 1
n_estimators = 89
# ExtraTreesClassifier - classifier 1
clf1 = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf2 = ExtraTreesClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
# RandomForestClassifier - classifier 2
n_estimators = 89
clf3 = RandomForestClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
clf4 = RandomForestClassifier(criterion='gini', random_state=random_state, n_jobs=n_jobs, verbose=verbose, n_estimators=n_estimators, max_features=None)
# KNeighborsClassifier - classifier 3
clf5 = KNeighborsClassifier(n_neighbors=20)
clf6 = KNeighborsClassifier(n_neighbors=20)

# Start training
print('training started')
clf1.fit(X, y)
X_new1 = clf1.transform(X, '1.25*median')",solution4b5.py,canast02/microsoft-malware-classification-challenge,1
"
    config = {}
    execfile(""params.conf"", config)
    inputfile = config[""histogram_dataset""]    
    trainingSamples = config[""trainingSamples""]
    testingSamples = config[""testingSamples""]

    selectedFeatures = ""all""
    features, labels = sc.Data_Preparation(inputfile, selectedFeatures)

    Scikit_RandomForest_Model = ensemble.RandomForestClassifier(n_estimators=510, criterion='gini', max_depth=None,
                                                                 min_samples_split=2, min_samples_leaf=1, max_features='sqrt',
                                                                 bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, verbose=0,
                                                                 min_density=None, compute_importances=None)

    
    numberOfSamples = trainingSamples + testingSamples
    #accuracy, testing_Labels, predict_Labels =  sc.Classification_CrossValidation(Scikit_RandomForest_Model, features, labels, numberOfSamples, 10)    
    accuracy, testing_Labels, predict_Labels =  sc.Classification(Scikit_RandomForest_Model, features, labels, trainingSamples, testingSamples)
    sc.Result_Evaluation('data/evaluation_result/evaluation_RF.txt', accuracy, testing_Labels, predict_Labels)",RandomForest.py,caikehe/YELP_DS,1
"


class RF(BaseLearner):
    def learn(self, ds ):
        _convert_max_features(self.param_dict, ds.x.shape[1])
#        print 'max_features : ', self.param_dict['max_features']

        if not 'n_estimators' in self.param_dict:
            self.param_dict['n_estimators'] = 100 # our default value
        return RandomForestClassifier(**self.param_dict).fit(ds.x, ds.y)

rf_grid = [
    VariableMap('max_depth', 1, 20, is_int=True ),
    VariableMap('max_features', 0, 1 )
    ]



class SVR(BaseLearner):",spearmint_salad/old/learners.py,recursix/spearmint-salad,1
"    # y_train = np.load('data/training_labels.npy')
    # X_test = np.load('data/testing_data.npy')
    # y_test = np.load('data/testing_data.npy')

    clf = LinearSVC(verbose=2)
    clf.fit(X_train, y_train)
    print('Linear SVC Training: ', clf.score(X_train, y_train))
    print('Linear SVC Testing: ', clf.score(X_test, y_test))

def build_classifier(X_train, y_train):
    clf = RandomForestClassifier()
    clf.fit(X_train, y_train)
    print('Random Forest Training ', clf.score(X_train, y_train))
    return clf

def test_classifier(clf, X_test, y_test):
    print('Random Forest Testing ', clf.score(X_test, y_test))

if __name__ == '__main__':
",random_forest.py,SSYoung/Gender-Census-ANN,1
"    models = [(""LR"", LogisticRegression()), 
              (""LDA"", LDA()), 
              (""QDA"", QDA()),
              (""LSVC"", LinearSVC()),
              (""RSVM"", SVC(
              	C=1000000.0, cache_size=200, class_weight=None,
                coef0=0.0, degree=3, gamma=0.0001, kernel='rbf',
                max_iter=-1, probability=False, random_state=None,
                shrinking=True, tol=0.001, verbose=False)
              ),
              (""RF"", RandomForestClassifier(
              	n_estimators=1000, criterion='gini', 
                max_depth=None, min_samples_split=2, 
                min_samples_leaf=1, max_features='auto', 
                bootstrap=True, oob_score=False, n_jobs=1, 
                random_state=None, verbose=0)
              )]

    # Iterate through the models
    for m in models:",SAT eBook/chapter16/train_test_split.py,Funtimezzhou/TradeBuildTools,1
"	y1 = le.fit_transform(y)
	print ""Vectorization completed!""
	#print (vectorizer.stop_words_)
	
	#testClassifiers(X_train, y_train)
	X_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.33, random_state=42)
	print len(y_test)
	
	#Initialize a Random Forest classifier with 100 trees
	print ""Preparing training""
	#forest = RandomForestClassifier(n_estimators = 100)
	clf = svm.SVC()
	print ""Training in progress...""
	#forest = forest.fit(X_train.toarray(), y_train)
	clf.fit(X_train, y_train) 
	
	print ""Training completed!""
	
	print ""Preparing classification""
	result = clf.predict(X_test)",classification/trainingAndTesting.py,pcomputo/webpage-classifier,1
"
np.set_printoptions(threshold=sys.maxint)


names = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Decision Tree"",""Random Forest"", ""AdaBoost"", ""Naive Bayes"", ""LDA"", ""QDA""]
classifiers = [
	KNeighborsClassifier(3),
	SVC(kernel=""linear"", C=0.025),
	SVC(gamma=2, C=1),
	DecisionTreeClassifier(max_depth=5),
	RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
	AdaBoostClassifier(),
	GaussianNB(),
	LDA(),
	QDA()
	]

#read training features,training label and testing set
i_filename = raw_input(""Your testing features file name? "")
o_filename = raw_input(""Your output label file name? "")",scikit_code/classifier_with_multi_components.py,chakpongchung/RIPS_2014_BGI_source_code,1
"
X_train_counts = count_vect.fit_transform(cleaned_post) 
#X_target_counts = count_vect.fit_transform(cleaned_target)

from sklearn.feature_extraction.text import TfidfTransformer
tf_transformer = TfidfTransformer(use_idf = False).fit(X_train_counts)
X_train_tf = tf_transformer.transform(X_train_counts)
#%% training a linear model
# METHOD 1: BUILD randomforestclassifier...
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 10)
forest = rf.fit(X_train_tf, cleaned_target)
#%% examine the result produced by METHOD 1: 
pred = rf.predict(X_train_tf)
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from collections import OrderedDict
import matplotlib.pyplot as plt
import itertools
",Stack_Exchange/py2_text.py,sadahanu/DataScience_SideProject,1
"

### your code here!  name your classifier object clf if you want the 
### visualization code (prettyPicture) to show you the decision boundary

# from sklearn.ensemble import AdaBoostClassifier
# clf = AdaBoostClassifier(n_estimators=10, learning_rate=1)
# clf.fit(features_train, labels_train)

# from sklearn.ensemble import RandomForestClassifier
# clf = RandomForestClassifier(n_estimators=50)
# clf.fit(features_train, labels_train)

from sklearn.neighbors import KNeighborsClassifier

for x in range(1, 10):
    clf = KNeighborsClassifier(n_neighbors=x)
    clf.fit(features_train, labels_train)
    accuracy = clf.score(features_test, labels_test)
    print(x, accuracy)",proj/ud120-projects/choose_your_own/your_algorithm.py,askldjd/udacity-machine-learning,1
"
    if ""class_weight"" in kwparams and kwparams[""class_weight""]:
        class_weight = kwparams[""class_weight""]
    else:
        class_weight = None

    # Separating target from inputs
    X, y = design_matrix(train_filename=train_filename)

    print ""Training Random Forest Classifier...""
    clf = ensemble.RandomForestClassifier(n_estimators=n_estimators, criterion=criterion,
                                          max_features=max_features, max_depth=max_depth,
                                          min_samples_split=min_samples_split,
                                          min_samples_leaf=min_samples_leaf,
#                                           min_weight_fraction_leaf=min_weight_fraction_leaf,
                                          max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap,
                                          oob_score=oob_score, n_jobs=n_jobs,
                                          random_state=random_state)
#                                           class_weight=class_weight)
",scikit_randf/train.py,broadinstitute/ebola-predictor,1
"training = pd.read_csv(""protoAlpha_training.csv"")
testing = pd.read_csv(""protoAlpha_testing.csv"")

X = training.iloc[:,1:-1]
y = training['country_destination']

x_train,x_valid,y_train,y_valid = train_test_split(X,y,test_size=0.3,random_state=9372)

# Train classifier
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=500,n_jobs=1,verbose=10)
clf.fit(x_train,y_train)

# Run Predictions
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
y_preds = clf.predict(x_valid)
print( confusion_matrix(y_valid,y_preds) );
print( ""Accuracy: %f"" % (accuracy_score(y_valid,y_preds)) );
print( ""Precision: %f"" % (precision_score(y_valid,y_preds)) );
print( ""Recall: %f"" % (recall_score(y_valid,y_preds)) );",prototype_beta/randomForest_take2.py,valexandersaulys/airbnb_kaggle_contest,1
"    assert_array_equal(X_transformed_sparse.toarray(), X_transformed.toarray())


def test_parallel_train():
    rng = check_random_state(12321)
    n_samples, n_features = 80, 30
    X_train = rng.randn(n_samples, n_features)
    y_train = rng.randint(0, 2, n_samples)

    clfs = [
        RandomForestClassifier(n_estimators=20, n_jobs=n_jobs,
                               random_state=12345).fit(X_train, y_train)
        for n_jobs in [1, 2, 3, 8, 16, 32]
        ]

    X_test = rng.randn(n_samples, n_features)
    probas = [clf.predict_proba(X_test) for clf in clfs]
    for proba1, proba2 in zip(probas, probas[1:]):
        assert_array_almost_equal(proba1, proba2)
",projects/scikit-learn-master/sklearn/ensemble/tests/test_forest.py,DailyActie/Surrogate-Model,1
"                  X, y_regr)


def test_base_estimator():
    """"""Test different base estimators.""""""
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC

    # XXX doesn't work with y_class because RF doesn't support classes_
    # Shouldn't AdaBoost run a LabelBinarizer?
    clf = AdaBoostClassifier(RandomForestClassifier())
    clf.fit(X, y_regr)

    clf = AdaBoostClassifier(SVC(), algorithm=""SAMME"")
    clf.fit(X, y_class)

    from sklearn.ensemble import RandomForestRegressor
    from sklearn.svm import SVR

    clf = AdaBoostRegressor(RandomForestRegressor(), random_state=0)",BCI_Framework/test_weight_boosting.py,lol/BCI-BO-old,1
"    end_forest = time.time()

    y_pred2 = [ensemble.random_forest.predict(forest, sample) for sample in x_test]
    forest_ca = 1 - (sum(y_pred2 != y_test) / float(len(y_test)))
    print ""RF"", forest_ca

    # SKLEARN DT and RF
    set_random_seed(args[""seed""])
    dt_clf = sk_tree.DecisionTreeClassifier(min_samples_leaf=args[""min_samples_leaf""],
                                            min_samples_split=args[""min_samples_split""])
    rf_clf = RandomForestClassifier(n_estimators=args[""num_trees""],
                                    criterion='entropy',
                                    max_depth=args[""max_tree_nodes""],
                                    min_samples_leaf=args[""min_samples_leaf""],
                                    min_samples_split=args[""min_samples_split""],
                                    max_features='auto',
                                    bootstrap=True,
                                    oob_score=False,
                                    n_jobs=2,
                                    random_state=args[""seed""],",ensemble/main.py,romanorac/ml,1
"        pred, bias, contribs = treeinterpreter.predict(dt, testX, joint_contribution=True)
        self.assertTrue(np.allclose(base_prediction, pred))
        
        self.assertTrue(np.allclose(base_prediction, np.array([sum(contrib.values()) for contrib in contribs]) + bias))

    def test_forest_classifier(self):
        idx = np.arange(len(self.iris.data))
        np.random.shuffle(idx)
        X = self.iris.data[idx]
        Y = self.iris.target[idx]
        dt = RandomForestClassifier(max_depth=3)
        dt.fit(X[:int(len(X)/2)], Y[:int(len(X)/2)])
        testX = X[int(len(X)/2):]
        base_prediction = dt.predict_proba(testX)
        pred, bias, contrib = treeinterpreter.predict(dt, testX)
        self.assertTrue(np.allclose(base_prediction, pred))
        self.assertTrue(np.allclose(pred, bias + np.sum(contrib, axis=1)))


    def test_forest_classifier_joint(self):",tests/test_treeinterpreter.py,andosa/treeinterpreter,1
"    ])
    # grid search 
    parameters = {
        'ensemble__n_estimators': np.linspace(1, 200, 20, dtype=np.dtype(np.int16)),
    }
    gs = grid_search.GridSearchCV(clf, parameters, verbose=1, refit=False, cv=kFolds)
    gs.fit(X_norm, y)
    return gs.best_params_['ensemble__n_estimators'], gs.best_score_


rf = RandomForestClassifier(max_depth=None, min_samples_split=1, max_features=7)
et = ExtraTreesClassifier(max_depth=None, min_samples_split=1, max_features=7)

# evaluate_cross_validation(clf_sgd_hinge, X_norm, y, 10)
n_estimators, score = optimizeEnsemble(X_norm, y, clf=rf, kFolds=10)
print 'n_estimators=',n_estimators, 'score=',score",finance/WeekTest/EnsembleTest.py,Ernestyj/PyStudy,1
"
log_mod = LogisticRegression()
log_mod.fit(dh.x, dh.y)

knn_mod = KNeighborsClassifier(50)
knn_mod.fit(dh.x, dh.y)

gbc_mod = GradientBoostingClassifier()
gbc_mod.fit(dh.x, dh.y)

rf_mod = RandomForestClassifier(1000)
rf_mod.fit(dh.x, dh.y)

#|---Predict using Test data---
dh.partition(ratio=1.0)

log_prob = log_mod.predict_proba(dh.x)

knn_prob = knn_mod.predict_proba(dh.x)
",depr/0.2.5/run.py,rosspalmer/DataTools,1
"    labelIndexer = StringIndexer(inputCol=""label"", outputCol=""indexedLabel"").fit(data)
    # Automatically identify categorical features, and index them.
    # Set maxCategories so features with > 4 distinct values are treated as continuous.
    featureIndexer =\
        VectorIndexer(inputCol=""features"", outputCol=""indexedFeatures"", maxCategories=4).fit(data)

    # Split the data into training and test sets (30% held out for testing)
    (trainingData, testData) = data.randomSplit([0.7, 0.3])

    # Train a RandomForest model.
    rf = RandomForestClassifier(labelCol=""indexedLabel"", featuresCol=""indexedFeatures"")

    # Chain indexers and forest in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])

    # Train model.  This also runs the indexers.
    model = pipeline.fit(trainingData)

    # Make predictions.
    predictions = model.transform(testData)",examples/src/main/python/ml/random_forest_classifier_example.py,gioenn/xSpark,1
"    [Ridge(random_state=42)],
])
def test_explain_linear_regression_multitarget(reg):
    assert_multitarget_linear_regression_explained(reg, explain_prediction)


@pytest.mark.parametrize(['clf'], [
    [DecisionTreeClassifier(random_state=42)],
    [ExtraTreesClassifier(random_state=42)],
    [GradientBoostingClassifier(learning_rate=0.075, random_state=42)],
    [RandomForestClassifier(random_state=42)],
])
def test_explain_tree_clf_multiclass(clf, iris_train):
    X, y, feature_names, target_names = iris_train
    clf.fit(X, y)
    res = explain_prediction(
        clf, X[0], target_names=target_names, feature_names=feature_names)
    for expl in format_as_all(res, clf):
        for target in target_names:
            assert target in expl",tests/test_sklearn_explain_prediction.py,TeamHG-Memex/eli5,1
"        dic_groundReference = self.getGroundTruth(raster, trainingOgrLayer, geoTransform, n_x, n_y, n_b)

        y = []
        X = np.empty((0, n_b))
        for key, values in dic_groundReference.iteritems():
            X = np.concatenate((X, values), axis = 0)
            y.extend(values.shape[0] * [key])

        mask = np.sum(raster, 0) != 0
        data_list = raster[:, mask].T
        clf = RandomForestClassifier(n_estimators = 100, n_jobs = -1)
        clf.fit(X, y)
        res = clf.predict(data_list)

        raster_result = np.zeros((n_x, n_y))
        raster_result[mask] = res

        if self.checkBox.isChecked():
            # Smooth a bit...
            raster_result = np.reshape(raster_result, (raster_result.shape[0], raster_result.shape[1]))",WilliamWallace/classify_dialog.py,gilliM/wallace,1
"        try:
            train.drop(col, axis=1, inplace=True)
            test.drop(col, axis=1, inplace=True)
        except:
            continue

    X_train = train
    X_test = test
    print ('Feature Set: %s minutes' % round(((time.time() - start_time) / 60),2))
    print ('Number of features:', len(X_train.columns.tolist())
    rfr = RandomForestClassifier(n_jobs=-1, random_state=2016, verbose=1)

    param = {}
    param['n_estimators'] = 500
    param['max_features'] = [10, 12, 14]
    model = GridSearchCV(estimator = rfr, param_grid = param, n_jobs = -1, cv=5, verbose=20)
    model.fit(X_train, y_train)

    print ('Grid Search Completed:%s minutes' % round(((time.time() - start_time()))
    print ('Best Parametes')",Machine Learning/Two Sigma Housing Comp/rfr.py,kshrimp/data-science,1
"    >>> import numpy
    >>> from numpy import allclose
    >>> from pyspark.ml.linalg import Vectors
    >>> from pyspark.ml.feature import StringIndexer
    >>> df = spark.createDataFrame([
    ...     (1.0, Vectors.dense(1.0)),
    ...     (0.0, Vectors.sparse(1, [], []))], [""label"", ""features""])
    >>> stringIndexer = StringIndexer(inputCol=""label"", outputCol=""indexed"")
    >>> si_model = stringIndexer.fit(df)
    >>> td = si_model.transform(df)
    >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=""indexed"", seed=42)
    >>> model = rf.fit(td)
    >>> model.featureImportances
    SparseVector(1, {0: 1.0})
    >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])
    True
    >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [""features""])
    >>> result = model.transform(test0).head()
    >>> result.prediction
    0.0",python/pyspark/ml/classification.py,MLnick/spark,1
"## SVM                                     
#from sklearn import svm
#clf = svm.SVC()

#from sklearn.multiclass import OneVsOneClassifier
#from sklearn.multiclass import OutputCodeClassifier
#clf = OutputCodeClassifier(svm.SVC())

## RandomForest
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(min_samples_leaf=10)

## SGD
#from sklearn.linear_model import SGDClassifier
#clf = SGDClassifier(loss=""log"", penalty=""l2"")

# CART
#from sklearn import tree
#clf = tree.DecisionTreeClassifier()
#",scikit_algo/All.py,sankar-mukherjee/CoFee,1
"        self.features = None
        self.classes = None


    def train(self, features, data, labels, n_trees=64):
        self.run.info('RF Train', ""%d observations with %d features grouped into %d classes."" % (len(data), len(features), len(set(labels))))
        filesnpaths.is_output_file_writable(self.classifier_object_path)

        self.progress.new('Training')
        self.progress.update('...')
        rf = sklearn.ensemble.RandomForestClassifier(n_estimators=n_trees)
        rf.fit(np.array(data), labels)
        self.progress.end()

        pickle.dump({'features': features, 'classes': rf.classes_, 'classifier': rf}, open(self.classifier_object_path, 'wb'))
        self.run.info('Classifier output', self.classifier_object_path)

    def predict_from_TAB_delimited_file(self, file_path):
        cols = utils.get_columns_of_TAB_delim_file(file_path)
        return self.predict(utils.get_TAB_delimited_file_as_dictionary(file_path, column_mapping=[str] + [float] * len(cols)))",anvio/learning.py,merenlab/anvio,1
